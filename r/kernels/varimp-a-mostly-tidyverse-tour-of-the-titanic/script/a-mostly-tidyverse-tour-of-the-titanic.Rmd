---
title: "A (mostly!) tidyverse tour of the Titanic"
author: "Andrew Kinsman"
date: "30 May 2017 (last updated 2 June 2017)"
output: html_document
---

## Introduction

The evolution of the R programming language has taken some major steps forward in recent years, in large part due to the creative efforts of R Studio's Chief Data Scientist, Hadley Wickham, who has given us ggplot2 (for visualisation), dpylr (for data manipulation), tidyr (for data tidying) etc. These (and many other) programs have now been brought together into a single package, known as "tidyverse". By installing this package and then opening the tidyverse library, users now have immediate access to the core ggplot2, dplyr, tidyr, readr, tibble and purrr packages without the need to open a series of individual packages one by one. Furthermore, the tidyverse package downloads numerous other, somewhat less general but still very helpful libraries (such as stringr, for string manipulation, and forcats, for dealing with categorical data), that can be accessed in the normal way, library(), negating the need to keep going off looking for individual packages to download.

However, the ease of access to these packages is only one small aspect of tidyverse. More importantly, the idea of tidyverse is to create a "universe" of related packages with a uniform interface - once you have learned how to use one of the tidyverse tools, the others should be quick to pick up since they all follow the same basic approach. It's well worth checking out Hadley Wickham's [tidy tools manifesto](https://mran.microsoft.com/web/packages/tidyverse/vignettes/manifesto.html) for a brief overview of his objectives.

So what is tidy data anyhow? Let's use [Hadley Wickham's explanation](http://vita.had.co.nz/papers/tidy-data.html): "Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table." Often it takes considerable cleaning and manipulation before a dataset can be considered "tidy". In the wild most data is very far from tidy, but sometimes (as with Kaggle Titanic) our data is provided in a neatly presented format that isn't too  messy. In that case, we are free to focus more of our attention on things like feature engineering and dealing with missing values, all of which can of course be carried out using tidyverse. 

Before moving on, I should mention that the excellent free online book [R for Data Science](http://r4ds.had.co.nz/), co-authored by Garrett Grolemund and Hadley Wickham, provides a comprehensive overview of tidy data, with plenty of examples and exercises to help you get underway.

Anyhow, that's enough introduction, let's crack on with some "tidyversing" of our own. We'll start by loading up the tidyverse library and importing the data using read_csv() (from readr). Note that compared to read.csv() in base R we don't need to set stringAsFactors to be False here - in readr strings default to characters rather than factors. We can make any decisions about factors later on. 

# Data Preparation

```{r, message=FALSE, warning=FALSE}
library(tidyverse)

train <- read_csv("../input/train.csv")
test <- read_csv("../input/test.csv")
class(train)
```

You may be wondering what a "tbl_df" is. It's short for tibble dataframe, which is just a new, enhanced version of the standard data frame that you are probably already very familiar with. You can still use all the usual functions with a tibble, but it has some additional capabilities that come in very handy.

dplyr contains the glimpse function that enables us to take a quick look at the data.

```{r}
glimpse(train)
```

glimpse() tells us how many observations and variables there in the dataset, and also provides the class of each variable and the first few observations of each variable. It's somewhat analogous to the str() function in base R. 

```{r}
glimpse(test)
```

Here we see that the train set is missing the Survived variable. 

As already mentioned, we can still run standard base R functions on tibbles, such as if we would like a summary of the train and test data:

```{r}
summary(train)
```

```{r}
summary(test)
```

Let's just quickly check that all the column names are the same, apart from the missing Survived column in the test data.   

```{r}
setdiff(names(train), names(test)) 
```

Here's a quick summary of our variables:
 
Variable Name | Description
--------------|-------------
PassengerID   | Passenger ID (just a row number, so obviously not useful for prediction) 
Survived      | Survived (1) or died (0)
Pclass        | Passenger class (first, second or third)
Name          | Passenger name
Sex           | Passenger sex
Age           | Passenger age
SibSp         | Number of siblings/spouses aboard
Parch         | Number of parents/children aboard
Ticket        | Ticket number
Fare          | Fare
Cabin         | Cabin
Embarked      | Port of embarkation (S = Southampton, C = Cherbourg, Q = Queenstown)

The first obvious question is: What proportion of people survived? Here we can use the summarise() (or summarize!) function from dplyr to do a quick calculation.

```{r}
summarise(train, SurvivalRate = sum(Survived)/nrow(train)*100)
```

The survival rate among the train set was only 38.38%. If we didn't have any information whatsoever about individual passengers then we could guess that they all died and be correct 61.62% of the time for the train data. Let's use this naive approach to make a prediction for the test data (setting Survived to 0 for everyone) and see what happens when we submit it to Kaggle:

```{r}
baseline_solution <- data.frame(PassengerID = test$PassengerId, Survived = 0)
# To submit this as an entry, just un-comment the next line and submit the .csv file 
# write.csv(baseline_solution, file = 'baseline_model.csv', row.names = F) 
```

This submission receives a Kaggle score of 0.62679. Essentially, this is the baseline score on which all our models should be judged. If a model we created were to receive a worse score than 0.62679, then we would have been better off just guessing that everyone died rather than using any machine learning!

So now that we at least have an idea of a minimum target to beat, let's collect all the data together using a full_join() (from dplyr) and get to work on it.

```{r, message=FALSE, warning=FALSE}
titanic <- full_join(train, test)
glimpse(titanic)
```

The Survived variable is binary, either someone died or survived. Let's make it a factor and give it each level a name so that it is more "readable". Here we use another useful package that is supplied with tidyverse, forcats, which (as the name suggests!) is specifically designed for manipulation of categorical data. 

In this code chunk we see a key tidyverse concept known as "piping". Here the pipes are the "%>%" symbols that take us from one line to the next. So we start with the titanic dataset, mutate (or change) the Survived variable to a factor and then mutate it again by recoding that factor with the levels No and Yes. 

Essentially, we are sending a bunch of instructions down a pipeline and ending up with one output, rather than having to stop and start after each individual instruction. This leads to more concise, readable code. If you'd like to find out more about piping, you can find a detailed exposition in Chapter 18 of [R for Data Science (Grolemund/Wickham)](http://r4ds.had.co.nz/pipes.html)

```{r, message=FALSE, warning=FALSE}
library(forcats)
titanic <- titanic %>%
                mutate(Survived = factor(Survived)) %>%
                mutate(Survived = fct_recode(Survived, "No" = "0", "Yes" = "1"))
```

## Feature Engineering

Okay, now that's the target variable tidied up. Now let's see what can be done with the predictor variables. 

Let's start by recoding Sex to a factor and tidying up the labels. Then we'll add a proportional bar plot to examine survival rate by gender.

```{r}
titanic <- titanic %>%
        mutate(Sex = factor(Sex)) %>%
        mutate(Sex = fct_recode(Sex, "Female" = "female", "Male" = "male"))
```

```{r}
ggplot(titanic[1:891,], aes(Sex, fill=Survived)) +
           geom_bar(position = "fill") +
           ylab("Survival Rate") +
           geom_hline(yintercept = (sum(train$Survived)/nrow(train)), col = "white", lty = 2) +
           ggtitle("Survival Rate by Gender")

```

Women were over three times more likely to survive than men (the white horizontal dotted line is a reference point that represents the baseline survival rate of 0.3838 that we identified earlier).

The Name variable clearly doesn't meet the definition of tidy data, since it contains strings of multiple words. Our next task is to extract useful information from those messy strings, for which we can use the stringr library (which, like forcats, downloads as part of tidyverse). Let's extract a person's title from their name, since that may provide clues about their age, marital status, and also whether or not they may be part of a family grouping.

```{r, message=FALSE, warning=FALSE}
library(stringr)
titanic <- mutate(titanic, Title = str_sub(Name, str_locate(Name, ",")[ , 1] + 2, str_locate(Name, "\\.")[ , 1] - 1))

titanic %>% group_by(Title) %>%
              summarise(count = n()) %>%
              arrange(desc(count))
```

So, there are 18 unique titles, but only four of them (Master, Ms, Mr and Mrs) are common. Incidentally, we have used the dplyr verbs group_by to collate the data by Title, and then arrange(), to sort from highest to lowest.

There's one simple piece of feature engineering that might be worth doing before we go any further. Let's try and identify mothers travelling with their children, using their Title and the Parch variable, which states how many children they were accompanying.

```{r}
titanic <- titanic %>%
          mutate(Mother = factor(ifelse(c(titanic$Title == "Mrs" | titanic$Title == "Mme" | titanic$Title == "the Countess" | titanic$Title == "Dona" | titanic$Title == "Lady") & titanic$Parch > 0, "Yes", "No"))) 

ggplot(titanic[1:891,], aes(x = Mother, fill = Survived)) +
          geom_bar(position = "fill") +
          ylab("Survival Rate") +
          geom_hline(yintercept = (sum(train$Survived)/nrow(train)), col = "white", lty = 2) +
          ggtitle("Survival Rate by Motherhood Status")
```

The survival chances of mothers was over twice as good as that of the other passengers.

Now let's tidy up the Title variable by adding a couple of groups and combining the other 14 titles accordingly. Again we can use the forcats library, this time collapsing the factors so that they are combined.

```{r}
titanic <- titanic %>%
          mutate(Title = factor(Title)) %>%
          mutate(Title = fct_collapse(Title, "Miss" = c("Mlle", "Ms"), "Mrs" = "Mme", 
                                      "Ranked" = c( "Major", "Dr", "Capt", "Col", "Rev"),
                                      "Royalty" = c("Lady", "Dona", "the Countess", "Don", "Sir", "Jonkheer"))) 

ggplot(titanic[1:891,], aes(x = Title, fill = Survived)) +
          geom_bar(position = "fill") +
          ylab("Survival Rate") +
          geom_hline(yintercept = (sum(train$Survived)/nrow(train)), col = "white", lty = 2) +
          ggtitle("Survival Rate by Title")
```

Having reduced the Title variable down to six levels, we can take a look at the survival rate of each Title group. The Mr and Ranked groups had worse than average survival chances, but those in any of the other four groups all had better than a 50/50 chance.

Remaining with the Name strings, it might also be interesting to extract the first element of each string, since that is the surname. The simplest way of doing this is probably to go outside tidyverse and use the beg2char() function from the qdap package instead of stringr, as follows:

```{r, message=FALSE, warning=FALSE}
library(qdap)
titanic <- titanic %>%
              mutate(Surname = factor(beg2char(Name, ","))) %>% 
              glimpse()
```

We have already seen how the Parch variable can be employed to identify mothers who were travelling with their children, using a combination of Title and Parch (the number of parents/children aboard). In similar fashion we can also determine family sizes using SibSp (the number of siblings/spouses aboard) and Parch.

First we can identify those who weren't travelling as part of a family group:

```{r}
titanic <- mutate(titanic, Solo = factor(ifelse(SibSp + Parch + 1 == 1, "Yes", "No")))

ggplot(titanic[1:891,], aes(x = Solo, fill=Survived)) +
          geom_bar(position = "fill") +
          ylab("Survival Rate") +
          geom_hline(yintercept = (sum(train$Survived)/nrow(train)), col = "white", lty = 2) +
          ggtitle("Survival Rate by Solo Passenger Status")
```

Those who weren't part of a family group had notably worse survival chances.

But perhaps we can take this a little further. We shall define a large family as one that has more than 4 people, a medium family as one that has more than 1 but fewer than 5 and single people as 1.

```{r}
titanic <- titanic %>% 
              mutate(FamilySize = SibSp + Parch + 1) %>% 
              mutate(FamilyType = factor(ifelse(FamilySize > 4, "Large", ifelse(FamilySize == 1, "Single", "Medium"))))

ggplot(titanic[1:891,], aes(x = FamilyType, fill = Survived)) +
          geom_bar(position = "fill") +
          ylab("Survival Rate") +
          geom_hline(yintercept = (sum(train$Survived)/nrow(train)), col = "white", lty = 2) + 
          ggtitle ("Survival Rate by Family Group Size")

```

So not only do those travelling without a family have poor survival chances, but those in larger families have even worse prospects. In all the chaos and confusion, one can imagine the difficulty of rounding up a large family, keeping everyone together and then finding a lifeboat with enough space to take them.   

## Dealing with Missing Values

First let's see how many missing values there are.
```{r, message=FALSE, warning=FALSE}
library(VIM)

titanic %>% map_dbl(~sum(is.na(.)))

```

Apart from the 418 missing survival observations that came from the test data, cabin and age are both severely affected by missing values. There are just two missing embarkation values and one fare value, so they should be easy to deal with, but cabin and age both present a challenge. 

```{r}
aggr(titanic, prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE)
```

This plot shows that 529 observations are missing just the cabin, 183 observations have no missing values, and 158 are missing both cabin and age, etc. Given that so many cabins are missing, it seems best to ignore these altogether.

We'll start with the easy ones. For the two missing Embarked observations, we shall use the most common embarkation point (the vast majority of people boarded at Southampton), while for fare we shall use the most common fare for that class of passenger. Let's take a look at the relevant passengers using filter() from dplyr:

```{r}
filter(titanic, is.na(Embarked)) 
```

The missing embarkation belongs to two first-class passengers travelling on the same ticket. 

```{r}
filter(titanic, is.na(Fare)) 
```

The missing fare belongs to third-class passenger travelling from Southampton. Let's take a quick look at the median fares from each port:

```{r}
titanic %>% group_by(Pclass, Embarked) %>%
                summarise(count = n(), median_fare = median(Fare, na.rm=TRUE))
```

And now let's insert the missing port as the most common port and the missing fare as the median fare for a 3rd-class passenger travelling from Southampton:

```{r}
titanic <- titanic %>%
              mutate(Embarked = factor(ifelse(is.na(Embarked), names(which.max(table(titanic$Embarked))), Embarked))) %>%
              group_by(Pclass, Embarked) %>%
              mutate(Fare = ifelse(is.na(Fare), round(median(Fare, na.rm = TRUE), 4), Fare))
```

While we're on the subject of embarkation ports, passenger classes and fares, let's tale a quick look at how they each relate to survival chances:

```{r}
ggplot(titanic[1:891,], aes(x = Embarked, fill = Survived)) +
      geom_bar(position = "fill") +
      ylab("Survival Rate") +
      geom_hline(yintercept = (sum(train$Survived)/nrow(train)), col = "white", lty = 2) +
      ggtitle("Survival Rates by Embarkation Port")
```

The Cherbourg passengers appear to have had the better prospects.

```{r}
ggplot(titanic[1:891,], aes(x = Pclass, fill = Survived)) +
      geom_bar(position = "fill") +
      ylab("Survival Rate") +
      geom_hline(yintercept = (sum(train$Survived)/nrow(train)), col = "white", lty = 2) +
      ggtitle("Survival Rates by Passenger Class")
```

As one would have expected, those in first-class had a far higher survival chance, whereas only around one in four of the third-class passengers made it through the night.

```{r}
ggplot(titanic[1:891,], aes(x = log(Fare), fill = Survived)) +
      geom_density(alpha = 0.4)  + 
      ggtitle("Density Plot of Fare related to Survival") 

```

Here we first transform Fare to a log scale to deal with the heavy right-skew. Clearly those with cheaper (i.e. third-class) tickets were by far the most at risk.

OK, now let's impute missing values for the Age variable. For this we shall use the median age for each Title group

```{r}
titanic %>% group_by(Title) %>%
             summarise(median = median(Age, na.rm = TRUE))
 
titanic <- titanic %>%
              group_by(Title) %>%
              mutate(Age = ifelse(is.na(Age), round(median(Age, na.rm = TRUE), 1), Age)) 

```

Let's take a look at the distribution of ages in relation to survival rates:

```{r}
ggplot(titanic[1:891,], aes(x = Age, fill = Survived)) +
      geom_density(alpha = 0.4)  + 
      ggtitle("Density Plot of Age related to Survival") 

```

Here we see that survival rates were good up until around the age of 18, with a poor survival rate for 20-35 year olds and few survivors over the age of 60. For simplicity, let's break this down into three groups: Child (under-18), Adult and OAP (over-60).

```{r}
titanic <- mutate(titanic, LifeStage = factor(ifelse(Age < 18, "Child", ifelse(Age <= 60, "Adult", "OAP"))))

ggplot(titanic[1:891,], aes(x = LifeStage, fill = Survived)) +
      geom_bar(position = "fill") +
      ylab("Survival Rate") +
      geom_hline(yintercept = (sum(train$Survived)/nrow(train)), col = "white", lty = 2) +
      ggtitle("Survival Rates by Life Stage")
```

This barplot clearly shows that over-60s had a poor survival rate and under-18s a much better than average survival rate.

We have now completed our feature engineering and missing values imputation. Having extracted what we required from the Name variable we can now remove that, and we can also discard a few other variables that we won't be using for the model: Ticket, Cabin (for which there are too many missing values) and Surname (which has too many levels to be used for the random forest tree below). Here we use the select() function from dplyr.

```{r}
titanic <- select(titanic, -c(Name, Ticket, Cabin, Surname)) %>%
              glimpse()
```

Now we finally have a tidy dataset, suitable for modelling, meeting the objective that "each variable is a column, each observation is a row, and each type of observational unit is a table." (Wickham)  

## Model Building

So let's build a model. Here we shall use the cforest() function from the party package, a type of random forest that is based on conditional inference trees. Note that we are using a log transformation of Fare for the reasons outlined above.

```{r, message=FALSE, warning=FALSE}
train1 <- titanic[1:891,]
test1 <- titanic[892:1309,]

library(party)
set.seed(144)
cf_model <- cforest(Survived ~ Sex + Age + SibSp + Parch + 
                 log(Fare) + Embarked + Pclass + Title + Mother + 
                 Solo + FamilySize + FamilyType + LifeStage,
                 data = train1, 
                 controls = cforest_unbiased(ntree = 2000, mtry = 3)) 
```

Let's take a look at this model. First the confusion matrix, which shows how many predictions were correct for each category (died or survived):

```{r, message=FALSE, warning=FALSE}
xtab <- table(predict(cf_model), train1$Survived)
library(caret) 
confusionMatrix(xtab)
```

Overall, our model achieved 84.06% accuracy on the training data. However, given that the model was actually built on that data it is highly unlikely that a similar level of accuracy can be achieved using the "unseen" test data.

It is, however, interesting to examine which variables are perceived as most important by the model:

```{r}
varimp(cf_model)
```

It may be easier to compare these by means of a plot:

```{r}
cforestImpPlot <- function(x) {
  cforest_importance <<- v <- varimp(x)
  dotchart(v[order(v)])
}

cforestImpPlot(cf_model)
```

Title and Sex are by far the most important variables, according to our model.

Finally, let's make our predictions, write a submission file and send it to Kaggle:

```{r}
cf_prediction <- predict(cf_model, test1, OOB=TRUE, type = "response")
cf_prediction <- ifelse(cf_prediction == "No", 0, 1)
cf_solution <- data.frame(PassengerID = test1$PassengerId, Survived = cf_prediction)
# To submit this as an entry, just un-comment the next line and submit the .csv file 
# write.csv(cf_solution, file = 'cf_model.csv', row.names = F)
```

This model scored 0.80861 when submitted to the Kaggle competition.

## Conclusion

We've now completed our tidyverse approach to the Kaggle Titanic problem. Our short journey took us through many parts of the tidyverse universe, including dplyr, readr, forcats, stringr and of course ggplot2. I hope you found it a useful exploration. There are undoubtedly some places in which I could (should?) have stuck more closely to "pure" tidyverse, but sometimes practicality and/or habit wins over!    
