<!DOCTYPE html>
<html lang="en">
<head>
    <title>Titanic Survival Prediction with R | Kaggle</title>
    <meta charset="utf-8" />
    <meta name="robots" content="index, follow" />
    <meta name="turbolinks-cache-control" content="no-cache" />
                <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">    <meta name="theme-color" content="#008ABC" />
    <script type="text/javascript">
        window["initialPageLoadStartTime"] = new Date().getTime();
    </script>
    <link rel="dns-prefetch" href="https://www.google-analytics.com" /><link rel="dns-prefetch" href="https://stats.g.doubleclick.net" /><link rel="dns-prefetch" href="https://js.intercomcdn.com" /><link rel="dns-prefetch" href="https://storage.googleapis.com/" />
    <link href="/static/images/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <link rel="manifest" href="/static/json/manifest.json">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:400,300,300italic,400italic,600,600italic,700,700italic" rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet" type='text/css'/>
        <link rel="canonical" href="/neillwhite0/titanic-survival-prediction-with-r" />                    <link rel="stylesheet" type="text/css" href="/static/assets/vendor.css?v=632d145d8598" />
        <link rel="stylesheet" type="text/css" href="/static/assets/app.css?v=1d00932a7505" />
    
    
 
        <script>
        try{(function(a,s,y,n,c,h,i,d,e){d=s.createElement("style");
        d.appendChild(s.createTextNode(""));s.head.appendChild(d);d=d.sheet;
        y=y.map(x => d.insertRule(x + "{ opacity: 0 !important }"));
        h.start=1*new Date;h.end=i=function(){y.forEach(x => d.deleteRule(x))};
        (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;
        })(window,document,['.site-header-react__nav'],'dataLayer',2000,{'GTM-52LNT9S':true});}catch{}
    </script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-12629138-1', {
            'optimize_id': 'GTM-52LNT9S',
            'displayFeaturesTask': null,
            'send_page_view': false
        });
    </script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12629138-1"></script>

    
<script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
            n.callMethod.apply(n,arguments):n.queue.push(arguments)};
        if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
        n.queue=[];t=b.createElement(e);t.async=!0;
        t.src=v;s=b.getElementsByTagName(e)[0];
        s.parentNode.insertBefore(t,s)}(window,document,'script',
        'https://connect.facebook.net/en_US/fbevents.js');
    fbq("set", "autoConfig", "false", "136809193586742");
    fbq('init', '136809193586742'); 
    fbq('track', 'PageView');
</script>
<noscript>
    <img height="1" width="1" src="https://www.facebook.com/tr?id=136809193586742&ev=PageView&noscript=1"/>
</noscript>

<script>window.intercomSettings = {"app_id":"koj6gxx6"};</script>        <script>(function () { var w = window; var ic = w.Intercom; if (typeof ic === "function") { ic('reattach_activator'); ic('update', intercomSettings); } else { var d = document; var i = function () { i.c(arguments) }; i.q = []; i.c = function (args) { i.q.push(args) }; w.Intercom = i; function l() { var s = d.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'https://widget.intercom.io/widget/koj6gxx6'; var x = d.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x); } if (w.attachEvent) { w.attachEvent('onload', l); } else { w.addEventListener('load', l, false); } } })()</script>
    
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@kaggledatasets" />
    <meta name="og:url" content="https://kaggle.com/neillwhite0/titanic-survival-prediction-with-r" />
    <meta name="og:title" content="Titanic Survival Prediction with R" />
    <meta name="og:description" content="Using data from Titanic: Machine Learning from Disaster" />
    <meta name="og:image" content="https://storage.googleapis.com/kaggle-avatars/thumbnails/226735-kg.jpg" />


    
    

    
    
    
<script type="text/javascript">
    var Kaggle = Kaggle || {};

    Kaggle.Current = {
        antiForgeryToken: 'CfDJ8LdUzqlsSWBPr4Ce3rb9VL-gcCZL_Dn4dfC3xFL6ttUspzLqFqrve0CUNudm4yg8zWtSnFIOAHRVqR_i5wLXhQRvwYc2tvH5m5oW6euJQBYeab3eI0Td85U6isGUmBjHq1jt4khbweaHNk0sLalvMsQ',
        isAnonymous: true,
        analyticsToken: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NjAzNDQwOTMsIlVzZXJJZCI6MH0.mkYvGekDDVNzE0nPhuOx06aKLSWy8GwjdBrrwjxsFV4',
        analyticsTokenExpiry: 15,
        internetKernelsEnabled: false,
        
        
        
        
        
        
        
        
        
        
        
    }
        Kaggle.Current.log = function(){};
        Kaggle.Current.warn = function(){};

    var decodeUserDisplayName = function () {
        var escapedUserDisplayName = Kaggle.Current.userDisplayNameEscaped || "";
        try {
            var textVersion = new DOMParser().parseFromString(escapedUserDisplayName, "text/html").documentElement.textContent;
            if (textVersion) {
                return textVersion;
            }
        } catch(ex) {}
        return escapedUserDisplayName;
    }
    Kaggle.Current.userDisplayName = decodeUserDisplayName();
</script>

    

<script type="text/javascript">
    var Kaggle = Kaggle || {};
    Kaggle.PageMessages = [];
</script>

    
<script type="text/javascript">
/* <![CDATA[ */
goog_snippet_vars = function() {
    var w = window;
    w.google_conversion_id = 955616553;
    w.google_conversion_label = "QSjvCKDksHMQqZrWxwM";
    w.google_conversion_value = 0.00;
    w.google_conversion_currency = "USD";
    w.google_remarketing_only = false;
    w.google_conversion_language = "en";
    w.google_conversion_format = "3";
    w.google_conversion_color = "ffffff";
}
// DO NOT CHANGE THE CODE BELOW.
goog_report_conversion = function(url) {
    goog_snippet_vars();
    window.google_conversion_format = "3";
    var opt = new Object();
    opt.onload_callback = function() {
        if (typeof(url) != 'undefined') {
            window.location = url;
        }
    }
    var conv_handler = window['google_trackConversion'];
    if (typeof(conv_handler) == 'function') {
        conv_handler(opt);
    }
}
/* ]]> */
</script>
<script type="text/javascript"
src="//www.googleadservices.com/pagead/conversion_async.js">
</script>



        <script>window['useKaggleAnalytics'] = true;</script>

    <script src="/static/assets/vendor.js?v=4721d2c14786" data-turbolinks-track="reload"></script>
    <script src="/static/assets/app.js?v=1a3cd8c35fe7" data-turbolinks-track="reload"></script>
        <script>
            (function() {
                if ('serviceWorker' in navigator) {
                    navigator.serviceWorker.register("/static/assets/service-worker.js").then(function(reg) {
                        reg.onupdatefound = function() {
                            var installingWorker = reg.installing;
                            installingWorker.onstatechange = function() {
                                switch (installingWorker.state) {
                                case 'installed':
                                    if (navigator.serviceWorker.controller) {
                                        console.log('New or updated content is available.');
                                    } else {
                                        console.log('Content is now available offline!');
                                    }
                                    break;
                                case 'redundant':
                                    console.error('The installing service worker became redundant.');
                                    break;
                                }
                            };
                        };
                    }).catch(function(e) {
                      console.error('Error during service worker registration:', e);
                    });
                }
            })();
        </script>
    <script>
        function handleClientLoad() {
            try {
                gapi.load('client:auth2');
            } catch (e) {
                // In Opera, readystatechange is an unreliable detection of script load, causing
                // this function to be called before gapi exists on the window. The onload callback
                // is still called at the correct time, so the feature works as expected - it's
                // just generating noisy errors.
            }
        }
    </script>
    <script async defer src="https://apis.google.com/js/api.js"
            onload="this.googleApiOnLoad=function(){};handleClientLoad()"
            onreadystatechange="if (this.readyState === 'complete') this.googleApiOnLoad()">
    </script>
</head>
<body data-turbolinks="true">
    <main>
        






<div class="site-layout">
        <div class="site-layout__header">
            <div data-component-name="SiteHeaderContainer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({});performance && performance.mark && performance.mark("SiteHeaderContainer.componentCouldBootstrap");</script>
        </div>

    <div class="site-layout__main-content">
        

<div data-component-name="KernelViewer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({"kernel":{"id":1549984,"title":"Titanic Survival Prediction with R","forkParent":null,"currentRunId":5439677,"mostRecentRunId":5439677,"url":"/neillwhite0/titanic-survival-prediction-with-r","tags":[],"commentCount":0,"upvoteCount":3,"viewCount":721,"forkCount":0,"bestPublicScore":null,"author":{"id":226735,"displayName":"Neill White","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"neillwhite0","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/226735-kg.jpg","profileUrl":"/neillwhite0","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":1,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"isPrivate":false,"updatedTime":"2018-09-01T00:28:53.4333333Z","selfLink":"/kernels/1549984","pinnedDockerImageVersionId":null,"isLanguageTemplate":false,"medal":null,"topicId":null,"readGroupId":null,"writeGroupId":null,"slug":"titanic-survival-prediction-with-r"},"kernelBlob":{"id":51511299,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"Competition","sourceId":3136,"databundleVersionId":null}],"sourceType":"script","language":"rmarkdown","isGpuEnabled":false,"isInternetEnabled":true},"source":"---\r\ntitle: \u0022Kaggle\u0027s Titanic Survival Prediction Competition\u0022\r\nauthor: \u0022Neill White\u0022\r\ndate: \u0022January 18, 2017\u0022\r\noutput:\r\n  html_document:\r\n    toc: TRUE\r\n    toc_depth: 4\r\n    toc_float: FALSE\r\n---\r\n\r\n```{r getpics, echo=FALSE}\r\n# Load images and store locally\r\nlibrary(\u0022curl\u0022);\r\nimageFile = \u0022titanic.jpg\u0022;\r\nif ( ! file.exists( imageFile )){\r\n    curl_download(url = \u0022http://cbsnews1.cbsistatic.com/hub/i/r/2017/01/04/b1b74071-3301-49ee-93bd-82e47c67d3a8/thumbnail/1200x630/0f08f16522eb0723e8d147cc809bc3d1/0103-eve-titanicfire-phillips-1223384-640x360.jpg\u0022, destfile = imageFile);\r\n}\r\nimageFile = \u0022TitanicModelSummary.png\u0022;\r\nif ( ! file.exists( imageFile )){\r\n    curl_download(url = \u0022http://neillwhite.dynu.net/DataScience/TitanicModelSummary.png\u0022, destfile = imageFile);\r\n}\r\n```\r\n\r\n## The Problem Statement\r\nOn April 14, 1912, the RMS Titanic struck an iceberg in the North Atlantic Ocean and sank.  Of the 2,224 people on board, only 706 survived.\r\n\r\nThe goal of this exercise is to predict survivors on the Titanic based on nine input variables, described below.  We are provided two datasets: (1) train.csv, containing 891 records and (2) test.csv, containing 418 records.  The two datasets are provided with the intent that models are formulated using the train dataset and model performance is evaluated on the test dataset.\r\n\r\n\u003c!--![](http://cbsnews1.cbsistatic.com/hub/i/r/2017/01/04/b1b74071-3301-49ee-93bd-82e47c67d3a8/thumbnail/1200x630/0f08f16522eb0723e8d147cc809bc3d1/0103-eve-titanicfire-phillips-1223384-640x360.jpg)--\u003e\r\n![](titanic.jpg)\r\n\r\n***\r\n## About the Data\r\n\r\nVariable|Definition|Key\r\n------- | ---------- | ---------\r\nsurvival|Survival|0 = No, 1 = Yes\r\npclass|Ticket|class\t1 = 1st, 2 = 2nd, 3 = 3rd\r\nsex|Sex|\r\nAge|Age in year|\r\nsibsp|\t# of siblings / spouses aboard the Titanic|\r\nparch|\t# of parents / children aboard the Titanic|\r\nticket|Ticket number|\r\nfare|Passenger fare|\r\ncabin|Cabin number|\r\nembarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|\r\n\r\n### Variable Notes\r\n\r\n**pclass**: A proxy for socio-economic status (SES)\r\n1st = Upper\r\n2nd = Middle\r\n3rd = Lower\r\n\r\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\r\n\r\n**sibsp**: The dataset defines family relations in this way...\r\nSibling = brother, sister, stepbrother, stepsister\r\nSpouse = husband, wife (mistresses and fiancés were ignored)\r\n\r\n**parch**: The dataset defines family relations in this way...\r\nParent = mother, father\r\nChild = daughter, son, stepdaughter, stepson\r\nSome children travelled only with a nanny, therefore parch=0 for them.\r\n\r\nsource: https://www.kaggle.com/c/titanic/data\r\n\r\n### Sample Records\r\n#### Training Data\r\n\r\n```{r trainsample, echo=FALSE}\r\n# Load raw data\r\ntrain \u003c- read.csv(\u0022../input/train.csv\u0022, header = TRUE);\r\nedaTrain = train;\r\nknitr::kable( head(train), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n#### Test Data\r\n\r\n```{r testsample, echo=FALSE, warning=FALSE}\r\n# Load libraries\r\nlibrary(stringr);\r\nlibrary(assertthat);\r\nlibrary(dplyr, quietly=TRUE);\r\nlibrary(leaps);\r\n# Load raw data\r\ntest \u003c- read.csv(\u0022../input/test.csv\u0022, header = TRUE);\r\n\r\n# Add a \u0022Survived\u0022 variable to the test set to allow for combining data sets\r\ntest.survived \u003c- data.frame(Survived = rep(NA, nrow(test)), test[,]);\r\n\r\n# Combine data sets\r\ndata_combined \u003c- rbind(train, test.survived);\r\n#data_combined$survived \u003c- as.factor(data_combined$survived)\r\n\r\nknitr::kable( head(test), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n***\r\n## Exploratory Data Analysis\r\n### Missing Values\r\nThe first step is to find any and all missing data in the train and test sets.  \r\n\r\n#### Train Dataset\r\n```{r eda1}\r\n# go through each variable and if it\u0027s an empty factor or numeric NA, sum each column\r\nnlTrainNA = sapply( train, function(x) switch( class(x), factor = sum(x==\u0022\u0022), sum( is.na(x) ) ) );\r\ntTrainNA = t(nlTrainNA);                                 # transpose named list\r\ndfTrainNA = data.frame( tTrainNA );                      # convert to dataframe\r\n```\r\nThe train.csv dataset had 3 columns with missing values: Age, Cabin, and Embarked.  Age is likely to be an important predictor of survival and we have data for 80% of the training subjects so imputing the missing values is likely to be beneficial.  The source of embarkation may not have obvious predictive power, but given that we have data for over 99% of the training subjects, imputing the missing values could provide value.  The Cabin variable is missing from over 77% of the test subjects.  At first impression, this variable seems like an unlikely candidate to impute values since so much source data is missing.\r\n```{r eda1table, echo=FALSE}\r\nknitr::kable( dfTrainNA, format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\n\r\n#### Test Dataset\r\n```{r eda2}\r\nnlTestNA = sapply( test, function(x) switch( class(x), factor = sum(x==\u0022\u0022), sum( is.na(x) ) ) );\r\ntTestNA = t(nlTestNA);                                 # transpose named list\r\ndfTestNA = data.frame( tTestNA );                      # convert to dataframe\r\n```\r\nThe test.csv dataset also had 3 columns with missing values: Age, Fare, and Cabin.  Like the train dataset, the Cabin variable is sparse, with over 78% subjects missing values.  The Age variable is populated for over 79% of the train subjects, and likely has good predictive power so it will likely be beneficial to impute values for subjects missing Age.  The Fare variable is missing for a single subject.  While Fare may not be an obvious predictor for survival, the fact that the dataset is over 99% complete for this variable indicates that it is a good candidate for imputation.\r\n```{r eda2table, echo=FALSE, warning=FALSE}\r\nlibrary( ggplot2 );\r\nknitr::kable( dfTestNA, format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\n\r\n### Variable Features\r\n\r\n#### PassengerId\r\nPassengerId is a primary key for each row of data in the train and test sets.  This variable will not be included in any of the predictive models.\r\n\r\n#### Survived\r\nSurvived is the class we\u0027re trying to predict.\r\n\r\n#### Pclass\r\nPassenger class is either 1st, 2nd, or 3rd.\r\n```{r eda_pclass1}\r\nggplot(train, aes(x = factor(Pclass), fill = factor(Pclass))) +\r\n  geom_bar( show.legend=FALSE) +\r\n  xlab(\u0022Pclass\u0022) +\r\n  ylab(\u0022Total Count\u0022)\r\n```\r\n\r\nThe Pclass variable shows that most passengers in the train set held a 3rd class ticket.  491 of the 891 passengers were 3rd class, more than 1st and 2nd class combined.  \r\n```{r eda_pclass2}\r\nggplot(train, aes(x = factor(Pclass), fill = factor(Survived))) +\r\n  geom_bar(width = 0.5, position=\u0022dodge\u0022) +\r\n  xlab(\u0022Pclass\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\n\r\nIf the Survived variable is plotted as a function of passenger class, it appears that Pclass will be a predictor for survivability.  A higher percentage of first class passengers survived than died, contrary to the overall trend, whereas a far higher percentage of 3rd class passengers died than survived.\r\n\r\n#### Name\r\nIt may seem that the passenger name is a lot like the PassengerId in that each name acts as a sort of primary key into the data and using name as a model feature would not generalize well.  However, the name field could possibly provide value.  The first twenty names in the train dataset:\r\n```{r eda_name}\r\nhead(as.character(train$Name),n=20);\r\n```\r\nEach name begins with a surname before a comma and a title.  If the passenger is a married woman, her maiden name appears in parentheses.  Some of the titles may help infer age (Master. and Miss.) and surnames could help determine extended family travelling together, even if they\u0027ve purchased separate tickets and are not in the same cabin.  Additionally, the presence of diacritical marks in a name could indicate that the passenger is a non-English speaker who might have had difficulty understanding instructions or the gravity of the situation.\r\n\r\n#### Sex\r\nSex is an unordered factor, male or female.\r\n```{r eda_sex1}\r\nggplot(train, aes(x = factor(Sex), fill = factor(Sex))) +\r\n  geom_bar( show.legend=FALSE) +\r\n  xlab(\u0022Sex\u0022) +\r\n  ylab(\u0022Total Count\u0022)\r\n```\r\n\r\nThe Sex variable shows that most passengers in the training set were male (nearly 2/3 male).  577 of the 891 passengers were male, or 65%.\r\n```{r eda_sex2}\r\nggplot(train, aes(x = factor(Sex), fill = factor(Survived))) +\r\n  geom_bar(width = 0.5, position=\u0022dodge\u0022) +\r\n  xlab(\u0022Sex\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022)\r\n```\r\nIf the Survived variable is plotted as a function of Sex, it appears that Sex will be a strong predictor for survivability.  Of the 314 female passengers in the training set, 233, or 74% survived.  On the other hand, of the 577 male passengers, only 109 survived, or 19%.\r\n\r\n#### Age\r\nAs noted, there are age values for 80% of the training subjects, missing for 177 passengers.  The distribution of ages is slightly skewed right, with a median of 28 years and a mean of 29.7 years.\r\n```{r eda_age1}\r\nggplot(subset(train, !is.na(Age)), aes(x = Age)) +\r\n  geom_histogram(binwidth=4) +\r\n  xlab(\u0022Age\u0022) +\r\n  ylab(\u0022Total Count\u0022);\r\nggplot(subset(train,!is.na(Age)), aes(y=Age,x=\u0022\u0022)) + geom_boxplot();\r\n```\r\n\r\nAge as a predictor of survivability:\r\n```{r eda_age2}\r\nggplot(subset(train,!is.na(Age)), aes(x = Age, fill = factor(Survived))) +\r\n  geom_density( position=\u0022stack\u0022) +\r\n  xlab(\u0022Age\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\nIf we plot the Survived value as a function of the Age density, we see that there is a higher likelihood of younger passengers surviving over older passengers.  Up until the mid-to-late teens, a training set passenger is more likely to survive than die, so age is likely to be a useful predictor for survivability.\r\n\r\n#### SibSp\r\nThis variable is unique in that it is combination of number of siblings or \u00221\u0022 if the passenger had a spouse on board the ship.  In some cases, it is not clear what the SibSp variable is encoding when a \u00221\u0022 is found - is the passenger travelling with a sibling or a spouse?  A SibSp of 2 or more is indicative of siblings.  It will likely be beneficial to disambiguate this variable into separate \u0027Siblings\u0027 and \u0027Spouses\u0027 variables.\r\n```{r eda_sibsp1}\r\nggplot(train, aes(x = SibSp )) +\r\n   geom_histogram(binwidth=0.5) +\r\n   xlab(\u0022Number of Siblings/Spouses\u0022) +\r\n   ylab(\u0022Total Count\u0022)\r\n```\r\n\r\nMost passengers in the training set (68%) had neither a spouse or sibling on board.  23% of the training set passengers had a single sibling or spouse on board.  The remaining 9% of the passengers had two or more (presumably) siblings on board.\r\n```{r eda_sibsp2}\r\nggplot(train, aes(x = SibSp, fill = factor(Survived))) +\r\n   geom_histogram(binwidth=0.5, position=\u0022dodge\u0022) +\r\n   xlab(\u0022Number of Siblings/Spouses\u0022) +\r\n   ylab(\u0022Total Count\u0022) +\r\n   labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\nSurvivability does appear to trend with the number of siblings/spouses on board.  A passenger having no siblings or spouses is most likely to have died, whereas a passenger with one or two siblings/spouses has around a 50% likelihood of surviving.  The remaining cases of three to eight siblings are likely too few from which to draw inferences individually, so it might make sense to pool the SibSp values as follows: 0, 1, 2, 3\u003e to avoid overfitting to specific training cases.  A close examination of the seven instances of the SibSp variable in which SibSp equals 8 reveals that all the subjects were from the same family and were in the same cabin.  Predicting that all families of size 8 will perish is unlikley to generalize well.\r\n\r\n#### Parch\r\nSimilar to SibSp, this variable convolves two separate pieces of data: the number of parents and the number of children this passenger has on board.  In some cases, it is not clear what the Parch variable is encoding when a \u00221\u0022 is found - is the passenger travelling with a parent or a child?  This can be inferred if the Age variable is present for the passenger, but if the Age is missing, it will be ambiguous and may need further analysis.  Perhaps the passenger\u0027s title (Mr., Miss., Master) could help.  Like SibSp, it will likely be beneficial to disambiguate this variable into separate \u0027Parents\u0027 and \u0027Children\u0027 variables.\r\n```{r eda_parch1}\r\nggplot(train, aes(x = Parch )) +\r\n   geom_histogram(binwidth=0.5) +\r\n   xlab(\u0022Number of Parents/Children\u0022) +\r\n   ylab(\u0022Total Count\u0022)\r\n```\r\n\r\nMost passengers in the training set (76%) had neither a parent or child on board.  13% of the training set passengers had a single parent or child on board.  About 9% of the passengers in the training set (80) had two or more parents or children on board.  The remaining 2% of the passengers had either 3, 4, 5, or 6 (presumably) children on board.\r\n```{r eda_parch2}\r\nggplot(train, aes(x = Parch, fill = factor(Survived))) +\r\n   geom_histogram(binwidth=0.5, position=\u0022dodge\u0022) +\r\n   xlab(\u0022Number of Parents/Children\u0022) +\r\n   ylab(\u0022Total Count\u0022) +\r\n   labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\nSurvivability does appear to trend with the number of parents/children on board.  A passenger having no parents or children is most likely to have died, whereas a passenger with one or two parents/children has around a 50% likelihood of surviving.  The remaining cases of three to six children are likely too few from which to draw inferences individually, so it might make sense to group the Parch values as follows: 0, 1, 2, 3\u003e= to avoid overfitting to specific training cases.  \r\n\r\n#### Ticket\r\nThe entries in the Ticket column do not seem to be of a uniform format.  Some ticket entries are just numbers - ranging from 693-392096.  Other ticket entries have character prefixes like \u0022C.A.\u0022 or \u0022SOTON/O2\u0022, followed by a (presumably) ticket number.  \r\n```{r eda_ticket}\r\nhead(as.character(train$Ticket),n=20);\r\n```\r\nAn inspection of the set of tickets shows that presumed families tend to share a single ticket number.  The first impression is that the ticket number seems an unlikely predictor for survivability and could lead to overfitting the training set.  However, the ticket number might help populate the missing Cabin information - and Cabin might be a good predictor for survivability.\r\n\r\n#### Fare\r\nThe fare (price paid per ticket) ranges from 0 to 512.3292.  The units are unclear, but are likely in English pounds.  The distribution is skewed to the right, with a median of 14.4542 and a mean of 32.20421.  A log transform of the data may be necessary to normalize the distribution of fares.  However, first the fare for passenger must be determined.  It appears to be the case that individual ticket numbers are not assigned per passenger, but rather a single ticket number is given to the purchaser of an allotment of tickets.  That is, families travelling together seem to be under the same ticket with the same fare.  So, it may be necessary to get to create an \u0022Amount Paid per Passenger\u0022 feature that takes into account the number of people for which a fare was purchased on a single ticket.  \r\n\r\n```{r eda_fare1}\r\nggplot(train, aes(x = Fare)) +\r\n  geom_histogram(binwidth=4) +\r\n  xlab(\u0022Fare\u0022) +\r\n  ylab(\u0022Total Count\u0022);\r\nggplot(train, aes(y=Fare,x=\u0022\u0022)) + geom_boxplot();\r\n```\r\n\r\nFare could conceivably be an important factor in determining survivability.  Perhaps the higher paying passengers received the first opportunity to board lifeboats.  Or perhaps, those higher paying passengers were more initially unwilling to leave their more comfortable accomodations for the plebian conditions aboard a lifeboat.  Fare as a predictor of survivability:\r\n```{r eda_fare2}\r\nggplot(train, aes(x = Fare, fill = factor(Survived))) +\r\n  geom_density( position=\u0022stack\u0022) +\r\n  xlab(\u0022Fare\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\n#### Cabin\r\nThe Cabin feature could be another strong predictor for survivability.  Perhaps cabins located nearest the lifeboats afforded the best survivability.  But, the Cabin variable has many empty values.  The empty values could mean that the information was not captured or it could mean that not all passengers received cabins and stayed in other accomodations.  Being assigned a cabin could be a proxy for one\u0027s social status and wealth.  If so, the Pclass variable might be co-linear.\r\n```{r eda_cabin1}\r\nlevels(train$Cabin);\r\n```\r\n\r\nThe cabin name mostly adheres to the rule of a single letter A-F,G,T, followed by a number up to 3 digits.  There are cases where a passenger has multiple cabins, each separated by whitespace.  The beginning letter of each cabin could denote a deck or particular region of the ship - which could help with predicting survivability.  Alternatively, the number of the cabin could be more informative than the beginning letter.  Perhaps cabins \u0022A19\u0022 and \u0022B19\u0022 are located right next to one another, for instance.  \r\n```{r eda_cabin2, warning=FALSE}\r\ncabinLetter = ifelse(train$Cabin == \u0022\u0022, NA, substr(train$Cabin,1,1));\r\ncabinREs = gregexpr(\u0022\\\\d+\u0022,train$Cabin, perl=TRUE);\r\ncnMatches = regmatches( train$Cabin, cabinREs);\r\ncabinNumber = numeric(length(cnMatches));\r\nfor ( i in 1:length(cnMatches) ){\r\n  cabinNumber[i] = mean( as.numeric( unlist( cnMatches[i] )));\r\n}\r\nedaTrain$CabinLetter \u003c- as.factor(cabinLetter);\r\nedaTrain$CabinNumber \u003c- cabinNumber;\r\nggplot( edaTrain, aes(x=CabinNumber,y=Survived,color=Survived ) ) + \r\n  geom_point( shape=1,position=position_jitter(height=0.25)) +\r\n  ggtitle(\u0022Survivability by Cabin Number\u0022) +\r\n  xlab(\u0022Cabin Number\u0022) +\r\n  ylab(\u0022Survived\u0022);\r\nggplot(subset(edaTrain, !is.na(cabinLetter)), aes(x = CabinLetter, fill = as.factor(Survived))) +\r\n  geom_bar() +\r\n  ggtitle(\u0022Survivability by Ticket Letter\u0022) +\r\n  xlab(\u0022Cabin Letter\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022);\r\n```\r\n\r\nWhen grouping the passengers by cabin number, there does not appear to be a relationship where survivability depends on cabin number.  If so, there should be identifiable pockets of clusters where there is a higher incidence of survivability.  If such clusters appeared to exist, the clusters could be defined and the group of clusters tested with a Chi-Square test to measure if survivability depends on cabin cluster.\r\n\r\nIt\u0027s not immediately obvious if there is a benefit to categorizing the cabins according to their first letter.  Are these groups statistically different from one another?  A Chi-Square test of independence should show if survivability is dependent on cabin letter or not.\r\n```{r eda_cabin3, warning=FALSE}\r\ntbl = table( edaTrain$Survived, edaTrain$CabinLetter);\r\ntbl\r\nchisq.test(tbl);\r\n```\r\n\r\nThe p-value is 0.17 so at a confidence level of 0.05, we cannot reject the null hypothesis that survivability is independent of the starting cabin letter.\r\n```{r eda_cabin4}\r\nedaTrain$CabinAssignment[ edaTrain$Cabin != \u0022\u0022 ] \u003c- \u0022Assigned\u0022;\r\nedaTrain$CabinAssignment[ edaTrain$Cabin == \u0022\u0022 ] \u003c- \u0022Unassigned\u0022;\r\ndata_combined$CabinAssignment[ data_combined$Cabin != \u0022\u0022 ] \u003c- \u0022Assigned\u0022;\r\ndata_combined$CabinAssignment[ data_combined$Cabin == \u0022\u0022 ] \u003c- \u0022Unassigned\u0022;\r\ndata_combined$CabinAssignment = factor( data_combined$CabinAssignment );\r\nggplot(edaTrain, aes(x=CabinAssignment, fill=factor(Survived))) +\r\n  geom_bar() + \r\n  facet_wrap(~Pclass) + \r\n  ggtitle(\u0022Pclass\u0022) + \r\n  xlab(\u0022Cabin Assignment\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill=\u0022Survived\u0022);\r\n```\r\n\r\nFrom the graph, it appears that for each passenger class, if a passenger is assigned a cabin, their chances of surviving the disaster are better than if they had not been assigned a cabin.\r\n\r\n#### Embarked\r\n\r\nPassengers boarded the Titanic from one of three ports: (S)outhampton, England; (C)herbourg, France, or (Q)ueenstown, Ireland.  As noted, two passengers in the training set have no record of port of Embarkation.\r\n```{r eda_embarked1, echo=FALSE}\r\nknitr::kable( train[train$Embarked == \u0022\u0022,], format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\n\r\nThe two passengers are both female and in first class.\r\n```{r eda_embarked2}\r\nggplot( edaTrain, aes(x=factor(Pclass),fill=factor(Survived)))+\r\n  geom_bar() + \r\n  facet_wrap(~Embarked) +\r\n  ggtitle(\u0022Survivability by Port of Embarkation and Passenger Class\u0022) +\r\n  xlab(\u0022Passenger Class\u0022) +\r\n  ylab(\u0022Count\u0022) +\r\n  labs(fill=\u0022Survived\u0022);\r\n```\r\n\r\n***\r\n\r\n### Errors in the Data\r\n\r\nErrors in the input dataset are often not apparent until deeper analyses are performed, such as feature engineering and imputing missing data.  However, once the data is corrected, it is necessary to regenerate the columns and feature analyses that will feed the predictive models.  In this dataset, it was found that a 16-year-old member of a family was incorrectly identified as being the father:\r\n```{r errors1, echo=FALSE}\r\nknitr::kable( data_combined[ data_combined$Ticket == \u0022W./C. 6608\u0022, ], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\nIn the Ford family, Mr. William Neal Ford has the value \u00271\u0027 for SibSp and \u00273\u0027 for Parch.  The value of \u00271\u0027 for SibSp would imply that William has either one spouse or one sibling.  In addition, William has a value of \u00273\u0027 for Parch.  This indicates that William has either 3 children, 1 parent and 2 children, 2 parents and 1 child, or 3 parents, which is clearly not possible according to the variable definition.  Since two of the children in the family, Miss. Doolina Margaret Ford (21), and Mr. Edward Watson Ford (18) are actually *older* than William, it is clear that he cannot be their father.  The other child, Miss. Robina Maggie Ford (9) is only 7 years younger than William, also indicating that William cannot be her father.  The matriarch of the family is Mrs. Edward Ford - not Mrs. William Neal Ford, which indicates that it\u0027s likely not a simple case of getting William Neal Ford\u0027s age wrong (for instance, 46 instead of 16).\r\n\r\nTherefore, the best fix would be to change the Parch variables for Miss. Robina Maggie Ford, Miss. Doolina Margaret Ford, and Mr. Edward Watson Ford from \u00272\u0027 to \u00271\u0027 (due to the inclusion of Mr. William Neal as a brother).  In addition, the Parch variable of Mrs. Edward Ford would rise from \u00273\u0027 to \u00274\u0027, indicating she\u0027s travelling with four of her children and the Parch variable of Mr. William Neal Ford would change from \u00273\u0027 to \u00271\u0027 indicating that he\u0027s travelling with a single parent, his mother.  The SibSp variables would change from \u00272\u0027 to \u00273\u0027 for Miss. Robina Maggie Ford, Miss. Doolina Margaret Ford, and Mr. Edward Watson Ford, indicating that each of them are travelling with 3 siblings.  Similarly, Mr. William Neal Ford\u0027s SibSp variable would change from \u00271\u0027 to \u00273\u0027.  Finally, Mrs. Edward Ford\u0027s SibSp would change from \u00271\u0027 to \u00270\u0027, indicating she was travelling without her husband (Mr. Edward Ford).  The changes appear below.\r\n```{r errors2, echo=FALSE}\r\n# Mr. William Neal Ford\r\ndata_combined[ data_combined$PassengerId ==  87,\u0022SibSp\u0022] = 3;  # From 1\r\ndata_combined[ data_combined$PassengerId ==  87,\u0022Parch\u0022] = 1;  # From 3\r\n# Miss. Robina Maggie Ford\r\ndata_combined[ data_combined$PassengerId == 148,\u0022SibSp\u0022] = 3;  # From 2\r\ndata_combined[ data_combined$PassengerId == 148,\u0022Parch\u0022] = 1;  # From 2\r\n# Miss. Doolina Margaret Ford\r\ndata_combined[ data_combined$PassengerId == 437,\u0022SibSp\u0022] = 3;  # From 2\r\ndata_combined[ data_combined$PassengerId == 437,\u0022Parch\u0022] = 1;  # From 2\r\n# Mrs. Edward Ford\r\ndata_combined[ data_combined$PassengerId == 737,\u0022SibSp\u0022] = 1;  # From 1; Note: sister is Mrs. Eliza Johnston\r\ndata_combined[ data_combined$PassengerId == 737,\u0022Parch\u0022] = 4;  # From 3\r\n# Mr. Edward Watson Ford\r\ndata_combined[ data_combined$PassengerId == 1059,\u0022SibSp\u0022] = 3;  # From 2\r\ndata_combined[ data_combined$PassengerId == 1059,\u0022Parch\u0022] = 1;  # From 2\r\nknitr::kable( data_combined[ data_combined$Ticket == \u0022W./C. 6608\u0022, ], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nAdditionally, there are problems with the Abbott family with ticket C.A., 2673:\r\n```{r errors3, echo=FALSE}\r\nknitr::kable( data_combined[ data_combined$Ticket == \u0022C.A. 2673\u0022, ], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nThirteen year old Master. Eugene Joseph Abbott has a Parch value of \u00272\u0027, meaning he has two parents on board.  A female on the same ticket, Mrs. Stanton Abbott, is 22 years older than Eugene at 35 years of age.  Mrs. Stanton Abbott has a SibSp of \u00271\u0027 and a Parch of \u00271\u0027.  The other person on the ticket is 16-year-old Mr. Rossmore Edward Abbott, with a SibSp of \u00271\u0027 and a Parch of \u00271\u0027.  It appears that 16-year-old Mr. Rossmore Edward Abbott was incorrectly designated the spouse of Mrs. Stanton Abbott instead of her son.  The corrections appear below.\r\n```{r errors4, echo=FALSE}\r\n# Master. Eugene Joseph Abbott\r\ndata_combined[ data_combined$PassengerId == 1284,\u0022SibSp\u0022] = 1;  # From 0\r\ndata_combined[ data_combined$PassengerId == 1284,\u0022Parch\u0022] = 1;  # From 2\r\n# Mrs. Stanton (Rosa Hunt)\r\ndata_combined[ data_combined$PassengerId == 280,\u0022SibSp\u0022] = 0;  # From 1\r\ndata_combined[ data_combined$PassengerId == 280,\u0022Parch\u0022] = 2;  # From 1\r\nknitr::kable( data_combined[ data_combined$Ticket == \u0022C.A. 2673\u0022, ], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\n## Feature Engineering\r\nWhen importing into R, factor/class variables solely composed of numeric entries are interpreted and imported as numeric types, implying an ordering and a distance between entries.  These variables include PassengerId, Survived, and Pclass.  PassengerId is like a primary key for each passenger, so it\u0027s not necessary to convert this variable into a factor.  However, the Survived data is encoded as a binary flag where 1=survived and 0=not survived.  Since our goal is to predict classes (i.e., survived or died), it is necessary to convert this value into a factor.  The Pclass variable is a factor, but it is an ordered factor.  An ordered factor indicates that there is an ordering between the classes (1st class, 2nd class, 3rd class), but no magnitude between each class level can be inferred.\r\n```{r feature1}\r\ntrain$Survived = as.factor(train$Survived);\r\ntrain$Pclass = as.ordered(train$Pclass);\r\n```\r\n\r\n### Imputing Missing Data\r\n\r\n#### Embarked\r\nTwo passengers had no information for their port of embarcation, Miss. Amelie Icard and Mrs. George Nelson Stone.  These first class passengers were both on the same ticket (113572) and stayed in Cabin B28.  The majority of passengers boarded at Southampton (70%), as compared to Cherbourg (21%) and Queenstown (9%).  There is quite a variety in the ticket numbers on the Titanic.  Their ticket number, 113572, is one in a series of similar ticket numbers, all in the 113XXX format.  There are 56 tickets in the 113XXX format and of the 54 that have a valid port of embarcation, 81% were from Southampton.  Considering the different data points, the two missing Embarked values will be considered \u0027S\u0027 for Southampton.\r\n```{r impute_embarked}\r\ndata_combined[62,\u0022Embarked\u0022] = as.factor(\u0022S\u0022);\r\ndata_combined[830,\u0022Embarked\u0022] = as.factor(\u0022S\u0022);\r\ndata_combined$Embarked = factor( data_combined$Embarked );  # removes empty factor \u0022\u0022\r\n```\r\n\r\n#### Cabin\r\nAs determined in the EDA section, over 77% of the passengers have no recorded cabin information.  In addition, the assigned cabins did not seem to have a predictable pattern from the available data.  Further, cursory analyses did not indicate that cabin assignments were a good predictor of survivability.  As such, the missing cabin data will not be imputed.\r\n\r\n#### Fare\r\nOut of 1309 records, one fare is missing:\r\n```{r impute_fare1, echo=FALSE}\r\nknitr::kable( data_combined[1044,], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nThe fare can be estimated by considering the variables that most likely affect the Fare variable, such as passenger class (Pclass), port of Embarcation (Embarked), Cabin, and how many other people are on the same ticket.  The passenger class and port of embarkation are readily available in each data row.  The Cabin variable, however, is not useful in this case since the Cabin variable is not defined for passenger 1044.  However, the fact that no Cabin was recorded for this passenger is significant since perhaps not all passengers were assigned cabins (i.e., wealthy passengers would be assigned cabins, and hence pay higher fares, while less privileged passengers may have to had large, unassigned community accomodations with a comparatively lower fare).  \r\n\r\n```{r impute_fare2}\r\nticketChar = as.character( data_combined$Ticket );\r\nnumPassengers = nrow( data_combined );\r\ndata_combined$NumPassengersOnTicket = 1;\r\nfor (i in 1:numPassengers ){\r\n    thisTicket = as.character( data_combined[i,\u0022Ticket\u0022] );\r\n    idxPeople = which( thisTicket == ticketChar );\r\n    numPeople = length( idxPeople );\r\n    data_combined$NumPassengersOnTicket[i] = numPeople;\r\n}\r\n# now constrain to those that have just 1 person on the ticket, as passenger 1044\r\nsinglePassengers = data_combined$NumPassengersOnTicket == 1;\r\nembarkedS = data_combined$Embarked == \u0027S\u0027;\r\nunassignedCabins = data_combined$CabinAssignment == \u0027Unassigned\u0027;\r\npClass3 = data_combined$Pclass == 3;\r\nsimilarPassengers = which( singlePassengers \u0026 embarkedS \u0026 unassignedCabins \u0026 pClass3 );\r\nsimilarPassengersFareData = data_combined[similarPassengers,];  # include all columns but the one we have no Fare info\r\nsimilarPassengersFareData = similarPassengersFareData[ similarPassengersFareData$PassengerId != 1044,];  # include all columns but the one we have no Fare info\r\nmedianFare = median( similarPassengersFareData$Fare );\r\ndata_combined[1044,\u0022Fare\u0022] = medianFare;\r\nhist(similarPassengersFareData$Fare, xlab = \u0022Fare\u0022, ylab = \u0022Passengers\u0022, main = \u0022Fare of Similar Passengers\u0022);\r\nsummary( similarPassengersFareData$Fare );\r\n```\r\n\r\nThere are 318 passengers that share the same characteristics as passenger 1044: passenger class 3, port of embarkation Southampton, cabin assignment (unassigned), and the number of people with the same ticket (1).  Excluding a fare of 3.1708 and two at 19.9667, the remaining fares are between 6.2375 and 10.5167.  Passenger 1044 is assigned the median fare of 7.896.\r\n\r\n#### Age\r\nAs found above, the Age data is missing for 177 of the train set cases and 86 of the test set cases.  Since Age is likely to be an important predictor of survival, the missing data should be imputed.  As will be shown in the next section on Feature Engineering, the passenger\u0027s title can be extracted from their name.  The title should allow for a better estimate of a passenger\u0027s age.\r\n\r\nThe missing passengers have the following titles:\r\n```{r impute_age1, echo=FALSE}\r\ndata_combined$FixedAge = data_combined$Age;\r\npassenger_names = as.character(data_combined$Name);\r\nnum_commas = unname(sapply( passenger_names, str_count, \u0022,\u0022));\r\nall_commas = assert_that( all( num_commas == 1 ) );  # Make sure each row has a comma\r\n# Now, extract titles.  Split on comma and take the tail end of the character string\r\nsurnames = sapply( strsplit(as.character(passenger_names), \u0022,\u0022), head, 1);\r\ndata_combined$Surname = as.factor( surnames );\r\ngiven_name_string = sapply( strsplit(as.character(passenger_names), \u0022,\u0022), tail, 1);\r\ngiven_name_string = trimws( given_name_string, \u0022left\u0022); # remove leading whitespace\r\ndata_combined$NameString = paste( given_name_string, surnames );\r\n# Remove leading whitespace, if any\r\ngiven_name_string = trimws( given_name_string, \u0022left\u0022);\r\n# To extract the title from the Name character string, split each string \r\nname_tokens = strsplit( given_name_string, \u0022\\\\s+\u0022);\r\n# Exract first token as the title\r\ntitles = lapply(name_tokens,\u0022[[\u0022,1);\r\ngiven_names = lapply(name_tokens,\u0022[[\u0022,2);\r\ndata_combined$GivenName = as.factor( unlist( given_names ) );\r\ntitle_regs = regexpr( \u0022[\\\\w+]+\\\\.\u0022, given_name_string, perl=TRUE );\r\nre_titles = regmatches( given_name_string, title_regs );\r\ndata_combined$Title = as.factor( unlist( re_titles ));\r\n# The set of titles that have missing age values\r\nageNA = is.na( data_combined$Age );\r\ntitleNA = data_combined$Title[ageNA];\r\n# The set of titles that have missing age values\r\ndata_combined$FixedTitle = data_combined$Title;\r\n\r\nFIX_TITLES = TRUE;\r\nif ( FIX_TITLES ){\r\n  # change the following titles to \u0022Mr.\u0022 or \u0022Mrs.\u0022\r\n  titlesToChange = c(\u0022Capt.\u0022, \u0022Col.\u0022, \u0022Countess.\u0022, \u0022Don.\u0022, \u0022Dona.\u0022, \u0022Dr.\u0022, \u0022Jonkheer.\u0022, \u0022Lady.\u0022, \u0022Major.\u0022, \u0022Mlle.\u0022, \u0022Mme.\u0022, \u0022Ms.\u0022, \u0022Rev.\u0022, \u0022Sir.\u0022);\r\n  for ( title in titlesToChange ){\r\n    idxChange = which( data_combined$FixedTitle == title );\r\n    idxChangeMale = intersect( idxChange, data_combined$Sex == \u0022male\u0022);\r\n    idxChangeFemale = setdiff( idxChange, idxChangeMale );\r\n    data_combined[idxChangeMale,\u0022FixedTitle\u0022] = \u0022Mr.\u0022;\r\n    data_combined[idxChangeFemale,\u0022FixedTitle\u0022] = \u0022Mrs.\u0022;\r\n  }\r\n  data_combined$FixedTitle = factor( data_combined$FixedTitle );\r\n}\r\n\r\ndata_combined$AgeTitle = data_combined$Title;\r\ndata_combined$AgeTitle[ which( data_combined$Title == \u0022Ms.\u0022 \u0026 ageNA )] = as.factor( \u0022Mrs.\u0022 );\r\nage_lm = lm( Age~AgeTitle, data=data_combined);\r\nmissingAgeIndices = which( is.na( data_combined$Age ) );\r\nmissingTitles = data_combined[missingAgeIndices,\u0022AgeTitle\u0022];\r\nmissingCount = table( missingTitles );\r\nsummary( missingTitles )\r\n```\r\n\r\nThe resultant linear model uses the following average ages for each missing passenger title:\r\n```{r impute_age2}\r\n# model is simple: use the passenger\u0027s title (i.e., Mr., Master., Mrs.) to determine age\r\n# May want to better this by using age of parents (if travelling with parents), age of siblings, spouse, etc.\r\nage_lm = lm( Age~AgeTitle, data=data_combined);\r\n\r\nmissingAges = predict( age_lm, data_combined[missingAgeIndices,]);\r\ndata_combined[missingAgeIndices,\u0022FixedAge\u0022] = missingAges;\r\ncoeffs = coefficients( age_lm );\r\ncoeffNames = names(coeffs);\r\ncoeffValues = unname(coeffs);\r\n# find (Intercept)\r\nidxIntercept = match( \u0022(Intercept)\u0022, coeffNames );\r\ninterceptValue = coeffValues[idxIntercept];\r\nallOtherCoeffIndices = setdiff(1:length(coeffNames),idxIntercept);\r\nallOtherCoeffValues = coeffValues[allOtherCoeffIndices];\r\ntitleAges = interceptValue + allOtherCoeffValues;\r\nnames(titleAges) =  gsub( \u0022AgeTitle\u0022, \u0022\u0022, coeffNames[allOtherCoeffIndices] );\r\nknitr::kable( bind_rows(titleAges), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n***\r\n### New Features\r\n\r\n#### Title and AgeTitle\r\nThe Name variable in the test and train datasets has some structure - surname followed by a comma, then a title and a given name.  Additionally, a maiden name will be present in parentheses if the passenger is a married woman.\r\n```{r feature2}\r\npassenger_names = as.character(data_combined$Name);\r\nhead(passenger_names);\r\n```\r\nThe implied structure is [SURNAME] COMMA [TITLE] [GIVEN NAME] (MAIDEN NAME if applicable).  Since the goal is to extract the title information from each row programmatically, it is  necessary to enforce some error checking - namely, that each row contain one and only one comma.  If this condition does not hold true, the title extraction task will be more tedious.\r\n\r\n```{r feature3}\r\nnum_commas = unname(sapply( passenger_names, str_count, \u0022,\u0022));\r\nall_commas = assert_that( all( num_commas == 1 ) );  # Make sure each row has a comma\r\n# Now, extract titles.  Split on comma and take the tail end of the character string\r\nsurnames = sapply( strsplit(as.character(passenger_names), \u0022,\u0022), head, 1);\r\ndata_combined$Surname = as.factor( surnames );\r\ngiven_name_string = sapply( strsplit(as.character(passenger_names), \u0022,\u0022), tail, 1);\r\n# Remove leading whitespace, if any\r\ngiven_name_string = trimws( given_name_string, \u0022left\u0022);\r\n# To extract the title from the Name character string, split each string \r\nname_tokens = strsplit( given_name_string, \u0022\\\\s+\u0022);\r\n# Exract first token as the title\r\ntitles = lapply(name_tokens,\u0022[[\u0022,1);\r\ngiven_names = lapply(name_tokens,\u0022[[\u0022,2);\r\ndata_combined$GivenName = as.factor( unlist( given_names ) );\r\n```\r\n\r\nThe full set of extracted titles:\r\n```{r feature4}\r\npassenger_titles = unlist( titles );\r\ntable( passenger_titles );\r\n```\r\n\r\nMost of these look correct and reasonable as titles (\u0022Don.\u0022 and \u0022Dona.\u0022 are Italian honorifics, as is \u0022Jonkheer.\u0022 a Dutch honorific).  However, the title \u0022the\u0022 is suspicious.  The complete row is as follows:\r\n```{r feature5}\r\ndata_combined[ which( data_combined$Title == \u0022Countess.\u0022 ), ]\r\n```\r\n\r\nAlthough it makes little difference in this case since there is only a single \u0022the Countess. of Rothes\u0022 on board, the title of this passenger should be changed from \u0022the\u0022 to \u0022Countess.\u0022.  This could be fixed by changing the single record, but the fact that this incorrect title occurred in the first place indicates that the methodology for finding titles isn\u0027t robust enough.  Since it appears that the only time a \u0022.\u0022 appears in the Name field is at the end of a title, that information could be used to find all titles.\r\n```{r feature6}\r\ntitle_regs = regexpr( \u0022[\\\\w+]+\\\\.\u0022, given_name_string, perl=TRUE );\r\nre_titles = regmatches( given_name_string, title_regs );\r\ndata_combined$Title = as.factor( unlist( re_titles ));\r\nsummary( data_combined$Title );\r\n```\r\n\r\nOf particular interest are the titles that are underrepresented and how they might relate to the most abundant titles (Mr., Mrs., Miss., and Master.).  For instance, the title \u0022Ms.\u0022 has only two occurences and as such, has limited predictive power.  \u0022Ms.\u0022 generally refers to an adult woman, either married or unmarried and so is much closer to either a \u0022Mrs.\u0022 or \u0022Miss.\u0022 title than a \u0022Mr.\u0022, for instance.  Additionally, the French designations for \u0022Miss.\u0022 and \u0022Mrs.\u0022 appear as Madamoiselle (\u0022Mlle.\u0022) and Madame (\u0022Mme.\u0022).  These titles would likely provide better predictive power if they were switched to their English equivalents.\r\n\r\nAdditionally, many of the Age attributes are missing from the dataset and since it seems likely that Age would be a strong predictor of survivability.  Title should be a fairly robust predictor for Age, so engineering a title feature that is reflective of Age could be beneficial (i.e., a Doctor with a missing Age value is much more likely to be an adult than a child and adults have a generally lower survival rate than children).  The titles of the passengers missing Age information are as follows:\r\n```{r feature7}\r\n# The set of titles that have missing age values\r\nageNA = is.na( data_combined$Age );\r\ntitleNA = data_combined$Title[ageNA];\r\nsummary( titleNA );\r\n```\r\n\r\nFor the purposes of estimating the Age variable for this group, the \u0022Ms.\u0022 entry is better suited as a \u0022Mrs.\u0022.  Although the \u0022Dr.\u0022 title has a relatively small number of instances, it is likely enough from which to draw a reasonable age estimate.\r\n```{r feature8}\r\n# The set of titles that have missing age values\r\ndata_combined$AgeTitle = data_combined$Title;\r\ndata_combined$AgeTitle[ which( data_combined$Title == \u0022Ms.\u0022 \u0026 ageNA )] = as.factor( \u0022Mrs.\u0022 );\r\nsummary( data_combined$AgeTitle );\r\n```\r\n\r\n#### Parents and Children\r\nNext, the Parch variable encodes two different measures: the number of parents a passenger has on board AND/OR the number of children a passenger has on board.  Since this variable is overloaded, a more informative variable set might disambiguate these measures into NumParents and NumChildren (aboard).  Such a set of variables may provide more predictive power than the single Parch variable.\r\n```{r feature9}\r\nnumParents = integer( nrow( data_combined ) );\r\nnumChildren = integer( nrow( data_combined ) );\r\n\r\nMAX_CHILD_AGE = 14;\r\n\r\n# Find all Parch \u003e 0\r\nposParch = data_combined$Parch \u003e 0;\r\nidxParch = which( posParch );\r\ncounter = 0;\r\nnumCases = length( idxParch );\r\nfor (thisRow in idxParch){\r\n  sibsp = data_combined[thisRow,\u0022Sibsp\u0022];\r\n  # if sibsp \u003e 1, then the passenger is travelling with siblings (and therefore, likely parents)\r\n  counter = counter + 1;\r\n  numParch = data_combined[thisRow,\u0022Parch\u0022];\r\n  thisSurname = data_combined[thisRow,\u0022Surname\u0022];\r\n  thisTicket = data_combined[thisRow,\u0022Ticket\u0022];\r\n  thisTitle = as.character( data_combined[thisRow,\u0022Title\u0022] );\r\n  thisAge = data_combined[thisRow,\u0022Age\u0022];\r\n  if ( (thisTitle == \u0022Master.\u0022) || (thisTitle == \u0022Miss.\u0022) ){\r\n    # if this passenger is a \u0022Master.\u0022 or Miss. and numParch \u003c= 2, he/she must be someone\u0027s child\r\n    numParents[thisRow] = numParch;\r\n    #next;\r\n  }\r\n  if (!is.na( thisAge ) \u0026\u0026 thisAge \u003c= MAX_CHILD_AGE ){\r\n    # if this passenger is young, declare they cannot be parents, must be a child\r\n    numParents[thisRow] = numParch;\r\n    #next;\r\n  }\r\n  # get passenger rows on same ticket\r\n  sameTickets = data_combined$Ticket == thisTicket;\r\n  sameSurnames = data_combined$Surname == thisSurname;\r\n  sameSurnameSameTicket = sameTickets \u0026 sameSurnames \u0026 posParch;\r\n  ticketTitles = as.character( data_combined[ sameSurnameSameTicket, \u0022Title\u0022] );\r\n  ticketParches = data_combined[ sameSurnameSameTicket, \u0022Parch\u0022];\r\n  # now, look for passengers with the same surname on the ticket and check their titles and ages\r\n  numSame = length( sameSurnameSameTicket );\r\n  ages = sort( data_combined[sameSurnameSameTicket,\u0022Age\u0022] );\r\n  thisAgePos = which( thisAge == ages )[1];  # find index of thisAge in sorted ages\r\n  gaps = diff( ages );\r\n  numGenerationalGaps = length( which( gaps \u003e MAX_CHILD_AGE) );\r\n  if ( numGenerationalGaps == 0 ){\r\n    if ( !is.na( thisAge )){\r\n      if ( ( thisAge \u003e= 40 ) || (thisTitle == \u0022Mrs.\u0022 ) ){\r\n        numChildren[thisRow] = numParch;\r\n      }\r\n      else if (numParch \u003e 2 ){\r\n        numChildren[thisRow] = numParch;\r\n      }\r\n      else{\r\n        numParents[thisRow] = numParch;\r\n      }\r\n    }\r\n    else{\r\n      # no age information. If travelling with \u0022kids\u0022, and title isn\u0027t a kid, then parent\r\n      if ( ! ( thisTitle %in% c(\u0022Master.\u0022,\u0022Miss.\u0022) ) ){\r\n        if( any( c(\u0022Master.\u0022,\u0022Miss.\u0022) %in% ticketTitles ) ){\r\n          # now, if there are two Mr. in this group, we need to choose the real father\r\n          # If there are three or more children, then Parch will be greater than 2 and\r\n          # will indicate this is the father.  Else, it will be a child\r\n          if( thisTitle == \u0022Mr.\u0022){\r\n            maxParches = max( ticketParches );\r\n            if ( ( maxParches \u003e 2 ) \u0026\u0026 ( maxParches == numParch ) ){\r\n              numChildren[thisRow] = numParch;\r\n            }\r\n            else{\r\n              numParents[thisRow] = numParch;\r\n            }\r\n          }\r\n          else{\r\n            numChildren[thisRow] = numParch;\r\n          }\r\n        }\r\n        else{\r\n          # all we have is a non Mr. or Miss. title.  Make them a child\r\n            numChildren[thisRow] = numParch;\r\n        }\r\n      }\r\n      else{\r\n        numParents[thisRow] = numParch;\r\n      }\r\n    }\r\n  }\r\n  else{\r\n    if ( !is.na(thisAge) ){  # use age in comparison to generation gap to classify kids/parents\r\n      maxGapPos = which.max( gaps ) + 0.5;  # the 0.5 puts it in the middle of the kids/parents\r\n      if ( thisAgePos \u003c maxGapPos ){\r\n        numParents[thisRow] = numParch;\r\n      }\r\n      else{\r\n        numChildren[thisRow] = numParch;\r\n      }\r\n    }\r\n    else{\r\n      # no age info, have to go with titles\r\n    }\r\n  }\r\n  totalParch = numParents[thisRow] + numChildren[thisRow];\r\n  if ( totalParch != numParch ){\r\n    stop( \u0022Number of Parents/Children assigned (\u0022, totalParch, \u0022) does not equal the Parch variable (\u0022, numParch, \u0022) for passenger \u0022, data_combined[thisRow,\u0022PassengerId\u0022], \u0022\\n\u0022);\r\n  }\r\n  data_combined$NumParents = numParents;\r\n  data_combined$NumChildren = numChildren;\r\n}\r\n```\r\n\r\n#### Siblings and Spouses\r\nSimilar to Parch, the SibSp variable indicates the number of Siblings AND/OR Spouses a passenger has on board.  A better set of variables would be separate variables for Siblings and Spouses as it would be possible to model the original SibSp vector as a simple combination of the separate columns.\r\n```{r feature10}\r\nnumSiblings = integer( nrow( data_combined ) );\r\nnumSpouses = integer( nrow( data_combined ) );\r\n\r\nMAX_CHILD_AGE = 14;\r\n\r\nposSibSp = data_combined$SibSp \u003e 0;\r\nidxSibSp = which( posSibSp );\r\ncounter = 0;\r\nnumCases = length( idxSibSp );\r\nfor (thisRow in idxSibSp){\r\n  counter = counter + 1;\r\n  numSibSp = data_combined[thisRow,\u0022SibSp\u0022];\r\n  numParch = data_combined[thisRow,\u0022Parch\u0022];\r\n  thisSurname = data_combined[thisRow,\u0022Surname\u0022];\r\n  thisTicket = data_combined[thisRow,\u0022Ticket\u0022];\r\n  thisTitle = as.character( data_combined[thisRow,\u0022Title\u0022] );\r\n  thisAge = data_combined[thisRow,\u0022Age\u0022];\r\n  thisSex = as.character( data_combined[thisRow,\u0022Sex\u0022] );\r\n  thisGivenName = as.character( data_combined[thisRow,\u0022GivenName\u0022] );\r\n  thisPassengerId = data_combined[thisRow,\u0022PassengerId\u0022];\r\n  if ( (thisTitle == \u0022Master.\u0022) || (thisTitle == \u0022Miss.\u0022) ){\r\n    # if this passenger is a \u0022Master.\u0022 or Miss., this passenger is not married, so must be spouse\r\n    numSiblings[thisRow] = numSibSp;\r\n    next;\r\n  }\r\n  if (!is.na( thisAge ) \u0026\u0026 thisAge \u003c= 10 ){\r\n    # if this passenger is young, declare they cannot be parents, must be a child\r\n    numSiblings[thisRow] = numSibSp;\r\n    next;\r\n  }\r\n  # get passenger rows on same ticket\r\n  sameTickets = data_combined$Ticket == thisTicket;\r\n  sameSurnames = data_combined$Surname == thisSurname;\r\n  sameSurnameSameTicket = sameTickets \u0026 sameSurnames \u0026 posSibSp;\r\n  # if married, look for spouse on the same ticket\r\n  # get given names on same ticket\r\n  same_ticket_rows = data_combined[which(sameSurnameSameTicket),];\r\n  given_names = as.character( same_ticket_rows$GivenName );\r\n  titles = as.character( same_ticket_rows$Title );\r\n  sexes = as.character( same_ticket_rows$Sex );\r\n  passengerIds = same_ticket_rows$PassengerId;\r\n  master_mask = ( titles != \u0022Master.\u0022 );  \r\n  sex_mask = ( sexes != thisSex );  # opposite sex marriage\r\n  miss_mask = ( titles != \u0022Miss.\u0022 );\r\n  this_mask = ( passengerIds != thisPassengerId );\r\n  idxMatch = which( ( thisGivenName == given_names ) \u0026 master_mask \u0026 sex_mask \u0026 miss_mask \u0026 this_mask);\r\n  if ( length( idxMatch ) == 1 ){\r\n    numSpouses[thisRow] = 1;\r\n    next;\r\n  }\r\n  ticketTitles = as.character( data_combined[ sameSurnameSameTicket, \u0022Title\u0022] );\r\n  ticketParches = data_combined[ sameSurnameSameTicket, \u0022Parch\u0022];\r\n  # now, look for passengers with the same surname on the ticket and check their titles and ages\r\n  numSame = length( sameSurnameSameTicket );\r\n  ages = sort( data_combined[sameSurnameSameTicket,\u0022Age\u0022] );\r\n  thisAgePos = which( thisAge == ages )[1];  # find index of thisAge in sorted ages\r\n  gaps = diff( ages );\r\n  numGenerationalGaps = length( which( gaps \u003e MAX_CHILD_AGE) );\r\n}\r\n\r\n# do some manual corrections\r\nnumSiblings[168] = 0;  # Mrs. William Skoog\r\nnumSpouses[168] = 1;  # Mrs. William Skoog\r\nnumSiblings[361] = 0;  # Mr. Wilhelm Skoog\r\nnumSpouses[361] = 1;  # Mr. Wilhelm Skoog\r\nnumSiblings[679] = 0;  # Mrs. Frederick Goodwin\r\nnumSpouses[679] = 1;  # Mrs. Frederick Goodwin\r\nnumSiblings[1031] = 0;  # Mr. Charles Frederick Goodwin\r\nnumSpouses[1031] = 1;  # Mr. Charles Frederick Goodwin\r\nnumSiblings[557] = 0;  # Lady. Duff Gordon\r\nnumSpouses[557] = 1;  # Lady. Duff Gordon\r\nnumSiblings[600] = 0;  # Sir. Duff Gordon\r\nnumSpouses[600] = 1;  # Sir. Duff Gordon\r\nnumSiblings[746] = 0;  # Capt. Edward Gifford Crosby\r\nnumSpouses[746] = 1;  # Capt. Edward Gifford Crosby\r\nnumSiblings[1197] = 0;  # Mrs. Edward Gifford Crosby\r\nnumSpouses[1197] = 1;  # Mrs. Edward Gifford Crosby\r\nnumSiblings[1059] = 3;  # Mr. Edward Watson Ford\r\nnumSpouses[1059] = 0;  # Mr. Edward Watson Ford\r\n\r\ncounter = 0;\r\nfor (thisRow in idxSibSp){\r\n  counter = counter + 1;\r\n  numSibSp = data_combined[thisRow,\u0022SibSp\u0022];\r\n  numParch = data_combined[thisRow,\u0022Parch\u0022];\r\n  thisSurname = data_combined[thisRow,\u0022Surname\u0022];\r\n  thisTicket = data_combined[thisRow,\u0022Ticket\u0022];\r\n  thisTitle = as.character( data_combined[thisRow,\u0022Title\u0022] );\r\n  thisAge = data_combined[thisRow,\u0022Age\u0022];\r\n  thisSex = as.character( data_combined[thisRow,\u0022Sex\u0022] );\r\n  thisGivenName = as.character( data_combined[thisRow,\u0022GivenName\u0022] );\r\n  thisPassengerId = data_combined[thisRow,\u0022PassengerId\u0022];\r\n  numSiblings[thisRow] = numSibSp - numSpouses[thisRow];\r\n}\r\n\r\ndata_combined$Spouses = numSpouses;\r\ndata_combined$SpousesFactor = factor( numSpouses );\r\ndata_combined$Siblings = numSiblings;\r\n```\r\n\r\n#### FarePerPassenger\r\nAs determined in the EDA section, the Fare variable is not per passenger, it is the price paid for the ticket the passenger is traveling under (and multiple passengers may travel on the same ticket).  \r\n```{r feature11}\r\nticketChar = as.character( data_combined$Ticket );\r\nuniqueTickets = unique( ticketChar );\r\nfarePerPassenger = data_combined$Fare;\r\nfor (i in 1:length(uniqueTickets) ){\r\n    thisTicket = uniqueTickets[i];\r\n    idxPeople = which( ticketChar == thisTicket );\r\n    theseFares = data_combined[idxPeople,\u0022Fare\u0022];\r\n    #if ( !all( theseFares == theseFares[1] ) ){\r\n    #    cat( thisTicket )\r\n    #}\r\n    # should only be a single fare\r\n    \r\n    numPeople = length( idxPeople );\r\n    farePerPassenger[idxPeople] = theseFares[1]/numPeople;\r\n}\r\ndata_combined$FarePerPassenger = farePerPassenger;\r\n```\r\n\r\n***\r\n## Models\r\nThis is a classification problem with two classes: { Died, Survived }.  This is encoded in the \u0027Survived\u0027 variable in the train dataset. \r\n\r\n```{r setup, include=FALSE}\r\nknitr::opts_chunk$set(echo = TRUE, fig.align=\u0022center\u0022)\r\n```\r\n\r\n\u003c!-- Globals --\u003e\r\n\u003c!-- Data set --\u003e\r\n\r\n```{r include = FALSE}\r\nMAX_HOURS = 10;\r\nlibrary( assertthat );\r\ndata_combined$pclass \u003c- as.factor(data_combined$Pclass);\r\n\r\n# A bit about R data types (e.g., factors)\r\nsummary(data_combined );\r\n\r\n```\r\n\r\n### Null Model\r\n```{r null1, echo=FALSE}\r\nnumPassengers = length(train$Survived);\r\ndied = sum( train$Survived == 0);\r\nsurvived = sum( train$Survived == 1);\r\n#assert_that( died + survived == numPassengers);\r\nmean_survived = survived/numPassengers;\r\ncat( \u0022Of \u0022, numPassengers, \u0022 passengers, \u0022, died, \u0022 died and \u0022, survived, \u0022 survived.\\nMean Survivability = \u0022, survived/numPassengers );\r\n```\r\nThe simplest model would be to assume that all test subjects are members of the most common class in the train dataset.  In the train dataset, 62% of the subjects died, so this simple model would assume that all the test passengers die.  This would yield a prediction accuracy of about 62%, and a corresponding misclassification rate of 38%, all false negatives.\r\n\r\nThis simple model is also the fastest to implement and serves as a good initial benchmark from which to improve upon.\r\n\r\nUpon submission to the Kaggle site, the model yielded a score of 0.62679.  In terms of the bias/variance tradeoff, this is a very biased model (very inflexible) and the variance will likely be very small (i.e., the choice of training set will not affect predictions on the test set except in rare cases where more survivors are selected than non-survivors).\r\n\r\n```{r null2, echo=FALSE}\r\n# Time strip\r\nnullModelTrain = data.frame(PassengerId=train$PassengerId, Survived=0);\r\nnullModel = data.frame(PassengerId=test$PassengerId, Survived=0);\r\nwrite.csv( nullModelTrain, \u0022NullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nullModel, \u0022NullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nnullModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=342/891);\r\nwrite.csv( nullModelTrainProbs, \u0022NullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nnullModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=342/891);\r\nwrite.csv( nullModelTestProbs, \u0022NullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\n#hours = 1;\r\n#par(mfrow=c(2,1));\r\n#barplot( hours, width=0.9, main=\u0022Time\u0022, xlab=\u0022hours\u0022, ylim=c(0,1), xlim=c(0,MAX_HOURS), horiz=TRUE );\r\n#bp = ggplot(data=hours,aes(x=hours,y=1)) + geom_bar(stat=\u0022identity\u0022) + coord_flip();\r\n#bp\r\n### Bias-Variance Strip\r\n\r\n```\r\n\r\n### Gender Submission Model\r\nAlong with the train.csv and test.csv file, Kaggle provides an additional file called \u0027gender_submission.csv\u0027.  This file is a sample submission to Kaggle in which all the female passengers are predicted to survive while all of the male passengers are predicted to die.\r\n\r\n\r\n```{r genderSubmissionModel, echo=FALSE}\r\nfemaleIndices = which( train$Sex == \u0027female\u0027 );\r\nmaleIndices = which( train$Sex == \u0027male\u0027 );\r\nnumFemales = length( femaleIndices );\r\nnumMales = length( maleIndices );\r\nproportionFemaleSurvival = length( intersect( femaleIndices, which( train$Survived == 1)))/numFemales;\r\nproportionMaleSurvival = length( intersect( maleIndices, which( train$Survived == 1)))/numMales;\r\n\r\ngenderModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=0);\r\ngenderModelTrainProbs$Survived[ femaleIndices ] = proportionFemaleSurvival;\r\ngenderModelTrainProbs$Survived[ maleIndices ] = proportionMaleSurvival;\r\nwrite.csv( genderModelTrainProbs, \u0022GenderModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\ngenderModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=0);\r\nwrite.csv( genderModelTestProbs, \u0022GenderModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nIn the training set, of the 891 passengers, 342 are female.  233 of the 342 female passengers in the training set survived (74.2%).  Of the 549 male passengers in the train set, 468 did not survive (81.1%).  This yields an overall classification rate of 78.7% for the test set.  On the bias/variance continuum, this model is very biased and should have small variance.\r\n\r\nUpon submission to Kaggle, this model scored 0.76555.\r\n\r\n### Women and Children First Model\r\nThe canonical model for a sinking ship is that women and children get preferential access to lifeboats while men are expected to defer.  If 100% accordance to this model is assumed and all women and children are boarded onto lifeboats while the men are left to fight it out for flotsam in the frigid waters, women and children would be expected to survive while the men would perish.\r\n\r\nSince the Age variable is missing for 177 of the train set cases and 86 test set cases, the imputed Age values from above will be used.  For this application, a child will be defined as a person under 15 years of age.\r\n```{r womenAndChildrenFirstTrain, echo=FALSE}\r\nlibrary(caret, quietly=TRUE);\r\nfemaleIndices = which( train$Sex == \u0027female\u0027 );\r\ndata_combined[[\u0022SurvivedFactor\u0022]] = factor(data_combined[[\u0022Survived\u0022]]);\r\nwhiten = preProcess(data_combined, c(\u0022center\u0022,\u0022scale\u0022,\u0022BoxCox\u0022) );\r\nwhitened_data = data.frame( predict( whiten, data_combined ));\r\n# now \u0022unwhiten\u0022 some data like Survived and PassengerId\r\nwhitened_data$PassengerId = data_combined$PassengerId;\r\nwhitened_data$Survived = data_combined$Survived;\r\nnewTrain = subset( data_combined, PassengerId \u003c= nrow(train) );\r\nnewTest = subset( data_combined, PassengerId \u003e nrow(train) );\r\nwhiteTrain = subset( whitened_data, PassengerId \u003c= nrow(train) );\r\nwhiteTest = subset( whitened_data, PassengerId \u003e nrow(train) );\r\nchildIndices = which( newTrain$FixedAge \u003c 15 );\r\nwomenOrChildren = union( femaleIndices, childIndices );\r\nmaleIndices = setdiff( 1:nrow(train), womenOrChildren );\r\nwomenOrChildrenSurvived = which( train[ womenOrChildren, \u0022Survived\u0022] == 1 );\r\nmenDied = which( train[ maleIndices, \u0022Survived\u0022] == 0);\r\nnumFemales = length( femaleIndices );\r\nnumMales = length( maleIndices );\r\ntrainProportionCorrect = ( length(womenOrChildrenSurvived) + length(menDied))/nrow(train);\r\n```\r\n\r\nIn the training set, the proportion of correct predictions is 0.79.  \r\n\r\n```{r womenAndChildrenFirstTest, echo=FALSE}\r\n# start with all perished model, then set women and children to survived\r\nwomenAndChildrenFirstModelTrain = data.frame(PassengerId=train$PassengerId, Survived=0);\r\nwomenAndChildrenFirstModel = data.frame(PassengerId=test$PassengerId, Survived=0);\r\nnewTest = subset( data_combined, PassengerId \u003e nrow(train) );\r\nfemaleIndices = which( newTest$Sex == \u0027female\u0027 );\r\ntrainFemaleIndices = which( newTrain$Sex == \u0027female\u0027 );\r\nchildIndices = which( newTest$FixedAge \u003c 15 );\r\ntrainChildIndices = which( newTrain$FixedAge \u003c 15 );\r\nwomenOrChildren = union( femaleIndices, childIndices );\r\nwomenOrChildrenTrain = union( trainFemaleIndices, trainChildIndices );\r\nwomenAndChildrenFirstModelTrain[womenOrChildrenTrain,\u0022Survived\u0022] = 1;\r\nwomenAndChildrenFirstModel[womenOrChildren,\u0022Survived\u0022] = 1;\r\nwrite.csv( womenAndChildrenFirstModelTrain, \u0022WomenAndChildrenFirstModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( womenAndChildrenFirstModel, \u0022WomenAndChildrenFirstModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\nprobWCSurvived = mean(newTrain$Survived[ womenOrChildrenTrain ]);\r\nprobMenSurvived = mean(newTrain$Survived[ -womenOrChildrenTrain ]);\r\nwcModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=newTrain$Survived);\r\nwcModelTrainProbs$Survived[womenOrChildrenTrain] = probWCSurvived;\r\nwcModelTrainProbs$Survived[-womenOrChildrenTrain] = probMenSurvived;\r\nwrite.csv( wcModelTrainProbs, \u0022WCModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwcModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=0);\r\nwcModelTestProbs$Survived[womenOrChildren] = probWCSurvived;\r\nwcModelTestProbs$Survived[-womenOrChildren] = probMenSurvived;\r\nwrite.csv( wcModelTestProbs, \u0022WCModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nUpon submission to Kaggle, the Women and Children First model scored 0.76076.\r\n\r\n### Linear Discriminant Analysis (LDA) Models\r\n\r\nIn Linear Discriminant Analysis, the distributions of each of the predictors (Passenger Class, Age, Sex, etc.) are modeled separately for the Survived and Died classes.  Then, Bayes\u0027 theorem can be flipped around to get estimates for the probability of Survived or Died given the set of predictors.  LDA assumes that the observations in each class (Survived, Died) are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix common to both classes.  This assumption does not hold for some of the predictors in this analysis, as not only are they not all normal, some are categorical (Sex, Embarked).  Nevertheless, the implementation in R converts the variables to a numeric albeit not-normal representation.\r\n\r\n#### LDA Model using all features\r\nThe first LDA model is one in which all predictors are considered: pClass, Sex, FixedAge, FarePerPassenger, FixedTitle, CabinAssignment, Embarked, NumPassengersOnTicket, NumParents, NumChildren, SpousesFactor, and Siblings.  Against the training data, the model scored 0.8373 (~84% accuracy).  When run against the test set, the model scored 0.78947.\r\n\r\n```{r lda1, warning=FALSE}\r\nlibrary(MASS, quietly=TRUE);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nlda.fit = lda( SurvivedFactor ~., data=xTrain);\r\nlda.fit\r\nplot( lda.fit );\r\nlda.train.pred = predict( lda.fit, xTrain );  \r\nldaTrainPredictions = as.numeric( lda.train.pred$class ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( ldaTrainPredictions, truth );\r\n# scored a 0.8373 against the training data\r\nlda.test.pred = predict( lda.fit, xTest );  \r\n# lda.pred$class is a factor; need to convert to 0, 1\r\nldaPredictions = as.numeric( lda.test.pred$class ) - 1;\r\nldaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=ldaTrainPredictions);\r\nldaModel = data.frame(PassengerId=test$PassengerId, Survived=ldaPredictions);\r\n\r\nwrite.csv( ldaModelTrain, \u0022LDAModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModelTrain, \u0022LDAModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModel, \u0022LDAModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModel, \u0022LDAModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.78947\r\n```\r\n\r\n#### LDA Model with only significant features\r\nIn subsequent analyses (logistic regression, random forests), the predictors are measured for effect on the response variable.  The logistic regression analysis indicates which covariates have a low p-value (i.e., \u003c0.05), indicating a significant effect.  Similarly, random forests produce a variable importance measure which shows which covariates are used most often in tree generation.  These measures give a good idea of which variables are likely to have the most predictive power.  Using these variables and leaving out the variables with higher p-values and those not often used in random forests will likely reduce overfitting.  The variables with the best predictive power in this set are pClass, Sex, FixedAge, FarePerPassenger, FixedTitle, CabinAssignment, NumChildren, SpousesFactor, and Siblings.\r\n\r\nThe LDA model using only significant features scored a 0.8339 against the training data and, similar to the full-featured LDA model, scored a 0.78947.\r\n\r\n```{r lda2, echo=TRUE, warning=FALSE}\r\nlibrary(MASS);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nlda.fit = lda( SurvivedFactor ~., data=xTrain);\r\nplot( lda.fit );\r\nlda.train.pred = predict( lda.fit, xTrain );  \r\nldaTrainPredictions = as.numeric( lda.train.pred$class ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( ldaTrainPredictions, truth );\r\n# scored a 0.8339 against the training data\r\nlda.test.pred = predict( lda.fit, xTest );  \r\n# lda.pred$class is a factor; need to convert to 0, 1\r\nldaPredictions = as.numeric( lda.test.pred$class ) - 1;\r\nldaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=ldaTrainPredictions);\r\nldaModel = data.frame(PassengerId=test$PassengerId, Survived=ldaPredictions);\r\n\r\nwrite.csv( ldaModelTrain, \u0022LDAReducedModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModelTrain, \u0022LDAReducedModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModel, \u0022LDAReducedModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModel, \u0022LDAReducedModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.78947\r\n```\r\n\r\n### Quadratic Discriminant Analysis (QDA) Models\r\n\r\nQuadratic Discriminant Analysis (QDA) is very much similar to LDA but with one difference: there is no assumption that the classes have a common variance.  In this model, each class has its own covariance matrix.  QDA is so named because the discriminant functions are quadratic in the predictor vector, as opposed to linear with LDA.  Since the discriminant function is quadratic, the decision boundary is not constrained to a line as with LDA and can assume curved shapes.\r\n\r\n#### QDA Model with all features\r\n\r\nThe QDA model with all variables scored a 0.8159 against the training set and 0.77033 against the test data.\r\n\r\n```{r qda1, echo=TRUE, warning=FALSE}\r\nlibrary(MASS);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nqda.fit = qda( SurvivedFactor ~., data=xTrain);\r\nqda.train.pred = predict( qda.fit, xTrain );  \r\nqdaTrainPredictions = as.numeric( qda.train.pred$class ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( qdaTrainPredictions, truth );\r\n# scored a 0.8159 against the training data\r\nqda.test.pred = predict( qda.fit, xTest );  \r\n# lda.pred$class is a factor; need to convert to 0, 1\r\nqdaPredictions = as.numeric( qda.test.pred$class ) - 1;\r\nqdaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=qdaTrainPredictions);\r\nqdaModel = data.frame(PassengerId=test$PassengerId, Survived=qdaPredictions);\r\n\r\nwrite.csv( qdaModelTrain, \u0022QDAModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModelTrain, \u0022QDAModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModel, \u0022QDAModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModel, \u0022QDAModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.77033\r\n```\r\n\r\n#### QDA Model with only significant features\r\n\r\nThe QDA model using only significant features scored a 0.7845 against the training set and 0.78947 against the test data.\r\n\r\n```{r qda2, echo=TRUE, warning=FALSE}\r\nlibrary(MASS);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022NumChildren\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022NumChildren\u0022,\u0022Siblings\u0022)];\r\nqda.fit = qda( SurvivedFactor ~., data=xTrain);\r\nqda.train.pred = predict( qda.fit, xTrain );  \r\nqdaTrainPredictions = as.numeric( qda.train.pred$class ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( qdaTrainPredictions, truth );\r\n# scored a 0.7845 against the training data\r\nqda.test.pred = predict( qda.fit, xTest );  \r\n# lda.pred$class is a factor; need to convert to 0, 1\r\nqdaPredictions = as.numeric( qda.test.pred$class ) - 1;\r\nqdaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=qdaTrainPredictions);\r\nqdaModel = data.frame(PassengerId=test$PassengerId, Survived=qdaPredictions);\r\n\r\nwrite.csv( qdaModelTrain, \u0022QDASigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModelTrain, \u0022QDASigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModel, \u0022QDASigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModel, \u0022QDASigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.78947\r\n```\r\n### Logistic Regression Models\r\n\r\nIn a linear regression model, the probability of the response variable is modeled as a linear combination of predictors and multiplicative coefficients.  The encoding of the response variable (Survived) as 0 and 1 allows for a linear regression model but covariates that produce a response greater than 1 and less than 0 are considered errors and can negatively impact parameter estimation.  In logistic regression, the probability of the response variable is modeled with the logistic function, or y = e^X/(1+e^X).  Logistic regression is a classic model for binary classification.\r\n\r\n#### Logistic Regression using all features\r\nA logistic regression model is fit using the train data.  Selected variables are shown below.  Some variables could be treated as either numeric or factors (i.e., NumPassengersOnTicket and NumChildren).  Because there was a natural ordering of these variables, they are treated as numeric values rather than factors.  This was done to capture the linear dependence of each variable to the survival response.\r\n\r\n```{r logisticRegression1, echo=TRUE, warning=FALSE}\r\nall_features = c(\u0022pclass\u0022, \u0022Sex\u0022, \u0022FixedAge\u0022, \u0022FarePerPassenger\u0022, \u0022FixedTitle\u0022, \u0022Siblings\u0022, \u0022CabinAssignment\u0022, \u0022Embarked\u0022, \u0022NumPassengersOnTicket\u0022, \u0022NumParents\u0022, \u0022NumChildren\u0022, \u0022SpousesFactor\u0022);\r\nfull_df = newTrain[, c(\u0022Survived\u0022, all_features)];\r\nfull.glm.fit = glm( Survived~., data=full_df, family=binomial);\r\nsummary( full.glm.fit );\r\ntrainSetProbs = predict( full.glm.fit, newTrain, type=\u0022response\u0022);  # now a vector of probabilities\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\n# training set performance\r\nthreshold = 0.5;\r\npredictions = as.numeric( trainSetProbs \u003e= threshold );\r\nconfusionMatrix( predictions, newTrain$Survived );\r\n\r\n# now, how\u0027d we do against the test set?\r\ntestSetProbs = predict( full.glm.fit, newTest, type=\u0022response\u0022);\r\nfullLogisticPredictions = as.numeric( testSetProbs \u003e= threshold );\r\nfullLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);\r\nfullLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullLogisticPredictions);\r\n\r\nfullLogisticRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=trainSetProbs);\r\nfullLogisticRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=testSetProbs);\r\n\r\nwrite.csv( fullLogisticRegressionModelTrainProbs, \u0022LogisticRegressionFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLogisticRegressionModelProbs, \u0022LogisticRegressionFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\nwrite.csv( fullLogisticRegressionModelTrain, \u0022LogisticRegressionFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLogisticRegressionModel, \u0022LogisticRegressionFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.78947 - not expected\r\n```\r\n\r\nThe logistic regression model was first fit with all 12 possible covariates. After fitting the model to the training data, the model was run on the training set yielding a training set test error of 0.1661055 (0.8338945 accuracy rate).  The model had an overall accuracy of 0.78947 on the test set (the score from Kaggle).  Since this was lower than the expected score (the training set yielded a score of 0.8338945), it appears the model may be overfitting the training data.  \r\n\r\n```{r logisticRegression2, echo=TRUE, warning=FALSE}\r\nlibrary(caret);\r\nlibrary(lattice);\r\ncat( \u0022Training Set performance:\\n\u0022);\r\nconfusionMatrix( predictions, newTrain$Survived);\r\n```\r\n\r\nBecause the logistic regression model with all 12 predictors has a test set error somewhat larger than the training set error, the model is likely fitting noise in the training set.  It\u0027s very likely that some of the features are co-linear, for instance, Passenger Class (pclass) and FarePerPassenger.  If the number of predictors can be reduced, the variance of the model can likely be lowered.  After running the full set of predictors, the model shows several variables as significant: FixedTitle, Sex, and Siblings.  Other variables that show nearly significant correlation to the response are FixedAge, pclass, CabinAssignment, and NumChildren.  If those variables are brought forward and tested separately in different combinations, a model with a lower cross validation error might result.  As a final measure, a separate model was run with the remaining \u0022leftover\u0022 features: FarePerPassenger, Embarked, NumPassengersOnTicket, SpousesFactor, and NumParents.  The idea is to see if any significant features can be determined in the absence of the significant features of the full model.\r\n\r\n```{r logisticRegression3, echo=TRUE, warning=FALSE}\r\nleftoverFeatures = c(\u0022FarePerPassenger\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022SpousesFactor\u0022,\u0022NumParents\u0022);\r\nleftover_df = newTrain[, c(\u0022Survived\u0022, leftoverFeatures)];\r\nleftover.glm.fit = glm( Survived ~., data = leftover_df, family=binomial);\r\nsummary(leftover.glm.fit);\r\n```\r\n\r\n#### Logistic Regression using only significant features\r\n\r\nThe logistic regression model with the \u0022leftover\u0022 features yielded two new feature possibilities: FarePerPassenger and NumParents.  These features are likely proxies for passenger class (pclass) and age (FixedAge).  i.e., passengers travelling with their parents are more likely to be children and thus, have a higher survival probability.  Similarly, FarePerPassenger is likely to reflect the passenger class separation.  However, unlike pclass, FarePerPassenger is a continuous variable and possibly more informative than the three passenger classes.  So, combined with the significant features from the full logistic regression model, a set of features can be drawn upon in different combinations in order to select the model with the lowest cross validation error.\r\n\r\n```{r logisticRegression4, echo=TRUE, warning=FALSE}\r\nlibrary(boot, quietly = TRUE);\r\nsignificant_features = c(\u0022FixedTitle\u0022, \u0022Sex\u0022, \u0022Siblings\u0022, \u0022FixedAge\u0022, \u0022CabinAssignment\u0022, \u0022pclass\u0022, \u0022NumChildren\u0022, \u0022FarePerPassenger\u0022, \u0022Embarked\u0022);\r\nall_combinations_of_significant_features = lapply(1:length(significant_features), function(x) combn(length(significant_features),x));\r\nnumFeatureSetClasses = length( all_combinations_of_significant_features );\r\ncorrect = list();\r\naccuracies = list();\r\nerrors = list();\r\nfeatures = list();\r\nnumFeatures = list();\r\ncounter = 1;\r\nfor ( featureClass in 1:numFeatureSetClasses){\r\n  featClassColumns = dim( all_combinations_of_significant_features[[featureClass]] )[2];\r\n  for ( thisCol in 1:featClassColumns ){\r\n    feature_set = significant_features[ all_combinations_of_significant_features[[featureClass]][,thisCol]];\r\n    nFeatures = length( feature_set );\r\n    df = newTrain[, c(\u0022Survived\u0022, feature_set)]\r\n    glm.fit = glm( Survived~.,data=df,family=binomial);\r\n    cv.glm.fit = cv.glm(df,glm.fit,K=10);\r\n    cv.error = cv.glm.fit$delta[1];\r\n    trainSetProbs = predict( glm.fit, newTrain, type=\u0022response\u0022);  # now a vector of probabilities\r\n    predictions = as.numeric( trainSetProbs \u003e= threshold );\r\n    numRight = sum( predictions == newTrain$Survived );\r\n    accuracy = mean( predictions == newTrain$Survived );\r\n    features[counter] = list( feature_set );\r\n    numFeatures[counter] = nFeatures;\r\n    accuracies[counter] = accuracy;\r\n    correct[counter] = numRight;\r\n    errors[counter] = cv.error;\r\n    counter = counter + 1;\r\n  }\r\n}\r\ncvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), Features=I(features) );\r\ncvResults = cvResults[order(cvResults$CVError),];\r\nknitr::kable( head(cvResults,n=20L), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nOf the 512 different feature combinations, the top 20 are listed.  The cross validation error estimates were made using 10-fold cross validation.  The top 10 are retested using LOOCV:\r\n\r\n```{r logisticRegression5, echo=TRUE, warning=FALSE}\r\ncorrect = list();\r\naccuracies = list();\r\nerrors = list();\r\nfeatures = list();\r\nnumFeatures = list();\r\ncounter = 1;\r\nfor ( rowNum in 502:511){\r\n    feature_set = cvResults$Features[[rowNum]];\r\n    nFeatures = cvResults$NumFeatures[rowNum];\r\n    df = newTrain[, c(\u0022Survived\u0022, feature_set)]\r\n    glm.fit = glm( Survived~.,data=df,family=binomial);\r\n    cv.glm.fit = cv.glm(df,glm.fit);\r\n    cv.error = cv.glm.fit$delta[1];\r\n    trainSetProbs = predict( glm.fit, newTrain, type=\u0022response\u0022);  # now a vector of probabilities\r\n    predictions = as.numeric( trainSetProbs \u003e= threshold );\r\n    numRight = sum( predictions == newTrain$Survived );\r\n    accuracy = mean( predictions == newTrain$Survived );\r\n    features[counter] = list( feature_set );\r\n    numFeatures[counter] = nFeatures;\r\n    accuracies[counter] = accuracy;\r\n    correct[counter] = numRight;\r\n    errors[counter] = cv.error;\r\n    counter = counter + 1;\r\n}\r\nloocvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), Features=I(features) );\r\nloocvResults = loocvResults[order(loocvResults$CVError),];\r\nknitr::kable( head(loocvResults,n=10L), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nThe model with the lowest cross validation error is a model with 8 features: FixedTitle, Sex, Siblings, FixedAge, CabinAssignment, pclass, NumChildren, and Embarked.  A logistic regression model is fit using these features and submitted to Kaggle.\r\n\r\n```{r logisticRegression6, echo=TRUE, warning=FALSE}\r\nbest_features = c(\u0022pclass\u0022, \u0022Sex\u0022, \u0022FixedAge\u0022, \u0022FixedTitle\u0022, \u0022Siblings\u0022, \u0022CabinAssignment\u0022, \u0022Embarked\u0022, \u0022NumChildren\u0022 );\r\nbest_df = newTrain[, c(\u0022Survived\u0022, best_features)];\r\nbest.glm.fit = glm( Survived~., data=best_df, family=binomial);\r\ntrainSetProbs = predict( best.glm.fit, newTrain, type=\u0022response\u0022);  # now a vector of probabilities\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nthreshold = 0.5;\r\npredictions = as.numeric( trainSetProbs \u003e= threshold );\r\nsummary( best.glm.fit );\r\n\r\n# How\u0027d we do?\r\n# now calculate the training set error\r\ncorrectRate = mean( predictions == newTrain$Survived );\r\n# scored a 0.8395062\r\n\r\n# now, how\u0027d we do against the test set?\r\ntestSetProbs = predict( best.glm.fit, newTest, type=\u0022response\u0022);\r\nbestTestPredictions = as.numeric( testSetProbs \u003e= threshold );\r\nbestLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);\r\nbestLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=bestTestPredictions);\r\n\r\nbestLogisticRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=trainSetProbs);\r\nbestLogisticRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=testSetProbs);\r\n\r\nwrite.csv( bestLogisticRegressionModelTrainProbs, \u0022LogisticRegressionSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLogisticRegressionModelProbs, \u0022LogisticRegressionSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\nwrite.csv( bestLogisticRegressionModelTrain, \u0022LogisticRegressionSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLogisticRegressionModel, \u0022LogisticRegressionSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.77990 - not as good as the full model\r\n```\r\nThe model based on significant features scored lower on the test set than the full model.  To reduce the model fit to noise in the training data, the regularization methods of ridge regression and lasso are considered.\r\n\r\n### Logistic Regression with Ridge Regression Regularization\r\nRidge regression introduces a coefficient penalty that reduces the effect of the covariates.  This can help with variance by making the solution slightly less flexible.  The glmnet package includes a method for determining the lambda penalty coefficient using cross validation (default is 10-fold cross validation).\r\n\r\n\u003c!--![](http://businessforecastblog.com/wp-content/uploads/2014/01/RRminization.png)--\u003e\r\n![](../RRminimization.png)\r\n\r\n#### Logistic Ridge Regression on full feature set\r\n```{r ridgeRegression1, echo=TRUE, warning=FALSE}\r\nlibrary( glmnet, quietly = TRUE );\r\nlambdas = c();\r\nset.seed(1);\r\n# use ridge regression\r\nxTrain = model.matrix( Survived~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,family=binomial);\r\nxTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest,family=binomial);\r\n#xTrain = model.matrix( Survived~.,data=full_df,family=binomial);\r\nfull_df_test = newTest[, all_features];\r\n#xTest = model.matrix( ~.,data=full_df_test,family=binomial);\r\ncv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=0, type.measure=\u0022class\u0022);\r\nplot( cv.out );\r\nlambdaMin = cv.out$lambda.min;\r\nrrmodel = glmnet( xTrain, newTrain$Survived, alpha=0);\r\ntrain.ridge.pred = predict( rrmodel, s=lambdaMin, newx=xTrain);\r\ntrainRidgePredictions = as.numeric( train.ridge.pred \u003e= threshold );\r\nnumCorrect = sum(trainRidgePredictions == newTrain$Survived);\r\naccuracy = numCorrect/nrow(newTrain);\r\nridge.pred = predict( rrmodel, s=lambdaMin, newx=xTest);\r\nfullRidgePredictions = as.numeric( ridge.pred \u003e= threshold );\r\nfullRidgeRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainRidgePredictions);\r\nfullRidgeRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullRidgePredictions);\r\nfullRidgeRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=train.ridge.pred);\r\nfullRidgeRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=ridge.pred);\r\nwrite.csv( fullRidgeRegressionModelTrainProbs, \u0022RidgeRegressionFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullRidgeRegressionModelProbs, \u0022RidgeRegressionFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullRidgeRegressionModelTrain, \u0022RidgeRegressionFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullRidgeRegressionModel, \u0022RidgeRegressionFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# score is 0.78947\r\n```\r\n\r\nThe training set mean squared error is the smallest at a lambda value of 0.02931267.  Against the training data, the model achieved an accuracy of 0.8226712.\r\n\r\n```{r ridgeRegression1b, echo=TRUE}\r\nconfusionMatrix(trainRidgePredictions, newTrain$Survived);\r\n```\r\n\r\nWhen comparing to the full logistic regression solution, the ridge predictions are the same as the least-squares logistic regression in 398 of the 418 cases (95.2% of the test set).  The model scored a 0.78468 upon submission to the Kaggle site - slightly less than the full logistic regression solution (see comparison below).\r\n\r\n```{r ridgeRegression2, echo=TRUE}\r\nfullLogisticPredictions = bestTestPredictions;\r\nconfusionMatrix( fullRidgePredictions, fullLogisticPredictions );\r\n```\r\nNext, a ridge regression model is fit using only the significant features determined from the logistic regression solution.  From the training data, lambda was determined to be 0.02931267.  Against the training data, the model achieved an accuracy of 0.8249158.\r\n\r\n#### Logistic Ridge Regression on significant features\r\n\r\n```{r ridgeRegression3, echo=TRUE, warning=FALSE}\r\nlibrary( glmnet );\r\nlambdas = c();\r\nset.seed(1);\r\n# use ridge regression\r\nxTrain = model.matrix( Survived~.,data=best_df,family=binomial);\r\nbest_df_test = newTest[, best_features];\r\nxTest = model.matrix( ~.,data=best_df_test,family=binomial);\r\ncv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=0, type.measure=\u0022class\u0022);\r\nplot( cv.out );\r\nlambdaMin = cv.out$lambda.min;\r\nrrmodel = glmnet( xTrain, newTrain$Survived, alpha=0);\r\ntrain.ridge.pred = predict( rrmodel, s=lambdaMin, newx=xTrain);\r\ntrainRidgePredictions = as.numeric( train.ridge.pred \u003e= threshold );\r\nnumCorrect = sum(trainRidgePredictions == newTrain$Survived);\r\naccuracy = numCorrect/nrow(newTrain);\r\nridge.pred = predict( rrmodel, s=lambdaMin, newx=xTest);\r\nbestRidgePredictions = as.numeric( ridge.pred \u003e= threshold );\r\nbestRidgeRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainRidgePredictions);\r\nbestRidgeRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=bestRidgePredictions);\r\n\r\nbestRidgeRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.ridge.pred));\r\nbestRidgeRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(ridge.pred));\r\nwrite.csv( bestRidgeRegressionModelTrainProbs, \u0022RidgeRegressionSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestRidgeRegressionModelProbs, \u0022RidgeRegressionSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\nwrite.csv( bestRidgeRegressionModelTrain, \u0022RidgeRegressionSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestRidgeRegressionModel, \u0022RidgeRegressionSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nWhen comparing to the full logistic regression solution, the best feature ridge predictions are the same as the least-squares logistic regression in 399 of the 418 cases (95.4% of the test set).  The model scored a 0.77990 upon submission to the Kaggle site - which is less than both the full logistic regression and full ridge regression models.\r\n\r\n```{r ridgeRegression4, echo=TRUE}\r\nfullLogisticPredictions = bestTestPredictions;\r\nconfusionMatrix( bestRidgePredictions, fullLogisticPredictions );\r\n```\r\n\r\n\r\n### Logistic Regression with Lasso Regularization\r\nSimilar to ridge regression, lasso regularization introduces a penalty to reduce the magnitude of coefficient estimates.  As with ridge regression, the glmnet implementation of the lasso uses cross validation to estimate the best value of lambda.  The Lasso regularization method multiplies lambda with the absolute value of the coefficients as the penalty and, unlike Ridge Regression, can be used to eliminate features altogether.\r\n\r\n\u003c!--![](http://www.statisticshowto.com/wp-content/uploads/2015/09/lasso-regression.png)--\u003e\r\n![](../lasso-regression.png)\r\n\r\n#### Lasso using all features\r\n```{r lassoRegression, echo=TRUE, warning=FALSE}\r\n# on to the lasso\r\nxTrain = model.matrix( Survived~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,family=binomial);\r\nxTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest,family=binomial);\r\ncv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=1, type.measure = \u0022class\u0022);\r\nplot( cv.out );\r\nlambdaMin = cv.out$lambda.min;\r\nlassomodel = glmnet( xTrain, newTrain$Survived, alpha=1);\r\ntrain.lasso.pred = predict( lassomodel, s=lambdaMin, newx=xTrain);\r\ntrainLassoPredictions = as.numeric( train.lasso.pred \u003e= threshold );\r\nnumCorrect = sum(trainLassoPredictions == newTrain$Survived);\r\naccuracy = numCorrect/nrow(newTrain);\r\nlasso.pred = predict( lassomodel, s=lambdaMin, newx=xTest);\r\nlassoPredictions = as.numeric( lasso.pred \u003e= threshold );\r\nfullLassoRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainLassoPredictions);\r\nfullLassoRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=lassoPredictions);\r\nfullLassoRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.lasso.pred));\r\nfullLassoRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(lasso.pred));\r\nwrite.csv( fullLassoRegressionModelTrainProbs, \u0022LassoRegressionFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLassoRegressionModelProbs, \u0022LassoRegressionFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLassoRegressionModelTrain, \u0022LassoRegressionFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLassoRegressionModel, \u0022LassoRegressionFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nIn the plot, as Lambda increases, the number of features in the model is reduced (see top axis).  In this application, no features were eliminated.  The Lasso model using all of the features had a training set error of 0.1638608 with a lambda of 0.00160114 (see confusion matrix below).  \r\n\r\n```{r lassoRegression2, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( trainLassoPredictions, newTrain$Survived );\r\n```\r\n\r\nWhen comparing to the full logistic regression model against the test data, the lasso model differed from the full logistic regression model in 11 cases, predicting the same outcome in 407 of 419 rows.  Against the Kaggle test data, the model scored 0.78947.\r\n\r\n```{r lassoRegression3, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( lassoPredictions, fullLogisticPredictions );\r\n```\r\n\r\n#### Lasso on significant features\r\nThe lasso model with only the significant features achieved a training set error of 0.1627385 using a lambda of 0.0002067951.\r\n```{r lassoRegression4, echo=TRUE, warning=FALSE}\r\n# on to the lasso\r\nxTrain = model.matrix( Survived~.,data=best_df,family=binomial);\r\nbest_df_test = newTest[, best_features];\r\nxTest = model.matrix( ~.,data=best_df_test,family=binomial);\r\ncv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=1, type.measure = \u0022class\u0022);\r\nplot( cv.out );\r\nlambdaMin = cv.out$lambda.min;\r\nlassomodel = glmnet( xTrain, newTrain$Survived, alpha=1);\r\ntrain.lasso.pred = predict( lassomodel, s=lambdaMin, newx=xTrain);\r\ntrainLassoPredictions = as.numeric( train.lasso.pred \u003e= threshold );\r\nnumCorrect = sum(trainLassoPredictions == newTrain$Survived);\r\naccuracy = numCorrect/nrow(newTrain);\r\nlasso.pred = predict( lassomodel, s=lambdaMin, newx=xTest);\r\nlassoPredictions = as.numeric( lasso.pred \u003e= threshold );\r\nbestLassoRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainLassoPredictions);\r\nbestLassoRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=lassoPredictions);\r\n\r\nbestLassoRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.lasso.pred));\r\nbestLassoRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(lasso.pred));\r\n\r\nwrite.csv( bestLassoRegressionModelTrainProbs, \u0022LassoRegressionSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLassoRegressionModelProbs, \u0022LassoRegressionSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLassoRegressionModelTrain, \u0022LassoRegressionSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLassoRegressionModel, \u0022LassoRegressionSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nThe \u0022best\u0022 feature Lasso model had a training set error of 0.1616162 with a lambda of 0.0005754197 (see confusion matrix below).  \r\n\r\n```{r lassoRegression5, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( trainLassoPredictions, newTrain$Survived );\r\n```\r\n\r\nWhen comparing to the full logistic regression model against the test data, the \u0022best feature\u0022 lasso model differed from the full logistic regression model in 11 cases, predicting the same outcome in 407 of 419 rows.  Against the Kaggle test data, the model scored 0.79425.\r\n\r\n```{r lassoRegression6, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( lassoPredictions, fullLogisticPredictions );\r\n```\r\n\r\nThe best feature lasso model differed from the full logistic regression model in 11 cases and from the best feature ridge regression model in 12 cases (see confusion matrices below).\r\n\r\n```{r lassoRegression7, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( lassoPredictions, fullLogisticPredictions );\r\nconfusionMatrix( bestRidgePredictions, lassoPredictions );\r\n# score is 0.78947\r\n```\r\n\r\n### K-Nearest Neighbors Models\r\n\r\nK-Nearest Neighbors is a conceptionally simple method of assigning the most common class label to a point based on the labels of the closest K points.  The lower the K, the higher the variance (at an extreme, K=1 will assign the class label based on the closest labeled point).  Conversely, the higher the K, the lower the variance and higher the bias.  With K as N-1 as an extreme, each point will be labeled as the most common class.  This method lends itself nicely to cross-validation to find the best value of K.\r\n\r\n#### KNN using all features\r\nUsing Leave One Out Cross Validation, the KNN model was trained using all features.  The model was cross-validated using 20 different K-values from 5 to 43, odd numbered.  The model achieved a training set accuracy of 0.8563 with a K of 9.\r\n\r\n```{r knn1, warning=FALSE}\r\nlibrary(caret);\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022);\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit );\r\ntest_pred = predict( knn_fit, newdata=newTrain);\r\nconfusionMatrix( test_pred, newTrain$SurvivedFactor)\r\n```\r\n\r\nAgainst the Kaggle test data, the model scored 0.76555.\r\n\r\n```{r knn2, echo=TRUE, warning=FALSE}\r\n# now the whole training set\r\nknnTrain = newTrain;\r\nknnTest = newTest;\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nfullKNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nfullKNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( fullKNNModelTrain, \u0022KNNFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullKNNModel, \u0022KNNFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullKNNModelTrain, \u0022KNNFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullKNNModel, \u0022KNNFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### KNN with subsets of significant features\r\n\r\nSince some features may not be informative and all features are used in finding closest points, it may be better to consider removing features that just add noise. The idea is to start with all significant features and then consider combinations as subsets for analysis.\r\n\r\n```{r knn3, warning=FALSE}\r\nsignificant_features = c(\u0022FixedTitle\u0022, \u0022Sex\u0022, \u0022Siblings\u0022, \u0022FixedAge\u0022, \u0022CabinAssignment\u0022, \u0022pclass\u0022, \u0022NumChildren\u0022, \u0022FarePerPassenger\u0022, \u0022Embarked\u0022);\r\nall_combinations_of_significant_features = lapply(1:length(significant_features), function(x) combn(length(significant_features),x));\r\nnumFeatureSetClasses = length( all_combinations_of_significant_features );\r\ncorrect = list();\r\naccuracies = list();\r\nerrors = list();\r\nfeatures = list();\r\nnumFeatures = list();\r\nk = list();\r\ncounter = 1;\r\nnumTrain = nrow( newTrain );\r\ntrainCtrl = trainControl( method=\u0022repeatedcv\u0022, number = 8, repeats = 1 );\r\nfor ( featureClass in 1:numFeatureSetClasses){\r\n  featClassColumns = dim( all_combinations_of_significant_features[[featureClass]] )[2];\r\n  for ( thisCol in 1:featClassColumns ){\r\n    feature_set = significant_features[ all_combinations_of_significant_features[[featureClass]][,thisCol]];\r\n    nFeatures = length( feature_set );\r\n    df = newTrain[, c(\u0022SurvivedFactor\u0022, feature_set)]\r\n    knn_fit = train( SurvivedFactor~.,data=df,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\n    knn_pred = predict( knn_fit, newdata=newTrain);\r\n    numRight = sum( knn_pred == newTrain$SurvivedFactor );\r\n    cv.error = ( numTrain - numRight )/numTrain;\r\n    accuracy = mean( knn_pred == newTrain$SurvivedFactor );\r\n    features[counter] = list( feature_set );\r\n    numFeatures[counter] = nFeatures;\r\n    accuracies[counter] = accuracy;\r\n    correct[counter] = numRight;\r\n    errors[counter] = cv.error;\r\n    k[counter] = knn_fit$bestTune$k;\r\n    counter = counter + 1;\r\n  }\r\n}\r\ncvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), K=unlist(k), Features=I(features) );\r\ncvResults = cvResults[order(cvResults$CVError),];\r\nknitr::kable( head(cvResults,n=20L), format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\n\r\nNow, take the top 10 results and repeat the analysis using LOOCV.  LOOCV provides a better estimate of the error but is computationally costly.\r\n\r\n```{r knn4, echo=TRUE, eval=FALSE, warning=FALSE}\r\ncorrect = list();\r\naccuracies = list();\r\nerrors = list();\r\nfeatures = list();\r\nnumFeatures = list();\r\nk = list();\r\ncounter = 1;\r\nnumTrain = nrow( newTrain );\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nfor ( rowNum in 1:10 ){\r\n  feature_set = cvResults$Features[[rowNum]];\r\n  nFeatures = cvResults$NumFeatures[rowNum];\r\n  df = newTrain[, c(\u0022SurvivedFactor\u0022, feature_set)]\r\n  knn_fit = train( SurvivedFactor~.,data=df,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\n  knn_pred = predict( knn_fit, newdata=newTrain);\r\n  numRight = sum( knn_pred == newTrain$SurvivedFactor );\r\n  cv.error = ( numTrain - numRight )/numTrain;\r\n  accuracy = mean( knn_pred == newTrain$SurvivedFactor );\r\n  features[counter] = list( feature_set );\r\n  numFeatures[counter] = nFeatures;\r\n  accuracies[counter] = accuracy;\r\n  correct[counter] = numRight;\r\n  errors[counter] = cv.error;\r\n  k[counter] = knn_fit$bestTune$k;\r\n  counter = counter + 1;\r\n  cat( counter, \u0022\\n\u0022)\r\n}\r\nloocvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), K=unlist(k), Features=I(features) );\r\nloocvResults = loocvResults[order(loocvResults$CVError),];\r\nknitr::kable( head(loocvResults,n=10L), format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\nNow, take the best 9, 8, 7, and 6 knn models.\r\n\r\n#### KNN with with best subset of 9 significant features\r\n\r\nWith 9 features, the model achieves a training set accuracy of 0.8754 using a cross-validated K of 5.  On the test set, the model scored 0.76076.\r\n\r\n```{r knn5, echo=TRUE, warning=FALSE}\r\n# best 9\r\nknnTrain = newTrain;\r\nknnTest = newTest;\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit )\r\n# training set performance\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nconfusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);\r\n# test set performance\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nbest9KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nbest9KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( best9KNNModelTrain, \u0022KNNBest9ModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best9KNNModel, \u0022KNNBest9ModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best9KNNModelTrain, \u0022KNNBest9ModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best9KNNModel, \u0022KNNBest9Model.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### KNN with with best subset of 8 significant features\r\n\r\nWith 8 features, the model achieves a training set accuracy of 0.881 using a cross-validated K of 5.  On the test set, the model scored 0.73684.\r\n\r\n```{r knn6, warning=FALSE}\r\n# best 8\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit )\r\n# training set performance\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nconfusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);\r\n# test set performance\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nbest8KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nbest8KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( best8KNNModelTrain, \u0022KNNBest8ModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best8KNNModel, \u0022KNNBest8ModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best8KNNModelTrain, \u0022KNNBest8ModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best8KNNModel, \u0022KNNBest8Model.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### KNN with with best subset of 7 significant features\r\n\r\nWith 7 features, the model achieves a training set accuracy of 0.8765 using a cross-validated K of 5.  On the test set, the model scored 0.73684.\r\n\r\n```{r knn7, echo=TRUE, warning=FALSE}\r\n# best 7\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit )\r\n# training set performance\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nconfusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);\r\n# test set performance\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nbest7KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nbest7KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( best7KNNModelTrain, \u0022KNNBest7ModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best7KNNModel, \u0022KNNBest7ModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best7KNNModelTrain, \u0022KNNBest7ModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best7KNNModel, \u0022KNNBest7Model.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### KNN with with best subset of 6 significant features\r\n\r\nWith 6 features, the model achieves a training set accuracy of 0.8765 using a cross-validated K of 5.  On the test set, the model scored 0.71770.\r\n\r\n```{r knn8, warning=FALSE}\r\n# best 6\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit )\r\n# training set performance\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nconfusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);\r\n# test set performance\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nbest6KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nbest6KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( best6KNNModelTrain, \u0022KNNBest6ModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best6KNNModel, \u0022KNNBest6ModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best6KNNModelTrain, \u0022KNNBest6ModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best6KNNModel, \u0022KNNBest6Model.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nThe test set score of 0.71770 is less than that of the simple Gender Submission Model (0.76555).\r\n\r\n### Decision Tree Models\r\n\r\n#### Classification Tree\r\n\r\nA classification tree is used to model the data.  Since the features contain nominal data (Embarked, Sex, for example), a decision tree is appropriate.  After the tree is fit, terminal nodes are pruned using cross validation.  \r\nThe tree model scored 0.8507 against the training data and 0.7727 against the test data.\r\n\r\n```{r treeModel1, echo=TRUE, warning=FALSE}\r\nlibrary(tree);\r\ntree_model = tree( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain );\r\nsummary( tree_model )\r\ncv_tree_model = cv.tree( tree_model, FUN=prune.misclass);\r\nbest_terminal_node_idx = which.min(cv_tree_model$dev);\r\nbest_terminal_nodes = cv_tree_model$size[best_terminal_node_idx];\r\npruned_tree = prune.misclass(tree_model,best=best_terminal_nodes);\r\nplot( pruned_tree );\r\ntext( pruned_tree, pretty=0);\r\n# training set performance against pruned tree\r\npruned_tree_train_pred = predict( pruned_tree, newTrain, type=\u0022class\u0022);\r\nconfusionMatrix(pruned_tree_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\ntree_pred = predict( pruned_tree, newTest, type=\u0022class\u0022);\r\ntreeModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(pruned_tree_train_pred)-1);\r\ntreeModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(tree_pred)-1);\r\nwrite.csv( treeModelTrain, \u0022TreeModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModel, \u0022TreeModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModelTrain, \u0022TreeModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModel, \u0022TreeModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### Tree model with significant features\r\n\r\nNext, a tree model using only significant features is considered.  Again, cross-validation is used to prune the tree.  The model achieved an accuracy of 0.8541 on the training data and scored 0.7608 against the test data.\r\n\r\n```{r treeModel2, warning=FALSE}\r\nlibrary(tree);\r\ntree_model = tree( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain );\r\nsummary( tree_model )\r\ncv_tree_model = cv.tree( tree_model, FUN=prune.misclass);\r\nbest_terminal_node_idx = which.min(cv_tree_model$dev);\r\nbest_terminal_nodes = cv_tree_model$size[best_terminal_node_idx];\r\npruned_tree = prune.misclass(tree_model,best=best_terminal_nodes);\r\nplot( pruned_tree );\r\ntext( pruned_tree, pretty=0);\r\n# training set performance against pruned tree\r\npruned_tree_train_pred = predict( pruned_tree, newTrain, type=\u0022class\u0022);\r\nconfusionMatrix(pruned_tree_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\ntree_pred = predict( pruned_tree, newTest, type=\u0022class\u0022);\r\n# scored 0.7608\r\ntreeModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(pruned_tree_train_pred)-1);\r\ntreeModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(tree_pred)-1);\r\nwrite.csv( treeModelTrain, \u0022TreeSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModel, \u0022TreeSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModelTrain, \u0022TreeSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModel, \u0022TreeSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n### Bootstrap Aggregation (Bagging) Tree Models\r\n\r\nBootstrap Aggregation is a method of resampling the data many times to generate a series of samples from the original training set.  This allows for the fitting of many trees.  In Bagging tree models, the results of these trees are accumulated and the class that a particular sample assumes most often is the winner (by majority vote).  The averaging of many trees reduces variance.\r\n\r\n#### Bagging model using all features\r\n\r\nThe Bagging model scored an impressive 0.9877 on the training set but only 0.7321 against the test set.  The high train set accuracy and relatively low test set accuracy is indicative of over-training.\r\n\r\n```{r baggingModel1, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nbag_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain, mtry=12, importance=TRUE );\r\nbag_model\r\nplot( bag_model )\r\n# Get training set performance\r\nbag_train_pred = predict( bag_model, newdata=newTrain);\r\nconfusionMatrix( bag_train_pred, newTrain$SurvivedFactor);\r\n# now test set\r\nbag_pred = predict( bag_model, newdata=newTest);\r\nbagModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(bag_train_pred)-1);\r\nbagModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(bag_pred)-1);\r\nwrite.csv( bagModelTrain, \u0022BagModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModel, \u0022BagModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModelTrain, \u0022BagModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModel, \u0022BagModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.73205\r\n```\r\n\r\nPlot shows the OOB error (black) as well as the class errors (0/Died = red, 1/Survived = green).\r\n\r\n#### Bagging model using only significant features\r\n\r\nLike the full featured model, the significant feature bagging model scored an impressive 0.9854 on the training set but only 0.7273 against the test set.  The high train set accuracy and relatively low test set accuracy is indicative of over-training.\r\n\r\n```{r baggingModel2, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nbag_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain, mtry=9, importance=TRUE );\r\nbag_model\r\nplot( bag_model )\r\n# Get training set performance\r\nbag_train_pred = predict( bag_model, newdata=newTrain);\r\nconfusionMatrix( bag_train_pred, newTrain$SurvivedFactor);\r\n# now test set\r\nbag_pred = predict( bag_model, newdata=newTest);\r\nbagModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(bag_train_pred)-1);\r\nbagModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(bag_pred)-1);\r\nwrite.csv( bagModelTrain, \u0022BagSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModel, \u0022BagSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModelTrain, \u0022BagSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModel, \u0022BagSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.7273\r\n```\r\n\r\nPlot shows the OOB error (black) as well as the class errors (0/Died = red, 1/Survived = green).\r\n\r\n### Random Forest Models\r\n\r\nA random forest is much like the bagging idea but has the constraint that each branch point is allowed a random selection of m features to use.  This random selection of predictors decorrelates the trees from one another and creates a much more diverse set of trees than does bagging.  As a rule of thumb, the number of predictors to choose from is set to the square root of the number of available predictors (for classification trees).\r\n\r\n#### Random Forest Model using all features\r\n\r\nUsing the standard sqrt(predictors) = 3 for mtry resulted in another overfit model.  But reducing the mtry to 1 (the number of predictors considered at each branch point), the random forest model scored 0.8575 against the training data and 0.7847 against the test data.\r\n\r\n```{r rfModel1, echo=TRUE, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nrf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain, mtry=1, importance=TRUE );\r\nrf_model\r\nimportance( rf_model );\r\nvarImpPlot( rf_model );\r\nplot( rf_model )\r\n# training set performance\r\nrf_train_pred = predict( rf_model, newdata=newTrain);\r\nconfusionMatrix( rf_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\nrf_pred = predict( rf_model, newdata=newTest);\r\nrfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);\r\nrfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);\r\nwrite.csv( rfModelTrain, \u0022RandomForestModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModelTrain, \u0022RandomForestModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.7847\r\n```\r\n\r\n#### Random Forest Model using only significant features\r\n\r\nUsing the standard sqrt(predictors) = 3 for mtry resulted in another overfit model.  But reducing the mtry to 1 (the number of predictors considered at each branch point), the random forest model scored 0.8608 against the training data and 0.7823 against the test data.\r\n\r\n```{r rfModel2, echo=TRUE, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nrf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain, importance=TRUE, mtry = 1 );\r\nrf_model\r\nimportance( rf_model );\r\nvarImpPlot( rf_model );\r\nplot( rf_model )\r\n# training set performance\r\nrf_train_pred = predict( rf_model, newdata=newTrain);\r\nconfusionMatrix( rf_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\nrf_pred = predict( rf_model, newdata=newTest);\r\nrfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);\r\nrfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);\r\nwrite.csv( rfModelTrain, \u0022RandomForestSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModelTrain, \u0022RandomForestSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.7823\r\n```\r\n\r\n#### Random Forest using only 9 selected features\r\n\r\nThe Random forest model using all features generated a variable importance plot.  The variable set that looked to be most informative is selected and used in this model.  This model scored 0.8485 on the training set and 0.7775 on the test set.\r\n\r\n```{r rfModel3, echo=TRUE, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nrf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+NumParents+Siblings+NumPassengersOnTicket,data=newTrain, importance=TRUE, mtry=1 );\r\nrf_model\r\nimportance( rf_model );\r\nvarImpPlot( rf_model );\r\nplot( rf_model )\r\n# training set performance\r\nrf_train_pred = predict( rf_model, newdata=newTrain);\r\nconfusionMatrix( rf_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\nrf_pred = predict( rf_model, newdata=newTest);\r\nrfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);\r\nrfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);\r\nwrite.csv( rfModelTrain, \u0022RandomForestTrimmedModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestTrimmedModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModelTrain, \u0022RandomForestTrimmedModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestTrimmedModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.7775\r\n```\r\n\r\n### Boosted Classification Tree Models\r\n\r\nGradient Boosting applied to classification trees involves fitting classification trees sequentially.  That is, instead of generating a bunch of trees and averaging them as with bagging and random forests, boosting involves adding trees sequentially into a single model.  Trees are successively fit to the residuals of the previous tree ensemble. \r\n\r\n#### Gradient Boosting using all features\r\n\r\nSince there are many parameters to consider for boosted classification trees, cross-validation is used to select the values which will generalize best.  This analysis is done using all predictors.\r\n\r\n```{r boostedModel0, echo=TRUE, warning=FALSE, eval=FALSE}\r\n# This is not run; ignore\r\nlibrary(gbm, quietly = TRUE);\r\ntree_lengths = 1:4;\r\nbest_error = Inf;\r\nbest_idx = 0;\r\nmin_error_idx = tree_lengths;\r\nmin_errors = tree_lengths;\r\nxTrain = newTrain[,c(\u0022Survived\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = newTest[,c(\u0022Survived\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nnumSamples = 10;\r\ntrainingProportion = 0.8;\r\nthreshold = 0.5;\r\ncvFolds = 8;  # n-fold validation\r\nnTrees = 15000; \r\ntreeDiv = 1000;\r\nnumIntTrees = nTrees/treeDiv - 1;\r\nnCores = 7; # adjust according to computer\r\n\r\ntrainMat = matrix( nrow=numSamples, ncol=numIntTrees);\r\ntestMat = matrix( nrow=numSamples, ncol=numIntTrees);\r\nfor ( depth in tree_lengths ){\r\n  set.seed(depth);\r\n  sampleSet = createDataPartition(1:nrow(newTrain),numSamples,p=trainingProportion);\r\n  for ( samp in 1:numSamples){\r\n    trainSample = sampleSet[[samp]];\r\n    testSample = setdiff(1:nrow(newTrain),trainSample);\r\n    boosted_model = gbm( formula=Survived~.,data=xTrain[trainSample,], distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, n.cores = nCores, verbose=FALSE );\r\n    best_tree_num = gbm.perf( boosted_model );\r\n    train_scores = 0;\r\n    test_scores = 0;\r\n    for ( ntree in 1:numIntTrees){\r\n      treelen = (ntree-1)*treeDiv;\r\n      # now that the model is trained, see how it does on training set for n trees.\r\n      train_pred = predict( boosted_model, newdata=xTrain[trainSample,], n.trees=treelen, type=\u0022response\u0022);\r\n      train_predictions = as.numeric( train_pred \u003e= threshold );\r\n      train_score = sum( train_predictions == newTrain$Survived[trainSample] )/length(trainSample);\r\n      train_scores[ ntree ] = train_score;\r\n      trainMat[samp,ntree] = train_score;\r\n      cat( \u0022trainMat[\u0022, samp, \u0022,\u0022, ntree, \u0022] = \u0022, train_score, \u0022\\n\u0022);\r\n      # now that the model is trained, see how it does on test set for n trees.\r\n      test_pred = predict( boosted_model, newdata=xTrain[testSample,], n.trees=treelen, type=\u0022response\u0022);\r\n      test_predictions = as.numeric( test_pred \u003e= threshold );\r\n      test_score = sum( test_predictions == newTrain$Survived[testSample] )/length(testSample);\r\n      test_scores[ ntree ] = test_score;\r\n      testMat[samp,ntree] = test_score;\r\n    }\r\n  }\r\n  boosted_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, n.cores = nCores, verbose=FALSE );\r\n  min_error_idx[depth] = which.min( boosted_model$cv.error );\r\n  min_errors[depth] = min( boosted_model$cv.error );\r\n  if ( min_errors[depth] \u003c best_error ){\r\n    best_error = min_errors[depth];\r\n    best_idx = depth;\r\n  }\r\n}\r\ngbtrees = data.frame( TreeDepth = tree_lengths, BernoulliDeviance = min_errors, NumTrees = min_error_idx );\r\nknitr::kable( gbtrees, format=\u0022markdown\u0022, longtable=TRUE);\r\n#summary( boosted_model );\r\n#plot( boosted_model );\r\nset.seed(best_idx);\r\nbest_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, n.cores = nCores, verbose=FALSE );\r\ngbm.perf( best_model );\r\nboosted_train_pred = predict( best_model, newdata=xTrain, n.trees=min_error_idx[best_idx], type=\u0022response\u0022);\r\nboosted_pred = predict( best_model, newdata=xTest, n.trees=min_error_idx[best_idx], type=\u0022response\u0022);\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nboosted_train_predictions = as.numeric( boosted_train_pred \u003e= threshold );\r\nboosted_predictions = as.numeric( boosted_pred \u003e= threshold );\r\nboostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_predictions);\r\nboostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);\r\nboostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));\r\nboostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));\r\nwrite.csv( boostedModelTrainProbs, \u0022BoostedTreeModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelProbs, \u0022BoostedTreeModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelTrain, \u0022BoostedTreeModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModel, \u0022BoostedTreeModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# originally scored 0.76076 with interaction depth =4, 0.77990 with interaction depth = 1\r\n# after 10-fold x-validation, considering interaction depths of 1 to 4, scored a 0.76555\r\n# same model , stopping at 4800 trees yields 0.77511\r\n```\r\n\r\n```{r boostedModel1, echo=TRUE, warning=FALSE}\r\nlibrary(gbm, quietly = TRUE);\r\ntree_lengths = 1:10;\r\nnumLengths = length( tree_lengths );\r\nbest_error = Inf;\r\nbest_idx = 0;\r\nbest_shrinkage = 0;\r\nbest_steps = 0;\r\nxTrain = newTrain[,c(\u0022Survived\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = newTest[,c(\u0022Survived\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nlearningRates = c(0.1,0.07,0.05,0.03,0.02,0.01);\r\nnumLR = length( learningRates );\r\nthreshold = 0.5;\r\ncvFolds = 10;  # n-fold validation\r\nnTrees = 3000; \r\nnCores = 7; # adjust according to computer\r\ntree_depths = rep(0,numLengths*numLR);\r\nsteps = rep(0,numLengths*numLR);\r\ndeviances = rep(0,numLengths*numLR);\r\nshrinkages = rep(0,numLengths*numLR);\r\nidx = 1;\r\n\r\nfor ( depth in tree_lengths ){\r\n  set.seed(depth);\r\n  for ( lrIdx in 1:numLR){\r\n      learningRate = learningRates[lrIdx];\r\n      boosted_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, shrinkage = learningRate, n.cores = nCores, verbose=FALSE );\r\n      idxCVErr = gbm.perf( boosted_model, plot.it=FALSE );\r\n      cvError = boosted_model$cv.error[idxCVErr];\r\n      if ( cvError \u003c best_error ){\r\n        best_error = cvError;\r\n        best_idx = depth;\r\n        best_shrinkage = learningRate;\r\n        best_steps = idxCVErr;\r\n      }\r\n      tree_depths[idx] = depth;\r\n      shrinkages[idx] = learningRate;\r\n      steps[idx] = idxCVErr;\r\n      deviances[idx] = cvError;\r\n      idx = idx + 1;\r\n  }\r\n}\r\ngbtrees = data.frame( TreeDepth = tree_depths, Shrinkage = shrinkages, NumTrees = steps, BernoulliDeviance = deviances );\r\nknitr::kable( gbtrees, format=\u0022markdown\u0022, longtable=TRUE);\r\n#summary( boosted_model );\r\n#plot( boosted_model );\r\n# create a line chart\r\nxrange = range( shrinkages );\r\nyrange = range( deviances );\r\nplot( xrange, yrange, type=\u0022n\u0022, xlab=\u0022Shrinkage\u0022, ylab=\u0022CV Accuracy (Bernoulli Deviance)\u0022);\r\ncolors = rainbow( numLengths );\r\nlinetype = tree_lengths;\r\nplotchar = seq(0,numLengths,1);\r\n# add lines\r\nfor ( i in tree_lengths){\r\n  thisDepthTrees = subset( gbtrees, gbtrees$TreeDepth == i );\r\n  lines(thisDepthTrees$Shrinkage, thisDepthTrees$BernoulliDeviance, type=\u0022b\u0022, lwd=1.5, lty=linetype[i], col=colors[i], pch=plotchar[i]);\r\n}\r\ntitle( \u0022Bernoulli Deviance by Shrinkage and Tree Depth\u0022);\r\nlegend( 0.08, yrange[2], 1:numLengths, cex=0.8, col=colors, pch=plotchar, lty=linetype, title=\u0022Tree Depth\u0022);\r\nset.seed(best_idx);\r\nbest_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, shrinkage = best_shrinkage, n.cores = nCores, verbose=FALSE );\r\ngbm.perf( best_model );\r\nboosted_train_pred = predict( best_model, newdata=xTrain, n.trees=best_steps, type=\u0022response\u0022);\r\nboosted_train_calls = as.numeric( boosted_train_pred \u003e= threshold );\r\nconfusionMatrix(boosted_train_calls,newTrain$Survived);\r\nboosted_pred = predict( best_model, newdata=xTest, n.trees=best_steps, type=\u0022response\u0022);\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nboosted_predictions = as.numeric( boosted_pred \u003e= threshold );\r\nboostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_calls);\r\nboostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);\r\nboostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));\r\nboostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));\r\nwrite.csv( boostedModelTrainProbs, \u0022BoostedTreeModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelProbs, \u0022BoostedTreeModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelTrain, \u0022BoostedTreeModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModel, \u0022BoostedTreeModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# interaction.depth = 8, nTrees = 118, learningRate = 0.05\r\n# scored 0.756\r\n```\r\n\r\nThe Cross-validated parameters (interaction depth = 8, number of trees = 118, learning rate = 0.05) resulted in a training set accuracy of 0.8943 on the training set and 0.756 on the test set.\r\n\r\n#### Gradient Boosting using only significant features\r\n\r\nThe same technique is used as above to find the best set of parameters for gradient boosting model for the significant features.\r\n\r\n```{r boostedModel2, echo=TRUE, warning=FALSE}\r\nlibrary(gbm);\r\ntree_lengths = 1:10;\r\nnumLengths = length( tree_lengths );\r\nbest_error = Inf;\r\nbest_idx = 0;\r\nbest_shrinkage = 0;\r\nbest_steps = 0;\r\nsignificant_features = c(\u0022FixedTitle\u0022, \u0022Sex\u0022, \u0022Siblings\u0022, \u0022FixedAge\u0022, \u0022CabinAssignment\u0022, \u0022pclass\u0022, \u0022NumChildren\u0022, \u0022FarePerPassenger\u0022, \u0022Embarked\u0022);\r\nxTrain = newTrain[,c(\u0022Survived\u0022, significant_features) ];\r\nxTest = newTest[,c(\u0022Survived\u0022, significant_features) ];\r\nlearningRates = c(0.1,0.07,0.05,0.03,0.02,0.01);\r\nnumLR = length( learningRates );\r\nthreshold = 0.5;\r\ncvFolds = 10;  # n-fold validation\r\nnTrees = 3000; \r\nnCores = 7; # adjust according to computer\r\ntree_depths = rep(0,numLengths*numLR);\r\nsteps = rep(0,numLengths*numLR);\r\ndeviances = rep(0,numLengths*numLR);\r\nshrinkages = rep(0,numLengths*numLR);\r\nidx = 1;\r\n\r\nfor ( depth in tree_lengths ){\r\n  set.seed(depth);\r\n  for ( lrIdx in 1:numLR){\r\n      learningRate = learningRates[lrIdx];\r\n      boosted_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, shrinkage = learningRate, n.cores = nCores, verbose=FALSE );\r\n      idxCVErr = gbm.perf( boosted_model, plot.it=FALSE );\r\n      cvError = boosted_model$cv.error[idxCVErr];\r\n      if ( cvError \u003c best_error ){\r\n        best_error = cvError;\r\n        best_idx = depth;\r\n        best_shrinkage = learningRate;\r\n        best_steps = idxCVErr;\r\n      }\r\n      tree_depths[idx] = depth;\r\n      shrinkages[idx] = learningRate;\r\n      steps[idx] = idxCVErr;\r\n      deviances[idx] = cvError;\r\n      idx = idx + 1;\r\n  }\r\n}\r\ngbtrees = data.frame( TreeDepth = tree_depths, Shrinkage = shrinkages, NumTrees = steps, BernoulliDeviance = deviances );\r\nknitr::kable( gbtrees, format=\u0022markdown\u0022, longtable=TRUE);\r\n#summary( boosted_model );\r\n#plot( boosted_model );\r\n# create a line chart\r\nxrange = range( shrinkages );\r\nyrange = range( deviances );\r\nplot( xrange, yrange, type=\u0022n\u0022, xlab=\u0022Shrinkage\u0022, ylab=\u0022Cross Validation Error (Bernoulli Deviance)\u0022);\r\ncolors = rainbow( numLengths );\r\nlinetype = tree_lengths;\r\nplotchar = seq(0,numLengths,1);\r\n# add lines\r\nfor ( i in tree_lengths){\r\n  thisDepthTrees = subset( gbtrees, gbtrees$TreeDepth == i );\r\n  lines(thisDepthTrees$Shrinkage, thisDepthTrees$BernoulliDeviance, type=\u0022b\u0022, lwd=1.5, lty=linetype[i], col=colors[i], pch=plotchar[i]);\r\n}\r\ntitle( \u0022Bernoulli Deviance by Shrinkage and Tree Depth\u0022);\r\nlegend( 0.08, yrange[2], 1:numLengths, cex=0.8, col=colors, pch=plotchar, lty=linetype, title=\u0022Tree Depth\u0022);\r\nset.seed(best_idx);\r\nbest_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, shrinkage = best_shrinkage, n.cores = nCores, verbose=FALSE );\r\ngbm.perf( best_model );\r\nboosted_train_pred = predict( best_model, newdata=xTrain, n.trees=best_steps, type=\u0022response\u0022);\r\nboosted_train_calls = as.numeric( boosted_train_pred \u003e= threshold );\r\nconfusionMatrix(boosted_train_calls,newTrain$Survived);\r\nboosted_pred = predict( best_model, newdata=xTest, n.trees=best_steps, type=\u0022response\u0022);\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nboosted_predictions = as.numeric( boosted_pred \u003e= threshold );\r\nboostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_calls);\r\nboostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);\r\nboostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));\r\nboostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));\r\nwrite.csv( boostedModelTrainProbs, \u0022BoostedTreeSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelProbs, \u0022BoostedTreeSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelTrain, \u0022BoostedTreeSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModel, \u0022BoostedTreeSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# interaction.depth = 8, nTrees = 328, learningRate = 0.02\r\n# scored 0.75119\r\n```\r\n\r\nThe cross validated parameters were found to be an interaction depth of 8, 328 trees, and a learning rate of 0.02.  The model achieved a score of 0.8923 on the training set and a sad 0.7368 on the test set.\r\n\r\n### Support Vector Machine (SVM) Model\r\n\r\nSupport Vector Machines seek to find the optimal separating hyperplane between two classes.  There are several parameters to consider: cost (of a constraint violation), choice of kernel (and corresponding gamma).  Cross-validation is used to estimate these parameters.\r\n\r\n```{r svm1, echo=TRUE, warning=FALSE}\r\nlibrary(e1071);\r\nlibrary(caret);\r\n#xTrain = newTrain[,c(\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\n#yTrain = newTrain[,\u0022SurvivedFactor\u0022];\r\nxTrain = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=whiteTrain);\r\nyTrain = newTrain[,\u0022SurvivedFactor\u0022];\r\n#xTest = newTest[,c(\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=whiteTest);\r\n#xTrainSVM = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain);\r\n#xTestSVM = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest);\r\ncosts = c(0.01, 0.1, 0.5, 0.9, 1, 1.1);\r\ngammas = c(0.0001, 0.01, 0.05, 0.1, 0.5);\r\nkernels = c(\u0022linear\u0022, \u0022polynomial\u0022, \u0022radial\u0022, \u0022sigmoid\u0022);\r\nbestCost = costs[1];\r\nbestGamma = gammas[1];\r\nbestKernel = kernels[1];\r\nbestError = Inf;\r\nerrors = rep(Inf,1,length(kernels));\r\nctr = 1;\r\nfor ( kernel in kernels ){\r\n  tune.out = tune( \u0022svm\u0022, xTrain, train.y=yTrain, kernel=kernel, ranges=list( cost=costs,gamma=gammas) );\r\n  tune.out$best.performance\r\n  tune.out$best.model\r\n  thisError = tune.out$best.performance;\r\n  errors[ctr] = thisError;\r\n  if ( thisError \u003c  bestError ){\r\n    bestError = thisError;\r\n    bestKernel = kernel;\r\n    bestGamma = tune.out$best.model$gamma;\r\n    bestCost = tune.out$best.model$cost;\r\n  }\r\n  ctr = ctr + 1;\r\n}\r\nsvm.fit = svm( x=xTrain, y=yTrain, kernel=bestKernel, cost=bestCost, gamma=bestGamma);\r\n#plot( svm.fit, xTrain );\r\nsvm.train.pred = predict( svm.fit, xTrain );\r\nsvmTrainingPreds = as.numeric( svm.train.pred ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( svmTrainingPreds, truth );\r\n# scores a 0.8294 on the training set\r\nsvm.test.pred = predict( svm.fit, xTest );  \r\nsvmPredictions = as.numeric( svm.test.pred ) - 1;\r\nsvmModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=svmTrainingPreds);\r\nsvmModel = data.frame(PassengerId=newTest$PassengerId, Survived=svmPredictions);\r\n\r\nwrite.csv( svmModelTrain, \u0022SVMModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( svmModel, \u0022SVMModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( svmModelTrain, \u0022SVMModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( svmModel, \u0022SVMModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.7727\r\n```\r\n\r\nThe SVM model scored a 0.8328 against the training set and a 0.7727 against the test set.\r\n\r\n### Neural Network Models\r\n\r\nThis neural network implementation uses a single layer.  The number of hidden nodes and learning rate is selected through 10-fold cross validation.\r\n\r\n#### Neural Network with one hidden layer utilizing all features\r\n```{r nnModel1, echo=TRUE, eval=FALSE, warning=FALSE}\r\n### NN Model\r\nlibrary(nnet);\r\n#xTrain = newTrain[,c(\u0022SurvivedFactor\u0022, significant_features) ];\r\n#xTest = newTest[,c(\u0022SurvivedFactor\u0022, significant_features) ];\r\n\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\n\r\nnn = nnet( SurvivedFactor ~ ., data = xTrain, size=12, maxit=500, trace=FALSE);\r\n# How do we do on the training data?\r\n\r\nnn_train_pred_class = predict( nn, xTrain, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nnn_train_pred = as.numeric( nn_train_pred_class );   # transform to 0, 1\r\nconfusionMatrix(nn_train_pred,xTrain$Survived);\r\n\r\n# try on test data\r\nnn_test_pred_class = predict( nn, xTest, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nnn_test_pred = as.numeric( nn_test_pred_class );   # transform to 0, 1\r\n# write to  file\r\nnnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=nn_train_pred);\r\nnnModel = data.frame(PassengerId=newTest$PassengerId, Survived=nn_test_pred);\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# best \u0022over\u0022trained model scores 0.73205 (306/418)\r\n\r\n# Now, we wish to select parameters for neural network by using 10-fold cross-validation with caret\r\ntuningGrid = expand.grid(size=1:12,decay=c(0,0.0001,0.05,0.1));\r\nset.seed( 1 );\r\ntrControl = trainControl( method=\u0022repeatedcv\u0022, number=10, repeats=10 );\r\nnn_cv = train( SurvivedFactor ~ ., data=xTrain, method=\u0022nnet\u0022, trControl = trControl, tuneGrid=tuningGrid, verbose=FALSE, trace=FALSE);\r\n\r\nbest_size = nn_cv$bestTune[1,\u0022size\u0022];\r\nbest_decay = nn_cv$bestTune[1,\u0022decay\u0022];\r\nbest_nn = nnet( SurvivedFactor ~ ., data = xTrain, size=best_size, decay=best_decay, maxit=500, trace=FALSE);\r\nbest_nn_train_pred_class = predict( best_nn, newdata=xTrain, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nbest_nn_train_pred = as.numeric( best_nn_train_pred_class );   # transform to 0, 1\r\nconfusionMatrix( best_nn_train_pred, xTrain$Survived);\r\n# now do on test data\r\nbest_nn_test_pred_class = predict( best_nn, newdata=xTest, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nbest_nn_test_pred = as.numeric( best_nn_test_pred_class );   # transform to 0, 1\r\n# write to  file\r\nnnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=best_nn_train_pred);\r\nnnModel = data.frame(PassengerId=newTest$PassengerId, Survived=best_nn_test_pred);\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkFullModelCVTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkFullModelCVProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkFullModelCVTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkFullModelCV.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# Neural network model scores 0.7847\r\n```\r\n\r\nThe cross validated Neural Network model scored 0.8418 against the training set and 0.7847 against the test set.\r\n\r\n#### Neural Network with one hidden layer utilizing significant features\r\n```{r nnModel2, echo=TRUE, eval=TRUE, warning=FALSE}\r\n### NN Model\r\nlibrary(nnet);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022, significant_features) ];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022, significant_features) ];\r\n\r\nnn = nnet( SurvivedFactor ~ ., data = xTrain, size=length(significant_features), maxit=500, trace=FALSE);\r\n# How do we do on the training data?\r\n\r\nnn_train_pred_class = predict( nn, xTrain, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nnn_train_pred = as.numeric( nn_train_pred_class );   # transform to 0, 1\r\nconfusionMatrix(nn_train_pred,xTrain$Survived);\r\n# 0.9068 accuracy on the training set\r\n\r\n# try on test data\r\nnn_test_pred_class = predict( nn, xTest, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nnn_test_pred = as.numeric( nn_test_pred_class );   # transform to 0, 1\r\n# write to  file\r\nnnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=nn_train_pred);\r\nnnModel = data.frame(PassengerId=newTest$PassengerId, Survived=nn_test_pred);\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# best \u0022over\u0022trained model scores 0.72727 (304/418)\r\n\r\n# Now, we wish to select parameters for neural network by using 10-fold cross-validation with caret\r\ntuningGrid = expand.grid(size=1:length(significant_features),decay=c(0,0.0001,0.05,0.1));\r\nset.seed( 1 );\r\ntrControl = trainControl( method=\u0022repeatedcv\u0022, number=10, repeats=10 );\r\nnn_cv = train( SurvivedFactor ~ ., data=xTrain, method=\u0022nnet\u0022, trControl = trControl, tuneGrid=tuningGrid, verbose=FALSE, trace=FALSE);\r\n\r\nbest_size = nn_cv$bestTune[1,\u0022size\u0022];\r\nbest_decay = nn_cv$bestTune[1,\u0022decay\u0022];\r\nbest_nn = nnet( SurvivedFactor ~ ., data = xTrain, size=best_size, decay=best_decay, maxit=500, trace=FALSE);\r\nbest_nn_train_pred_class = predict( best_nn, newdata=xTrain, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nbest_nn_train_pred = as.numeric( best_nn_train_pred_class );   # transform to 0, 1\r\nconfusionMatrix( best_nn_train_pred, xTrain$Survived);\r\n# now do on test data\r\nbest_nn_test_pred_class = predict( best_nn, newdata=xTest, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nbest_nn_test_pred = as.numeric( best_nn_test_pred_class );   # transform to 0, 1\r\n# write to  file\r\nnnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=best_nn_train_pred);\r\nnnModel = data.frame(PassengerId=newTest$PassengerId, Survived=best_nn_test_pred);\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkSigModelCVTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkSigModelCVProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkSigModelCVTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkSigModelCV.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# Neural network model scores 0.7847 (334/418) - same as full featured model\r\n```\r\n\r\nThe cross validated Neural Network model with only significant features scored 0.853 against the training set and 0.7751 against the test set.\r\n\r\n### Ensemble Model\r\n\r\nEnsemble models use multiple models to make predictions.  Ensemble models usually fall into one of three different types: bagging, boosting, and stacking.  In a bagging model paradigm, a single model (usually) is run multiple times over different subsamples of the data.  These models are usually not complex to avoid overfitting and hopefully learn one part of the data and so are uncorrelated with one another.  The output of the models are then combined to make a prediction.  The Gradient Boosting model that we used with classification trees is an example of an ensemble model where successive models are employed to reduce the residuals of the previous model.  Finally, a stacking ensemble model is much like a traditional machine learning model only the features or inputs to the learning model are themselves outputs of different types of machine learning models.  The caveat, however, is that in order for ensembling to work, the models must be uncorrelated.\r\n\r\nThe idea in this section is to gather the results from all the trained models.  Where possible, I used the probabilties of individual predictions (i.e., 0.983 chance of survival) over class calls (binary 0=dead, 1=survived) as that data is a little more fine-tuned.  For this dataset, many of the models that I have trained are drastically overfit.  The gradient boosted tree models, with training accuracies of 98% are extremely overfit.  Other models are just not well-suited for this type of problem (KNN, LDA, QDA).  Once I pared the list of models to just include the models that I thought were well-suited for this problem, I computed all of the cross-correlations between models and then enumerated all of the sets of models that were below a specified cross-correlation (I picked 0.75 to signify a high correlation).  Once I did this, I found that *all* of the remaining models were highly correlated to one another, so I coudn\u0027t make an ensemble!  But since I\u0027m determined to make an ensemble, I pressed on.\r\n\r\nI added back the LDA, QDA, and neural network models and reran the correlation analysis.  I purposely excluded the KNN and Bagging models since I believe KNN to not be well-suited for this problem and the bagging models are massively overfit.  The correlation analysis showed which models could be combined into a single stacked model (see below).  I manually selected the set of 3 models: RandomForest, Gender model, and QDA, since they represent three models of different complexity (simple: gender model, balanced: Random Forest, complex: QDA), hoping the strengths of each would complement each other.\r\n\r\nIn addition, I decided to use all the models in a different ensemble model, just taking row means.  This set included all of the models, including the overfit bagging models, KNN, boosting, etc.  Many of these models are highly correlated so the ensemble method is just a simple mean across all of the models in each row.\r\n\r\n```{r ensemble, echo=TRUE, eval=TRUE, warning=FALSE}\r\nlibrary(matlab,quietly=TRUE);\r\nlibrary(ROCR,quietly=TRUE);\r\nlibrary(tidyr,quietly=TRUE);\r\nlibrary(dplyr,quietly=TRUE);\r\nlibrary(tibble,quietly=TRUE);\r\n\r\nall_train_models = c( \u0022BagModelTrainProbs.csv\u0022, \u0022BagSigModelTrainProbs.csv\u0022, \u0022BoostedTreeModelTrainProbs.csv\u0022, \u0022BoostedTreeSigModelTrainProbs.csv\u0022, \u0022GenderModelTrainProbs.csv\u0022, \u0022KNNBest6ModelTrainProbs.csv\u0022, \u0022KNNBest7ModelTrainProbs.csv\u0022, \u0022KNNBest8ModelTrainProbs.csv\u0022, \u0022KNNBest9ModelTrainProbs.csv\u0022, \u0022KNNFullModelTrainProbs.csv\u0022, \u0022LassoRegressionFullModelTrainProbs.csv\u0022, \u0022LassoRegressionSigModelTrainProbs.csv\u0022, \u0022LDAModelTrainProbs.csv\u0022, \u0022LDAReducedModelTrainProbs.csv\u0022, \u0022LogisticRegressionFullModelTrainProbs.csv\u0022, \u0022LogisticRegressionSigModelTrainProbs.csv\u0022, \u0022NeuralNetworkSigModelCVTrainProbs.csv\u0022, \u0022NeuralNetworkSigModelTrainProbs.csv\u0022, \u0022NullModelTrainProbs.csv\u0022, \u0022QDAModelTrainProbs.csv\u0022, \u0022QDASigModelTrainProbs.csv\u0022, \u0022RandomForestModelTrainProbs.csv\u0022, \u0022RandomForestSigModelTrainProbs.csv\u0022, \u0022RandomForestTrimmedModelTrainProbs.csv\u0022, \u0022RidgeRegressionFullModelTrainProbs.csv\u0022, \u0022RidgeRegressionSigModelTrainProbs.csv\u0022, \u0022SVMModelTrainProbs.csv\u0022, \u0022TreeModelTrainProbs.csv\u0022, \u0022TreeSigModelTrainProbs.csv\u0022, \u0022WCModelTrainProbs.csv\u0022 );\r\n\r\ntrain_models = c( \u0022BoostedTreeModelTrainProbs.csv\u0022, \u0022BoostedTreeSigModelTrainProbs.csv\u0022, \u0022GenderModelTrainProbs.csv\u0022, \u0022LassoRegressionFullModelTrainProbs.csv\u0022, \u0022LassoRegressionSigModelTrainProbs.csv\u0022, \u0022LDAModelTrainProbs.csv\u0022, \u0022LDAReducedModelTrainProbs.csv\u0022, \u0022LogisticRegressionFullModelTrainProbs.csv\u0022, \u0022LogisticRegressionSigModelTrainProbs.csv\u0022, \u0022NeuralNetworkSigModelCVTrainProbs.csv\u0022, \u0022NeuralNetworkSigModelTrainProbs.csv\u0022, \u0022QDAModelTrainProbs.csv\u0022, \u0022QDASigModelTrainProbs.csv\u0022, \u0022RandomForestModelTrainProbs.csv\u0022, \u0022RandomForestSigModelTrainProbs.csv\u0022, \u0022RandomForestTrimmedModelTrainProbs.csv\u0022, \u0022RidgeRegressionFullModelTrainProbs.csv\u0022, \u0022RidgeRegressionSigModelTrainProbs.csv\u0022, \u0022SVMModelTrainProbs.csv\u0022, \u0022TreeModelTrainProbs.csv\u0022, \u0022TreeSigModelTrainProbs.csv\u0022, \u0022WCModelTrainProbs.csv\u0022 );\r\n\r\nall_test_models = c( \u0022BagModelProbs.csv\u0022, \u0022BagSigModelProbs.csv\u0022, \u0022BoostedTreeModelProbs.csv\u0022, \u0022BoostedTreeSigModelProbs.csv\u0022, \u0022GenderModelProbs.csv\u0022, \u0022KNNBest6ModelProbs.csv\u0022, \u0022KNNBest7ModelProbs.csv\u0022, \u0022KNNBest8ModelProbs.csv\u0022, \u0022KNNBest9ModelProbs.csv\u0022, \u0022KNNFullModelProbs.csv\u0022, \u0022LassoRegressionFullModelProbs.csv\u0022, \u0022LassoRegressionSigModelProbs.csv\u0022, \u0022LDAModelProbs.csv\u0022, \u0022LDAReducedModelProbs.csv\u0022, \u0022LogisticRegressionFullModelProbs.csv\u0022, \u0022LogisticRegressionSigModelProbs.csv\u0022, \u0022NeuralNetworkSigModelCVProbs.csv\u0022, \u0022NeuralNetworkSigModelProbs.csv\u0022, \u0022NullModelProbs.csv\u0022, \u0022QDAModelProbs.csv\u0022, \u0022QDASigModelProbs.csv\u0022, \u0022RandomForestModelProbs.csv\u0022, \u0022RandomForestSigModelProbs.csv\u0022, \u0022RandomForestTrimmedModelProbs.csv\u0022, \u0022RidgeRegressionFullModelProbs.csv\u0022, \u0022RidgeRegressionSigModelProbs.csv\u0022, \u0022SVMModelProbs.csv\u0022, \u0022TreeModelProbs.csv\u0022, \u0022TreeSigModelProbs.csv\u0022, \u0022WCModelProbs.csv\u0022 );\r\n\r\ntest_models = c( \u0022BoostedTreeModelProbs.csv\u0022, \u0022BoostedTreeSigModelProbs.csv\u0022, \u0022GenderModelProbs.csv\u0022, \u0022LassoRegressionFullModelProbs.csv\u0022, \u0022LassoRegressionSigModelProbs.csv\u0022, \u0022LDAModelProbs.csv\u0022, \u0022LDAReducedModelProbs.csv\u0022, \u0022LogisticRegressionFullModelProbs.csv\u0022, \u0022LogisticRegressionSigModelProbs.csv\u0022, \u0022NeuralNetworkSigModelCVProbs.csv\u0022, \u0022NeuralNetworkSigModelProbs.csv\u0022, \u0022QDAModelProbs.csv\u0022, \u0022QDASigModelProbs.csv\u0022, \u0022RandomForestModelProbs.csv\u0022, \u0022RandomForestSigModelProbs.csv\u0022, \u0022RandomForestTrimmedModelProbs.csv\u0022, \u0022RidgeRegressionFullModelProbs.csv\u0022, \u0022RidgeRegressionSigModelProbs.csv\u0022, \u0022SVMModelProbs.csv\u0022, \u0022TreeModelProbs.csv\u0022, \u0022TreeSigModelProbs.csv\u0022, \u0022WCModelProbs.csv\u0022 );\r\n\r\ntrainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );\r\ntestModels = data.frame(PassengerId=1:nrow(newTest));\r\n\r\n# read all the train model predictions and store in dataframe\r\nnumModels = length( train_models );\r\nfor ( i in 1:numModels ){\r\n  trainFileName = train_models[i];\r\n  fp = fileparts( trainFileName );\r\n  modelName = gsub( \u0022TrainProbs\u0022, \u0022\u0022, fp$name );\r\n  trainModelFrame = read.csv( trainFileName );\r\n  trainModels[ modelName ] = trainModelFrame$Survived;\r\n  # now for test\r\n  testFileName = test_models[i];\r\n  fp = fileparts( testFileName );\r\n  modelName = gsub( \u0022Probs\u0022, \u0022\u0022, fp$name );\r\n  testModelFrame = read.csv( testFileName );\r\n  testModels[ modelName ] = testModelFrame$Survived;\r\n}\r\n\r\nallTrainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );\r\nallTestModels = data.frame(PassengerId=1:nrow(newTest));\r\n\r\nnumAllModels = length( all_train_models );\r\nfor ( i in 1:numAllModels ){\r\n  trainFileName = all_train_models[i];\r\n  fp = fileparts( trainFileName );\r\n  modelName = gsub( \u0022TrainProbs\u0022, \u0022\u0022, fp$name );\r\n  trainModelFrame = read.csv( trainFileName );\r\n  allTrainModels[ modelName ] = trainModelFrame$Survived;\r\n  # now for test\r\n  testFileName = all_test_models[i];\r\n  fp = fileparts( testFileName );\r\n  modelName = gsub( \u0022Probs\u0022, \u0022\u0022, fp$name );\r\n  testModelFrame = read.csv( testFileName );\r\n  allTestModels[ modelName ] = testModelFrame$Survived;\r\n}\r\n\r\nall_train_model_names = setdiff( names(allTrainModels), c(\u0022PassengerId\u0022, \u0022Survived\u0022) );\r\n\r\n# this calculates all correlations between models.  Some corr calcs break because\r\n# they\u0027re all one value so standard deviation calc blows up.\r\nmodelCors \u003c- trainModels[,!names(trainModels) %in% c(\u0022PassengerId\u0022,\u0022Survived\u0022)] %\u003e% \r\n  as.matrix %\u003e%\r\n  cor %\u003e%\r\n  as.data.frame %\u003e%\r\n  rownames_to_column(var = \u0027Model_A\u0027) %\u003e%\r\n  gather(Model_B, correlation, -Model_A)\r\n\r\ncorrThresh = 0.75;\r\nmodelNames = unique(modelCors$Model_A);\r\n\r\n# this recursive function is intended to output groups of models that \r\n# have inter correlations below threshold\r\naddToModelSet = function( modelSet, corrs, model, corrThresh ){\r\n    #modelSet[ length(modelSet)+1 ] = model;\r\n    if ( ! model %in% modelSet ){\r\n        modelSet = union( modelSet, model );\r\n    }\r\n    idxUsed = c();\r\n    idxUnused = 1:nrow(corrs);\r\n   \r\n    toUse = c();\r\n    for ( idx in idxUnused ){\r\n        thisModel = corrs$Model_A[idx];\r\n        if ( thisModel %in% modelSet ){\r\n            toUse[ length(toUse) + 1] = idx;\r\n        }\r\n    }\r\n   \r\n    for ( idx in toUse ){\r\n        addModel = TRUE;\r\n        modelA = corrs$Model_A[idx];\r\n        modelB = corrs$Model_B[idx];\r\n        correlation = corrs$correlation[idx];\r\n        if ( correlation \u003c corrThresh ){\r\n            # now, check corr with every other member of modelSet\r\n            for ( model in modelSet ){\r\n                # want to check modelB corr with all models in modelSet\r\n              idxModelA = which( corrs$Model_A == model );\r\n              idxModelB = which( corrs$Model_B == modelB );\r\n                if ( model == modelB ){\r\n                    idxUsed[ length(idxUsed) + 1 ] = idx;\r\n                    # correlations of the same model\r\n                    next;\r\n                }\r\n              rightRowIdx = intersect( idxModelA, idxModelB );\r\n              if( length( rightRowIdx ) == 1 \u0026\u0026 rightRowIdx \u003c nrow(corrs) ){\r\n                rightRow = corrs[ rightRowIdx, ];\r\n              }\r\n              else{\r\n                next;\r\n              }\r\n                rightCorr = rightRow$correlation;\r\n                if ( rightCorr \u003e corrThresh ){\r\n                    idxUsed[ length(idxUsed) + 1 ] = idx;\r\n                    addModel = FALSE;\r\n                    break;\r\n                }\r\n            }\r\n            idxUsed[ length(idxUsed) + 1 ] = idx;\r\n        }\r\n        else{\r\n          addModel = FALSE;\r\n        }\r\n        if ( addModel ){\r\n          idxToUse = setdiff( idxUnused, idxUsed );\r\n          newCorrs = corrs[idxToUse,]\r\n          #newCorrs = corrs;\r\n          modelSet = addToModelSet( modelSet, newCorrs, modelB, corrThresh);\r\n        }\r\n    }\r\n    \r\n    return( modelSet );\r\n}\r\n\r\nmodelSets = list();\r\nprint( \u0022Model sets that have inter-correlations less than threshold:\u0022)\r\nfor ( model in modelNames ){\r\n  modelSet = addToModelSet( c(), modelCors, model, corrThresh );\r\n  cat( paste(paste(modelSet, collapse=\u0022, \u0022),\u0022\\n\u0022));\r\n  modelSets[ length(modelSets) + 1 ] = list( modelSet );\r\n}\r\n\r\n# now go through each model and find the other models which are correlated below\r\n# threshold (0.75)\r\n# manually choose the set #14\r\nuncorrelatedModelSet = modelSets[[14]];\r\n\r\n# now for all models that have passed criteria, create a model\r\nselectedTrainModels = trainModels[,c(\u0022Survived\u0022, uncorrelatedModelSet)];\r\nensemble_lr = glm( Survived~., data=selectedTrainModels, family=binomial);\r\nsummary( ensemble_lr );\r\ntrainSetProbs = predict( ensemble_lr, trainModels, type=\u0022response\u0022);  # now a vector of probabilities\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nthreshold = 0.5;\r\nselectedTrainPredictions = as.numeric( trainSetProbs \u003e= threshold );\r\nconfusionMatrix( selectedTrainPredictions, trainModels$Survived );\r\n\r\n# check to see if threshold 0.5 is appropriate\r\nrocr.pred = prediction( trainSetProbs, trainModels$Survived);\r\nacc.perf = performance( rocr.pred, measure=\u0022acc\u0022);\r\nplot( acc.perf, main=\u0022Manual Ensemble Logistic Regression Model Training Set Accuracy by Threshold\u0022 );\r\n# threshold 0.5 is appropriate\r\n\r\n# now, how\u0027d we do against the test set?\r\ntestSetProbs = predict( ensemble_lr, testModels, type=\u0022response\u0022);\r\ntestPredictions = as.numeric( testSetProbs \u003e= threshold );\r\nensembleModelManual = data.frame(PassengerId=newTest$PassengerId, Survived=testPredictions);\r\nwrite.csv( ensembleModelManual, \u0022EnsembleModelManual.csv\u0022, row.names=FALSE, quote=FALSE );\r\nensembleModelTrainManual = data.frame(PassengerId=newTrain$PassengerId, Survived=selectedTrainPredictions);\r\nwrite.csv( ensembleModelTrainManual, \u0022EnsembleModelManualTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\n# now try just an ensemble model, taking means\r\nensembleTrainModels = allTrainModels[,all_train_model_names];\r\n# simply take the means across the rows to get \u0022probabilities\u0022 of survival for each passenger\r\nensembleTrainPredictions = rowMeans( ensembleTrainModels );\r\nensembleTrainCalls = round( ensembleTrainPredictions );\r\nconfusionMatrix( ensembleTrainCalls, allTrainModels$Survived );\r\n\r\n# check to see if threshold 0.5 is appropriate\r\nrocr.pred = prediction( ensembleTrainPredictions, allTrainModels$Survived);\r\nacc.perf = performance( rocr.pred, measure=\u0022acc\u0022);\r\nplot( acc.perf, main=\u0022All Means Ensemble Logistic Regression Model Training Set Accuracy by Threshold\u0022 );\r\n# threshold 0.5 is appropriate\r\n\r\n# How\u0027d the ensemble model do against the test set?\r\nensembleTestModels = testModels[,modelNames];\r\n# simply take the means across the rows to get \u0022probabilities\u0022 of survival for each passenger\r\nensembleTestPredictions = rowMeans( ensembleTestModels );\r\nensembleTestCalls = round( ensembleTestPredictions );\r\n\r\n#fullLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);\r\n#fullLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullLogisticPredictions);\r\n# now, create a new test and train data frame with these models as predictors\r\n\r\n# write to  file\r\nensembleModelMeans = data.frame(PassengerId=newTest$PassengerId, Survived=ensembleTestCalls);\r\nwrite.csv( ensembleModelMeans, \u0022EnsembleModelMeans.csv\u0022, row.names=FALSE, quote=FALSE );\r\nensembleModelTrainMeans = data.frame(PassengerId=newTrain$PassengerId, Survived=ensembleTrainCalls);\r\nwrite.csv( ensembleModelTrainMeans, \u0022EnsembleModelMeansTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nThe manually selected linear regression ensemble model was constructed using three predictors: Random Forest, Gender model, and QDA.  The training set score was 0.8575 and test set score was 0.7847.  The simple row means model scored a 0.8664 against the training set and a 0.7703 against the test set.  As would be expected, the row means model, which used some very overfit models, was itself overfit - scoring high against the training set and low against the test set - lower than the manually selected ensemble model.\r\n\r\n### Cheat Models\r\n\r\nCheaters never prosper.  Nevertheless.\r\n\r\n#### Google Data-Mining\r\n\r\nAll of the Titanic data is available online.  It is possible to find resource pages listing who died and who survived.  Of course, much of it is not in a standardized format and has to be screenscraped.  In addition, there are errors and discrepancies both in the Kaggle data set and in the data online.  Word for word searches fail in many cases.  \r\n\r\n```{r cheatModel1, echo=TRUE, eval=FALSE, warning=FALSE}\r\nlibrary(RCurl);\r\nlibrary(XML);\r\nlibrary(readr);\r\n#testIDs = 892:nrow(data_combined);\r\n#userAgents = c( \u0022Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0\u0022, \u0022Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0\u0022, \u0022Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\u0022, \u0022Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_1 like Mac OS X) AppleWebKit/603.1.30 (KHTML, like Gecko) Version/10.0 Mobile/14E304 Safari/602.1\u0022, \u0022Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0)\u0022);\r\nuserAgents = c( \u0022Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36\u0022, \u0022Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36 Edge/15.15063\u0022 );\r\nnumUserAgents = length( userAgents );\r\ntrainIDs = 1:891;\r\ntestIDs = 892:nrow(data_combined);\r\nsetIDs = 1:nrow(data_combined);\r\ntrainURLS = c();\r\nsetHits = data_combined[,c(\u0022PassengerId\u0022,\u0022Survived\u0022)];\r\nsetHits$SurvivorHits = 0;\r\nsetHits$VictimHits = 0;\r\nsetHits$SurvivorFirst = 0;\r\nsetHits$SurvivedPrediction = 0;\r\nfor ( pass_idx in 1:length(setIDs) ){\r\n  i = setIDs[pass_idx];\r\n  name = gsub( \u0022 \u0022, \u0022+\u0022, as.character( data_combined[i,\u0022NameString\u0022] ) ); # remove whitespace\r\n  sex = as.character( data_combined[i,\u0022Sex\u0022] );\r\n  searchURL = paste0( \u0022https://www.google.com/search?q=\u0022, name, \u0022+\u0022, sex, \u0022+titanic\u0022 );\r\n  trainURLS[i] = searchURL;\r\n  randomUserAgent = userAgents[ round( runif(1, 1, numUserAgents) ) ];\r\n  wd = getwd();\r\n  localFile = paste0( wd, \u0022/html/\u0022, i, \u0022.html\u0022 );\r\n  if ( file.exists( localFile )){\r\n    resultsHTML = read_file( localFile );\r\n  }else{\r\n    resultsHTML = getURL( searchURL, httpheader=c(\u0027User-Agent\u0027=randomUserAgent) );\r\n    write( resultsHTML, localFile );\r\n    Sys.sleep( runif( 1, 27, 67)); # randomly sleep between 27 and 67 seconds\r\n    # this is necessary because if Google thinks you\u0027re mining their data with a bot\r\n    # you will be grounded and sent to your room for a timeout.\r\n  }\r\n  # now search for \u0022victim\u0022 and \u0022survivor\u0022\r\n  victimHits = gregexpr(\u0022victim\u0022, resultsHTML, ignore.case=TRUE);\r\n  victimHitIndices = which( victimHits[[1]] \u003e -1 );\r\n  numVictimHits = length( victimHitIndices );\r\n  setHits[pass_idx,\u0022VictimHits\u0022] = numVictimHits;\r\n  firstVictimHit = victimHits[[1]][1];\r\n  survivorHits = gregexpr(\u0022survivor\u0022, resultsHTML, ignore.case=TRUE);\r\n  survivorHitIndices = which( survivorHits[[1]] \u003e -1 );\r\n  numSurvivorHits = length( survivorHitIndices );\r\n  setHits[pass_idx,\u0022SurvivorHits\u0022] = numSurvivorHits;\r\n  firstSurvivorHit = survivorHits[[1]][1];\r\n  #cat( \u0022row \u0022, i, \u0022\\n\u0022)\r\n  #assert_that( ( numSurvivorHits \u003e 0 ) || (numVictimHits \u003e 0) );\r\n  if (!( ( numSurvivorHits \u003e 0 ) || (numVictimHits \u003e 0) ) ){\r\n    cat( \u0022Could not find \u0027victim\u0027 or \u0027survivor\u0027 keyword for PassengerID \u0022, i, \u0022.  Manually investigate and update .html file with \u0027survived\u0027 or \u0027victim\u0027.\\n\u0022)\r\n  }\r\n  survived = FALSE;\r\n  if ( numSurvivorHits \u003e numVictimHits ){\r\n      survived = TRUE;\r\n  }\r\n  else if ( ( numSurvivorHits \u003e 0 ) \u0026\u0026 ( numVictimHits \u003e 0 ) ){\r\n    # both are positive, take the min\r\n    if ( firstSurvivorHit \u003c firstVictimHit ){\r\n      survived = TRUE;\r\n    }\r\n    else{\r\n      survived = FALSE;\r\n    }\r\n    setHits[pass_idx,\u0022SurvivorFirst\u0022] = as.numeric( survived );\r\n  }\r\n  survived_flag = 0;\r\n  if( survived ){\r\n    survived_flag = 1;\r\n  }\r\n  setHits[pass_idx,\u0022SurvivedPrediction\u0022] = survived_flag;\r\n  if ( !is.na( data_combined[i,\u0022Survived\u0022] ) \u0026\u0026 survived_flag != data_combined[i,\u0022Survived\u0022] ){\r\n    #cat( \u0022Prediction does not match truth at \u0022, i, as.character( data_combined[i,\u0022Name\u0022] ), \u0022\\n\u0022);\r\n  }\r\n}\r\n# evaluate training set performance \r\ntrainingPredictions = setHits$SurvivedPrediction[1:891];\r\nconfusionMatrix( trainingPredictions, data_combined$Survived[1:891])\r\n\r\ncheatTestPredictions = setHits$SurvivedPrediction[892:nrow(data_combined)];\r\n\r\nwrite.csv( setHits, file=\u0022CheatModel1.csv\u0022, quote=FALSE);\r\n```\r\n\r\n#### Screen Scraping Titanic Resource Webpage\r\n\r\nA webpage exists that lists all the survivors in bold and all the passengers who died in normal font.  It is possible to build a scraper to parse the passengers into two lists and then try to match the names in the Kaggle test set with the names in the Survived and Died sets derived from the webpage.  The idea is to use both a bag-of-words model and a distance model to capture both number of token hits and handle any differences in spelling.\r\n\r\n```{r cheatModel2, echo=TRUE, eval=FALSE, warning=FALSE}\r\n# read in CheatModel1\r\ncheat \u003c- read.csv(\u0022CheatModel1.csv\u0022, header = TRUE);\r\ncheat$SurvivedFactor = as.factor( cheat$Survived );\r\ncheat$SurvivedTokenMatches = 0;\r\ncheat$DiedTokenMatches = 0;\r\ncheat$SurvivedTokenDistance = 0;\r\ncheat$DiedTokenDistance = 0;\r\nresourceURL = \u0022http://www.titanic-whitestarships.com/1st_Class_Pass.htm\u0022;\r\n# Example HTML:\r\n#\u003cstrong\u003eGordon, Sir Cosmo Duff\u003cbr\u003e\r\n#Gordon, Lady Lucile Duff\u003cbr\u003e\r\n#and Maid (Miss Laura Mabel Francatelli)\u003cbr\u003e\r\n#Gracie, Colonel Archibald IV\u003c/strong\u003e\u003cbr\u003e\r\n#Graham, Mr. George Edward\u003cbr\u003e\r\n#Graham1, Mr. George Edward\u003cbr\u003e\r\n#Graham2, Mr. George Edward\u003cbr\u003e\r\n#Graham3, Mr. George Edward\u003cbr\u003e\r\n#\u003cstrong\u003eGraham, Mrs. William Thompson\u003cbr\u003e\r\n#(nee Edith Junkins)\u003cbr\u003e\r\n#Graham, Miss Margaret\u003cbr\u003e\r\n#Greenfield, Mrs. Leo David\u003cbr\u003e\r\n#(nee Blanche Strouse)\u003cbr\u003e\r\n#Greenfield, Mr. William Bertram\u003c/strong\u003e\u003cbr\u003e\r\n#Guggenheim, Mr. Benjamin\u003cbr\u003e\r\n#and Manservant (Victor Giglio)\u003c/font\u003e\u003c/p\u003e\r\ncheatSurvived = numeric(nrow(newTest));\r\nwd = getwd();\r\nlocalFile = file.path( wd, \u0022TitanicResource.htm\u0022 );  \r\n# know the file is approx 2100 lines.  Initialize each died/survived list to 3000\r\ndied = character(3000);\r\nsurvived = character(3000);\r\nif ( !file.exists( localFile )){  # Do this so we can load the URL once and not spam it everytime we run this script\r\n  resultsHTML = getURL( resourceURL );\r\n  write( resultsHTML, localFile );\r\n}\r\n# now, parse the file, line-by-line.  Survivors are in bold, denoted by the \u003cstrong\u003e tag:\r\nfid = file( localFile, \u0022r\u0022);\r\nisStrong = FALSE;\r\ndiedCtr = 1;\r\nsurvivedCtr = 1;\r\nwhile( TRUE ){\r\n  line = readLines(fid,n=1);\r\n  if( length(line) == 0 ){\r\n    break;\r\n  }\r\n  # look for \u003c/strong\u003e\r\n  thisLineStartStrong = grepl( \u0022^\u003cstrong\u003e\u0022, line );\r\n  thisLineEndStrong = grepl( \u0022\u003c/strong\u003e\u0022, line );\r\n  # if line contains \u003cstrong\r\n  # strip html tags\r\n  rawText = gsub( \u0022\u003c.*?\u003e\u0022, \u0022\u0022, line );\r\n  # skip if no comma\r\n  thisLineHasComma = grepl( \u0022,\u0022, rawText );\r\n  if ( !thisLineHasComma ){\r\n    next;\r\n  }\r\n  # remove punctuation\r\n  rawText = gsub( \u0022[[:punct:]]\u0022, \u0022\u0022, rawText );\r\n  # skip if line (name) is too long\r\n  if ( nchar( rawText) \u003e 100 ){\r\n    next;\r\n  }\r\n  if( isStrong || thisLineStartStrong ){\r\n    survived[survivedCtr] = rawText;\r\n    survivedCtr = survivedCtr + 1;\r\n  }\r\n  else{\r\n    died[diedCtr] = rawText;\r\n    diedCtr = diedCtr + 1;\r\n  }\r\n  isStrong = thisLineStartStrong \u0026\u0026 !thisLineEndStrong;\r\n}\r\nclose(fid);\r\n# truncate the died/survived lists\r\nsurvived = sort( survived[1:survivedCtr-1] );\r\ndied = sort( died[1:diedCtr-1] );  # yes, this includes crap lines at the top of list\r\n#\r\n\r\n# now have two lists and need to do bag of words searching vs. each name\r\n# for each name, tokenize, then query the died/survived lists\r\nfor( i in 1:nrow( newTest ) ){\r\n  thisName = newTest[i,\u0022Name\u0022];\r\n  # remove non-word characters \r\n  thisName = gsub( \u0022[[:punct:]]\u0022, \u0022\u0022, thisName );\r\n  # now tokenize each name and search each died/survived for distance from each token\r\n  nameTokens = strsplit( thisName, \u0022\\\\s+\u0022)[[1]];\r\n  numTokens = length( nameTokens );\r\n  # search through survived\r\n  survivedTokenMatches = numeric(length(survived));\r\n  survivedTokenDistance = numeric(length(survived));\r\n  for ( j in 1:length( survived ) ){\r\n    thisScore = 0;\r\n    thisSurvived = survived[j];\r\n    survivedTokens = strsplit( thisSurvived, \u0022\\\\s+\u0022)[[1]];\r\n    numSurvivedTokens = length( survivedTokens );\r\n    # now, take each name token and search for match in survived tokens\r\n    for ( k in 1:numTokens){\r\n      thisNameToken = nameTokens[k];\r\n      if ( thisNameToken %in% survivedTokens ){\r\n        thisScore = thisScore + 1;\r\n      }\r\n    }\r\n    survivedTokenMatches[j] = thisScore;\r\n    # go through each name token and find the min distance to survived tokens\r\n    totalDist = 0;\r\n    for ( k in 1:numTokens){\r\n      thisNameToken = nameTokens[k];\r\n      wordDist = 10;\r\n      distances = numeric(numSurvivedTokens);\r\n      for ( m in 1:numSurvivedTokens){\r\n        thisSurvivedToken = survivedTokens[m];\r\n        distances[m] = adist( thisNameToken, thisSurvivedToken );\r\n      }\r\n      totalDist = totalDist + min( distances );\r\n    }\r\n    survivedTokenDistance[j] = totalDist;\r\n  }\r\n  bestSurvivedScore = max( survivedTokenMatches );\r\n  bestSurvivedDistance = min( survivedTokenDistance );\r\n  cheat[i,\u0022SurvivedTokenHits\u0022] = bestSurvivedScore;\r\n  cheat[i,\u0022SurvivedTokenDistance\u0022] = bestSurvivedDistance;\r\n  \r\n  # search through died\r\n  diedTokenMatches = numeric(length(died));\r\n  diedTokenDistance = numeric(length(died));\r\n  for ( j in 1:length( died ) ){\r\n    thisScore = 0;\r\n    thisDied = died[j];\r\n    diedTokens = strsplit( thisDied, \u0022\\\\s+\u0022)[[1]];\r\n    numDiedTokens = length( diedTokens );\r\n    # now, take each name token and search for match in survived tokens\r\n    for ( k in 1:numTokens){\r\n      thisNameToken = nameTokens[k];\r\n      if ( thisNameToken %in% diedTokens ){\r\n        thisScore = thisScore + 1;\r\n      }\r\n    }\r\n    diedTokenMatches[j] = thisScore;\r\n    # go through each name token and find the min distance to survived tokens\r\n    totalDist = 0;\r\n    for ( k in 1:numTokens){\r\n      thisNameToken = nameTokens[k];\r\n      wordDist = 10;\r\n      distances = numeric(numDiedTokens);\r\n      for ( m in 1:numDiedTokens){\r\n        thisDiedToken = diedTokens[m];\r\n        distances[m] = adist( thisNameToken, thisDiedToken );\r\n      }\r\n      totalDist = totalDist + min( distances );\r\n    }\r\n    diedTokenDistance[j] = totalDist;\r\n  }\r\n  \r\n  bestDiedScore = max( diedTokenMatches );\r\n  bestDiedDistance = min( diedTokenDistance );\r\n  cheat[i,\u0022DiedTokenHits\u0022] = bestSurvivedScore;\r\n  cheat[i,\u0022DiedTokenDistance\u0022] = bestSurvivedDistance;\r\n  #cat( thisName, \u0022 bestScore = \u0022, bestScore, \u0022, bestMatch = \u0022, survived[bestIdx], \u0022\\n\u0022);\r\n}\r\n\r\n# now that we have some cheating scores, build a simple tree model\r\nlibrary(tree);\r\ncheat_train = cheat[1:891,];  # use the training data\r\ncheat_test= cheat[892:nrow(cheat),];  # use the test data\r\ntree_cheat_model = tree( SurvivedFactor~SurvivorHits+VictimHits+SurvivorFirst+SurvivedTokenHits+SurvivedTokenDistance+DiedTokenHits+DiedTokenDistance,data=cheat_train );\r\nsummary( tree_cheat_model )\r\nplot( tree_cheat_model )\r\ntext( tree_cheat_model, pretty=0);\r\ncheat_train_pred = predict( tree_cheat_model, cheat_train, type=\u0022class\u0022);\r\ncheat_test_pred = predict( tree_cheat_model, cheat_test, type=\u0022class\u0022);\r\n# performance\r\nconfusionMatrix(cheat_train_pred, cheat_train$Survived);\r\n\r\ncheatModelTest = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(cheat_test_pred)-1);\r\nwrite.csv( cheatModelTest, \u0022CheatModelFull.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\n```\r\n\r\n### Summary\r\n\r\nHere is the summary of training and test results for the different models:\r\n```{r summary, eval=FALSE, echo=FALSE}\r\nlibrary(matlab,quietly=TRUE);\r\nlibrary(ROCR,quietly=TRUE);\r\n\r\n# these csv models represent the binary 0, 1 predictions of Survival for the different models\r\nmodels = c( \u0022BagModel.csv\u0022, \u0022BagSigModel.csv\u0022, \u0022BoostedTreeModel.csv\u0022, \u0022BoostedTreeSigModel.csv\u0022, \u0022gender_submission.csv\u0022, \u0022KNNBest6Model.csv\u0022, \u0022KNNBest7Model.csv\u0022, \u0022KNNBest8Model.csv\u0022, \u0022KNNBest9Model.csv\u0022, \u0022KNNFullModel.csv\u0022, \u0022LassoRegressionFullModel.csv\u0022, \u0022LassoRegressionSigModel.csv\u0022, \u0022LDAModel.csv\u0022, \u0022LDAReducedModel.csv\u0022, \u0022LogisticRegressionFullModel.csv\u0022, \u0022LogisticRegressionSigModel.csv\u0022, \u0022NeuralNetworkFullModel.csv\u0022, \u0022NeuralNetworkFullModelCV.csv\u0022, \u0022NeuralNetworkSigModel.csv\u0022, \u0022NeuralNetworkSigModelCV.csv\u0022, \u0022QDAModel.csv\u0022, \u0022QDASigModel.csv\u0022, \u0022RandomForestModel.csv\u0022, \u0022RandomForestSigModel.csv\u0022, \u0022RandomForestTrimmedModel.csv\u0022, \u0022RidgeRegressionFullModel.csv\u0022, \u0022RidgeRegressionSigModel.csv\u0022, \u0022SVMModel.csv\u0022, \u0022NullModel.csv\u0022, \u0022TreeModel.csv\u0022, \u0022TreeSigModel.csv\u0022, \u0022WomenAndChildrenFirstModel.csv\u0022, \u0022EnsembleModelManual.csv\u0022, \u0022EnsembleModelMeans.csv\u0022 );\r\n\r\ntrainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );\r\ntestModels = data.frame(PassengerId=1:nrow(newTest));\r\nmodelNames = c();\r\n\r\n# testAnswers is a .csv file representing all the correct predictions for the test set.\r\n# I created this through trial and error and some cheat models and use it here to compare\r\n# predictions with the answer key to show in a table (easier than hitting Kaggle every\r\n# time for a score calculation).  If you don\u0027t have this file, just make a default of\r\n# everyone dying\r\ntestAnswers = data.frame(PassengerId=1:nrow(newTest), Survived=0);\r\nif( file.exists(\u0022TestAnswers.csv\u0022)){\r\n    testAnswers = read.csv( \u0022TestAnswers.csv\u0022, header=TRUE );\r\n}\r\n\r\ntrainScores = c();\r\ntestScores = c();\r\n\r\nnumModels = length( models );\r\nfor ( i in 1:numModels ){\r\n  testFileName = models[i];\r\n  fp = fileparts( testFileName );\r\n  testName = fp$name;\r\n  modelNames[i] = testName;\r\n  trainName = paste0( testName, \u0022Train\u0022 );\r\n  trainFileName = paste0( trainName, fp$ext );\r\n  trainModelFrame = read.csv( trainFileName );\r\n  testModelFrame = read.csv( testFileName );\r\n  trainScores[i] = mean( trainModelFrame$Survived == trainModels$Survived );\r\n  testScores[i] = mean( testModelFrame$Survived == testAnswers$Survived );\r\n  trainModelFrame$Score = trainScores[i];\r\n  testModelFrame$Score = testScores[i];\r\n}\r\ntypes = c( rep(\u0022Training Score\u0022, length(modelNames)), rep(\u0022Test Score\u0022, length(modelNames)));\r\ntrainOrder = sort( trainScores, decreasing = TRUE, index.return = TRUE );\r\n# now reorder according to training score\r\nmodelNames = modelNames[trainOrder$ix];\r\ntrainScores = trainScores[trainOrder$ix];\r\ntestScores = testScores[trainOrder$ix];\r\nscores = data.frame( Model = c(modelNames,modelNames), Accuracy = c(trainScores,testScores), Type = types );\r\nscores$Model = factor(scores$Model, levels=modelNames);\r\nscores$Type = factor(scores$Type, levels=c(\u0022Training Score\u0022, \u0022Test Score\u0022));\r\nscores$CVMethod = \u0022-\u0022\r\nif( !file.exists(\u0022ModelResults.csv\u0022)){\r\n    write.csv( scores, \u0022ModelResults.csv\u0022, row.names=FALSE, quote=FALSE );\r\n} else{\r\n  # why read this in?  Because it might be annotated.  And if you\u0027re missing the TestAnswers.csv\r\n  # file, all the test scores will be wrong\r\n  scores = read.csv(\u0022ModelResults.csv\u0022, header=TRUE);\r\n}\r\n\r\nggplot( data=scores, aes(x=Model, y=Accuracy, group=Type, colour=Type)) + \r\n  geom_line() + \r\n  geom_point() +\r\n  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))\r\n\r\nmodel_results = scores;\r\nmodels = as.character( model_results$Model );\r\nuniqueModels= unique( as.character( model_results$Model) );\r\ntrainingScores = sort(model_results[model_results$Type==\u0022Training Score\u0022,\u0022Accuracy\u0022], decreasing=TRUE, index.return=TRUE);\r\ntrainingModels = models[trainingScores$ix];\r\ncvMethods = model_results[model_results$Type==\u0022Training Score\u0022,\u0022CVMethod\u0022];\r\ntestScores = c();\r\nfor ( z in 1:length(trainingModels)){\r\n  thisModel = trainingModels[z];\r\n  idxTestScore = which(( model_results$Model == thisModel ) \u0026 (model_results$Type == \u0022Test Score\u0022) );\r\n  testScores[z] = model_results$Accuracy[idxTestScore];\r\n}\r\nannotatedCVMethods = cvMethods[trainingScores$ix];\r\nif( ! file.exists(\u0022TestAnswers.csv\u0022)){\r\n  testScores = c();\r\n  annotatedCVMethods = c();\r\n}\r\nsummaryTable = data.frame( Model = trainingModels, CVMethod=annotatedCVMethods, TrainingScore = trainingScores$x, TestScore = testScores);\r\nknitr::kable( summaryTable, format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\n![Model Summary](TitanicModelSummary.png)\r\n\r\nModel|CVMethod|Training Score|Test Score\r\n-------------------------------- | ---------- | ---------- | -----------\r\nBag Model|-|0.9876543|0.7320574\r\nBag Sig Model|-|0.9854097|0.7272727\r\nNeural Network Full Model|-|0.9203143|0.6746411\r\nNeural Network Sig Model|-|0.9124579|0.7105263\r\nBoosted Tree Model|8-fold CV|0.8933782|0.7559809\r\nBoosted Tree Sig Model|8-fold CV|0.8922559|0.7368421\r\nKNN Best 8 Model|LOOCV|0.8787879|0.7559809\r\nKNN Best 6 Model|LOOCV|0.8754209|0.7200957\r\nKNN Best 7 Model|LOOCV|0.8754209|0.7344498\r\nKNN Best 9 Model|LOOCV|0.8742985|0.7631579\r\nEnsemble Means Model|-|0.8664000|0.7703000\r\nRandomForest Sig Model|-|0.8608305|0.7822967\r\nEnsemble Manual Model|-|0.8575000|0.7847000\r\nRandomForest Model|-|0.8574635|0.7846890\r\nKNN Full Model|LOOCV|0.8563412|0.7607656\r\nTree Sig Model|Pruning|0.8540965|0.7607656\r\nNeural Network Sig Model|10-fold CV|0.8529742|0.7751196\r\nTree Model|Pruning|0.8507295|0.7727273\r\nRandomForest Trimmed Model|-|0.8484848|0.7775120\r\nNeural Network Full Model|10-fold CV|0.8417508|0.7846890\r\nLogistic Regression Sig Model|-|0.8395062|0.7607656\r\nSVM Model|10-fold CV|0.8395062|0.7679426\r\nLasso Regression Sig Model|10-fold CV|0.8372615|0.7775120\r\nLDA Model|-|0.8372615|0.7727273\r\nLasso Regression Full Model|10-fold CV|0.8361392|0.7703349\r\nLDA Reduced Model|-|0.8338945|0.7727273\r\nLogistic Regression Full Model|-|0.8338945|0.7607656\r\nRidge Regression Sig Model|10-fold CV|0.8249158|0.7870813\r\nRidge Regression Full Model|10-fold CV|0.8226712|0.7846890\r\nQDAModel|-|0.8159371|0.7296651\r\nWomenAndChildrenFirstModel|-|0.7901235|0.7511962\r\ngender_submission|-|0.7867565|0.7631579\r\nQDASigModel|-|0.7845118|0.7272727\r\nNullModel|-|0.6161616|0.6244019\r\n\r\nNow then, how does one go about picking a model out of all of these?  Clearly, the training scores of some of the more complex models cannot be trusted as they\u0027re obviously overfit.  The bagged tree model has training accuracies of about 0.98!  But how would we know that they\u0027re overfit without peeking at the test scores?  The boosted tree cross-validation accuracies were about 0.89 - but these also proved to be vastly overfit.\r\n\r\nBut suppose all of these training results were more or less equal - some at 0.84 cross-validated training set accuracy, some at 0.83 cross-validated training set accuracy, and one at 0.85 cross-validated training set accuracy.  What then would be the criterion for choosing a model?  At first blush, it might be obvious to choose the one that that has the highest cross-validated training set accuracy of 0.85.  But what if that model is very complex with a lot of parameterization and there is a much simpler model that scored at 0.84.  Then would it be better to choose the simpler, but lower-scoring model?  Is the difference between 0.84 and 0.85 statistically significant or is the difference just due to random chance?  Is there a way to quantify the statistical significance?  I\u0027ve seen McNemar\u0027s test but it looks like it operates on the correct counts between only two different models, not when num_models \u003e 2.  And if we run McNemar\u0027s test over all pairs of models, then we\u0027re likely to see something \u0022significant\u0022 just through chance and need to somehow correct for it.\r\n\r\nIf I had to pick the \u0022best\u0022 model out of these, it would probably be the ridge regression model.  The squared regularization penalty helps to drive down the coefficients of some of the predictors better than the similar lasso regression model.  \r\n\r\n## Lessons Learned\r\n\r\nThis report is a little stream-of-consciousness as it is/was my first Kaggle competition and kernel.  But what a great introduction to overfitting with R.  The EDA portion was really enjoyable as was the feature engineering section.  I thought that time put in to the early part would pay off later on but in retrospect, I probably should have tried to regularize the data a little more with outlier detection and deletion.  But I was reluctant to delete any data that might help me eek out a few more percentage points!\r\n\r\nAlso, in retrospect, it is *much* easier to run all of the models through the caret framework.  In this model, a single feature set can be run through different models fairly seemlessly - and then even run as ensembles at a later step.\r\n\r\nThanks for reading along!","dateCreated":"2018-09-01T00:28:50.8416023Z"},"kernelRun":{"id":5439677,"kernelId":1549984,"status":"complete","type":"batch","sourceType":"script","language":"rmarkdown","title":"Titanic Survival Prediction with R","dateCreated":"2018-09-01T00:28:52.653Z","dateEvaluated":"2018-09-01T00:28:53.433Z","workerContainerPort":null,"workerUptimeSeconds":10866,"workerIPAddress":"172.16.21.149  ","scriptLanguageId":5,"scriptLanguageName":"RMarkdown","renderedOutputUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..WakQ2QiDJa-dHi8dBXMQxg.mQqR5kTLjSFPvDflZHvca_m4omtcUSTmkTE0kMvuJ8yTvi3hdjB8ZvA_oox5cM9ng79rOIU1DGfAic7eDwjD8w6a0EzKZTLX57F-8lfEcHUQrRJIJoW0o333JHS2oXAOfRsDT81Sf4xAHJ9dGo8-R3p6y23mc0lOLNAh92Qsn9w.MbBG-c1dyt8bsfEYMY_6dA/__results__.html","commit":{"id":51511299,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"Competition","sourceId":3136,"databundleVersionId":null}],"sourceType":"script","language":"rmarkdown","isGpuEnabled":false,"isInternetEnabled":true},"source":"---\r\ntitle: \u0022Kaggle\u0027s Titanic Survival Prediction Competition\u0022\r\nauthor: \u0022Neill White\u0022\r\ndate: \u0022January 18, 2017\u0022\r\noutput:\r\n  html_document:\r\n    toc: TRUE\r\n    toc_depth: 4\r\n    toc_float: FALSE\r\n---\r\n\r\n```{r getpics, echo=FALSE}\r\n# Load images and store locally\r\nlibrary(\u0022curl\u0022);\r\nimageFile = \u0022titanic.jpg\u0022;\r\nif ( ! file.exists( imageFile )){\r\n    curl_download(url = \u0022http://cbsnews1.cbsistatic.com/hub/i/r/2017/01/04/b1b74071-3301-49ee-93bd-82e47c67d3a8/thumbnail/1200x630/0f08f16522eb0723e8d147cc809bc3d1/0103-eve-titanicfire-phillips-1223384-640x360.jpg\u0022, destfile = imageFile);\r\n}\r\nimageFile = \u0022TitanicModelSummary.png\u0022;\r\nif ( ! file.exists( imageFile )){\r\n    curl_download(url = \u0022http://neillwhite.dynu.net/DataScience/TitanicModelSummary.png\u0022, destfile = imageFile);\r\n}\r\n```\r\n\r\n## The Problem Statement\r\nOn April 14, 1912, the RMS Titanic struck an iceberg in the North Atlantic Ocean and sank.  Of the 2,224 people on board, only 706 survived.\r\n\r\nThe goal of this exercise is to predict survivors on the Titanic based on nine input variables, described below.  We are provided two datasets: (1) train.csv, containing 891 records and (2) test.csv, containing 418 records.  The two datasets are provided with the intent that models are formulated using the train dataset and model performance is evaluated on the test dataset.\r\n\r\n\u003c!--![](http://cbsnews1.cbsistatic.com/hub/i/r/2017/01/04/b1b74071-3301-49ee-93bd-82e47c67d3a8/thumbnail/1200x630/0f08f16522eb0723e8d147cc809bc3d1/0103-eve-titanicfire-phillips-1223384-640x360.jpg)--\u003e\r\n![](titanic.jpg)\r\n\r\n***\r\n## About the Data\r\n\r\nVariable|Definition|Key\r\n------- | ---------- | ---------\r\nsurvival|Survival|0 = No, 1 = Yes\r\npclass|Ticket|class\t1 = 1st, 2 = 2nd, 3 = 3rd\r\nsex|Sex|\r\nAge|Age in year|\r\nsibsp|\t# of siblings / spouses aboard the Titanic|\r\nparch|\t# of parents / children aboard the Titanic|\r\nticket|Ticket number|\r\nfare|Passenger fare|\r\ncabin|Cabin number|\r\nembarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|\r\n\r\n### Variable Notes\r\n\r\n**pclass**: A proxy for socio-economic status (SES)\r\n1st = Upper\r\n2nd = Middle\r\n3rd = Lower\r\n\r\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\r\n\r\n**sibsp**: The dataset defines family relations in this way...\r\nSibling = brother, sister, stepbrother, stepsister\r\nSpouse = husband, wife (mistresses and fiancés were ignored)\r\n\r\n**parch**: The dataset defines family relations in this way...\r\nParent = mother, father\r\nChild = daughter, son, stepdaughter, stepson\r\nSome children travelled only with a nanny, therefore parch=0 for them.\r\n\r\nsource: https://www.kaggle.com/c/titanic/data\r\n\r\n### Sample Records\r\n#### Training Data\r\n\r\n```{r trainsample, echo=FALSE}\r\n# Load raw data\r\ntrain \u003c- read.csv(\u0022../input/train.csv\u0022, header = TRUE);\r\nedaTrain = train;\r\nknitr::kable( head(train), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n#### Test Data\r\n\r\n```{r testsample, echo=FALSE, warning=FALSE}\r\n# Load libraries\r\nlibrary(stringr);\r\nlibrary(assertthat);\r\nlibrary(dplyr, quietly=TRUE);\r\nlibrary(leaps);\r\n# Load raw data\r\ntest \u003c- read.csv(\u0022../input/test.csv\u0022, header = TRUE);\r\n\r\n# Add a \u0022Survived\u0022 variable to the test set to allow for combining data sets\r\ntest.survived \u003c- data.frame(Survived = rep(NA, nrow(test)), test[,]);\r\n\r\n# Combine data sets\r\ndata_combined \u003c- rbind(train, test.survived);\r\n#data_combined$survived \u003c- as.factor(data_combined$survived)\r\n\r\nknitr::kable( head(test), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n***\r\n## Exploratory Data Analysis\r\n### Missing Values\r\nThe first step is to find any and all missing data in the train and test sets.  \r\n\r\n#### Train Dataset\r\n```{r eda1}\r\n# go through each variable and if it\u0027s an empty factor or numeric NA, sum each column\r\nnlTrainNA = sapply( train, function(x) switch( class(x), factor = sum(x==\u0022\u0022), sum( is.na(x) ) ) );\r\ntTrainNA = t(nlTrainNA);                                 # transpose named list\r\ndfTrainNA = data.frame( tTrainNA );                      # convert to dataframe\r\n```\r\nThe train.csv dataset had 3 columns with missing values: Age, Cabin, and Embarked.  Age is likely to be an important predictor of survival and we have data for 80% of the training subjects so imputing the missing values is likely to be beneficial.  The source of embarkation may not have obvious predictive power, but given that we have data for over 99% of the training subjects, imputing the missing values could provide value.  The Cabin variable is missing from over 77% of the test subjects.  At first impression, this variable seems like an unlikely candidate to impute values since so much source data is missing.\r\n```{r eda1table, echo=FALSE}\r\nknitr::kable( dfTrainNA, format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\n\r\n#### Test Dataset\r\n```{r eda2}\r\nnlTestNA = sapply( test, function(x) switch( class(x), factor = sum(x==\u0022\u0022), sum( is.na(x) ) ) );\r\ntTestNA = t(nlTestNA);                                 # transpose named list\r\ndfTestNA = data.frame( tTestNA );                      # convert to dataframe\r\n```\r\nThe test.csv dataset also had 3 columns with missing values: Age, Fare, and Cabin.  Like the train dataset, the Cabin variable is sparse, with over 78% subjects missing values.  The Age variable is populated for over 79% of the train subjects, and likely has good predictive power so it will likely be beneficial to impute values for subjects missing Age.  The Fare variable is missing for a single subject.  While Fare may not be an obvious predictor for survival, the fact that the dataset is over 99% complete for this variable indicates that it is a good candidate for imputation.\r\n```{r eda2table, echo=FALSE, warning=FALSE}\r\nlibrary( ggplot2 );\r\nknitr::kable( dfTestNA, format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\n\r\n### Variable Features\r\n\r\n#### PassengerId\r\nPassengerId is a primary key for each row of data in the train and test sets.  This variable will not be included in any of the predictive models.\r\n\r\n#### Survived\r\nSurvived is the class we\u0027re trying to predict.\r\n\r\n#### Pclass\r\nPassenger class is either 1st, 2nd, or 3rd.\r\n```{r eda_pclass1}\r\nggplot(train, aes(x = factor(Pclass), fill = factor(Pclass))) +\r\n  geom_bar( show.legend=FALSE) +\r\n  xlab(\u0022Pclass\u0022) +\r\n  ylab(\u0022Total Count\u0022)\r\n```\r\n\r\nThe Pclass variable shows that most passengers in the train set held a 3rd class ticket.  491 of the 891 passengers were 3rd class, more than 1st and 2nd class combined.  \r\n```{r eda_pclass2}\r\nggplot(train, aes(x = factor(Pclass), fill = factor(Survived))) +\r\n  geom_bar(width = 0.5, position=\u0022dodge\u0022) +\r\n  xlab(\u0022Pclass\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\n\r\nIf the Survived variable is plotted as a function of passenger class, it appears that Pclass will be a predictor for survivability.  A higher percentage of first class passengers survived than died, contrary to the overall trend, whereas a far higher percentage of 3rd class passengers died than survived.\r\n\r\n#### Name\r\nIt may seem that the passenger name is a lot like the PassengerId in that each name acts as a sort of primary key into the data and using name as a model feature would not generalize well.  However, the name field could possibly provide value.  The first twenty names in the train dataset:\r\n```{r eda_name}\r\nhead(as.character(train$Name),n=20);\r\n```\r\nEach name begins with a surname before a comma and a title.  If the passenger is a married woman, her maiden name appears in parentheses.  Some of the titles may help infer age (Master. and Miss.) and surnames could help determine extended family travelling together, even if they\u0027ve purchased separate tickets and are not in the same cabin.  Additionally, the presence of diacritical marks in a name could indicate that the passenger is a non-English speaker who might have had difficulty understanding instructions or the gravity of the situation.\r\n\r\n#### Sex\r\nSex is an unordered factor, male or female.\r\n```{r eda_sex1}\r\nggplot(train, aes(x = factor(Sex), fill = factor(Sex))) +\r\n  geom_bar( show.legend=FALSE) +\r\n  xlab(\u0022Sex\u0022) +\r\n  ylab(\u0022Total Count\u0022)\r\n```\r\n\r\nThe Sex variable shows that most passengers in the training set were male (nearly 2/3 male).  577 of the 891 passengers were male, or 65%.\r\n```{r eda_sex2}\r\nggplot(train, aes(x = factor(Sex), fill = factor(Survived))) +\r\n  geom_bar(width = 0.5, position=\u0022dodge\u0022) +\r\n  xlab(\u0022Sex\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022)\r\n```\r\nIf the Survived variable is plotted as a function of Sex, it appears that Sex will be a strong predictor for survivability.  Of the 314 female passengers in the training set, 233, or 74% survived.  On the other hand, of the 577 male passengers, only 109 survived, or 19%.\r\n\r\n#### Age\r\nAs noted, there are age values for 80% of the training subjects, missing for 177 passengers.  The distribution of ages is slightly skewed right, with a median of 28 years and a mean of 29.7 years.\r\n```{r eda_age1}\r\nggplot(subset(train, !is.na(Age)), aes(x = Age)) +\r\n  geom_histogram(binwidth=4) +\r\n  xlab(\u0022Age\u0022) +\r\n  ylab(\u0022Total Count\u0022);\r\nggplot(subset(train,!is.na(Age)), aes(y=Age,x=\u0022\u0022)) + geom_boxplot();\r\n```\r\n\r\nAge as a predictor of survivability:\r\n```{r eda_age2}\r\nggplot(subset(train,!is.na(Age)), aes(x = Age, fill = factor(Survived))) +\r\n  geom_density( position=\u0022stack\u0022) +\r\n  xlab(\u0022Age\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\nIf we plot the Survived value as a function of the Age density, we see that there is a higher likelihood of younger passengers surviving over older passengers.  Up until the mid-to-late teens, a training set passenger is more likely to survive than die, so age is likely to be a useful predictor for survivability.\r\n\r\n#### SibSp\r\nThis variable is unique in that it is combination of number of siblings or \u00221\u0022 if the passenger had a spouse on board the ship.  In some cases, it is not clear what the SibSp variable is encoding when a \u00221\u0022 is found - is the passenger travelling with a sibling or a spouse?  A SibSp of 2 or more is indicative of siblings.  It will likely be beneficial to disambiguate this variable into separate \u0027Siblings\u0027 and \u0027Spouses\u0027 variables.\r\n```{r eda_sibsp1}\r\nggplot(train, aes(x = SibSp )) +\r\n   geom_histogram(binwidth=0.5) +\r\n   xlab(\u0022Number of Siblings/Spouses\u0022) +\r\n   ylab(\u0022Total Count\u0022)\r\n```\r\n\r\nMost passengers in the training set (68%) had neither a spouse or sibling on board.  23% of the training set passengers had a single sibling or spouse on board.  The remaining 9% of the passengers had two or more (presumably) siblings on board.\r\n```{r eda_sibsp2}\r\nggplot(train, aes(x = SibSp, fill = factor(Survived))) +\r\n   geom_histogram(binwidth=0.5, position=\u0022dodge\u0022) +\r\n   xlab(\u0022Number of Siblings/Spouses\u0022) +\r\n   ylab(\u0022Total Count\u0022) +\r\n   labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\nSurvivability does appear to trend with the number of siblings/spouses on board.  A passenger having no siblings or spouses is most likely to have died, whereas a passenger with one or two siblings/spouses has around a 50% likelihood of surviving.  The remaining cases of three to eight siblings are likely too few from which to draw inferences individually, so it might make sense to pool the SibSp values as follows: 0, 1, 2, 3\u003e to avoid overfitting to specific training cases.  A close examination of the seven instances of the SibSp variable in which SibSp equals 8 reveals that all the subjects were from the same family and were in the same cabin.  Predicting that all families of size 8 will perish is unlikley to generalize well.\r\n\r\n#### Parch\r\nSimilar to SibSp, this variable convolves two separate pieces of data: the number of parents and the number of children this passenger has on board.  In some cases, it is not clear what the Parch variable is encoding when a \u00221\u0022 is found - is the passenger travelling with a parent or a child?  This can be inferred if the Age variable is present for the passenger, but if the Age is missing, it will be ambiguous and may need further analysis.  Perhaps the passenger\u0027s title (Mr., Miss., Master) could help.  Like SibSp, it will likely be beneficial to disambiguate this variable into separate \u0027Parents\u0027 and \u0027Children\u0027 variables.\r\n```{r eda_parch1}\r\nggplot(train, aes(x = Parch )) +\r\n   geom_histogram(binwidth=0.5) +\r\n   xlab(\u0022Number of Parents/Children\u0022) +\r\n   ylab(\u0022Total Count\u0022)\r\n```\r\n\r\nMost passengers in the training set (76%) had neither a parent or child on board.  13% of the training set passengers had a single parent or child on board.  About 9% of the passengers in the training set (80) had two or more parents or children on board.  The remaining 2% of the passengers had either 3, 4, 5, or 6 (presumably) children on board.\r\n```{r eda_parch2}\r\nggplot(train, aes(x = Parch, fill = factor(Survived))) +\r\n   geom_histogram(binwidth=0.5, position=\u0022dodge\u0022) +\r\n   xlab(\u0022Number of Parents/Children\u0022) +\r\n   ylab(\u0022Total Count\u0022) +\r\n   labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\nSurvivability does appear to trend with the number of parents/children on board.  A passenger having no parents or children is most likely to have died, whereas a passenger with one or two parents/children has around a 50% likelihood of surviving.  The remaining cases of three to six children are likely too few from which to draw inferences individually, so it might make sense to group the Parch values as follows: 0, 1, 2, 3\u003e= to avoid overfitting to specific training cases.  \r\n\r\n#### Ticket\r\nThe entries in the Ticket column do not seem to be of a uniform format.  Some ticket entries are just numbers - ranging from 693-392096.  Other ticket entries have character prefixes like \u0022C.A.\u0022 or \u0022SOTON/O2\u0022, followed by a (presumably) ticket number.  \r\n```{r eda_ticket}\r\nhead(as.character(train$Ticket),n=20);\r\n```\r\nAn inspection of the set of tickets shows that presumed families tend to share a single ticket number.  The first impression is that the ticket number seems an unlikely predictor for survivability and could lead to overfitting the training set.  However, the ticket number might help populate the missing Cabin information - and Cabin might be a good predictor for survivability.\r\n\r\n#### Fare\r\nThe fare (price paid per ticket) ranges from 0 to 512.3292.  The units are unclear, but are likely in English pounds.  The distribution is skewed to the right, with a median of 14.4542 and a mean of 32.20421.  A log transform of the data may be necessary to normalize the distribution of fares.  However, first the fare for passenger must be determined.  It appears to be the case that individual ticket numbers are not assigned per passenger, but rather a single ticket number is given to the purchaser of an allotment of tickets.  That is, families travelling together seem to be under the same ticket with the same fare.  So, it may be necessary to get to create an \u0022Amount Paid per Passenger\u0022 feature that takes into account the number of people for which a fare was purchased on a single ticket.  \r\n\r\n```{r eda_fare1}\r\nggplot(train, aes(x = Fare)) +\r\n  geom_histogram(binwidth=4) +\r\n  xlab(\u0022Fare\u0022) +\r\n  ylab(\u0022Total Count\u0022);\r\nggplot(train, aes(y=Fare,x=\u0022\u0022)) + geom_boxplot();\r\n```\r\n\r\nFare could conceivably be an important factor in determining survivability.  Perhaps the higher paying passengers received the first opportunity to board lifeboats.  Or perhaps, those higher paying passengers were more initially unwilling to leave their more comfortable accomodations for the plebian conditions aboard a lifeboat.  Fare as a predictor of survivability:\r\n```{r eda_fare2}\r\nggplot(train, aes(x = Fare, fill = factor(Survived))) +\r\n  geom_density( position=\u0022stack\u0022) +\r\n  xlab(\u0022Fare\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022)\r\n```\r\n\r\n#### Cabin\r\nThe Cabin feature could be another strong predictor for survivability.  Perhaps cabins located nearest the lifeboats afforded the best survivability.  But, the Cabin variable has many empty values.  The empty values could mean that the information was not captured or it could mean that not all passengers received cabins and stayed in other accomodations.  Being assigned a cabin could be a proxy for one\u0027s social status and wealth.  If so, the Pclass variable might be co-linear.\r\n```{r eda_cabin1}\r\nlevels(train$Cabin);\r\n```\r\n\r\nThe cabin name mostly adheres to the rule of a single letter A-F,G,T, followed by a number up to 3 digits.  There are cases where a passenger has multiple cabins, each separated by whitespace.  The beginning letter of each cabin could denote a deck or particular region of the ship - which could help with predicting survivability.  Alternatively, the number of the cabin could be more informative than the beginning letter.  Perhaps cabins \u0022A19\u0022 and \u0022B19\u0022 are located right next to one another, for instance.  \r\n```{r eda_cabin2, warning=FALSE}\r\ncabinLetter = ifelse(train$Cabin == \u0022\u0022, NA, substr(train$Cabin,1,1));\r\ncabinREs = gregexpr(\u0022\\\\d+\u0022,train$Cabin, perl=TRUE);\r\ncnMatches = regmatches( train$Cabin, cabinREs);\r\ncabinNumber = numeric(length(cnMatches));\r\nfor ( i in 1:length(cnMatches) ){\r\n  cabinNumber[i] = mean( as.numeric( unlist( cnMatches[i] )));\r\n}\r\nedaTrain$CabinLetter \u003c- as.factor(cabinLetter);\r\nedaTrain$CabinNumber \u003c- cabinNumber;\r\nggplot( edaTrain, aes(x=CabinNumber,y=Survived,color=Survived ) ) + \r\n  geom_point( shape=1,position=position_jitter(height=0.25)) +\r\n  ggtitle(\u0022Survivability by Cabin Number\u0022) +\r\n  xlab(\u0022Cabin Number\u0022) +\r\n  ylab(\u0022Survived\u0022);\r\nggplot(subset(edaTrain, !is.na(cabinLetter)), aes(x = CabinLetter, fill = as.factor(Survived))) +\r\n  geom_bar() +\r\n  ggtitle(\u0022Survivability by Ticket Letter\u0022) +\r\n  xlab(\u0022Cabin Letter\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill = \u0022Survived\u0022);\r\n```\r\n\r\nWhen grouping the passengers by cabin number, there does not appear to be a relationship where survivability depends on cabin number.  If so, there should be identifiable pockets of clusters where there is a higher incidence of survivability.  If such clusters appeared to exist, the clusters could be defined and the group of clusters tested with a Chi-Square test to measure if survivability depends on cabin cluster.\r\n\r\nIt\u0027s not immediately obvious if there is a benefit to categorizing the cabins according to their first letter.  Are these groups statistically different from one another?  A Chi-Square test of independence should show if survivability is dependent on cabin letter or not.\r\n```{r eda_cabin3, warning=FALSE}\r\ntbl = table( edaTrain$Survived, edaTrain$CabinLetter);\r\ntbl\r\nchisq.test(tbl);\r\n```\r\n\r\nThe p-value is 0.17 so at a confidence level of 0.05, we cannot reject the null hypothesis that survivability is independent of the starting cabin letter.\r\n```{r eda_cabin4}\r\nedaTrain$CabinAssignment[ edaTrain$Cabin != \u0022\u0022 ] \u003c- \u0022Assigned\u0022;\r\nedaTrain$CabinAssignment[ edaTrain$Cabin == \u0022\u0022 ] \u003c- \u0022Unassigned\u0022;\r\ndata_combined$CabinAssignment[ data_combined$Cabin != \u0022\u0022 ] \u003c- \u0022Assigned\u0022;\r\ndata_combined$CabinAssignment[ data_combined$Cabin == \u0022\u0022 ] \u003c- \u0022Unassigned\u0022;\r\ndata_combined$CabinAssignment = factor( data_combined$CabinAssignment );\r\nggplot(edaTrain, aes(x=CabinAssignment, fill=factor(Survived))) +\r\n  geom_bar() + \r\n  facet_wrap(~Pclass) + \r\n  ggtitle(\u0022Pclass\u0022) + \r\n  xlab(\u0022Cabin Assignment\u0022) +\r\n  ylab(\u0022Total Count\u0022) +\r\n  labs(fill=\u0022Survived\u0022);\r\n```\r\n\r\nFrom the graph, it appears that for each passenger class, if a passenger is assigned a cabin, their chances of surviving the disaster are better than if they had not been assigned a cabin.\r\n\r\n#### Embarked\r\n\r\nPassengers boarded the Titanic from one of three ports: (S)outhampton, England; (C)herbourg, France, or (Q)ueenstown, Ireland.  As noted, two passengers in the training set have no record of port of Embarkation.\r\n```{r eda_embarked1, echo=FALSE}\r\nknitr::kable( train[train$Embarked == \u0022\u0022,], format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\n\r\nThe two passengers are both female and in first class.\r\n```{r eda_embarked2}\r\nggplot( edaTrain, aes(x=factor(Pclass),fill=factor(Survived)))+\r\n  geom_bar() + \r\n  facet_wrap(~Embarked) +\r\n  ggtitle(\u0022Survivability by Port of Embarkation and Passenger Class\u0022) +\r\n  xlab(\u0022Passenger Class\u0022) +\r\n  ylab(\u0022Count\u0022) +\r\n  labs(fill=\u0022Survived\u0022);\r\n```\r\n\r\n***\r\n\r\n### Errors in the Data\r\n\r\nErrors in the input dataset are often not apparent until deeper analyses are performed, such as feature engineering and imputing missing data.  However, once the data is corrected, it is necessary to regenerate the columns and feature analyses that will feed the predictive models.  In this dataset, it was found that a 16-year-old member of a family was incorrectly identified as being the father:\r\n```{r errors1, echo=FALSE}\r\nknitr::kable( data_combined[ data_combined$Ticket == \u0022W./C. 6608\u0022, ], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\nIn the Ford family, Mr. William Neal Ford has the value \u00271\u0027 for SibSp and \u00273\u0027 for Parch.  The value of \u00271\u0027 for SibSp would imply that William has either one spouse or one sibling.  In addition, William has a value of \u00273\u0027 for Parch.  This indicates that William has either 3 children, 1 parent and 2 children, 2 parents and 1 child, or 3 parents, which is clearly not possible according to the variable definition.  Since two of the children in the family, Miss. Doolina Margaret Ford (21), and Mr. Edward Watson Ford (18) are actually *older* than William, it is clear that he cannot be their father.  The other child, Miss. Robina Maggie Ford (9) is only 7 years younger than William, also indicating that William cannot be her father.  The matriarch of the family is Mrs. Edward Ford - not Mrs. William Neal Ford, which indicates that it\u0027s likely not a simple case of getting William Neal Ford\u0027s age wrong (for instance, 46 instead of 16).\r\n\r\nTherefore, the best fix would be to change the Parch variables for Miss. Robina Maggie Ford, Miss. Doolina Margaret Ford, and Mr. Edward Watson Ford from \u00272\u0027 to \u00271\u0027 (due to the inclusion of Mr. William Neal as a brother).  In addition, the Parch variable of Mrs. Edward Ford would rise from \u00273\u0027 to \u00274\u0027, indicating she\u0027s travelling with four of her children and the Parch variable of Mr. William Neal Ford would change from \u00273\u0027 to \u00271\u0027 indicating that he\u0027s travelling with a single parent, his mother.  The SibSp variables would change from \u00272\u0027 to \u00273\u0027 for Miss. Robina Maggie Ford, Miss. Doolina Margaret Ford, and Mr. Edward Watson Ford, indicating that each of them are travelling with 3 siblings.  Similarly, Mr. William Neal Ford\u0027s SibSp variable would change from \u00271\u0027 to \u00273\u0027.  Finally, Mrs. Edward Ford\u0027s SibSp would change from \u00271\u0027 to \u00270\u0027, indicating she was travelling without her husband (Mr. Edward Ford).  The changes appear below.\r\n```{r errors2, echo=FALSE}\r\n# Mr. William Neal Ford\r\ndata_combined[ data_combined$PassengerId ==  87,\u0022SibSp\u0022] = 3;  # From 1\r\ndata_combined[ data_combined$PassengerId ==  87,\u0022Parch\u0022] = 1;  # From 3\r\n# Miss. Robina Maggie Ford\r\ndata_combined[ data_combined$PassengerId == 148,\u0022SibSp\u0022] = 3;  # From 2\r\ndata_combined[ data_combined$PassengerId == 148,\u0022Parch\u0022] = 1;  # From 2\r\n# Miss. Doolina Margaret Ford\r\ndata_combined[ data_combined$PassengerId == 437,\u0022SibSp\u0022] = 3;  # From 2\r\ndata_combined[ data_combined$PassengerId == 437,\u0022Parch\u0022] = 1;  # From 2\r\n# Mrs. Edward Ford\r\ndata_combined[ data_combined$PassengerId == 737,\u0022SibSp\u0022] = 1;  # From 1; Note: sister is Mrs. Eliza Johnston\r\ndata_combined[ data_combined$PassengerId == 737,\u0022Parch\u0022] = 4;  # From 3\r\n# Mr. Edward Watson Ford\r\ndata_combined[ data_combined$PassengerId == 1059,\u0022SibSp\u0022] = 3;  # From 2\r\ndata_combined[ data_combined$PassengerId == 1059,\u0022Parch\u0022] = 1;  # From 2\r\nknitr::kable( data_combined[ data_combined$Ticket == \u0022W./C. 6608\u0022, ], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nAdditionally, there are problems with the Abbott family with ticket C.A., 2673:\r\n```{r errors3, echo=FALSE}\r\nknitr::kable( data_combined[ data_combined$Ticket == \u0022C.A. 2673\u0022, ], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nThirteen year old Master. Eugene Joseph Abbott has a Parch value of \u00272\u0027, meaning he has two parents on board.  A female on the same ticket, Mrs. Stanton Abbott, is 22 years older than Eugene at 35 years of age.  Mrs. Stanton Abbott has a SibSp of \u00271\u0027 and a Parch of \u00271\u0027.  The other person on the ticket is 16-year-old Mr. Rossmore Edward Abbott, with a SibSp of \u00271\u0027 and a Parch of \u00271\u0027.  It appears that 16-year-old Mr. Rossmore Edward Abbott was incorrectly designated the spouse of Mrs. Stanton Abbott instead of her son.  The corrections appear below.\r\n```{r errors4, echo=FALSE}\r\n# Master. Eugene Joseph Abbott\r\ndata_combined[ data_combined$PassengerId == 1284,\u0022SibSp\u0022] = 1;  # From 0\r\ndata_combined[ data_combined$PassengerId == 1284,\u0022Parch\u0022] = 1;  # From 2\r\n# Mrs. Stanton (Rosa Hunt)\r\ndata_combined[ data_combined$PassengerId == 280,\u0022SibSp\u0022] = 0;  # From 1\r\ndata_combined[ data_combined$PassengerId == 280,\u0022Parch\u0022] = 2;  # From 1\r\nknitr::kable( data_combined[ data_combined$Ticket == \u0022C.A. 2673\u0022, ], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\n## Feature Engineering\r\nWhen importing into R, factor/class variables solely composed of numeric entries are interpreted and imported as numeric types, implying an ordering and a distance between entries.  These variables include PassengerId, Survived, and Pclass.  PassengerId is like a primary key for each passenger, so it\u0027s not necessary to convert this variable into a factor.  However, the Survived data is encoded as a binary flag where 1=survived and 0=not survived.  Since our goal is to predict classes (i.e., survived or died), it is necessary to convert this value into a factor.  The Pclass variable is a factor, but it is an ordered factor.  An ordered factor indicates that there is an ordering between the classes (1st class, 2nd class, 3rd class), but no magnitude between each class level can be inferred.\r\n```{r feature1}\r\ntrain$Survived = as.factor(train$Survived);\r\ntrain$Pclass = as.ordered(train$Pclass);\r\n```\r\n\r\n### Imputing Missing Data\r\n\r\n#### Embarked\r\nTwo passengers had no information for their port of embarcation, Miss. Amelie Icard and Mrs. George Nelson Stone.  These first class passengers were both on the same ticket (113572) and stayed in Cabin B28.  The majority of passengers boarded at Southampton (70%), as compared to Cherbourg (21%) and Queenstown (9%).  There is quite a variety in the ticket numbers on the Titanic.  Their ticket number, 113572, is one in a series of similar ticket numbers, all in the 113XXX format.  There are 56 tickets in the 113XXX format and of the 54 that have a valid port of embarcation, 81% were from Southampton.  Considering the different data points, the two missing Embarked values will be considered \u0027S\u0027 for Southampton.\r\n```{r impute_embarked}\r\ndata_combined[62,\u0022Embarked\u0022] = as.factor(\u0022S\u0022);\r\ndata_combined[830,\u0022Embarked\u0022] = as.factor(\u0022S\u0022);\r\ndata_combined$Embarked = factor( data_combined$Embarked );  # removes empty factor \u0022\u0022\r\n```\r\n\r\n#### Cabin\r\nAs determined in the EDA section, over 77% of the passengers have no recorded cabin information.  In addition, the assigned cabins did not seem to have a predictable pattern from the available data.  Further, cursory analyses did not indicate that cabin assignments were a good predictor of survivability.  As such, the missing cabin data will not be imputed.\r\n\r\n#### Fare\r\nOut of 1309 records, one fare is missing:\r\n```{r impute_fare1, echo=FALSE}\r\nknitr::kable( data_combined[1044,], format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nThe fare can be estimated by considering the variables that most likely affect the Fare variable, such as passenger class (Pclass), port of Embarcation (Embarked), Cabin, and how many other people are on the same ticket.  The passenger class and port of embarkation are readily available in each data row.  The Cabin variable, however, is not useful in this case since the Cabin variable is not defined for passenger 1044.  However, the fact that no Cabin was recorded for this passenger is significant since perhaps not all passengers were assigned cabins (i.e., wealthy passengers would be assigned cabins, and hence pay higher fares, while less privileged passengers may have to had large, unassigned community accomodations with a comparatively lower fare).  \r\n\r\n```{r impute_fare2}\r\nticketChar = as.character( data_combined$Ticket );\r\nnumPassengers = nrow( data_combined );\r\ndata_combined$NumPassengersOnTicket = 1;\r\nfor (i in 1:numPassengers ){\r\n    thisTicket = as.character( data_combined[i,\u0022Ticket\u0022] );\r\n    idxPeople = which( thisTicket == ticketChar );\r\n    numPeople = length( idxPeople );\r\n    data_combined$NumPassengersOnTicket[i] = numPeople;\r\n}\r\n# now constrain to those that have just 1 person on the ticket, as passenger 1044\r\nsinglePassengers = data_combined$NumPassengersOnTicket == 1;\r\nembarkedS = data_combined$Embarked == \u0027S\u0027;\r\nunassignedCabins = data_combined$CabinAssignment == \u0027Unassigned\u0027;\r\npClass3 = data_combined$Pclass == 3;\r\nsimilarPassengers = which( singlePassengers \u0026 embarkedS \u0026 unassignedCabins \u0026 pClass3 );\r\nsimilarPassengersFareData = data_combined[similarPassengers,];  # include all columns but the one we have no Fare info\r\nsimilarPassengersFareData = similarPassengersFareData[ similarPassengersFareData$PassengerId != 1044,];  # include all columns but the one we have no Fare info\r\nmedianFare = median( similarPassengersFareData$Fare );\r\ndata_combined[1044,\u0022Fare\u0022] = medianFare;\r\nhist(similarPassengersFareData$Fare, xlab = \u0022Fare\u0022, ylab = \u0022Passengers\u0022, main = \u0022Fare of Similar Passengers\u0022);\r\nsummary( similarPassengersFareData$Fare );\r\n```\r\n\r\nThere are 318 passengers that share the same characteristics as passenger 1044: passenger class 3, port of embarkation Southampton, cabin assignment (unassigned), and the number of people with the same ticket (1).  Excluding a fare of 3.1708 and two at 19.9667, the remaining fares are between 6.2375 and 10.5167.  Passenger 1044 is assigned the median fare of 7.896.\r\n\r\n#### Age\r\nAs found above, the Age data is missing for 177 of the train set cases and 86 of the test set cases.  Since Age is likely to be an important predictor of survival, the missing data should be imputed.  As will be shown in the next section on Feature Engineering, the passenger\u0027s title can be extracted from their name.  The title should allow for a better estimate of a passenger\u0027s age.\r\n\r\nThe missing passengers have the following titles:\r\n```{r impute_age1, echo=FALSE}\r\ndata_combined$FixedAge = data_combined$Age;\r\npassenger_names = as.character(data_combined$Name);\r\nnum_commas = unname(sapply( passenger_names, str_count, \u0022,\u0022));\r\nall_commas = assert_that( all( num_commas == 1 ) );  # Make sure each row has a comma\r\n# Now, extract titles.  Split on comma and take the tail end of the character string\r\nsurnames = sapply( strsplit(as.character(passenger_names), \u0022,\u0022), head, 1);\r\ndata_combined$Surname = as.factor( surnames );\r\ngiven_name_string = sapply( strsplit(as.character(passenger_names), \u0022,\u0022), tail, 1);\r\ngiven_name_string = trimws( given_name_string, \u0022left\u0022); # remove leading whitespace\r\ndata_combined$NameString = paste( given_name_string, surnames );\r\n# Remove leading whitespace, if any\r\ngiven_name_string = trimws( given_name_string, \u0022left\u0022);\r\n# To extract the title from the Name character string, split each string \r\nname_tokens = strsplit( given_name_string, \u0022\\\\s+\u0022);\r\n# Exract first token as the title\r\ntitles = lapply(name_tokens,\u0022[[\u0022,1);\r\ngiven_names = lapply(name_tokens,\u0022[[\u0022,2);\r\ndata_combined$GivenName = as.factor( unlist( given_names ) );\r\ntitle_regs = regexpr( \u0022[\\\\w+]+\\\\.\u0022, given_name_string, perl=TRUE );\r\nre_titles = regmatches( given_name_string, title_regs );\r\ndata_combined$Title = as.factor( unlist( re_titles ));\r\n# The set of titles that have missing age values\r\nageNA = is.na( data_combined$Age );\r\ntitleNA = data_combined$Title[ageNA];\r\n# The set of titles that have missing age values\r\ndata_combined$FixedTitle = data_combined$Title;\r\n\r\nFIX_TITLES = TRUE;\r\nif ( FIX_TITLES ){\r\n  # change the following titles to \u0022Mr.\u0022 or \u0022Mrs.\u0022\r\n  titlesToChange = c(\u0022Capt.\u0022, \u0022Col.\u0022, \u0022Countess.\u0022, \u0022Don.\u0022, \u0022Dona.\u0022, \u0022Dr.\u0022, \u0022Jonkheer.\u0022, \u0022Lady.\u0022, \u0022Major.\u0022, \u0022Mlle.\u0022, \u0022Mme.\u0022, \u0022Ms.\u0022, \u0022Rev.\u0022, \u0022Sir.\u0022);\r\n  for ( title in titlesToChange ){\r\n    idxChange = which( data_combined$FixedTitle == title );\r\n    idxChangeMale = intersect( idxChange, data_combined$Sex == \u0022male\u0022);\r\n    idxChangeFemale = setdiff( idxChange, idxChangeMale );\r\n    data_combined[idxChangeMale,\u0022FixedTitle\u0022] = \u0022Mr.\u0022;\r\n    data_combined[idxChangeFemale,\u0022FixedTitle\u0022] = \u0022Mrs.\u0022;\r\n  }\r\n  data_combined$FixedTitle = factor( data_combined$FixedTitle );\r\n}\r\n\r\ndata_combined$AgeTitle = data_combined$Title;\r\ndata_combined$AgeTitle[ which( data_combined$Title == \u0022Ms.\u0022 \u0026 ageNA )] = as.factor( \u0022Mrs.\u0022 );\r\nage_lm = lm( Age~AgeTitle, data=data_combined);\r\nmissingAgeIndices = which( is.na( data_combined$Age ) );\r\nmissingTitles = data_combined[missingAgeIndices,\u0022AgeTitle\u0022];\r\nmissingCount = table( missingTitles );\r\nsummary( missingTitles )\r\n```\r\n\r\nThe resultant linear model uses the following average ages for each missing passenger title:\r\n```{r impute_age2}\r\n# model is simple: use the passenger\u0027s title (i.e., Mr., Master., Mrs.) to determine age\r\n# May want to better this by using age of parents (if travelling with parents), age of siblings, spouse, etc.\r\nage_lm = lm( Age~AgeTitle, data=data_combined);\r\n\r\nmissingAges = predict( age_lm, data_combined[missingAgeIndices,]);\r\ndata_combined[missingAgeIndices,\u0022FixedAge\u0022] = missingAges;\r\ncoeffs = coefficients( age_lm );\r\ncoeffNames = names(coeffs);\r\ncoeffValues = unname(coeffs);\r\n# find (Intercept)\r\nidxIntercept = match( \u0022(Intercept)\u0022, coeffNames );\r\ninterceptValue = coeffValues[idxIntercept];\r\nallOtherCoeffIndices = setdiff(1:length(coeffNames),idxIntercept);\r\nallOtherCoeffValues = coeffValues[allOtherCoeffIndices];\r\ntitleAges = interceptValue + allOtherCoeffValues;\r\nnames(titleAges) =  gsub( \u0022AgeTitle\u0022, \u0022\u0022, coeffNames[allOtherCoeffIndices] );\r\nknitr::kable( bind_rows(titleAges), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n***\r\n### New Features\r\n\r\n#### Title and AgeTitle\r\nThe Name variable in the test and train datasets has some structure - surname followed by a comma, then a title and a given name.  Additionally, a maiden name will be present in parentheses if the passenger is a married woman.\r\n```{r feature2}\r\npassenger_names = as.character(data_combined$Name);\r\nhead(passenger_names);\r\n```\r\nThe implied structure is [SURNAME] COMMA [TITLE] [GIVEN NAME] (MAIDEN NAME if applicable).  Since the goal is to extract the title information from each row programmatically, it is  necessary to enforce some error checking - namely, that each row contain one and only one comma.  If this condition does not hold true, the title extraction task will be more tedious.\r\n\r\n```{r feature3}\r\nnum_commas = unname(sapply( passenger_names, str_count, \u0022,\u0022));\r\nall_commas = assert_that( all( num_commas == 1 ) );  # Make sure each row has a comma\r\n# Now, extract titles.  Split on comma and take the tail end of the character string\r\nsurnames = sapply( strsplit(as.character(passenger_names), \u0022,\u0022), head, 1);\r\ndata_combined$Surname = as.factor( surnames );\r\ngiven_name_string = sapply( strsplit(as.character(passenger_names), \u0022,\u0022), tail, 1);\r\n# Remove leading whitespace, if any\r\ngiven_name_string = trimws( given_name_string, \u0022left\u0022);\r\n# To extract the title from the Name character string, split each string \r\nname_tokens = strsplit( given_name_string, \u0022\\\\s+\u0022);\r\n# Exract first token as the title\r\ntitles = lapply(name_tokens,\u0022[[\u0022,1);\r\ngiven_names = lapply(name_tokens,\u0022[[\u0022,2);\r\ndata_combined$GivenName = as.factor( unlist( given_names ) );\r\n```\r\n\r\nThe full set of extracted titles:\r\n```{r feature4}\r\npassenger_titles = unlist( titles );\r\ntable( passenger_titles );\r\n```\r\n\r\nMost of these look correct and reasonable as titles (\u0022Don.\u0022 and \u0022Dona.\u0022 are Italian honorifics, as is \u0022Jonkheer.\u0022 a Dutch honorific).  However, the title \u0022the\u0022 is suspicious.  The complete row is as follows:\r\n```{r feature5}\r\ndata_combined[ which( data_combined$Title == \u0022Countess.\u0022 ), ]\r\n```\r\n\r\nAlthough it makes little difference in this case since there is only a single \u0022the Countess. of Rothes\u0022 on board, the title of this passenger should be changed from \u0022the\u0022 to \u0022Countess.\u0022.  This could be fixed by changing the single record, but the fact that this incorrect title occurred in the first place indicates that the methodology for finding titles isn\u0027t robust enough.  Since it appears that the only time a \u0022.\u0022 appears in the Name field is at the end of a title, that information could be used to find all titles.\r\n```{r feature6}\r\ntitle_regs = regexpr( \u0022[\\\\w+]+\\\\.\u0022, given_name_string, perl=TRUE );\r\nre_titles = regmatches( given_name_string, title_regs );\r\ndata_combined$Title = as.factor( unlist( re_titles ));\r\nsummary( data_combined$Title );\r\n```\r\n\r\nOf particular interest are the titles that are underrepresented and how they might relate to the most abundant titles (Mr., Mrs., Miss., and Master.).  For instance, the title \u0022Ms.\u0022 has only two occurences and as such, has limited predictive power.  \u0022Ms.\u0022 generally refers to an adult woman, either married or unmarried and so is much closer to either a \u0022Mrs.\u0022 or \u0022Miss.\u0022 title than a \u0022Mr.\u0022, for instance.  Additionally, the French designations for \u0022Miss.\u0022 and \u0022Mrs.\u0022 appear as Madamoiselle (\u0022Mlle.\u0022) and Madame (\u0022Mme.\u0022).  These titles would likely provide better predictive power if they were switched to their English equivalents.\r\n\r\nAdditionally, many of the Age attributes are missing from the dataset and since it seems likely that Age would be a strong predictor of survivability.  Title should be a fairly robust predictor for Age, so engineering a title feature that is reflective of Age could be beneficial (i.e., a Doctor with a missing Age value is much more likely to be an adult than a child and adults have a generally lower survival rate than children).  The titles of the passengers missing Age information are as follows:\r\n```{r feature7}\r\n# The set of titles that have missing age values\r\nageNA = is.na( data_combined$Age );\r\ntitleNA = data_combined$Title[ageNA];\r\nsummary( titleNA );\r\n```\r\n\r\nFor the purposes of estimating the Age variable for this group, the \u0022Ms.\u0022 entry is better suited as a \u0022Mrs.\u0022.  Although the \u0022Dr.\u0022 title has a relatively small number of instances, it is likely enough from which to draw a reasonable age estimate.\r\n```{r feature8}\r\n# The set of titles that have missing age values\r\ndata_combined$AgeTitle = data_combined$Title;\r\ndata_combined$AgeTitle[ which( data_combined$Title == \u0022Ms.\u0022 \u0026 ageNA )] = as.factor( \u0022Mrs.\u0022 );\r\nsummary( data_combined$AgeTitle );\r\n```\r\n\r\n#### Parents and Children\r\nNext, the Parch variable encodes two different measures: the number of parents a passenger has on board AND/OR the number of children a passenger has on board.  Since this variable is overloaded, a more informative variable set might disambiguate these measures into NumParents and NumChildren (aboard).  Such a set of variables may provide more predictive power than the single Parch variable.\r\n```{r feature9}\r\nnumParents = integer( nrow( data_combined ) );\r\nnumChildren = integer( nrow( data_combined ) );\r\n\r\nMAX_CHILD_AGE = 14;\r\n\r\n# Find all Parch \u003e 0\r\nposParch = data_combined$Parch \u003e 0;\r\nidxParch = which( posParch );\r\ncounter = 0;\r\nnumCases = length( idxParch );\r\nfor (thisRow in idxParch){\r\n  sibsp = data_combined[thisRow,\u0022Sibsp\u0022];\r\n  # if sibsp \u003e 1, then the passenger is travelling with siblings (and therefore, likely parents)\r\n  counter = counter + 1;\r\n  numParch = data_combined[thisRow,\u0022Parch\u0022];\r\n  thisSurname = data_combined[thisRow,\u0022Surname\u0022];\r\n  thisTicket = data_combined[thisRow,\u0022Ticket\u0022];\r\n  thisTitle = as.character( data_combined[thisRow,\u0022Title\u0022] );\r\n  thisAge = data_combined[thisRow,\u0022Age\u0022];\r\n  if ( (thisTitle == \u0022Master.\u0022) || (thisTitle == \u0022Miss.\u0022) ){\r\n    # if this passenger is a \u0022Master.\u0022 or Miss. and numParch \u003c= 2, he/she must be someone\u0027s child\r\n    numParents[thisRow] = numParch;\r\n    #next;\r\n  }\r\n  if (!is.na( thisAge ) \u0026\u0026 thisAge \u003c= MAX_CHILD_AGE ){\r\n    # if this passenger is young, declare they cannot be parents, must be a child\r\n    numParents[thisRow] = numParch;\r\n    #next;\r\n  }\r\n  # get passenger rows on same ticket\r\n  sameTickets = data_combined$Ticket == thisTicket;\r\n  sameSurnames = data_combined$Surname == thisSurname;\r\n  sameSurnameSameTicket = sameTickets \u0026 sameSurnames \u0026 posParch;\r\n  ticketTitles = as.character( data_combined[ sameSurnameSameTicket, \u0022Title\u0022] );\r\n  ticketParches = data_combined[ sameSurnameSameTicket, \u0022Parch\u0022];\r\n  # now, look for passengers with the same surname on the ticket and check their titles and ages\r\n  numSame = length( sameSurnameSameTicket );\r\n  ages = sort( data_combined[sameSurnameSameTicket,\u0022Age\u0022] );\r\n  thisAgePos = which( thisAge == ages )[1];  # find index of thisAge in sorted ages\r\n  gaps = diff( ages );\r\n  numGenerationalGaps = length( which( gaps \u003e MAX_CHILD_AGE) );\r\n  if ( numGenerationalGaps == 0 ){\r\n    if ( !is.na( thisAge )){\r\n      if ( ( thisAge \u003e= 40 ) || (thisTitle == \u0022Mrs.\u0022 ) ){\r\n        numChildren[thisRow] = numParch;\r\n      }\r\n      else if (numParch \u003e 2 ){\r\n        numChildren[thisRow] = numParch;\r\n      }\r\n      else{\r\n        numParents[thisRow] = numParch;\r\n      }\r\n    }\r\n    else{\r\n      # no age information. If travelling with \u0022kids\u0022, and title isn\u0027t a kid, then parent\r\n      if ( ! ( thisTitle %in% c(\u0022Master.\u0022,\u0022Miss.\u0022) ) ){\r\n        if( any( c(\u0022Master.\u0022,\u0022Miss.\u0022) %in% ticketTitles ) ){\r\n          # now, if there are two Mr. in this group, we need to choose the real father\r\n          # If there are three or more children, then Parch will be greater than 2 and\r\n          # will indicate this is the father.  Else, it will be a child\r\n          if( thisTitle == \u0022Mr.\u0022){\r\n            maxParches = max( ticketParches );\r\n            if ( ( maxParches \u003e 2 ) \u0026\u0026 ( maxParches == numParch ) ){\r\n              numChildren[thisRow] = numParch;\r\n            }\r\n            else{\r\n              numParents[thisRow] = numParch;\r\n            }\r\n          }\r\n          else{\r\n            numChildren[thisRow] = numParch;\r\n          }\r\n        }\r\n        else{\r\n          # all we have is a non Mr. or Miss. title.  Make them a child\r\n            numChildren[thisRow] = numParch;\r\n        }\r\n      }\r\n      else{\r\n        numParents[thisRow] = numParch;\r\n      }\r\n    }\r\n  }\r\n  else{\r\n    if ( !is.na(thisAge) ){  # use age in comparison to generation gap to classify kids/parents\r\n      maxGapPos = which.max( gaps ) + 0.5;  # the 0.5 puts it in the middle of the kids/parents\r\n      if ( thisAgePos \u003c maxGapPos ){\r\n        numParents[thisRow] = numParch;\r\n      }\r\n      else{\r\n        numChildren[thisRow] = numParch;\r\n      }\r\n    }\r\n    else{\r\n      # no age info, have to go with titles\r\n    }\r\n  }\r\n  totalParch = numParents[thisRow] + numChildren[thisRow];\r\n  if ( totalParch != numParch ){\r\n    stop( \u0022Number of Parents/Children assigned (\u0022, totalParch, \u0022) does not equal the Parch variable (\u0022, numParch, \u0022) for passenger \u0022, data_combined[thisRow,\u0022PassengerId\u0022], \u0022\\n\u0022);\r\n  }\r\n  data_combined$NumParents = numParents;\r\n  data_combined$NumChildren = numChildren;\r\n}\r\n```\r\n\r\n#### Siblings and Spouses\r\nSimilar to Parch, the SibSp variable indicates the number of Siblings AND/OR Spouses a passenger has on board.  A better set of variables would be separate variables for Siblings and Spouses as it would be possible to model the original SibSp vector as a simple combination of the separate columns.\r\n```{r feature10}\r\nnumSiblings = integer( nrow( data_combined ) );\r\nnumSpouses = integer( nrow( data_combined ) );\r\n\r\nMAX_CHILD_AGE = 14;\r\n\r\nposSibSp = data_combined$SibSp \u003e 0;\r\nidxSibSp = which( posSibSp );\r\ncounter = 0;\r\nnumCases = length( idxSibSp );\r\nfor (thisRow in idxSibSp){\r\n  counter = counter + 1;\r\n  numSibSp = data_combined[thisRow,\u0022SibSp\u0022];\r\n  numParch = data_combined[thisRow,\u0022Parch\u0022];\r\n  thisSurname = data_combined[thisRow,\u0022Surname\u0022];\r\n  thisTicket = data_combined[thisRow,\u0022Ticket\u0022];\r\n  thisTitle = as.character( data_combined[thisRow,\u0022Title\u0022] );\r\n  thisAge = data_combined[thisRow,\u0022Age\u0022];\r\n  thisSex = as.character( data_combined[thisRow,\u0022Sex\u0022] );\r\n  thisGivenName = as.character( data_combined[thisRow,\u0022GivenName\u0022] );\r\n  thisPassengerId = data_combined[thisRow,\u0022PassengerId\u0022];\r\n  if ( (thisTitle == \u0022Master.\u0022) || (thisTitle == \u0022Miss.\u0022) ){\r\n    # if this passenger is a \u0022Master.\u0022 or Miss., this passenger is not married, so must be spouse\r\n    numSiblings[thisRow] = numSibSp;\r\n    next;\r\n  }\r\n  if (!is.na( thisAge ) \u0026\u0026 thisAge \u003c= 10 ){\r\n    # if this passenger is young, declare they cannot be parents, must be a child\r\n    numSiblings[thisRow] = numSibSp;\r\n    next;\r\n  }\r\n  # get passenger rows on same ticket\r\n  sameTickets = data_combined$Ticket == thisTicket;\r\n  sameSurnames = data_combined$Surname == thisSurname;\r\n  sameSurnameSameTicket = sameTickets \u0026 sameSurnames \u0026 posSibSp;\r\n  # if married, look for spouse on the same ticket\r\n  # get given names on same ticket\r\n  same_ticket_rows = data_combined[which(sameSurnameSameTicket),];\r\n  given_names = as.character( same_ticket_rows$GivenName );\r\n  titles = as.character( same_ticket_rows$Title );\r\n  sexes = as.character( same_ticket_rows$Sex );\r\n  passengerIds = same_ticket_rows$PassengerId;\r\n  master_mask = ( titles != \u0022Master.\u0022 );  \r\n  sex_mask = ( sexes != thisSex );  # opposite sex marriage\r\n  miss_mask = ( titles != \u0022Miss.\u0022 );\r\n  this_mask = ( passengerIds != thisPassengerId );\r\n  idxMatch = which( ( thisGivenName == given_names ) \u0026 master_mask \u0026 sex_mask \u0026 miss_mask \u0026 this_mask);\r\n  if ( length( idxMatch ) == 1 ){\r\n    numSpouses[thisRow] = 1;\r\n    next;\r\n  }\r\n  ticketTitles = as.character( data_combined[ sameSurnameSameTicket, \u0022Title\u0022] );\r\n  ticketParches = data_combined[ sameSurnameSameTicket, \u0022Parch\u0022];\r\n  # now, look for passengers with the same surname on the ticket and check their titles and ages\r\n  numSame = length( sameSurnameSameTicket );\r\n  ages = sort( data_combined[sameSurnameSameTicket,\u0022Age\u0022] );\r\n  thisAgePos = which( thisAge == ages )[1];  # find index of thisAge in sorted ages\r\n  gaps = diff( ages );\r\n  numGenerationalGaps = length( which( gaps \u003e MAX_CHILD_AGE) );\r\n}\r\n\r\n# do some manual corrections\r\nnumSiblings[168] = 0;  # Mrs. William Skoog\r\nnumSpouses[168] = 1;  # Mrs. William Skoog\r\nnumSiblings[361] = 0;  # Mr. Wilhelm Skoog\r\nnumSpouses[361] = 1;  # Mr. Wilhelm Skoog\r\nnumSiblings[679] = 0;  # Mrs. Frederick Goodwin\r\nnumSpouses[679] = 1;  # Mrs. Frederick Goodwin\r\nnumSiblings[1031] = 0;  # Mr. Charles Frederick Goodwin\r\nnumSpouses[1031] = 1;  # Mr. Charles Frederick Goodwin\r\nnumSiblings[557] = 0;  # Lady. Duff Gordon\r\nnumSpouses[557] = 1;  # Lady. Duff Gordon\r\nnumSiblings[600] = 0;  # Sir. Duff Gordon\r\nnumSpouses[600] = 1;  # Sir. Duff Gordon\r\nnumSiblings[746] = 0;  # Capt. Edward Gifford Crosby\r\nnumSpouses[746] = 1;  # Capt. Edward Gifford Crosby\r\nnumSiblings[1197] = 0;  # Mrs. Edward Gifford Crosby\r\nnumSpouses[1197] = 1;  # Mrs. Edward Gifford Crosby\r\nnumSiblings[1059] = 3;  # Mr. Edward Watson Ford\r\nnumSpouses[1059] = 0;  # Mr. Edward Watson Ford\r\n\r\ncounter = 0;\r\nfor (thisRow in idxSibSp){\r\n  counter = counter + 1;\r\n  numSibSp = data_combined[thisRow,\u0022SibSp\u0022];\r\n  numParch = data_combined[thisRow,\u0022Parch\u0022];\r\n  thisSurname = data_combined[thisRow,\u0022Surname\u0022];\r\n  thisTicket = data_combined[thisRow,\u0022Ticket\u0022];\r\n  thisTitle = as.character( data_combined[thisRow,\u0022Title\u0022] );\r\n  thisAge = data_combined[thisRow,\u0022Age\u0022];\r\n  thisSex = as.character( data_combined[thisRow,\u0022Sex\u0022] );\r\n  thisGivenName = as.character( data_combined[thisRow,\u0022GivenName\u0022] );\r\n  thisPassengerId = data_combined[thisRow,\u0022PassengerId\u0022];\r\n  numSiblings[thisRow] = numSibSp - numSpouses[thisRow];\r\n}\r\n\r\ndata_combined$Spouses = numSpouses;\r\ndata_combined$SpousesFactor = factor( numSpouses );\r\ndata_combined$Siblings = numSiblings;\r\n```\r\n\r\n#### FarePerPassenger\r\nAs determined in the EDA section, the Fare variable is not per passenger, it is the price paid for the ticket the passenger is traveling under (and multiple passengers may travel on the same ticket).  \r\n```{r feature11}\r\nticketChar = as.character( data_combined$Ticket );\r\nuniqueTickets = unique( ticketChar );\r\nfarePerPassenger = data_combined$Fare;\r\nfor (i in 1:length(uniqueTickets) ){\r\n    thisTicket = uniqueTickets[i];\r\n    idxPeople = which( ticketChar == thisTicket );\r\n    theseFares = data_combined[idxPeople,\u0022Fare\u0022];\r\n    #if ( !all( theseFares == theseFares[1] ) ){\r\n    #    cat( thisTicket )\r\n    #}\r\n    # should only be a single fare\r\n    \r\n    numPeople = length( idxPeople );\r\n    farePerPassenger[idxPeople] = theseFares[1]/numPeople;\r\n}\r\ndata_combined$FarePerPassenger = farePerPassenger;\r\n```\r\n\r\n***\r\n## Models\r\nThis is a classification problem with two classes: { Died, Survived }.  This is encoded in the \u0027Survived\u0027 variable in the train dataset. \r\n\r\n```{r setup, include=FALSE}\r\nknitr::opts_chunk$set(echo = TRUE, fig.align=\u0022center\u0022)\r\n```\r\n\r\n\u003c!-- Globals --\u003e\r\n\u003c!-- Data set --\u003e\r\n\r\n```{r include = FALSE}\r\nMAX_HOURS = 10;\r\nlibrary( assertthat );\r\ndata_combined$pclass \u003c- as.factor(data_combined$Pclass);\r\n\r\n# A bit about R data types (e.g., factors)\r\nsummary(data_combined );\r\n\r\n```\r\n\r\n### Null Model\r\n```{r null1, echo=FALSE}\r\nnumPassengers = length(train$Survived);\r\ndied = sum( train$Survived == 0);\r\nsurvived = sum( train$Survived == 1);\r\n#assert_that( died + survived == numPassengers);\r\nmean_survived = survived/numPassengers;\r\ncat( \u0022Of \u0022, numPassengers, \u0022 passengers, \u0022, died, \u0022 died and \u0022, survived, \u0022 survived.\\nMean Survivability = \u0022, survived/numPassengers );\r\n```\r\nThe simplest model would be to assume that all test subjects are members of the most common class in the train dataset.  In the train dataset, 62% of the subjects died, so this simple model would assume that all the test passengers die.  This would yield a prediction accuracy of about 62%, and a corresponding misclassification rate of 38%, all false negatives.\r\n\r\nThis simple model is also the fastest to implement and serves as a good initial benchmark from which to improve upon.\r\n\r\nUpon submission to the Kaggle site, the model yielded a score of 0.62679.  In terms of the bias/variance tradeoff, this is a very biased model (very inflexible) and the variance will likely be very small (i.e., the choice of training set will not affect predictions on the test set except in rare cases where more survivors are selected than non-survivors).\r\n\r\n```{r null2, echo=FALSE}\r\n# Time strip\r\nnullModelTrain = data.frame(PassengerId=train$PassengerId, Survived=0);\r\nnullModel = data.frame(PassengerId=test$PassengerId, Survived=0);\r\nwrite.csv( nullModelTrain, \u0022NullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nullModel, \u0022NullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nnullModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=342/891);\r\nwrite.csv( nullModelTrainProbs, \u0022NullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nnullModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=342/891);\r\nwrite.csv( nullModelTestProbs, \u0022NullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\n#hours = 1;\r\n#par(mfrow=c(2,1));\r\n#barplot( hours, width=0.9, main=\u0022Time\u0022, xlab=\u0022hours\u0022, ylim=c(0,1), xlim=c(0,MAX_HOURS), horiz=TRUE );\r\n#bp = ggplot(data=hours,aes(x=hours,y=1)) + geom_bar(stat=\u0022identity\u0022) + coord_flip();\r\n#bp\r\n### Bias-Variance Strip\r\n\r\n```\r\n\r\n### Gender Submission Model\r\nAlong with the train.csv and test.csv file, Kaggle provides an additional file called \u0027gender_submission.csv\u0027.  This file is a sample submission to Kaggle in which all the female passengers are predicted to survive while all of the male passengers are predicted to die.\r\n\r\n\r\n```{r genderSubmissionModel, echo=FALSE}\r\nfemaleIndices = which( train$Sex == \u0027female\u0027 );\r\nmaleIndices = which( train$Sex == \u0027male\u0027 );\r\nnumFemales = length( femaleIndices );\r\nnumMales = length( maleIndices );\r\nproportionFemaleSurvival = length( intersect( femaleIndices, which( train$Survived == 1)))/numFemales;\r\nproportionMaleSurvival = length( intersect( maleIndices, which( train$Survived == 1)))/numMales;\r\n\r\ngenderModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=0);\r\ngenderModelTrainProbs$Survived[ femaleIndices ] = proportionFemaleSurvival;\r\ngenderModelTrainProbs$Survived[ maleIndices ] = proportionMaleSurvival;\r\nwrite.csv( genderModelTrainProbs, \u0022GenderModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\ngenderModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=0);\r\nwrite.csv( genderModelTestProbs, \u0022GenderModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nIn the training set, of the 891 passengers, 342 are female.  233 of the 342 female passengers in the training set survived (74.2%).  Of the 549 male passengers in the train set, 468 did not survive (81.1%).  This yields an overall classification rate of 78.7% for the test set.  On the bias/variance continuum, this model is very biased and should have small variance.\r\n\r\nUpon submission to Kaggle, this model scored 0.76555.\r\n\r\n### Women and Children First Model\r\nThe canonical model for a sinking ship is that women and children get preferential access to lifeboats while men are expected to defer.  If 100% accordance to this model is assumed and all women and children are boarded onto lifeboats while the men are left to fight it out for flotsam in the frigid waters, women and children would be expected to survive while the men would perish.\r\n\r\nSince the Age variable is missing for 177 of the train set cases and 86 test set cases, the imputed Age values from above will be used.  For this application, a child will be defined as a person under 15 years of age.\r\n```{r womenAndChildrenFirstTrain, echo=FALSE}\r\nlibrary(caret, quietly=TRUE);\r\nfemaleIndices = which( train$Sex == \u0027female\u0027 );\r\ndata_combined[[\u0022SurvivedFactor\u0022]] = factor(data_combined[[\u0022Survived\u0022]]);\r\nwhiten = preProcess(data_combined, c(\u0022center\u0022,\u0022scale\u0022,\u0022BoxCox\u0022) );\r\nwhitened_data = data.frame( predict( whiten, data_combined ));\r\n# now \u0022unwhiten\u0022 some data like Survived and PassengerId\r\nwhitened_data$PassengerId = data_combined$PassengerId;\r\nwhitened_data$Survived = data_combined$Survived;\r\nnewTrain = subset( data_combined, PassengerId \u003c= nrow(train) );\r\nnewTest = subset( data_combined, PassengerId \u003e nrow(train) );\r\nwhiteTrain = subset( whitened_data, PassengerId \u003c= nrow(train) );\r\nwhiteTest = subset( whitened_data, PassengerId \u003e nrow(train) );\r\nchildIndices = which( newTrain$FixedAge \u003c 15 );\r\nwomenOrChildren = union( femaleIndices, childIndices );\r\nmaleIndices = setdiff( 1:nrow(train), womenOrChildren );\r\nwomenOrChildrenSurvived = which( train[ womenOrChildren, \u0022Survived\u0022] == 1 );\r\nmenDied = which( train[ maleIndices, \u0022Survived\u0022] == 0);\r\nnumFemales = length( femaleIndices );\r\nnumMales = length( maleIndices );\r\ntrainProportionCorrect = ( length(womenOrChildrenSurvived) + length(menDied))/nrow(train);\r\n```\r\n\r\nIn the training set, the proportion of correct predictions is 0.79.  \r\n\r\n```{r womenAndChildrenFirstTest, echo=FALSE}\r\n# start with all perished model, then set women and children to survived\r\nwomenAndChildrenFirstModelTrain = data.frame(PassengerId=train$PassengerId, Survived=0);\r\nwomenAndChildrenFirstModel = data.frame(PassengerId=test$PassengerId, Survived=0);\r\nnewTest = subset( data_combined, PassengerId \u003e nrow(train) );\r\nfemaleIndices = which( newTest$Sex == \u0027female\u0027 );\r\ntrainFemaleIndices = which( newTrain$Sex == \u0027female\u0027 );\r\nchildIndices = which( newTest$FixedAge \u003c 15 );\r\ntrainChildIndices = which( newTrain$FixedAge \u003c 15 );\r\nwomenOrChildren = union( femaleIndices, childIndices );\r\nwomenOrChildrenTrain = union( trainFemaleIndices, trainChildIndices );\r\nwomenAndChildrenFirstModelTrain[womenOrChildrenTrain,\u0022Survived\u0022] = 1;\r\nwomenAndChildrenFirstModel[womenOrChildren,\u0022Survived\u0022] = 1;\r\nwrite.csv( womenAndChildrenFirstModelTrain, \u0022WomenAndChildrenFirstModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( womenAndChildrenFirstModel, \u0022WomenAndChildrenFirstModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\nprobWCSurvived = mean(newTrain$Survived[ womenOrChildrenTrain ]);\r\nprobMenSurvived = mean(newTrain$Survived[ -womenOrChildrenTrain ]);\r\nwcModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=newTrain$Survived);\r\nwcModelTrainProbs$Survived[womenOrChildrenTrain] = probWCSurvived;\r\nwcModelTrainProbs$Survived[-womenOrChildrenTrain] = probMenSurvived;\r\nwrite.csv( wcModelTrainProbs, \u0022WCModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwcModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=0);\r\nwcModelTestProbs$Survived[womenOrChildren] = probWCSurvived;\r\nwcModelTestProbs$Survived[-womenOrChildren] = probMenSurvived;\r\nwrite.csv( wcModelTestProbs, \u0022WCModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nUpon submission to Kaggle, the Women and Children First model scored 0.76076.\r\n\r\n### Linear Discriminant Analysis (LDA) Models\r\n\r\nIn Linear Discriminant Analysis, the distributions of each of the predictors (Passenger Class, Age, Sex, etc.) are modeled separately for the Survived and Died classes.  Then, Bayes\u0027 theorem can be flipped around to get estimates for the probability of Survived or Died given the set of predictors.  LDA assumes that the observations in each class (Survived, Died) are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix common to both classes.  This assumption does not hold for some of the predictors in this analysis, as not only are they not all normal, some are categorical (Sex, Embarked).  Nevertheless, the implementation in R converts the variables to a numeric albeit not-normal representation.\r\n\r\n#### LDA Model using all features\r\nThe first LDA model is one in which all predictors are considered: pClass, Sex, FixedAge, FarePerPassenger, FixedTitle, CabinAssignment, Embarked, NumPassengersOnTicket, NumParents, NumChildren, SpousesFactor, and Siblings.  Against the training data, the model scored 0.8373 (~84% accuracy).  When run against the test set, the model scored 0.78947.\r\n\r\n```{r lda1, warning=FALSE}\r\nlibrary(MASS, quietly=TRUE);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nlda.fit = lda( SurvivedFactor ~., data=xTrain);\r\nlda.fit\r\nplot( lda.fit );\r\nlda.train.pred = predict( lda.fit, xTrain );  \r\nldaTrainPredictions = as.numeric( lda.train.pred$class ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( ldaTrainPredictions, truth );\r\n# scored a 0.8373 against the training data\r\nlda.test.pred = predict( lda.fit, xTest );  \r\n# lda.pred$class is a factor; need to convert to 0, 1\r\nldaPredictions = as.numeric( lda.test.pred$class ) - 1;\r\nldaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=ldaTrainPredictions);\r\nldaModel = data.frame(PassengerId=test$PassengerId, Survived=ldaPredictions);\r\n\r\nwrite.csv( ldaModelTrain, \u0022LDAModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModelTrain, \u0022LDAModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModel, \u0022LDAModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModel, \u0022LDAModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.78947\r\n```\r\n\r\n#### LDA Model with only significant features\r\nIn subsequent analyses (logistic regression, random forests), the predictors are measured for effect on the response variable.  The logistic regression analysis indicates which covariates have a low p-value (i.e., \u003c0.05), indicating a significant effect.  Similarly, random forests produce a variable importance measure which shows which covariates are used most often in tree generation.  These measures give a good idea of which variables are likely to have the most predictive power.  Using these variables and leaving out the variables with higher p-values and those not often used in random forests will likely reduce overfitting.  The variables with the best predictive power in this set are pClass, Sex, FixedAge, FarePerPassenger, FixedTitle, CabinAssignment, NumChildren, SpousesFactor, and Siblings.\r\n\r\nThe LDA model using only significant features scored a 0.8339 against the training data and, similar to the full-featured LDA model, scored a 0.78947.\r\n\r\n```{r lda2, echo=TRUE, warning=FALSE}\r\nlibrary(MASS);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nlda.fit = lda( SurvivedFactor ~., data=xTrain);\r\nplot( lda.fit );\r\nlda.train.pred = predict( lda.fit, xTrain );  \r\nldaTrainPredictions = as.numeric( lda.train.pred$class ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( ldaTrainPredictions, truth );\r\n# scored a 0.8339 against the training data\r\nlda.test.pred = predict( lda.fit, xTest );  \r\n# lda.pred$class is a factor; need to convert to 0, 1\r\nldaPredictions = as.numeric( lda.test.pred$class ) - 1;\r\nldaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=ldaTrainPredictions);\r\nldaModel = data.frame(PassengerId=test$PassengerId, Survived=ldaPredictions);\r\n\r\nwrite.csv( ldaModelTrain, \u0022LDAReducedModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModelTrain, \u0022LDAReducedModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModel, \u0022LDAReducedModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( ldaModel, \u0022LDAReducedModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.78947\r\n```\r\n\r\n### Quadratic Discriminant Analysis (QDA) Models\r\n\r\nQuadratic Discriminant Analysis (QDA) is very much similar to LDA but with one difference: there is no assumption that the classes have a common variance.  In this model, each class has its own covariance matrix.  QDA is so named because the discriminant functions are quadratic in the predictor vector, as opposed to linear with LDA.  Since the discriminant function is quadratic, the decision boundary is not constrained to a line as with LDA and can assume curved shapes.\r\n\r\n#### QDA Model with all features\r\n\r\nThe QDA model with all variables scored a 0.8159 against the training set and 0.77033 against the test data.\r\n\r\n```{r qda1, echo=TRUE, warning=FALSE}\r\nlibrary(MASS);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nqda.fit = qda( SurvivedFactor ~., data=xTrain);\r\nqda.train.pred = predict( qda.fit, xTrain );  \r\nqdaTrainPredictions = as.numeric( qda.train.pred$class ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( qdaTrainPredictions, truth );\r\n# scored a 0.8159 against the training data\r\nqda.test.pred = predict( qda.fit, xTest );  \r\n# lda.pred$class is a factor; need to convert to 0, 1\r\nqdaPredictions = as.numeric( qda.test.pred$class ) - 1;\r\nqdaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=qdaTrainPredictions);\r\nqdaModel = data.frame(PassengerId=test$PassengerId, Survived=qdaPredictions);\r\n\r\nwrite.csv( qdaModelTrain, \u0022QDAModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModelTrain, \u0022QDAModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModel, \u0022QDAModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModel, \u0022QDAModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.77033\r\n```\r\n\r\n#### QDA Model with only significant features\r\n\r\nThe QDA model using only significant features scored a 0.7845 against the training set and 0.78947 against the test data.\r\n\r\n```{r qda2, echo=TRUE, warning=FALSE}\r\nlibrary(MASS);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022NumChildren\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022NumChildren\u0022,\u0022Siblings\u0022)];\r\nqda.fit = qda( SurvivedFactor ~., data=xTrain);\r\nqda.train.pred = predict( qda.fit, xTrain );  \r\nqdaTrainPredictions = as.numeric( qda.train.pred$class ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( qdaTrainPredictions, truth );\r\n# scored a 0.7845 against the training data\r\nqda.test.pred = predict( qda.fit, xTest );  \r\n# lda.pred$class is a factor; need to convert to 0, 1\r\nqdaPredictions = as.numeric( qda.test.pred$class ) - 1;\r\nqdaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=qdaTrainPredictions);\r\nqdaModel = data.frame(PassengerId=test$PassengerId, Survived=qdaPredictions);\r\n\r\nwrite.csv( qdaModelTrain, \u0022QDASigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModelTrain, \u0022QDASigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModel, \u0022QDASigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( qdaModel, \u0022QDASigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.78947\r\n```\r\n### Logistic Regression Models\r\n\r\nIn a linear regression model, the probability of the response variable is modeled as a linear combination of predictors and multiplicative coefficients.  The encoding of the response variable (Survived) as 0 and 1 allows for a linear regression model but covariates that produce a response greater than 1 and less than 0 are considered errors and can negatively impact parameter estimation.  In logistic regression, the probability of the response variable is modeled with the logistic function, or y = e^X/(1+e^X).  Logistic regression is a classic model for binary classification.\r\n\r\n#### Logistic Regression using all features\r\nA logistic regression model is fit using the train data.  Selected variables are shown below.  Some variables could be treated as either numeric or factors (i.e., NumPassengersOnTicket and NumChildren).  Because there was a natural ordering of these variables, they are treated as numeric values rather than factors.  This was done to capture the linear dependence of each variable to the survival response.\r\n\r\n```{r logisticRegression1, echo=TRUE, warning=FALSE}\r\nall_features = c(\u0022pclass\u0022, \u0022Sex\u0022, \u0022FixedAge\u0022, \u0022FarePerPassenger\u0022, \u0022FixedTitle\u0022, \u0022Siblings\u0022, \u0022CabinAssignment\u0022, \u0022Embarked\u0022, \u0022NumPassengersOnTicket\u0022, \u0022NumParents\u0022, \u0022NumChildren\u0022, \u0022SpousesFactor\u0022);\r\nfull_df = newTrain[, c(\u0022Survived\u0022, all_features)];\r\nfull.glm.fit = glm( Survived~., data=full_df, family=binomial);\r\nsummary( full.glm.fit );\r\ntrainSetProbs = predict( full.glm.fit, newTrain, type=\u0022response\u0022);  # now a vector of probabilities\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\n# training set performance\r\nthreshold = 0.5;\r\npredictions = as.numeric( trainSetProbs \u003e= threshold );\r\nconfusionMatrix( predictions, newTrain$Survived );\r\n\r\n# now, how\u0027d we do against the test set?\r\ntestSetProbs = predict( full.glm.fit, newTest, type=\u0022response\u0022);\r\nfullLogisticPredictions = as.numeric( testSetProbs \u003e= threshold );\r\nfullLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);\r\nfullLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullLogisticPredictions);\r\n\r\nfullLogisticRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=trainSetProbs);\r\nfullLogisticRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=testSetProbs);\r\n\r\nwrite.csv( fullLogisticRegressionModelTrainProbs, \u0022LogisticRegressionFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLogisticRegressionModelProbs, \u0022LogisticRegressionFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\nwrite.csv( fullLogisticRegressionModelTrain, \u0022LogisticRegressionFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLogisticRegressionModel, \u0022LogisticRegressionFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.78947 - not expected\r\n```\r\n\r\nThe logistic regression model was first fit with all 12 possible covariates. After fitting the model to the training data, the model was run on the training set yielding a training set test error of 0.1661055 (0.8338945 accuracy rate).  The model had an overall accuracy of 0.78947 on the test set (the score from Kaggle).  Since this was lower than the expected score (the training set yielded a score of 0.8338945), it appears the model may be overfitting the training data.  \r\n\r\n```{r logisticRegression2, echo=TRUE, warning=FALSE}\r\nlibrary(caret);\r\nlibrary(lattice);\r\ncat( \u0022Training Set performance:\\n\u0022);\r\nconfusionMatrix( predictions, newTrain$Survived);\r\n```\r\n\r\nBecause the logistic regression model with all 12 predictors has a test set error somewhat larger than the training set error, the model is likely fitting noise in the training set.  It\u0027s very likely that some of the features are co-linear, for instance, Passenger Class (pclass) and FarePerPassenger.  If the number of predictors can be reduced, the variance of the model can likely be lowered.  After running the full set of predictors, the model shows several variables as significant: FixedTitle, Sex, and Siblings.  Other variables that show nearly significant correlation to the response are FixedAge, pclass, CabinAssignment, and NumChildren.  If those variables are brought forward and tested separately in different combinations, a model with a lower cross validation error might result.  As a final measure, a separate model was run with the remaining \u0022leftover\u0022 features: FarePerPassenger, Embarked, NumPassengersOnTicket, SpousesFactor, and NumParents.  The idea is to see if any significant features can be determined in the absence of the significant features of the full model.\r\n\r\n```{r logisticRegression3, echo=TRUE, warning=FALSE}\r\nleftoverFeatures = c(\u0022FarePerPassenger\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022SpousesFactor\u0022,\u0022NumParents\u0022);\r\nleftover_df = newTrain[, c(\u0022Survived\u0022, leftoverFeatures)];\r\nleftover.glm.fit = glm( Survived ~., data = leftover_df, family=binomial);\r\nsummary(leftover.glm.fit);\r\n```\r\n\r\n#### Logistic Regression using only significant features\r\n\r\nThe logistic regression model with the \u0022leftover\u0022 features yielded two new feature possibilities: FarePerPassenger and NumParents.  These features are likely proxies for passenger class (pclass) and age (FixedAge).  i.e., passengers travelling with their parents are more likely to be children and thus, have a higher survival probability.  Similarly, FarePerPassenger is likely to reflect the passenger class separation.  However, unlike pclass, FarePerPassenger is a continuous variable and possibly more informative than the three passenger classes.  So, combined with the significant features from the full logistic regression model, a set of features can be drawn upon in different combinations in order to select the model with the lowest cross validation error.\r\n\r\n```{r logisticRegression4, echo=TRUE, warning=FALSE}\r\nlibrary(boot, quietly = TRUE);\r\nsignificant_features = c(\u0022FixedTitle\u0022, \u0022Sex\u0022, \u0022Siblings\u0022, \u0022FixedAge\u0022, \u0022CabinAssignment\u0022, \u0022pclass\u0022, \u0022NumChildren\u0022, \u0022FarePerPassenger\u0022, \u0022Embarked\u0022);\r\nall_combinations_of_significant_features = lapply(1:length(significant_features), function(x) combn(length(significant_features),x));\r\nnumFeatureSetClasses = length( all_combinations_of_significant_features );\r\ncorrect = list();\r\naccuracies = list();\r\nerrors = list();\r\nfeatures = list();\r\nnumFeatures = list();\r\ncounter = 1;\r\nfor ( featureClass in 1:numFeatureSetClasses){\r\n  featClassColumns = dim( all_combinations_of_significant_features[[featureClass]] )[2];\r\n  for ( thisCol in 1:featClassColumns ){\r\n    feature_set = significant_features[ all_combinations_of_significant_features[[featureClass]][,thisCol]];\r\n    nFeatures = length( feature_set );\r\n    df = newTrain[, c(\u0022Survived\u0022, feature_set)]\r\n    glm.fit = glm( Survived~.,data=df,family=binomial);\r\n    cv.glm.fit = cv.glm(df,glm.fit,K=10);\r\n    cv.error = cv.glm.fit$delta[1];\r\n    trainSetProbs = predict( glm.fit, newTrain, type=\u0022response\u0022);  # now a vector of probabilities\r\n    predictions = as.numeric( trainSetProbs \u003e= threshold );\r\n    numRight = sum( predictions == newTrain$Survived );\r\n    accuracy = mean( predictions == newTrain$Survived );\r\n    features[counter] = list( feature_set );\r\n    numFeatures[counter] = nFeatures;\r\n    accuracies[counter] = accuracy;\r\n    correct[counter] = numRight;\r\n    errors[counter] = cv.error;\r\n    counter = counter + 1;\r\n  }\r\n}\r\ncvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), Features=I(features) );\r\ncvResults = cvResults[order(cvResults$CVError),];\r\nknitr::kable( head(cvResults,n=20L), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nOf the 512 different feature combinations, the top 20 are listed.  The cross validation error estimates were made using 10-fold cross validation.  The top 10 are retested using LOOCV:\r\n\r\n```{r logisticRegression5, echo=TRUE, warning=FALSE}\r\ncorrect = list();\r\naccuracies = list();\r\nerrors = list();\r\nfeatures = list();\r\nnumFeatures = list();\r\ncounter = 1;\r\nfor ( rowNum in 502:511){\r\n    feature_set = cvResults$Features[[rowNum]];\r\n    nFeatures = cvResults$NumFeatures[rowNum];\r\n    df = newTrain[, c(\u0022Survived\u0022, feature_set)]\r\n    glm.fit = glm( Survived~.,data=df,family=binomial);\r\n    cv.glm.fit = cv.glm(df,glm.fit);\r\n    cv.error = cv.glm.fit$delta[1];\r\n    trainSetProbs = predict( glm.fit, newTrain, type=\u0022response\u0022);  # now a vector of probabilities\r\n    predictions = as.numeric( trainSetProbs \u003e= threshold );\r\n    numRight = sum( predictions == newTrain$Survived );\r\n    accuracy = mean( predictions == newTrain$Survived );\r\n    features[counter] = list( feature_set );\r\n    numFeatures[counter] = nFeatures;\r\n    accuracies[counter] = accuracy;\r\n    correct[counter] = numRight;\r\n    errors[counter] = cv.error;\r\n    counter = counter + 1;\r\n}\r\nloocvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), Features=I(features) );\r\nloocvResults = loocvResults[order(loocvResults$CVError),];\r\nknitr::kable( head(loocvResults,n=10L), format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\nThe model with the lowest cross validation error is a model with 8 features: FixedTitle, Sex, Siblings, FixedAge, CabinAssignment, pclass, NumChildren, and Embarked.  A logistic regression model is fit using these features and submitted to Kaggle.\r\n\r\n```{r logisticRegression6, echo=TRUE, warning=FALSE}\r\nbest_features = c(\u0022pclass\u0022, \u0022Sex\u0022, \u0022FixedAge\u0022, \u0022FixedTitle\u0022, \u0022Siblings\u0022, \u0022CabinAssignment\u0022, \u0022Embarked\u0022, \u0022NumChildren\u0022 );\r\nbest_df = newTrain[, c(\u0022Survived\u0022, best_features)];\r\nbest.glm.fit = glm( Survived~., data=best_df, family=binomial);\r\ntrainSetProbs = predict( best.glm.fit, newTrain, type=\u0022response\u0022);  # now a vector of probabilities\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nthreshold = 0.5;\r\npredictions = as.numeric( trainSetProbs \u003e= threshold );\r\nsummary( best.glm.fit );\r\n\r\n# How\u0027d we do?\r\n# now calculate the training set error\r\ncorrectRate = mean( predictions == newTrain$Survived );\r\n# scored a 0.8395062\r\n\r\n# now, how\u0027d we do against the test set?\r\ntestSetProbs = predict( best.glm.fit, newTest, type=\u0022response\u0022);\r\nbestTestPredictions = as.numeric( testSetProbs \u003e= threshold );\r\nbestLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);\r\nbestLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=bestTestPredictions);\r\n\r\nbestLogisticRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=trainSetProbs);\r\nbestLogisticRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=testSetProbs);\r\n\r\nwrite.csv( bestLogisticRegressionModelTrainProbs, \u0022LogisticRegressionSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLogisticRegressionModelProbs, \u0022LogisticRegressionSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\nwrite.csv( bestLogisticRegressionModelTrain, \u0022LogisticRegressionSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLogisticRegressionModel, \u0022LogisticRegressionSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.77990 - not as good as the full model\r\n```\r\nThe model based on significant features scored lower on the test set than the full model.  To reduce the model fit to noise in the training data, the regularization methods of ridge regression and lasso are considered.\r\n\r\n### Logistic Regression with Ridge Regression Regularization\r\nRidge regression introduces a coefficient penalty that reduces the effect of the covariates.  This can help with variance by making the solution slightly less flexible.  The glmnet package includes a method for determining the lambda penalty coefficient using cross validation (default is 10-fold cross validation).\r\n\r\n\u003c!--![](http://businessforecastblog.com/wp-content/uploads/2014/01/RRminization.png)--\u003e\r\n![](../RRminimization.png)\r\n\r\n#### Logistic Ridge Regression on full feature set\r\n```{r ridgeRegression1, echo=TRUE, warning=FALSE}\r\nlibrary( glmnet, quietly = TRUE );\r\nlambdas = c();\r\nset.seed(1);\r\n# use ridge regression\r\nxTrain = model.matrix( Survived~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,family=binomial);\r\nxTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest,family=binomial);\r\n#xTrain = model.matrix( Survived~.,data=full_df,family=binomial);\r\nfull_df_test = newTest[, all_features];\r\n#xTest = model.matrix( ~.,data=full_df_test,family=binomial);\r\ncv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=0, type.measure=\u0022class\u0022);\r\nplot( cv.out );\r\nlambdaMin = cv.out$lambda.min;\r\nrrmodel = glmnet( xTrain, newTrain$Survived, alpha=0);\r\ntrain.ridge.pred = predict( rrmodel, s=lambdaMin, newx=xTrain);\r\ntrainRidgePredictions = as.numeric( train.ridge.pred \u003e= threshold );\r\nnumCorrect = sum(trainRidgePredictions == newTrain$Survived);\r\naccuracy = numCorrect/nrow(newTrain);\r\nridge.pred = predict( rrmodel, s=lambdaMin, newx=xTest);\r\nfullRidgePredictions = as.numeric( ridge.pred \u003e= threshold );\r\nfullRidgeRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainRidgePredictions);\r\nfullRidgeRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullRidgePredictions);\r\nfullRidgeRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=train.ridge.pred);\r\nfullRidgeRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=ridge.pred);\r\nwrite.csv( fullRidgeRegressionModelTrainProbs, \u0022RidgeRegressionFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullRidgeRegressionModelProbs, \u0022RidgeRegressionFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullRidgeRegressionModelTrain, \u0022RidgeRegressionFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullRidgeRegressionModel, \u0022RidgeRegressionFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# score is 0.78947\r\n```\r\n\r\nThe training set mean squared error is the smallest at a lambda value of 0.02931267.  Against the training data, the model achieved an accuracy of 0.8226712.\r\n\r\n```{r ridgeRegression1b, echo=TRUE}\r\nconfusionMatrix(trainRidgePredictions, newTrain$Survived);\r\n```\r\n\r\nWhen comparing to the full logistic regression solution, the ridge predictions are the same as the least-squares logistic regression in 398 of the 418 cases (95.2% of the test set).  The model scored a 0.78468 upon submission to the Kaggle site - slightly less than the full logistic regression solution (see comparison below).\r\n\r\n```{r ridgeRegression2, echo=TRUE}\r\nfullLogisticPredictions = bestTestPredictions;\r\nconfusionMatrix( fullRidgePredictions, fullLogisticPredictions );\r\n```\r\nNext, a ridge regression model is fit using only the significant features determined from the logistic regression solution.  From the training data, lambda was determined to be 0.02931267.  Against the training data, the model achieved an accuracy of 0.8249158.\r\n\r\n#### Logistic Ridge Regression on significant features\r\n\r\n```{r ridgeRegression3, echo=TRUE, warning=FALSE}\r\nlibrary( glmnet );\r\nlambdas = c();\r\nset.seed(1);\r\n# use ridge regression\r\nxTrain = model.matrix( Survived~.,data=best_df,family=binomial);\r\nbest_df_test = newTest[, best_features];\r\nxTest = model.matrix( ~.,data=best_df_test,family=binomial);\r\ncv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=0, type.measure=\u0022class\u0022);\r\nplot( cv.out );\r\nlambdaMin = cv.out$lambda.min;\r\nrrmodel = glmnet( xTrain, newTrain$Survived, alpha=0);\r\ntrain.ridge.pred = predict( rrmodel, s=lambdaMin, newx=xTrain);\r\ntrainRidgePredictions = as.numeric( train.ridge.pred \u003e= threshold );\r\nnumCorrect = sum(trainRidgePredictions == newTrain$Survived);\r\naccuracy = numCorrect/nrow(newTrain);\r\nridge.pred = predict( rrmodel, s=lambdaMin, newx=xTest);\r\nbestRidgePredictions = as.numeric( ridge.pred \u003e= threshold );\r\nbestRidgeRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainRidgePredictions);\r\nbestRidgeRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=bestRidgePredictions);\r\n\r\nbestRidgeRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.ridge.pred));\r\nbestRidgeRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(ridge.pred));\r\nwrite.csv( bestRidgeRegressionModelTrainProbs, \u0022RidgeRegressionSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestRidgeRegressionModelProbs, \u0022RidgeRegressionSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\nwrite.csv( bestRidgeRegressionModelTrain, \u0022RidgeRegressionSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestRidgeRegressionModel, \u0022RidgeRegressionSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nWhen comparing to the full logistic regression solution, the best feature ridge predictions are the same as the least-squares logistic regression in 399 of the 418 cases (95.4% of the test set).  The model scored a 0.77990 upon submission to the Kaggle site - which is less than both the full logistic regression and full ridge regression models.\r\n\r\n```{r ridgeRegression4, echo=TRUE}\r\nfullLogisticPredictions = bestTestPredictions;\r\nconfusionMatrix( bestRidgePredictions, fullLogisticPredictions );\r\n```\r\n\r\n\r\n### Logistic Regression with Lasso Regularization\r\nSimilar to ridge regression, lasso regularization introduces a penalty to reduce the magnitude of coefficient estimates.  As with ridge regression, the glmnet implementation of the lasso uses cross validation to estimate the best value of lambda.  The Lasso regularization method multiplies lambda with the absolute value of the coefficients as the penalty and, unlike Ridge Regression, can be used to eliminate features altogether.\r\n\r\n\u003c!--![](http://www.statisticshowto.com/wp-content/uploads/2015/09/lasso-regression.png)--\u003e\r\n![](../lasso-regression.png)\r\n\r\n#### Lasso using all features\r\n```{r lassoRegression, echo=TRUE, warning=FALSE}\r\n# on to the lasso\r\nxTrain = model.matrix( Survived~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,family=binomial);\r\nxTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest,family=binomial);\r\ncv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=1, type.measure = \u0022class\u0022);\r\nplot( cv.out );\r\nlambdaMin = cv.out$lambda.min;\r\nlassomodel = glmnet( xTrain, newTrain$Survived, alpha=1);\r\ntrain.lasso.pred = predict( lassomodel, s=lambdaMin, newx=xTrain);\r\ntrainLassoPredictions = as.numeric( train.lasso.pred \u003e= threshold );\r\nnumCorrect = sum(trainLassoPredictions == newTrain$Survived);\r\naccuracy = numCorrect/nrow(newTrain);\r\nlasso.pred = predict( lassomodel, s=lambdaMin, newx=xTest);\r\nlassoPredictions = as.numeric( lasso.pred \u003e= threshold );\r\nfullLassoRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainLassoPredictions);\r\nfullLassoRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=lassoPredictions);\r\nfullLassoRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.lasso.pred));\r\nfullLassoRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(lasso.pred));\r\nwrite.csv( fullLassoRegressionModelTrainProbs, \u0022LassoRegressionFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLassoRegressionModelProbs, \u0022LassoRegressionFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLassoRegressionModelTrain, \u0022LassoRegressionFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullLassoRegressionModel, \u0022LassoRegressionFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nIn the plot, as Lambda increases, the number of features in the model is reduced (see top axis).  In this application, no features were eliminated.  The Lasso model using all of the features had a training set error of 0.1638608 with a lambda of 0.00160114 (see confusion matrix below).  \r\n\r\n```{r lassoRegression2, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( trainLassoPredictions, newTrain$Survived );\r\n```\r\n\r\nWhen comparing to the full logistic regression model against the test data, the lasso model differed from the full logistic regression model in 11 cases, predicting the same outcome in 407 of 419 rows.  Against the Kaggle test data, the model scored 0.78947.\r\n\r\n```{r lassoRegression3, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( lassoPredictions, fullLogisticPredictions );\r\n```\r\n\r\n#### Lasso on significant features\r\nThe lasso model with only the significant features achieved a training set error of 0.1627385 using a lambda of 0.0002067951.\r\n```{r lassoRegression4, echo=TRUE, warning=FALSE}\r\n# on to the lasso\r\nxTrain = model.matrix( Survived~.,data=best_df,family=binomial);\r\nbest_df_test = newTest[, best_features];\r\nxTest = model.matrix( ~.,data=best_df_test,family=binomial);\r\ncv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=1, type.measure = \u0022class\u0022);\r\nplot( cv.out );\r\nlambdaMin = cv.out$lambda.min;\r\nlassomodel = glmnet( xTrain, newTrain$Survived, alpha=1);\r\ntrain.lasso.pred = predict( lassomodel, s=lambdaMin, newx=xTrain);\r\ntrainLassoPredictions = as.numeric( train.lasso.pred \u003e= threshold );\r\nnumCorrect = sum(trainLassoPredictions == newTrain$Survived);\r\naccuracy = numCorrect/nrow(newTrain);\r\nlasso.pred = predict( lassomodel, s=lambdaMin, newx=xTest);\r\nlassoPredictions = as.numeric( lasso.pred \u003e= threshold );\r\nbestLassoRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainLassoPredictions);\r\nbestLassoRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=lassoPredictions);\r\n\r\nbestLassoRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.lasso.pred));\r\nbestLassoRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(lasso.pred));\r\n\r\nwrite.csv( bestLassoRegressionModelTrainProbs, \u0022LassoRegressionSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLassoRegressionModelProbs, \u0022LassoRegressionSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLassoRegressionModelTrain, \u0022LassoRegressionSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bestLassoRegressionModel, \u0022LassoRegressionSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nThe \u0022best\u0022 feature Lasso model had a training set error of 0.1616162 with a lambda of 0.0005754197 (see confusion matrix below).  \r\n\r\n```{r lassoRegression5, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( trainLassoPredictions, newTrain$Survived );\r\n```\r\n\r\nWhen comparing to the full logistic regression model against the test data, the \u0022best feature\u0022 lasso model differed from the full logistic regression model in 11 cases, predicting the same outcome in 407 of 419 rows.  Against the Kaggle test data, the model scored 0.79425.\r\n\r\n```{r lassoRegression6, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( lassoPredictions, fullLogisticPredictions );\r\n```\r\n\r\nThe best feature lasso model differed from the full logistic regression model in 11 cases and from the best feature ridge regression model in 12 cases (see confusion matrices below).\r\n\r\n```{r lassoRegression7, echo=TRUE, warning=FALSE}\r\nconfusionMatrix( lassoPredictions, fullLogisticPredictions );\r\nconfusionMatrix( bestRidgePredictions, lassoPredictions );\r\n# score is 0.78947\r\n```\r\n\r\n### K-Nearest Neighbors Models\r\n\r\nK-Nearest Neighbors is a conceptionally simple method of assigning the most common class label to a point based on the labels of the closest K points.  The lower the K, the higher the variance (at an extreme, K=1 will assign the class label based on the closest labeled point).  Conversely, the higher the K, the lower the variance and higher the bias.  With K as N-1 as an extreme, each point will be labeled as the most common class.  This method lends itself nicely to cross-validation to find the best value of K.\r\n\r\n#### KNN using all features\r\nUsing Leave One Out Cross Validation, the KNN model was trained using all features.  The model was cross-validated using 20 different K-values from 5 to 43, odd numbered.  The model achieved a training set accuracy of 0.8563 with a K of 9.\r\n\r\n```{r knn1, warning=FALSE}\r\nlibrary(caret);\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022);\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit );\r\ntest_pred = predict( knn_fit, newdata=newTrain);\r\nconfusionMatrix( test_pred, newTrain$SurvivedFactor)\r\n```\r\n\r\nAgainst the Kaggle test data, the model scored 0.76555.\r\n\r\n```{r knn2, echo=TRUE, warning=FALSE}\r\n# now the whole training set\r\nknnTrain = newTrain;\r\nknnTest = newTest;\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nfullKNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nfullKNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( fullKNNModelTrain, \u0022KNNFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullKNNModel, \u0022KNNFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullKNNModelTrain, \u0022KNNFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( fullKNNModel, \u0022KNNFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### KNN with subsets of significant features\r\n\r\nSince some features may not be informative and all features are used in finding closest points, it may be better to consider removing features that just add noise. The idea is to start with all significant features and then consider combinations as subsets for analysis.\r\n\r\n```{r knn3, warning=FALSE}\r\nsignificant_features = c(\u0022FixedTitle\u0022, \u0022Sex\u0022, \u0022Siblings\u0022, \u0022FixedAge\u0022, \u0022CabinAssignment\u0022, \u0022pclass\u0022, \u0022NumChildren\u0022, \u0022FarePerPassenger\u0022, \u0022Embarked\u0022);\r\nall_combinations_of_significant_features = lapply(1:length(significant_features), function(x) combn(length(significant_features),x));\r\nnumFeatureSetClasses = length( all_combinations_of_significant_features );\r\ncorrect = list();\r\naccuracies = list();\r\nerrors = list();\r\nfeatures = list();\r\nnumFeatures = list();\r\nk = list();\r\ncounter = 1;\r\nnumTrain = nrow( newTrain );\r\ntrainCtrl = trainControl( method=\u0022repeatedcv\u0022, number = 8, repeats = 1 );\r\nfor ( featureClass in 1:numFeatureSetClasses){\r\n  featClassColumns = dim( all_combinations_of_significant_features[[featureClass]] )[2];\r\n  for ( thisCol in 1:featClassColumns ){\r\n    feature_set = significant_features[ all_combinations_of_significant_features[[featureClass]][,thisCol]];\r\n    nFeatures = length( feature_set );\r\n    df = newTrain[, c(\u0022SurvivedFactor\u0022, feature_set)]\r\n    knn_fit = train( SurvivedFactor~.,data=df,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\n    knn_pred = predict( knn_fit, newdata=newTrain);\r\n    numRight = sum( knn_pred == newTrain$SurvivedFactor );\r\n    cv.error = ( numTrain - numRight )/numTrain;\r\n    accuracy = mean( knn_pred == newTrain$SurvivedFactor );\r\n    features[counter] = list( feature_set );\r\n    numFeatures[counter] = nFeatures;\r\n    accuracies[counter] = accuracy;\r\n    correct[counter] = numRight;\r\n    errors[counter] = cv.error;\r\n    k[counter] = knn_fit$bestTune$k;\r\n    counter = counter + 1;\r\n  }\r\n}\r\ncvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), K=unlist(k), Features=I(features) );\r\ncvResults = cvResults[order(cvResults$CVError),];\r\nknitr::kable( head(cvResults,n=20L), format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\n\r\nNow, take the top 10 results and repeat the analysis using LOOCV.  LOOCV provides a better estimate of the error but is computationally costly.\r\n\r\n```{r knn4, echo=TRUE, eval=FALSE, warning=FALSE}\r\ncorrect = list();\r\naccuracies = list();\r\nerrors = list();\r\nfeatures = list();\r\nnumFeatures = list();\r\nk = list();\r\ncounter = 1;\r\nnumTrain = nrow( newTrain );\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nfor ( rowNum in 1:10 ){\r\n  feature_set = cvResults$Features[[rowNum]];\r\n  nFeatures = cvResults$NumFeatures[rowNum];\r\n  df = newTrain[, c(\u0022SurvivedFactor\u0022, feature_set)]\r\n  knn_fit = train( SurvivedFactor~.,data=df,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\n  knn_pred = predict( knn_fit, newdata=newTrain);\r\n  numRight = sum( knn_pred == newTrain$SurvivedFactor );\r\n  cv.error = ( numTrain - numRight )/numTrain;\r\n  accuracy = mean( knn_pred == newTrain$SurvivedFactor );\r\n  features[counter] = list( feature_set );\r\n  numFeatures[counter] = nFeatures;\r\n  accuracies[counter] = accuracy;\r\n  correct[counter] = numRight;\r\n  errors[counter] = cv.error;\r\n  k[counter] = knn_fit$bestTune$k;\r\n  counter = counter + 1;\r\n  cat( counter, \u0022\\n\u0022)\r\n}\r\nloocvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), K=unlist(k), Features=I(features) );\r\nloocvResults = loocvResults[order(loocvResults$CVError),];\r\nknitr::kable( head(loocvResults,n=10L), format=\u0022markdown\u0022, longtable=TRUE);\r\n```\r\nNow, take the best 9, 8, 7, and 6 knn models.\r\n\r\n#### KNN with with best subset of 9 significant features\r\n\r\nWith 9 features, the model achieves a training set accuracy of 0.8754 using a cross-validated K of 5.  On the test set, the model scored 0.76076.\r\n\r\n```{r knn5, echo=TRUE, warning=FALSE}\r\n# best 9\r\nknnTrain = newTrain;\r\nknnTest = newTest;\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit )\r\n# training set performance\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nconfusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);\r\n# test set performance\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nbest9KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nbest9KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( best9KNNModelTrain, \u0022KNNBest9ModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best9KNNModel, \u0022KNNBest9ModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best9KNNModelTrain, \u0022KNNBest9ModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best9KNNModel, \u0022KNNBest9Model.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### KNN with with best subset of 8 significant features\r\n\r\nWith 8 features, the model achieves a training set accuracy of 0.881 using a cross-validated K of 5.  On the test set, the model scored 0.73684.\r\n\r\n```{r knn6, warning=FALSE}\r\n# best 8\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit )\r\n# training set performance\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nconfusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);\r\n# test set performance\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nbest8KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nbest8KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( best8KNNModelTrain, \u0022KNNBest8ModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best8KNNModel, \u0022KNNBest8ModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best8KNNModelTrain, \u0022KNNBest8ModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best8KNNModel, \u0022KNNBest8Model.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### KNN with with best subset of 7 significant features\r\n\r\nWith 7 features, the model achieves a training set accuracy of 0.8765 using a cross-validated K of 5.  On the test set, the model scored 0.73684.\r\n\r\n```{r knn7, echo=TRUE, warning=FALSE}\r\n# best 7\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit )\r\n# training set performance\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nconfusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);\r\n# test set performance\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nbest7KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nbest7KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( best7KNNModelTrain, \u0022KNNBest7ModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best7KNNModel, \u0022KNNBest7ModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best7KNNModelTrain, \u0022KNNBest7ModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best7KNNModel, \u0022KNNBest7Model.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### KNN with with best subset of 6 significant features\r\n\r\nWith 6 features, the model achieves a training set accuracy of 0.8765 using a cross-validated K of 5.  On the test set, the model scored 0.71770.\r\n\r\n```{r knn8, warning=FALSE}\r\n# best 6\r\ntrainCtrl = trainControl( method=\u0022LOOCV\u0022 );\r\nknn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+Siblings,data=knnTrain,method=\u0022knn\u0022, trControl=trainCtrl, preProcess=c(\u0022center\u0022,\u0022scale\u0022), tuneLength=20 );\r\nknn_fit\r\nplot( knn_fit )\r\n# training set performance\r\nknn_train_pred = predict( knn_fit, newdata=knnTrain);\r\nconfusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);\r\n# test set performance\r\nknn_pred = predict( knn_fit, newdata=knnTest);\r\nbest6KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);\r\nbest6KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);\r\nwrite.csv( best6KNNModelTrain, \u0022KNNBest6ModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best6KNNModel, \u0022KNNBest6ModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best6KNNModelTrain, \u0022KNNBest6ModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( best6KNNModel, \u0022KNNBest6Model.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nThe test set score of 0.71770 is less than that of the simple Gender Submission Model (0.76555).\r\n\r\n### Decision Tree Models\r\n\r\n#### Classification Tree\r\n\r\nA classification tree is used to model the data.  Since the features contain nominal data (Embarked, Sex, for example), a decision tree is appropriate.  After the tree is fit, terminal nodes are pruned using cross validation.  \r\nThe tree model scored 0.8507 against the training data and 0.7727 against the test data.\r\n\r\n```{r treeModel1, echo=TRUE, warning=FALSE}\r\nlibrary(tree);\r\ntree_model = tree( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain );\r\nsummary( tree_model )\r\ncv_tree_model = cv.tree( tree_model, FUN=prune.misclass);\r\nbest_terminal_node_idx = which.min(cv_tree_model$dev);\r\nbest_terminal_nodes = cv_tree_model$size[best_terminal_node_idx];\r\npruned_tree = prune.misclass(tree_model,best=best_terminal_nodes);\r\nplot( pruned_tree );\r\ntext( pruned_tree, pretty=0);\r\n# training set performance against pruned tree\r\npruned_tree_train_pred = predict( pruned_tree, newTrain, type=\u0022class\u0022);\r\nconfusionMatrix(pruned_tree_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\ntree_pred = predict( pruned_tree, newTest, type=\u0022class\u0022);\r\ntreeModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(pruned_tree_train_pred)-1);\r\ntreeModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(tree_pred)-1);\r\nwrite.csv( treeModelTrain, \u0022TreeModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModel, \u0022TreeModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModelTrain, \u0022TreeModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModel, \u0022TreeModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n#### Tree model with significant features\r\n\r\nNext, a tree model using only significant features is considered.  Again, cross-validation is used to prune the tree.  The model achieved an accuracy of 0.8541 on the training data and scored 0.7608 against the test data.\r\n\r\n```{r treeModel2, warning=FALSE}\r\nlibrary(tree);\r\ntree_model = tree( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain );\r\nsummary( tree_model )\r\ncv_tree_model = cv.tree( tree_model, FUN=prune.misclass);\r\nbest_terminal_node_idx = which.min(cv_tree_model$dev);\r\nbest_terminal_nodes = cv_tree_model$size[best_terminal_node_idx];\r\npruned_tree = prune.misclass(tree_model,best=best_terminal_nodes);\r\nplot( pruned_tree );\r\ntext( pruned_tree, pretty=0);\r\n# training set performance against pruned tree\r\npruned_tree_train_pred = predict( pruned_tree, newTrain, type=\u0022class\u0022);\r\nconfusionMatrix(pruned_tree_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\ntree_pred = predict( pruned_tree, newTest, type=\u0022class\u0022);\r\n# scored 0.7608\r\ntreeModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(pruned_tree_train_pred)-1);\r\ntreeModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(tree_pred)-1);\r\nwrite.csv( treeModelTrain, \u0022TreeSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModel, \u0022TreeSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModelTrain, \u0022TreeSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( treeModel, \u0022TreeSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\n### Bootstrap Aggregation (Bagging) Tree Models\r\n\r\nBootstrap Aggregation is a method of resampling the data many times to generate a series of samples from the original training set.  This allows for the fitting of many trees.  In Bagging tree models, the results of these trees are accumulated and the class that a particular sample assumes most often is the winner (by majority vote).  The averaging of many trees reduces variance.\r\n\r\n#### Bagging model using all features\r\n\r\nThe Bagging model scored an impressive 0.9877 on the training set but only 0.7321 against the test set.  The high train set accuracy and relatively low test set accuracy is indicative of over-training.\r\n\r\n```{r baggingModel1, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nbag_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain, mtry=12, importance=TRUE );\r\nbag_model\r\nplot( bag_model )\r\n# Get training set performance\r\nbag_train_pred = predict( bag_model, newdata=newTrain);\r\nconfusionMatrix( bag_train_pred, newTrain$SurvivedFactor);\r\n# now test set\r\nbag_pred = predict( bag_model, newdata=newTest);\r\nbagModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(bag_train_pred)-1);\r\nbagModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(bag_pred)-1);\r\nwrite.csv( bagModelTrain, \u0022BagModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModel, \u0022BagModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModelTrain, \u0022BagModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModel, \u0022BagModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.73205\r\n```\r\n\r\nPlot shows the OOB error (black) as well as the class errors (0/Died = red, 1/Survived = green).\r\n\r\n#### Bagging model using only significant features\r\n\r\nLike the full featured model, the significant feature bagging model scored an impressive 0.9854 on the training set but only 0.7273 against the test set.  The high train set accuracy and relatively low test set accuracy is indicative of over-training.\r\n\r\n```{r baggingModel2, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nbag_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain, mtry=9, importance=TRUE );\r\nbag_model\r\nplot( bag_model )\r\n# Get training set performance\r\nbag_train_pred = predict( bag_model, newdata=newTrain);\r\nconfusionMatrix( bag_train_pred, newTrain$SurvivedFactor);\r\n# now test set\r\nbag_pred = predict( bag_model, newdata=newTest);\r\nbagModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(bag_train_pred)-1);\r\nbagModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(bag_pred)-1);\r\nwrite.csv( bagModelTrain, \u0022BagSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModel, \u0022BagSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModelTrain, \u0022BagSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( bagModel, \u0022BagSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.7273\r\n```\r\n\r\nPlot shows the OOB error (black) as well as the class errors (0/Died = red, 1/Survived = green).\r\n\r\n### Random Forest Models\r\n\r\nA random forest is much like the bagging idea but has the constraint that each branch point is allowed a random selection of m features to use.  This random selection of predictors decorrelates the trees from one another and creates a much more diverse set of trees than does bagging.  As a rule of thumb, the number of predictors to choose from is set to the square root of the number of available predictors (for classification trees).\r\n\r\n#### Random Forest Model using all features\r\n\r\nUsing the standard sqrt(predictors) = 3 for mtry resulted in another overfit model.  But reducing the mtry to 1 (the number of predictors considered at each branch point), the random forest model scored 0.8575 against the training data and 0.7847 against the test data.\r\n\r\n```{r rfModel1, echo=TRUE, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nrf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain, mtry=1, importance=TRUE );\r\nrf_model\r\nimportance( rf_model );\r\nvarImpPlot( rf_model );\r\nplot( rf_model )\r\n# training set performance\r\nrf_train_pred = predict( rf_model, newdata=newTrain);\r\nconfusionMatrix( rf_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\nrf_pred = predict( rf_model, newdata=newTest);\r\nrfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);\r\nrfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);\r\nwrite.csv( rfModelTrain, \u0022RandomForestModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModelTrain, \u0022RandomForestModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.7847\r\n```\r\n\r\n#### Random Forest Model using only significant features\r\n\r\nUsing the standard sqrt(predictors) = 3 for mtry resulted in another overfit model.  But reducing the mtry to 1 (the number of predictors considered at each branch point), the random forest model scored 0.8608 against the training data and 0.7823 against the test data.\r\n\r\n```{r rfModel2, echo=TRUE, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nrf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain, importance=TRUE, mtry = 1 );\r\nrf_model\r\nimportance( rf_model );\r\nvarImpPlot( rf_model );\r\nplot( rf_model )\r\n# training set performance\r\nrf_train_pred = predict( rf_model, newdata=newTrain);\r\nconfusionMatrix( rf_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\nrf_pred = predict( rf_model, newdata=newTest);\r\nrfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);\r\nrfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);\r\nwrite.csv( rfModelTrain, \u0022RandomForestSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModelTrain, \u0022RandomForestSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.7823\r\n```\r\n\r\n#### Random Forest using only 9 selected features\r\n\r\nThe Random forest model using all features generated a variable importance plot.  The variable set that looked to be most informative is selected and used in this model.  This model scored 0.8485 on the training set and 0.7775 on the test set.\r\n\r\n```{r rfModel3, echo=TRUE, warning=FALSE}\r\nlibrary(randomForest);\r\nset.seed(1);\r\nrf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+NumParents+Siblings+NumPassengersOnTicket,data=newTrain, importance=TRUE, mtry=1 );\r\nrf_model\r\nimportance( rf_model );\r\nvarImpPlot( rf_model );\r\nplot( rf_model )\r\n# training set performance\r\nrf_train_pred = predict( rf_model, newdata=newTrain);\r\nconfusionMatrix( rf_train_pred, newTrain$SurvivedFactor);\r\n# test set performance\r\nrf_pred = predict( rf_model, newdata=newTest);\r\nrfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);\r\nrfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);\r\nwrite.csv( rfModelTrain, \u0022RandomForestTrimmedModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestTrimmedModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModelTrain, \u0022RandomForestTrimmedModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( rfModel, \u0022RandomForestTrimmedModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored 0.7775\r\n```\r\n\r\n### Boosted Classification Tree Models\r\n\r\nGradient Boosting applied to classification trees involves fitting classification trees sequentially.  That is, instead of generating a bunch of trees and averaging them as with bagging and random forests, boosting involves adding trees sequentially into a single model.  Trees are successively fit to the residuals of the previous tree ensemble. \r\n\r\n#### Gradient Boosting using all features\r\n\r\nSince there are many parameters to consider for boosted classification trees, cross-validation is used to select the values which will generalize best.  This analysis is done using all predictors.\r\n\r\n```{r boostedModel0, echo=TRUE, warning=FALSE, eval=FALSE}\r\n# This is not run; ignore\r\nlibrary(gbm, quietly = TRUE);\r\ntree_lengths = 1:4;\r\nbest_error = Inf;\r\nbest_idx = 0;\r\nmin_error_idx = tree_lengths;\r\nmin_errors = tree_lengths;\r\nxTrain = newTrain[,c(\u0022Survived\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = newTest[,c(\u0022Survived\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nnumSamples = 10;\r\ntrainingProportion = 0.8;\r\nthreshold = 0.5;\r\ncvFolds = 8;  # n-fold validation\r\nnTrees = 15000; \r\ntreeDiv = 1000;\r\nnumIntTrees = nTrees/treeDiv - 1;\r\nnCores = 7; # adjust according to computer\r\n\r\ntrainMat = matrix( nrow=numSamples, ncol=numIntTrees);\r\ntestMat = matrix( nrow=numSamples, ncol=numIntTrees);\r\nfor ( depth in tree_lengths ){\r\n  set.seed(depth);\r\n  sampleSet = createDataPartition(1:nrow(newTrain),numSamples,p=trainingProportion);\r\n  for ( samp in 1:numSamples){\r\n    trainSample = sampleSet[[samp]];\r\n    testSample = setdiff(1:nrow(newTrain),trainSample);\r\n    boosted_model = gbm( formula=Survived~.,data=xTrain[trainSample,], distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, n.cores = nCores, verbose=FALSE );\r\n    best_tree_num = gbm.perf( boosted_model );\r\n    train_scores = 0;\r\n    test_scores = 0;\r\n    for ( ntree in 1:numIntTrees){\r\n      treelen = (ntree-1)*treeDiv;\r\n      # now that the model is trained, see how it does on training set for n trees.\r\n      train_pred = predict( boosted_model, newdata=xTrain[trainSample,], n.trees=treelen, type=\u0022response\u0022);\r\n      train_predictions = as.numeric( train_pred \u003e= threshold );\r\n      train_score = sum( train_predictions == newTrain$Survived[trainSample] )/length(trainSample);\r\n      train_scores[ ntree ] = train_score;\r\n      trainMat[samp,ntree] = train_score;\r\n      cat( \u0022trainMat[\u0022, samp, \u0022,\u0022, ntree, \u0022] = \u0022, train_score, \u0022\\n\u0022);\r\n      # now that the model is trained, see how it does on test set for n trees.\r\n      test_pred = predict( boosted_model, newdata=xTrain[testSample,], n.trees=treelen, type=\u0022response\u0022);\r\n      test_predictions = as.numeric( test_pred \u003e= threshold );\r\n      test_score = sum( test_predictions == newTrain$Survived[testSample] )/length(testSample);\r\n      test_scores[ ntree ] = test_score;\r\n      testMat[samp,ntree] = test_score;\r\n    }\r\n  }\r\n  boosted_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, n.cores = nCores, verbose=FALSE );\r\n  min_error_idx[depth] = which.min( boosted_model$cv.error );\r\n  min_errors[depth] = min( boosted_model$cv.error );\r\n  if ( min_errors[depth] \u003c best_error ){\r\n    best_error = min_errors[depth];\r\n    best_idx = depth;\r\n  }\r\n}\r\ngbtrees = data.frame( TreeDepth = tree_lengths, BernoulliDeviance = min_errors, NumTrees = min_error_idx );\r\nknitr::kable( gbtrees, format=\u0022markdown\u0022, longtable=TRUE);\r\n#summary( boosted_model );\r\n#plot( boosted_model );\r\nset.seed(best_idx);\r\nbest_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, n.cores = nCores, verbose=FALSE );\r\ngbm.perf( best_model );\r\nboosted_train_pred = predict( best_model, newdata=xTrain, n.trees=min_error_idx[best_idx], type=\u0022response\u0022);\r\nboosted_pred = predict( best_model, newdata=xTest, n.trees=min_error_idx[best_idx], type=\u0022response\u0022);\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nboosted_train_predictions = as.numeric( boosted_train_pred \u003e= threshold );\r\nboosted_predictions = as.numeric( boosted_pred \u003e= threshold );\r\nboostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_predictions);\r\nboostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);\r\nboostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));\r\nboostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));\r\nwrite.csv( boostedModelTrainProbs, \u0022BoostedTreeModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelProbs, \u0022BoostedTreeModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelTrain, \u0022BoostedTreeModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModel, \u0022BoostedTreeModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# originally scored 0.76076 with interaction depth =4, 0.77990 with interaction depth = 1\r\n# after 10-fold x-validation, considering interaction depths of 1 to 4, scored a 0.76555\r\n# same model , stopping at 4800 trees yields 0.77511\r\n```\r\n\r\n```{r boostedModel1, echo=TRUE, warning=FALSE}\r\nlibrary(gbm, quietly = TRUE);\r\ntree_lengths = 1:10;\r\nnumLengths = length( tree_lengths );\r\nbest_error = Inf;\r\nbest_idx = 0;\r\nbest_shrinkage = 0;\r\nbest_steps = 0;\r\nxTrain = newTrain[,c(\u0022Survived\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = newTest[,c(\u0022Survived\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nlearningRates = c(0.1,0.07,0.05,0.03,0.02,0.01);\r\nnumLR = length( learningRates );\r\nthreshold = 0.5;\r\ncvFolds = 10;  # n-fold validation\r\nnTrees = 3000; \r\nnCores = 7; # adjust according to computer\r\ntree_depths = rep(0,numLengths*numLR);\r\nsteps = rep(0,numLengths*numLR);\r\ndeviances = rep(0,numLengths*numLR);\r\nshrinkages = rep(0,numLengths*numLR);\r\nidx = 1;\r\n\r\nfor ( depth in tree_lengths ){\r\n  set.seed(depth);\r\n  for ( lrIdx in 1:numLR){\r\n      learningRate = learningRates[lrIdx];\r\n      boosted_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, shrinkage = learningRate, n.cores = nCores, verbose=FALSE );\r\n      idxCVErr = gbm.perf( boosted_model, plot.it=FALSE );\r\n      cvError = boosted_model$cv.error[idxCVErr];\r\n      if ( cvError \u003c best_error ){\r\n        best_error = cvError;\r\n        best_idx = depth;\r\n        best_shrinkage = learningRate;\r\n        best_steps = idxCVErr;\r\n      }\r\n      tree_depths[idx] = depth;\r\n      shrinkages[idx] = learningRate;\r\n      steps[idx] = idxCVErr;\r\n      deviances[idx] = cvError;\r\n      idx = idx + 1;\r\n  }\r\n}\r\ngbtrees = data.frame( TreeDepth = tree_depths, Shrinkage = shrinkages, NumTrees = steps, BernoulliDeviance = deviances );\r\nknitr::kable( gbtrees, format=\u0022markdown\u0022, longtable=TRUE);\r\n#summary( boosted_model );\r\n#plot( boosted_model );\r\n# create a line chart\r\nxrange = range( shrinkages );\r\nyrange = range( deviances );\r\nplot( xrange, yrange, type=\u0022n\u0022, xlab=\u0022Shrinkage\u0022, ylab=\u0022CV Accuracy (Bernoulli Deviance)\u0022);\r\ncolors = rainbow( numLengths );\r\nlinetype = tree_lengths;\r\nplotchar = seq(0,numLengths,1);\r\n# add lines\r\nfor ( i in tree_lengths){\r\n  thisDepthTrees = subset( gbtrees, gbtrees$TreeDepth == i );\r\n  lines(thisDepthTrees$Shrinkage, thisDepthTrees$BernoulliDeviance, type=\u0022b\u0022, lwd=1.5, lty=linetype[i], col=colors[i], pch=plotchar[i]);\r\n}\r\ntitle( \u0022Bernoulli Deviance by Shrinkage and Tree Depth\u0022);\r\nlegend( 0.08, yrange[2], 1:numLengths, cex=0.8, col=colors, pch=plotchar, lty=linetype, title=\u0022Tree Depth\u0022);\r\nset.seed(best_idx);\r\nbest_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, shrinkage = best_shrinkage, n.cores = nCores, verbose=FALSE );\r\ngbm.perf( best_model );\r\nboosted_train_pred = predict( best_model, newdata=xTrain, n.trees=best_steps, type=\u0022response\u0022);\r\nboosted_train_calls = as.numeric( boosted_train_pred \u003e= threshold );\r\nconfusionMatrix(boosted_train_calls,newTrain$Survived);\r\nboosted_pred = predict( best_model, newdata=xTest, n.trees=best_steps, type=\u0022response\u0022);\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nboosted_predictions = as.numeric( boosted_pred \u003e= threshold );\r\nboostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_calls);\r\nboostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);\r\nboostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));\r\nboostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));\r\nwrite.csv( boostedModelTrainProbs, \u0022BoostedTreeModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelProbs, \u0022BoostedTreeModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelTrain, \u0022BoostedTreeModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModel, \u0022BoostedTreeModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# interaction.depth = 8, nTrees = 118, learningRate = 0.05\r\n# scored 0.756\r\n```\r\n\r\nThe Cross-validated parameters (interaction depth = 8, number of trees = 118, learning rate = 0.05) resulted in a training set accuracy of 0.8943 on the training set and 0.756 on the test set.\r\n\r\n#### Gradient Boosting using only significant features\r\n\r\nThe same technique is used as above to find the best set of parameters for gradient boosting model for the significant features.\r\n\r\n```{r boostedModel2, echo=TRUE, warning=FALSE}\r\nlibrary(gbm);\r\ntree_lengths = 1:10;\r\nnumLengths = length( tree_lengths );\r\nbest_error = Inf;\r\nbest_idx = 0;\r\nbest_shrinkage = 0;\r\nbest_steps = 0;\r\nsignificant_features = c(\u0022FixedTitle\u0022, \u0022Sex\u0022, \u0022Siblings\u0022, \u0022FixedAge\u0022, \u0022CabinAssignment\u0022, \u0022pclass\u0022, \u0022NumChildren\u0022, \u0022FarePerPassenger\u0022, \u0022Embarked\u0022);\r\nxTrain = newTrain[,c(\u0022Survived\u0022, significant_features) ];\r\nxTest = newTest[,c(\u0022Survived\u0022, significant_features) ];\r\nlearningRates = c(0.1,0.07,0.05,0.03,0.02,0.01);\r\nnumLR = length( learningRates );\r\nthreshold = 0.5;\r\ncvFolds = 10;  # n-fold validation\r\nnTrees = 3000; \r\nnCores = 7; # adjust according to computer\r\ntree_depths = rep(0,numLengths*numLR);\r\nsteps = rep(0,numLengths*numLR);\r\ndeviances = rep(0,numLengths*numLR);\r\nshrinkages = rep(0,numLengths*numLR);\r\nidx = 1;\r\n\r\nfor ( depth in tree_lengths ){\r\n  set.seed(depth);\r\n  for ( lrIdx in 1:numLR){\r\n      learningRate = learningRates[lrIdx];\r\n      boosted_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, shrinkage = learningRate, n.cores = nCores, verbose=FALSE );\r\n      idxCVErr = gbm.perf( boosted_model, plot.it=FALSE );\r\n      cvError = boosted_model$cv.error[idxCVErr];\r\n      if ( cvError \u003c best_error ){\r\n        best_error = cvError;\r\n        best_idx = depth;\r\n        best_shrinkage = learningRate;\r\n        best_steps = idxCVErr;\r\n      }\r\n      tree_depths[idx] = depth;\r\n      shrinkages[idx] = learningRate;\r\n      steps[idx] = idxCVErr;\r\n      deviances[idx] = cvError;\r\n      idx = idx + 1;\r\n  }\r\n}\r\ngbtrees = data.frame( TreeDepth = tree_depths, Shrinkage = shrinkages, NumTrees = steps, BernoulliDeviance = deviances );\r\nknitr::kable( gbtrees, format=\u0022markdown\u0022, longtable=TRUE);\r\n#summary( boosted_model );\r\n#plot( boosted_model );\r\n# create a line chart\r\nxrange = range( shrinkages );\r\nyrange = range( deviances );\r\nplot( xrange, yrange, type=\u0022n\u0022, xlab=\u0022Shrinkage\u0022, ylab=\u0022Cross Validation Error (Bernoulli Deviance)\u0022);\r\ncolors = rainbow( numLengths );\r\nlinetype = tree_lengths;\r\nplotchar = seq(0,numLengths,1);\r\n# add lines\r\nfor ( i in tree_lengths){\r\n  thisDepthTrees = subset( gbtrees, gbtrees$TreeDepth == i );\r\n  lines(thisDepthTrees$Shrinkage, thisDepthTrees$BernoulliDeviance, type=\u0022b\u0022, lwd=1.5, lty=linetype[i], col=colors[i], pch=plotchar[i]);\r\n}\r\ntitle( \u0022Bernoulli Deviance by Shrinkage and Tree Depth\u0022);\r\nlegend( 0.08, yrange[2], 1:numLengths, cex=0.8, col=colors, pch=plotchar, lty=linetype, title=\u0022Tree Depth\u0022);\r\nset.seed(best_idx);\r\nbest_model = gbm( formula=Survived~.,data=xTrain, distribution=\u0022bernoulli\u0022, n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, shrinkage = best_shrinkage, n.cores = nCores, verbose=FALSE );\r\ngbm.perf( best_model );\r\nboosted_train_pred = predict( best_model, newdata=xTrain, n.trees=best_steps, type=\u0022response\u0022);\r\nboosted_train_calls = as.numeric( boosted_train_pred \u003e= threshold );\r\nconfusionMatrix(boosted_train_calls,newTrain$Survived);\r\nboosted_pred = predict( best_model, newdata=xTest, n.trees=best_steps, type=\u0022response\u0022);\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nboosted_predictions = as.numeric( boosted_pred \u003e= threshold );\r\nboostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_calls);\r\nboostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);\r\nboostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));\r\nboostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));\r\nwrite.csv( boostedModelTrainProbs, \u0022BoostedTreeSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelProbs, \u0022BoostedTreeSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModelTrain, \u0022BoostedTreeSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( boostedModel, \u0022BoostedTreeSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# interaction.depth = 8, nTrees = 328, learningRate = 0.02\r\n# scored 0.75119\r\n```\r\n\r\nThe cross validated parameters were found to be an interaction depth of 8, 328 trees, and a learning rate of 0.02.  The model achieved a score of 0.8923 on the training set and a sad 0.7368 on the test set.\r\n\r\n### Support Vector Machine (SVM) Model\r\n\r\nSupport Vector Machines seek to find the optimal separating hyperplane between two classes.  There are several parameters to consider: cost (of a constraint violation), choice of kernel (and corresponding gamma).  Cross-validation is used to estimate these parameters.\r\n\r\n```{r svm1, echo=TRUE, warning=FALSE}\r\nlibrary(e1071);\r\nlibrary(caret);\r\n#xTrain = newTrain[,c(\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\n#yTrain = newTrain[,\u0022SurvivedFactor\u0022];\r\nxTrain = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=whiteTrain);\r\nyTrain = newTrain[,\u0022SurvivedFactor\u0022];\r\n#xTest = newTest[,c(\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=whiteTest);\r\n#xTrainSVM = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain);\r\n#xTestSVM = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest);\r\ncosts = c(0.01, 0.1, 0.5, 0.9, 1, 1.1);\r\ngammas = c(0.0001, 0.01, 0.05, 0.1, 0.5);\r\nkernels = c(\u0022linear\u0022, \u0022polynomial\u0022, \u0022radial\u0022, \u0022sigmoid\u0022);\r\nbestCost = costs[1];\r\nbestGamma = gammas[1];\r\nbestKernel = kernels[1];\r\nbestError = Inf;\r\nerrors = rep(Inf,1,length(kernels));\r\nctr = 1;\r\nfor ( kernel in kernels ){\r\n  tune.out = tune( \u0022svm\u0022, xTrain, train.y=yTrain, kernel=kernel, ranges=list( cost=costs,gamma=gammas) );\r\n  tune.out$best.performance\r\n  tune.out$best.model\r\n  thisError = tune.out$best.performance;\r\n  errors[ctr] = thisError;\r\n  if ( thisError \u003c  bestError ){\r\n    bestError = thisError;\r\n    bestKernel = kernel;\r\n    bestGamma = tune.out$best.model$gamma;\r\n    bestCost = tune.out$best.model$cost;\r\n  }\r\n  ctr = ctr + 1;\r\n}\r\nsvm.fit = svm( x=xTrain, y=yTrain, kernel=bestKernel, cost=bestCost, gamma=bestGamma);\r\n#plot( svm.fit, xTrain );\r\nsvm.train.pred = predict( svm.fit, xTrain );\r\nsvmTrainingPreds = as.numeric( svm.train.pred ) - 1;\r\ntruth = newTrain$Survived;\r\nconfusionMatrix( svmTrainingPreds, truth );\r\n# scores a 0.8294 on the training set\r\nsvm.test.pred = predict( svm.fit, xTest );  \r\nsvmPredictions = as.numeric( svm.test.pred ) - 1;\r\nsvmModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=svmTrainingPreds);\r\nsvmModel = data.frame(PassengerId=newTest$PassengerId, Survived=svmPredictions);\r\n\r\nwrite.csv( svmModelTrain, \u0022SVMModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( svmModel, \u0022SVMModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( svmModelTrain, \u0022SVMModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( svmModel, \u0022SVMModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# scored a 0.7727\r\n```\r\n\r\nThe SVM model scored a 0.8328 against the training set and a 0.7727 against the test set.\r\n\r\n### Neural Network Models\r\n\r\nThis neural network implementation uses a single layer.  The number of hidden nodes and learning rate is selected through 10-fold cross validation.\r\n\r\n#### Neural Network with one hidden layer utilizing all features\r\n```{r nnModel1, echo=TRUE, eval=FALSE, warning=FALSE}\r\n### NN Model\r\nlibrary(nnet);\r\n#xTrain = newTrain[,c(\u0022SurvivedFactor\u0022, significant_features) ];\r\n#xTest = newTest[,c(\u0022SurvivedFactor\u0022, significant_features) ];\r\n\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022,\u0022pclass\u0022,\u0022Sex\u0022,\u0022FixedAge\u0022,\u0022FarePerPassenger\u0022,\u0022FixedTitle\u0022,\u0022CabinAssignment\u0022,\u0022Embarked\u0022,\u0022NumPassengersOnTicket\u0022,\u0022NumParents\u0022,\u0022NumChildren\u0022,\u0022SpousesFactor\u0022,\u0022Siblings\u0022)];\r\n\r\nnn = nnet( SurvivedFactor ~ ., data = xTrain, size=12, maxit=500, trace=FALSE);\r\n# How do we do on the training data?\r\n\r\nnn_train_pred_class = predict( nn, xTrain, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nnn_train_pred = as.numeric( nn_train_pred_class );   # transform to 0, 1\r\nconfusionMatrix(nn_train_pred,xTrain$Survived);\r\n\r\n# try on test data\r\nnn_test_pred_class = predict( nn, xTest, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nnn_test_pred = as.numeric( nn_test_pred_class );   # transform to 0, 1\r\n# write to  file\r\nnnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=nn_train_pred);\r\nnnModel = data.frame(PassengerId=newTest$PassengerId, Survived=nn_test_pred);\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkFullModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkFullModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkFullModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkFullModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# best \u0022over\u0022trained model scores 0.73205 (306/418)\r\n\r\n# Now, we wish to select parameters for neural network by using 10-fold cross-validation with caret\r\ntuningGrid = expand.grid(size=1:12,decay=c(0,0.0001,0.05,0.1));\r\nset.seed( 1 );\r\ntrControl = trainControl( method=\u0022repeatedcv\u0022, number=10, repeats=10 );\r\nnn_cv = train( SurvivedFactor ~ ., data=xTrain, method=\u0022nnet\u0022, trControl = trControl, tuneGrid=tuningGrid, verbose=FALSE, trace=FALSE);\r\n\r\nbest_size = nn_cv$bestTune[1,\u0022size\u0022];\r\nbest_decay = nn_cv$bestTune[1,\u0022decay\u0022];\r\nbest_nn = nnet( SurvivedFactor ~ ., data = xTrain, size=best_size, decay=best_decay, maxit=500, trace=FALSE);\r\nbest_nn_train_pred_class = predict( best_nn, newdata=xTrain, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nbest_nn_train_pred = as.numeric( best_nn_train_pred_class );   # transform to 0, 1\r\nconfusionMatrix( best_nn_train_pred, xTrain$Survived);\r\n# now do on test data\r\nbest_nn_test_pred_class = predict( best_nn, newdata=xTest, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nbest_nn_test_pred = as.numeric( best_nn_test_pred_class );   # transform to 0, 1\r\n# write to  file\r\nnnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=best_nn_train_pred);\r\nnnModel = data.frame(PassengerId=newTest$PassengerId, Survived=best_nn_test_pred);\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkFullModelCVTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkFullModelCVProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkFullModelCVTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkFullModelCV.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# Neural network model scores 0.7847\r\n```\r\n\r\nThe cross validated Neural Network model scored 0.8418 against the training set and 0.7847 against the test set.\r\n\r\n#### Neural Network with one hidden layer utilizing significant features\r\n```{r nnModel2, echo=TRUE, eval=TRUE, warning=FALSE}\r\n### NN Model\r\nlibrary(nnet);\r\nxTrain = whiteTrain[,c(\u0022SurvivedFactor\u0022, significant_features) ];\r\nxTest = whiteTest[,c(\u0022SurvivedFactor\u0022, significant_features) ];\r\n\r\nnn = nnet( SurvivedFactor ~ ., data = xTrain, size=length(significant_features), maxit=500, trace=FALSE);\r\n# How do we do on the training data?\r\n\r\nnn_train_pred_class = predict( nn, xTrain, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nnn_train_pred = as.numeric( nn_train_pred_class );   # transform to 0, 1\r\nconfusionMatrix(nn_train_pred,xTrain$Survived);\r\n# 0.9068 accuracy on the training set\r\n\r\n# try on test data\r\nnn_test_pred_class = predict( nn, xTest, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nnn_test_pred = as.numeric( nn_test_pred_class );   # transform to 0, 1\r\n# write to  file\r\nnnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=nn_train_pred);\r\nnnModel = data.frame(PassengerId=newTest$PassengerId, Survived=nn_test_pred);\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkSigModelTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkSigModelProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkSigModelTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkSigModel.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# best \u0022over\u0022trained model scores 0.72727 (304/418)\r\n\r\n# Now, we wish to select parameters for neural network by using 10-fold cross-validation with caret\r\ntuningGrid = expand.grid(size=1:length(significant_features),decay=c(0,0.0001,0.05,0.1));\r\nset.seed( 1 );\r\ntrControl = trainControl( method=\u0022repeatedcv\u0022, number=10, repeats=10 );\r\nnn_cv = train( SurvivedFactor ~ ., data=xTrain, method=\u0022nnet\u0022, trControl = trControl, tuneGrid=tuningGrid, verbose=FALSE, trace=FALSE);\r\n\r\nbest_size = nn_cv$bestTune[1,\u0022size\u0022];\r\nbest_decay = nn_cv$bestTune[1,\u0022decay\u0022];\r\nbest_nn = nnet( SurvivedFactor ~ ., data = xTrain, size=best_size, decay=best_decay, maxit=500, trace=FALSE);\r\nbest_nn_train_pred_class = predict( best_nn, newdata=xTrain, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nbest_nn_train_pred = as.numeric( best_nn_train_pred_class );   # transform to 0, 1\r\nconfusionMatrix( best_nn_train_pred, xTrain$Survived);\r\n# now do on test data\r\nbest_nn_test_pred_class = predict( best_nn, newdata=xTest, type=\u0022class\u0022 );  # yields \u00220\u0022, \u00221\u0022\r\nbest_nn_test_pred = as.numeric( best_nn_test_pred_class );   # transform to 0, 1\r\n# write to  file\r\nnnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=best_nn_train_pred);\r\nnnModel = data.frame(PassengerId=newTest$PassengerId, Survived=best_nn_test_pred);\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkSigModelCVTrainProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkSigModelCVProbs.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModelTrain, \u0022NeuralNetworkSigModelCVTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\nwrite.csv( nnModel, \u0022NeuralNetworkSigModelCV.csv\u0022, row.names=FALSE, quote=FALSE );\r\n# Neural network model scores 0.7847 (334/418) - same as full featured model\r\n```\r\n\r\nThe cross validated Neural Network model with only significant features scored 0.853 against the training set and 0.7751 against the test set.\r\n\r\n### Ensemble Model\r\n\r\nEnsemble models use multiple models to make predictions.  Ensemble models usually fall into one of three different types: bagging, boosting, and stacking.  In a bagging model paradigm, a single model (usually) is run multiple times over different subsamples of the data.  These models are usually not complex to avoid overfitting and hopefully learn one part of the data and so are uncorrelated with one another.  The output of the models are then combined to make a prediction.  The Gradient Boosting model that we used with classification trees is an example of an ensemble model where successive models are employed to reduce the residuals of the previous model.  Finally, a stacking ensemble model is much like a traditional machine learning model only the features or inputs to the learning model are themselves outputs of different types of machine learning models.  The caveat, however, is that in order for ensembling to work, the models must be uncorrelated.\r\n\r\nThe idea in this section is to gather the results from all the trained models.  Where possible, I used the probabilties of individual predictions (i.e., 0.983 chance of survival) over class calls (binary 0=dead, 1=survived) as that data is a little more fine-tuned.  For this dataset, many of the models that I have trained are drastically overfit.  The gradient boosted tree models, with training accuracies of 98% are extremely overfit.  Other models are just not well-suited for this type of problem (KNN, LDA, QDA).  Once I pared the list of models to just include the models that I thought were well-suited for this problem, I computed all of the cross-correlations between models and then enumerated all of the sets of models that were below a specified cross-correlation (I picked 0.75 to signify a high correlation).  Once I did this, I found that *all* of the remaining models were highly correlated to one another, so I coudn\u0027t make an ensemble!  But since I\u0027m determined to make an ensemble, I pressed on.\r\n\r\nI added back the LDA, QDA, and neural network models and reran the correlation analysis.  I purposely excluded the KNN and Bagging models since I believe KNN to not be well-suited for this problem and the bagging models are massively overfit.  The correlation analysis showed which models could be combined into a single stacked model (see below).  I manually selected the set of 3 models: RandomForest, Gender model, and QDA, since they represent three models of different complexity (simple: gender model, balanced: Random Forest, complex: QDA), hoping the strengths of each would complement each other.\r\n\r\nIn addition, I decided to use all the models in a different ensemble model, just taking row means.  This set included all of the models, including the overfit bagging models, KNN, boosting, etc.  Many of these models are highly correlated so the ensemble method is just a simple mean across all of the models in each row.\r\n\r\n```{r ensemble, echo=TRUE, eval=TRUE, warning=FALSE}\r\nlibrary(matlab,quietly=TRUE);\r\nlibrary(ROCR,quietly=TRUE);\r\nlibrary(tidyr,quietly=TRUE);\r\nlibrary(dplyr,quietly=TRUE);\r\nlibrary(tibble,quietly=TRUE);\r\n\r\nall_train_models = c( \u0022BagModelTrainProbs.csv\u0022, \u0022BagSigModelTrainProbs.csv\u0022, \u0022BoostedTreeModelTrainProbs.csv\u0022, \u0022BoostedTreeSigModelTrainProbs.csv\u0022, \u0022GenderModelTrainProbs.csv\u0022, \u0022KNNBest6ModelTrainProbs.csv\u0022, \u0022KNNBest7ModelTrainProbs.csv\u0022, \u0022KNNBest8ModelTrainProbs.csv\u0022, \u0022KNNBest9ModelTrainProbs.csv\u0022, \u0022KNNFullModelTrainProbs.csv\u0022, \u0022LassoRegressionFullModelTrainProbs.csv\u0022, \u0022LassoRegressionSigModelTrainProbs.csv\u0022, \u0022LDAModelTrainProbs.csv\u0022, \u0022LDAReducedModelTrainProbs.csv\u0022, \u0022LogisticRegressionFullModelTrainProbs.csv\u0022, \u0022LogisticRegressionSigModelTrainProbs.csv\u0022, \u0022NeuralNetworkSigModelCVTrainProbs.csv\u0022, \u0022NeuralNetworkSigModelTrainProbs.csv\u0022, \u0022NullModelTrainProbs.csv\u0022, \u0022QDAModelTrainProbs.csv\u0022, \u0022QDASigModelTrainProbs.csv\u0022, \u0022RandomForestModelTrainProbs.csv\u0022, \u0022RandomForestSigModelTrainProbs.csv\u0022, \u0022RandomForestTrimmedModelTrainProbs.csv\u0022, \u0022RidgeRegressionFullModelTrainProbs.csv\u0022, \u0022RidgeRegressionSigModelTrainProbs.csv\u0022, \u0022SVMModelTrainProbs.csv\u0022, \u0022TreeModelTrainProbs.csv\u0022, \u0022TreeSigModelTrainProbs.csv\u0022, \u0022WCModelTrainProbs.csv\u0022 );\r\n\r\ntrain_models = c( \u0022BoostedTreeModelTrainProbs.csv\u0022, \u0022BoostedTreeSigModelTrainProbs.csv\u0022, \u0022GenderModelTrainProbs.csv\u0022, \u0022LassoRegressionFullModelTrainProbs.csv\u0022, \u0022LassoRegressionSigModelTrainProbs.csv\u0022, \u0022LDAModelTrainProbs.csv\u0022, \u0022LDAReducedModelTrainProbs.csv\u0022, \u0022LogisticRegressionFullModelTrainProbs.csv\u0022, \u0022LogisticRegressionSigModelTrainProbs.csv\u0022, \u0022NeuralNetworkSigModelCVTrainProbs.csv\u0022, \u0022NeuralNetworkSigModelTrainProbs.csv\u0022, \u0022QDAModelTrainProbs.csv\u0022, \u0022QDASigModelTrainProbs.csv\u0022, \u0022RandomForestModelTrainProbs.csv\u0022, \u0022RandomForestSigModelTrainProbs.csv\u0022, \u0022RandomForestTrimmedModelTrainProbs.csv\u0022, \u0022RidgeRegressionFullModelTrainProbs.csv\u0022, \u0022RidgeRegressionSigModelTrainProbs.csv\u0022, \u0022SVMModelTrainProbs.csv\u0022, \u0022TreeModelTrainProbs.csv\u0022, \u0022TreeSigModelTrainProbs.csv\u0022, \u0022WCModelTrainProbs.csv\u0022 );\r\n\r\nall_test_models = c( \u0022BagModelProbs.csv\u0022, \u0022BagSigModelProbs.csv\u0022, \u0022BoostedTreeModelProbs.csv\u0022, \u0022BoostedTreeSigModelProbs.csv\u0022, \u0022GenderModelProbs.csv\u0022, \u0022KNNBest6ModelProbs.csv\u0022, \u0022KNNBest7ModelProbs.csv\u0022, \u0022KNNBest8ModelProbs.csv\u0022, \u0022KNNBest9ModelProbs.csv\u0022, \u0022KNNFullModelProbs.csv\u0022, \u0022LassoRegressionFullModelProbs.csv\u0022, \u0022LassoRegressionSigModelProbs.csv\u0022, \u0022LDAModelProbs.csv\u0022, \u0022LDAReducedModelProbs.csv\u0022, \u0022LogisticRegressionFullModelProbs.csv\u0022, \u0022LogisticRegressionSigModelProbs.csv\u0022, \u0022NeuralNetworkSigModelCVProbs.csv\u0022, \u0022NeuralNetworkSigModelProbs.csv\u0022, \u0022NullModelProbs.csv\u0022, \u0022QDAModelProbs.csv\u0022, \u0022QDASigModelProbs.csv\u0022, \u0022RandomForestModelProbs.csv\u0022, \u0022RandomForestSigModelProbs.csv\u0022, \u0022RandomForestTrimmedModelProbs.csv\u0022, \u0022RidgeRegressionFullModelProbs.csv\u0022, \u0022RidgeRegressionSigModelProbs.csv\u0022, \u0022SVMModelProbs.csv\u0022, \u0022TreeModelProbs.csv\u0022, \u0022TreeSigModelProbs.csv\u0022, \u0022WCModelProbs.csv\u0022 );\r\n\r\ntest_models = c( \u0022BoostedTreeModelProbs.csv\u0022, \u0022BoostedTreeSigModelProbs.csv\u0022, \u0022GenderModelProbs.csv\u0022, \u0022LassoRegressionFullModelProbs.csv\u0022, \u0022LassoRegressionSigModelProbs.csv\u0022, \u0022LDAModelProbs.csv\u0022, \u0022LDAReducedModelProbs.csv\u0022, \u0022LogisticRegressionFullModelProbs.csv\u0022, \u0022LogisticRegressionSigModelProbs.csv\u0022, \u0022NeuralNetworkSigModelCVProbs.csv\u0022, \u0022NeuralNetworkSigModelProbs.csv\u0022, \u0022QDAModelProbs.csv\u0022, \u0022QDASigModelProbs.csv\u0022, \u0022RandomForestModelProbs.csv\u0022, \u0022RandomForestSigModelProbs.csv\u0022, \u0022RandomForestTrimmedModelProbs.csv\u0022, \u0022RidgeRegressionFullModelProbs.csv\u0022, \u0022RidgeRegressionSigModelProbs.csv\u0022, \u0022SVMModelProbs.csv\u0022, \u0022TreeModelProbs.csv\u0022, \u0022TreeSigModelProbs.csv\u0022, \u0022WCModelProbs.csv\u0022 );\r\n\r\ntrainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );\r\ntestModels = data.frame(PassengerId=1:nrow(newTest));\r\n\r\n# read all the train model predictions and store in dataframe\r\nnumModels = length( train_models );\r\nfor ( i in 1:numModels ){\r\n  trainFileName = train_models[i];\r\n  fp = fileparts( trainFileName );\r\n  modelName = gsub( \u0022TrainProbs\u0022, \u0022\u0022, fp$name );\r\n  trainModelFrame = read.csv( trainFileName );\r\n  trainModels[ modelName ] = trainModelFrame$Survived;\r\n  # now for test\r\n  testFileName = test_models[i];\r\n  fp = fileparts( testFileName );\r\n  modelName = gsub( \u0022Probs\u0022, \u0022\u0022, fp$name );\r\n  testModelFrame = read.csv( testFileName );\r\n  testModels[ modelName ] = testModelFrame$Survived;\r\n}\r\n\r\nallTrainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );\r\nallTestModels = data.frame(PassengerId=1:nrow(newTest));\r\n\r\nnumAllModels = length( all_train_models );\r\nfor ( i in 1:numAllModels ){\r\n  trainFileName = all_train_models[i];\r\n  fp = fileparts( trainFileName );\r\n  modelName = gsub( \u0022TrainProbs\u0022, \u0022\u0022, fp$name );\r\n  trainModelFrame = read.csv( trainFileName );\r\n  allTrainModels[ modelName ] = trainModelFrame$Survived;\r\n  # now for test\r\n  testFileName = all_test_models[i];\r\n  fp = fileparts( testFileName );\r\n  modelName = gsub( \u0022Probs\u0022, \u0022\u0022, fp$name );\r\n  testModelFrame = read.csv( testFileName );\r\n  allTestModels[ modelName ] = testModelFrame$Survived;\r\n}\r\n\r\nall_train_model_names = setdiff( names(allTrainModels), c(\u0022PassengerId\u0022, \u0022Survived\u0022) );\r\n\r\n# this calculates all correlations between models.  Some corr calcs break because\r\n# they\u0027re all one value so standard deviation calc blows up.\r\nmodelCors \u003c- trainModels[,!names(trainModels) %in% c(\u0022PassengerId\u0022,\u0022Survived\u0022)] %\u003e% \r\n  as.matrix %\u003e%\r\n  cor %\u003e%\r\n  as.data.frame %\u003e%\r\n  rownames_to_column(var = \u0027Model_A\u0027) %\u003e%\r\n  gather(Model_B, correlation, -Model_A)\r\n\r\ncorrThresh = 0.75;\r\nmodelNames = unique(modelCors$Model_A);\r\n\r\n# this recursive function is intended to output groups of models that \r\n# have inter correlations below threshold\r\naddToModelSet = function( modelSet, corrs, model, corrThresh ){\r\n    #modelSet[ length(modelSet)+1 ] = model;\r\n    if ( ! model %in% modelSet ){\r\n        modelSet = union( modelSet, model );\r\n    }\r\n    idxUsed = c();\r\n    idxUnused = 1:nrow(corrs);\r\n   \r\n    toUse = c();\r\n    for ( idx in idxUnused ){\r\n        thisModel = corrs$Model_A[idx];\r\n        if ( thisModel %in% modelSet ){\r\n            toUse[ length(toUse) + 1] = idx;\r\n        }\r\n    }\r\n   \r\n    for ( idx in toUse ){\r\n        addModel = TRUE;\r\n        modelA = corrs$Model_A[idx];\r\n        modelB = corrs$Model_B[idx];\r\n        correlation = corrs$correlation[idx];\r\n        if ( correlation \u003c corrThresh ){\r\n            # now, check corr with every other member of modelSet\r\n            for ( model in modelSet ){\r\n                # want to check modelB corr with all models in modelSet\r\n              idxModelA = which( corrs$Model_A == model );\r\n              idxModelB = which( corrs$Model_B == modelB );\r\n                if ( model == modelB ){\r\n                    idxUsed[ length(idxUsed) + 1 ] = idx;\r\n                    # correlations of the same model\r\n                    next;\r\n                }\r\n              rightRowIdx = intersect( idxModelA, idxModelB );\r\n              if( length( rightRowIdx ) == 1 \u0026\u0026 rightRowIdx \u003c nrow(corrs) ){\r\n                rightRow = corrs[ rightRowIdx, ];\r\n              }\r\n              else{\r\n                next;\r\n              }\r\n                rightCorr = rightRow$correlation;\r\n                if ( rightCorr \u003e corrThresh ){\r\n                    idxUsed[ length(idxUsed) + 1 ] = idx;\r\n                    addModel = FALSE;\r\n                    break;\r\n                }\r\n            }\r\n            idxUsed[ length(idxUsed) + 1 ] = idx;\r\n        }\r\n        else{\r\n          addModel = FALSE;\r\n        }\r\n        if ( addModel ){\r\n          idxToUse = setdiff( idxUnused, idxUsed );\r\n          newCorrs = corrs[idxToUse,]\r\n          #newCorrs = corrs;\r\n          modelSet = addToModelSet( modelSet, newCorrs, modelB, corrThresh);\r\n        }\r\n    }\r\n    \r\n    return( modelSet );\r\n}\r\n\r\nmodelSets = list();\r\nprint( \u0022Model sets that have inter-correlations less than threshold:\u0022)\r\nfor ( model in modelNames ){\r\n  modelSet = addToModelSet( c(), modelCors, model, corrThresh );\r\n  cat( paste(paste(modelSet, collapse=\u0022, \u0022),\u0022\\n\u0022));\r\n  modelSets[ length(modelSets) + 1 ] = list( modelSet );\r\n}\r\n\r\n# now go through each model and find the other models which are correlated below\r\n# threshold (0.75)\r\n# manually choose the set #14\r\nuncorrelatedModelSet = modelSets[[14]];\r\n\r\n# now for all models that have passed criteria, create a model\r\nselectedTrainModels = trainModels[,c(\u0022Survived\u0022, uncorrelatedModelSet)];\r\nensemble_lr = glm( Survived~., data=selectedTrainModels, family=binomial);\r\nsummary( ensemble_lr );\r\ntrainSetProbs = predict( ensemble_lr, trainModels, type=\u0022response\u0022);  # now a vector of probabilities\r\n# probabilities \u003e= 0.5 mean survived, \u003c 0.5 mean perished\r\nthreshold = 0.5;\r\nselectedTrainPredictions = as.numeric( trainSetProbs \u003e= threshold );\r\nconfusionMatrix( selectedTrainPredictions, trainModels$Survived );\r\n\r\n# check to see if threshold 0.5 is appropriate\r\nrocr.pred = prediction( trainSetProbs, trainModels$Survived);\r\nacc.perf = performance( rocr.pred, measure=\u0022acc\u0022);\r\nplot( acc.perf, main=\u0022Manual Ensemble Logistic Regression Model Training Set Accuracy by Threshold\u0022 );\r\n# threshold 0.5 is appropriate\r\n\r\n# now, how\u0027d we do against the test set?\r\ntestSetProbs = predict( ensemble_lr, testModels, type=\u0022response\u0022);\r\ntestPredictions = as.numeric( testSetProbs \u003e= threshold );\r\nensembleModelManual = data.frame(PassengerId=newTest$PassengerId, Survived=testPredictions);\r\nwrite.csv( ensembleModelManual, \u0022EnsembleModelManual.csv\u0022, row.names=FALSE, quote=FALSE );\r\nensembleModelTrainManual = data.frame(PassengerId=newTrain$PassengerId, Survived=selectedTrainPredictions);\r\nwrite.csv( ensembleModelTrainManual, \u0022EnsembleModelManualTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\n# now try just an ensemble model, taking means\r\nensembleTrainModels = allTrainModels[,all_train_model_names];\r\n# simply take the means across the rows to get \u0022probabilities\u0022 of survival for each passenger\r\nensembleTrainPredictions = rowMeans( ensembleTrainModels );\r\nensembleTrainCalls = round( ensembleTrainPredictions );\r\nconfusionMatrix( ensembleTrainCalls, allTrainModels$Survived );\r\n\r\n# check to see if threshold 0.5 is appropriate\r\nrocr.pred = prediction( ensembleTrainPredictions, allTrainModels$Survived);\r\nacc.perf = performance( rocr.pred, measure=\u0022acc\u0022);\r\nplot( acc.perf, main=\u0022All Means Ensemble Logistic Regression Model Training Set Accuracy by Threshold\u0022 );\r\n# threshold 0.5 is appropriate\r\n\r\n# How\u0027d the ensemble model do against the test set?\r\nensembleTestModels = testModels[,modelNames];\r\n# simply take the means across the rows to get \u0022probabilities\u0022 of survival for each passenger\r\nensembleTestPredictions = rowMeans( ensembleTestModels );\r\nensembleTestCalls = round( ensembleTestPredictions );\r\n\r\n#fullLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);\r\n#fullLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullLogisticPredictions);\r\n# now, create a new test and train data frame with these models as predictors\r\n\r\n# write to  file\r\nensembleModelMeans = data.frame(PassengerId=newTest$PassengerId, Survived=ensembleTestCalls);\r\nwrite.csv( ensembleModelMeans, \u0022EnsembleModelMeans.csv\u0022, row.names=FALSE, quote=FALSE );\r\nensembleModelTrainMeans = data.frame(PassengerId=newTrain$PassengerId, Survived=ensembleTrainCalls);\r\nwrite.csv( ensembleModelTrainMeans, \u0022EnsembleModelMeansTrain.csv\u0022, row.names=FALSE, quote=FALSE );\r\n```\r\n\r\nThe manually selected linear regression ensemble model was constructed using three predictors: Random Forest, Gender model, and QDA.  The training set score was 0.8575 and test set score was 0.7847.  The simple row means model scored a 0.8664 against the training set and a 0.7703 against the test set.  As would be expected, the row means model, which used some very overfit models, was itself overfit - scoring high against the training set and low against the test set - lower than the manually selected ensemble model.\r\n\r\n### Cheat Models\r\n\r\nCheaters never prosper.  Nevertheless.\r\n\r\n#### Google Data-Mining\r\n\r\nAll of the Titanic data is available online.  It is possible to find resource pages listing who died and who survived.  Of course, much of it is not in a standardized format and has to be screenscraped.  In addition, there are errors and discrepancies both in the Kaggle data set and in the data online.  Word for word searches fail in many cases.  \r\n\r\n```{r cheatModel1, echo=TRUE, eval=FALSE, warning=FALSE}\r\nlibrary(RCurl);\r\nlibrary(XML);\r\nlibrary(readr);\r\n#testIDs = 892:nrow(data_combined);\r\n#userAgents = c( \u0022Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0\u0022, \u0022Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0\u0022, \u0022Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\u0022, \u0022Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_1 like Mac OS X) AppleWebKit/603.1.30 (KHTML, like Gecko) Version/10.0 Mobile/14E304 Safari/602.1\u0022, \u0022Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0)\u0022);\r\nuserAgents = c( \u0022Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36\u0022, \u0022Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36 Edge/15.15063\u0022 );\r\nnumUserAgents = length( userAgents );\r\ntrainIDs = 1:891;\r\ntestIDs = 892:nrow(data_combined);\r\nsetIDs = 1:nrow(data_combined);\r\ntrainURLS = c();\r\nsetHits = data_combined[,c(\u0022PassengerId\u0022,\u0022Survived\u0022)];\r\nsetHits$SurvivorHits = 0;\r\nsetHits$VictimHits = 0;\r\nsetHits$SurvivorFirst = 0;\r\nsetHits$SurvivedPrediction = 0;\r\nfor ( pass_idx in 1:length(setIDs) ){\r\n  i = setIDs[pass_idx];\r\n  name = gsub( \u0022 \u0022, \u0022+\u0022, as.character( data_combined[i,\u0022NameString\u0022] ) ); # remove whitespace\r\n  sex = as.character( data_combined[i,\u0022Sex\u0022] );\r\n  searchURL = paste0( \u0022https://www.google.com/search?q=\u0022, name, \u0022+\u0022, sex, \u0022+titanic\u0022 );\r\n  trainURLS[i] = searchURL;\r\n  randomUserAgent = userAgents[ round( runif(1, 1, numUserAgents) ) ];\r\n  wd = getwd();\r\n  localFile = paste0( wd, \u0022/html/\u0022, i, \u0022.html\u0022 );\r\n  if ( file.exists( localFile )){\r\n    resultsHTML = read_file( localFile );\r\n  }else{\r\n    resultsHTML = getURL( searchURL, httpheader=c(\u0027User-Agent\u0027=randomUserAgent) );\r\n    write( resultsHTML, localFile );\r\n    Sys.sleep( runif( 1, 27, 67)); # randomly sleep between 27 and 67 seconds\r\n    # this is necessary because if Google thinks you\u0027re mining their data with a bot\r\n    # you will be grounded and sent to your room for a timeout.\r\n  }\r\n  # now search for \u0022victim\u0022 and \u0022survivor\u0022\r\n  victimHits = gregexpr(\u0022victim\u0022, resultsHTML, ignore.case=TRUE);\r\n  victimHitIndices = which( victimHits[[1]] \u003e -1 );\r\n  numVictimHits = length( victimHitIndices );\r\n  setHits[pass_idx,\u0022VictimHits\u0022] = numVictimHits;\r\n  firstVictimHit = victimHits[[1]][1];\r\n  survivorHits = gregexpr(\u0022survivor\u0022, resultsHTML, ignore.case=TRUE);\r\n  survivorHitIndices = which( survivorHits[[1]] \u003e -1 );\r\n  numSurvivorHits = length( survivorHitIndices );\r\n  setHits[pass_idx,\u0022SurvivorHits\u0022] = numSurvivorHits;\r\n  firstSurvivorHit = survivorHits[[1]][1];\r\n  #cat( \u0022row \u0022, i, \u0022\\n\u0022)\r\n  #assert_that( ( numSurvivorHits \u003e 0 ) || (numVictimHits \u003e 0) );\r\n  if (!( ( numSurvivorHits \u003e 0 ) || (numVictimHits \u003e 0) ) ){\r\n    cat( \u0022Could not find \u0027victim\u0027 or \u0027survivor\u0027 keyword for PassengerID \u0022, i, \u0022.  Manually investigate and update .html file with \u0027survived\u0027 or \u0027victim\u0027.\\n\u0022)\r\n  }\r\n  survived = FALSE;\r\n  if ( numSurvivorHits \u003e numVictimHits ){\r\n      survived = TRUE;\r\n  }\r\n  else if ( ( numSurvivorHits \u003e 0 ) \u0026\u0026 ( numVictimHits \u003e 0 ) ){\r\n    # both are positive, take the min\r\n    if ( firstSurvivorHit \u003c firstVictimHit ){\r\n      survived = TRUE;\r\n    }\r\n    else{\r\n      survived = FALSE;\r\n    }\r\n    setHits[pass_idx,\u0022SurvivorFirst\u0022] = as.numeric( survived );\r\n  }\r\n  survived_flag = 0;\r\n  if( survived ){\r\n    survived_flag = 1;\r\n  }\r\n  setHits[pass_idx,\u0022SurvivedPrediction\u0022] = survived_flag;\r\n  if ( !is.na( data_combined[i,\u0022Survived\u0022] ) \u0026\u0026 survived_flag != data_combined[i,\u0022Survived\u0022] ){\r\n    #cat( \u0022Prediction does not match truth at \u0022, i, as.character( data_combined[i,\u0022Name\u0022] ), \u0022\\n\u0022);\r\n  }\r\n}\r\n# evaluate training set performance \r\ntrainingPredictions = setHits$SurvivedPrediction[1:891];\r\nconfusionMatrix( trainingPredictions, data_combined$Survived[1:891])\r\n\r\ncheatTestPredictions = setHits$SurvivedPrediction[892:nrow(data_combined)];\r\n\r\nwrite.csv( setHits, file=\u0022CheatModel1.csv\u0022, quote=FALSE);\r\n```\r\n\r\n#### Screen Scraping Titanic Resource Webpage\r\n\r\nA webpage exists that lists all the survivors in bold and all the passengers who died in normal font.  It is possible to build a scraper to parse the passengers into two lists and then try to match the names in the Kaggle test set with the names in the Survived and Died sets derived from the webpage.  The idea is to use both a bag-of-words model and a distance model to capture both number of token hits and handle any differences in spelling.\r\n\r\n```{r cheatModel2, echo=TRUE, eval=FALSE, warning=FALSE}\r\n# read in CheatModel1\r\ncheat \u003c- read.csv(\u0022CheatModel1.csv\u0022, header = TRUE);\r\ncheat$SurvivedFactor = as.factor( cheat$Survived );\r\ncheat$SurvivedTokenMatches = 0;\r\ncheat$DiedTokenMatches = 0;\r\ncheat$SurvivedTokenDistance = 0;\r\ncheat$DiedTokenDistance = 0;\r\nresourceURL = \u0022http://www.titanic-whitestarships.com/1st_Class_Pass.htm\u0022;\r\n# Example HTML:\r\n#\u003cstrong\u003eGordon, Sir Cosmo Duff\u003cbr\u003e\r\n#Gordon, Lady Lucile Duff\u003cbr\u003e\r\n#and Maid (Miss Laura Mabel Francatelli)\u003cbr\u003e\r\n#Gracie, Colonel Archibald IV\u003c/strong\u003e\u003cbr\u003e\r\n#Graham, Mr. George Edward\u003cbr\u003e\r\n#Graham1, Mr. George Edward\u003cbr\u003e\r\n#Graham2, Mr. George Edward\u003cbr\u003e\r\n#Graham3, Mr. George Edward\u003cbr\u003e\r\n#\u003cstrong\u003eGraham, Mrs. William Thompson\u003cbr\u003e\r\n#(nee Edith Junkins)\u003cbr\u003e\r\n#Graham, Miss Margaret\u003cbr\u003e\r\n#Greenfield, Mrs. Leo David\u003cbr\u003e\r\n#(nee Blanche Strouse)\u003cbr\u003e\r\n#Greenfield, Mr. William Bertram\u003c/strong\u003e\u003cbr\u003e\r\n#Guggenheim, Mr. Benjamin\u003cbr\u003e\r\n#and Manservant (Victor Giglio)\u003c/font\u003e\u003c/p\u003e\r\ncheatSurvived = numeric(nrow(newTest));\r\nwd = getwd();\r\nlocalFile = file.path( wd, \u0022TitanicResource.htm\u0022 );  \r\n# know the file is approx 2100 lines.  Initialize each died/survived list to 3000\r\ndied = character(3000);\r\nsurvived = character(3000);\r\nif ( !file.exists( localFile )){  # Do this so we can load the URL once and not spam it everytime we run this script\r\n  resultsHTML = getURL( resourceURL );\r\n  write( resultsHTML, localFile );\r\n}\r\n# now, parse the file, line-by-line.  Survivors are in bold, denoted by the \u003cstrong\u003e tag:\r\nfid = file( localFile, \u0022r\u0022);\r\nisStrong = FALSE;\r\ndiedCtr = 1;\r\nsurvivedCtr = 1;\r\nwhile( TRUE ){\r\n  line = readLines(fid,n=1);\r\n  if( length(line) == 0 ){\r\n    break;\r\n  }\r\n  # look for \u003c/strong\u003e\r\n  thisLineStartStrong = grepl( \u0022^\u003cstrong\u003e\u0022, line );\r\n  thisLineEndStrong = grepl( \u0022\u003c/strong\u003e\u0022, line );\r\n  # if line contains \u003cstrong\r\n  # strip html tags\r\n  rawText = gsub( \u0022\u003c.*?\u003e\u0022, \u0022\u0022, line );\r\n  # skip if no comma\r\n  thisLineHasComma = grepl( \u0022,\u0022, rawText );\r\n  if ( !thisLineHasComma ){\r\n    next;\r\n  }\r\n  # remove punctuation\r\n  rawText = gsub( \u0022[[:punct:]]\u0022, \u0022\u0022, rawText );\r\n  # skip if line (name) is too long\r\n  if ( nchar( rawText) \u003e 100 ){\r\n    next;\r\n  }\r\n  if( isStrong || thisLineStartStrong ){\r\n    survived[survivedCtr] = rawText;\r\n    survivedCtr = survivedCtr + 1;\r\n  }\r\n  else{\r\n    died[diedCtr] = rawText;\r\n    diedCtr = diedCtr + 1;\r\n  }\r\n  isStrong = thisLineStartStrong \u0026\u0026 !thisLineEndStrong;\r\n}\r\nclose(fid);\r\n# truncate the died/survived lists\r\nsurvived = sort( survived[1:survivedCtr-1] );\r\ndied = sort( died[1:diedCtr-1] );  # yes, this includes crap lines at the top of list\r\n#\r\n\r\n# now have two lists and need to do bag of words searching vs. each name\r\n# for each name, tokenize, then query the died/survived lists\r\nfor( i in 1:nrow( newTest ) ){\r\n  thisName = newTest[i,\u0022Name\u0022];\r\n  # remove non-word characters \r\n  thisName = gsub( \u0022[[:punct:]]\u0022, \u0022\u0022, thisName );\r\n  # now tokenize each name and search each died/survived for distance from each token\r\n  nameTokens = strsplit( thisName, \u0022\\\\s+\u0022)[[1]];\r\n  numTokens = length( nameTokens );\r\n  # search through survived\r\n  survivedTokenMatches = numeric(length(survived));\r\n  survivedTokenDistance = numeric(length(survived));\r\n  for ( j in 1:length( survived ) ){\r\n    thisScore = 0;\r\n    thisSurvived = survived[j];\r\n    survivedTokens = strsplit( thisSurvived, \u0022\\\\s+\u0022)[[1]];\r\n    numSurvivedTokens = length( survivedTokens );\r\n    # now, take each name token and search for match in survived tokens\r\n    for ( k in 1:numTokens){\r\n      thisNameToken = nameTokens[k];\r\n      if ( thisNameToken %in% survivedTokens ){\r\n        thisScore = thisScore + 1;\r\n      }\r\n    }\r\n    survivedTokenMatches[j] = thisScore;\r\n    # go through each name token and find the min distance to survived tokens\r\n    totalDist = 0;\r\n    for ( k in 1:numTokens){\r\n      thisNameToken = nameTokens[k];\r\n      wordDist = 10;\r\n      distances = numeric(numSurvivedTokens);\r\n      for ( m in 1:numSurvivedTokens){\r\n        thisSurvivedToken = survivedTokens[m];\r\n        distances[m] = adist( thisNameToken, thisSurvivedToken );\r\n      }\r\n      totalDist = totalDist + min( distances );\r\n    }\r\n    survivedTokenDistance[j] = totalDist;\r\n  }\r\n  bestSurvivedScore = max( survivedTokenMatches );\r\n  bestSurvivedDistance = min( survivedTokenDistance );\r\n  cheat[i,\u0022SurvivedTokenHits\u0022] = bestSurvivedScore;\r\n  cheat[i,\u0022SurvivedTokenDistance\u0022] = bestSurvivedDistance;\r\n  \r\n  # search through died\r\n  diedTokenMatches = numeric(length(died));\r\n  diedTokenDistance = numeric(length(died));\r\n  for ( j in 1:length( died ) ){\r\n    thisScore = 0;\r\n    thisDied = died[j];\r\n    diedTokens = strsplit( thisDied, \u0022\\\\s+\u0022)[[1]];\r\n    numDiedTokens = length( diedTokens );\r\n    # now, take each name token and search for match in survived tokens\r\n    for ( k in 1:numTokens){\r\n      thisNameToken = nameTokens[k];\r\n      if ( thisNameToken %in% diedTokens ){\r\n        thisScore = thisScore + 1;\r\n      }\r\n    }\r\n    diedTokenMatches[j] = thisScore;\r\n    # go through each name token and find the min distance to survived tokens\r\n    totalDist = 0;\r\n    for ( k in 1:numTokens){\r\n      thisNameToken = nameTokens[k];\r\n      wordDist = 10;\r\n      distances = numeric(numDiedTokens);\r\n      for ( m in 1:numDiedTokens){\r\n        thisDiedToken = diedTokens[m];\r\n        distances[m] = adist( thisNameToken, thisDiedToken );\r\n      }\r\n      totalDist = totalDist + min( distances );\r\n    }\r\n    diedTokenDistance[j] = totalDist;\r\n  }\r\n  \r\n  bestDiedScore = max( diedTokenMatches );\r\n  bestDiedDistance = min( diedTokenDistance );\r\n  cheat[i,\u0022DiedTokenHits\u0022] = bestSurvivedScore;\r\n  cheat[i,\u0022DiedTokenDistance\u0022] = bestSurvivedDistance;\r\n  #cat( thisName, \u0022 bestScore = \u0022, bestScore, \u0022, bestMatch = \u0022, survived[bestIdx], \u0022\\n\u0022);\r\n}\r\n\r\n# now that we have some cheating scores, build a simple tree model\r\nlibrary(tree);\r\ncheat_train = cheat[1:891,];  # use the training data\r\ncheat_test= cheat[892:nrow(cheat),];  # use the test data\r\ntree_cheat_model = tree( SurvivedFactor~SurvivorHits+VictimHits+SurvivorFirst+SurvivedTokenHits+SurvivedTokenDistance+DiedTokenHits+DiedTokenDistance,data=cheat_train );\r\nsummary( tree_cheat_model )\r\nplot( tree_cheat_model )\r\ntext( tree_cheat_model, pretty=0);\r\ncheat_train_pred = predict( tree_cheat_model, cheat_train, type=\u0022class\u0022);\r\ncheat_test_pred = predict( tree_cheat_model, cheat_test, type=\u0022class\u0022);\r\n# performance\r\nconfusionMatrix(cheat_train_pred, cheat_train$Survived);\r\n\r\ncheatModelTest = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(cheat_test_pred)-1);\r\nwrite.csv( cheatModelTest, \u0022CheatModelFull.csv\u0022, row.names=FALSE, quote=FALSE );\r\n\r\n```\r\n\r\n### Summary\r\n\r\nHere is the summary of training and test results for the different models:\r\n```{r summary, eval=FALSE, echo=FALSE}\r\nlibrary(matlab,quietly=TRUE);\r\nlibrary(ROCR,quietly=TRUE);\r\n\r\n# these csv models represent the binary 0, 1 predictions of Survival for the different models\r\nmodels = c( \u0022BagModel.csv\u0022, \u0022BagSigModel.csv\u0022, \u0022BoostedTreeModel.csv\u0022, \u0022BoostedTreeSigModel.csv\u0022, \u0022gender_submission.csv\u0022, \u0022KNNBest6Model.csv\u0022, \u0022KNNBest7Model.csv\u0022, \u0022KNNBest8Model.csv\u0022, \u0022KNNBest9Model.csv\u0022, \u0022KNNFullModel.csv\u0022, \u0022LassoRegressionFullModel.csv\u0022, \u0022LassoRegressionSigModel.csv\u0022, \u0022LDAModel.csv\u0022, \u0022LDAReducedModel.csv\u0022, \u0022LogisticRegressionFullModel.csv\u0022, \u0022LogisticRegressionSigModel.csv\u0022, \u0022NeuralNetworkFullModel.csv\u0022, \u0022NeuralNetworkFullModelCV.csv\u0022, \u0022NeuralNetworkSigModel.csv\u0022, \u0022NeuralNetworkSigModelCV.csv\u0022, \u0022QDAModel.csv\u0022, \u0022QDASigModel.csv\u0022, \u0022RandomForestModel.csv\u0022, \u0022RandomForestSigModel.csv\u0022, \u0022RandomForestTrimmedModel.csv\u0022, \u0022RidgeRegressionFullModel.csv\u0022, \u0022RidgeRegressionSigModel.csv\u0022, \u0022SVMModel.csv\u0022, \u0022NullModel.csv\u0022, \u0022TreeModel.csv\u0022, \u0022TreeSigModel.csv\u0022, \u0022WomenAndChildrenFirstModel.csv\u0022, \u0022EnsembleModelManual.csv\u0022, \u0022EnsembleModelMeans.csv\u0022 );\r\n\r\ntrainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );\r\ntestModels = data.frame(PassengerId=1:nrow(newTest));\r\nmodelNames = c();\r\n\r\n# testAnswers is a .csv file representing all the correct predictions for the test set.\r\n# I created this through trial and error and some cheat models and use it here to compare\r\n# predictions with the answer key to show in a table (easier than hitting Kaggle every\r\n# time for a score calculation).  If you don\u0027t have this file, just make a default of\r\n# everyone dying\r\ntestAnswers = data.frame(PassengerId=1:nrow(newTest), Survived=0);\r\nif( file.exists(\u0022TestAnswers.csv\u0022)){\r\n    testAnswers = read.csv( \u0022TestAnswers.csv\u0022, header=TRUE );\r\n}\r\n\r\ntrainScores = c();\r\ntestScores = c();\r\n\r\nnumModels = length( models );\r\nfor ( i in 1:numModels ){\r\n  testFileName = models[i];\r\n  fp = fileparts( testFileName );\r\n  testName = fp$name;\r\n  modelNames[i] = testName;\r\n  trainName = paste0( testName, \u0022Train\u0022 );\r\n  trainFileName = paste0( trainName, fp$ext );\r\n  trainModelFrame = read.csv( trainFileName );\r\n  testModelFrame = read.csv( testFileName );\r\n  trainScores[i] = mean( trainModelFrame$Survived == trainModels$Survived );\r\n  testScores[i] = mean( testModelFrame$Survived == testAnswers$Survived );\r\n  trainModelFrame$Score = trainScores[i];\r\n  testModelFrame$Score = testScores[i];\r\n}\r\ntypes = c( rep(\u0022Training Score\u0022, length(modelNames)), rep(\u0022Test Score\u0022, length(modelNames)));\r\ntrainOrder = sort( trainScores, decreasing = TRUE, index.return = TRUE );\r\n# now reorder according to training score\r\nmodelNames = modelNames[trainOrder$ix];\r\ntrainScores = trainScores[trainOrder$ix];\r\ntestScores = testScores[trainOrder$ix];\r\nscores = data.frame( Model = c(modelNames,modelNames), Accuracy = c(trainScores,testScores), Type = types );\r\nscores$Model = factor(scores$Model, levels=modelNames);\r\nscores$Type = factor(scores$Type, levels=c(\u0022Training Score\u0022, \u0022Test Score\u0022));\r\nscores$CVMethod = \u0022-\u0022\r\nif( !file.exists(\u0022ModelResults.csv\u0022)){\r\n    write.csv( scores, \u0022ModelResults.csv\u0022, row.names=FALSE, quote=FALSE );\r\n} else{\r\n  # why read this in?  Because it might be annotated.  And if you\u0027re missing the TestAnswers.csv\r\n  # file, all the test scores will be wrong\r\n  scores = read.csv(\u0022ModelResults.csv\u0022, header=TRUE);\r\n}\r\n\r\nggplot( data=scores, aes(x=Model, y=Accuracy, group=Type, colour=Type)) + \r\n  geom_line() + \r\n  geom_point() +\r\n  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))\r\n\r\nmodel_results = scores;\r\nmodels = as.character( model_results$Model );\r\nuniqueModels= unique( as.character( model_results$Model) );\r\ntrainingScores = sort(model_results[model_results$Type==\u0022Training Score\u0022,\u0022Accuracy\u0022], decreasing=TRUE, index.return=TRUE);\r\ntrainingModels = models[trainingScores$ix];\r\ncvMethods = model_results[model_results$Type==\u0022Training Score\u0022,\u0022CVMethod\u0022];\r\ntestScores = c();\r\nfor ( z in 1:length(trainingModels)){\r\n  thisModel = trainingModels[z];\r\n  idxTestScore = which(( model_results$Model == thisModel ) \u0026 (model_results$Type == \u0022Test Score\u0022) );\r\n  testScores[z] = model_results$Accuracy[idxTestScore];\r\n}\r\nannotatedCVMethods = cvMethods[trainingScores$ix];\r\nif( ! file.exists(\u0022TestAnswers.csv\u0022)){\r\n  testScores = c();\r\n  annotatedCVMethods = c();\r\n}\r\nsummaryTable = data.frame( Model = trainingModels, CVMethod=annotatedCVMethods, TrainingScore = trainingScores$x, TestScore = testScores);\r\nknitr::kable( summaryTable, format=\u0022markdown\u0022, longtable=TRUE)\r\n```\r\n\r\n![Model Summary](TitanicModelSummary.png)\r\n\r\nModel|CVMethod|Training Score|Test Score\r\n-------------------------------- | ---------- | ---------- | -----------\r\nBag Model|-|0.9876543|0.7320574\r\nBag Sig Model|-|0.9854097|0.7272727\r\nNeural Network Full Model|-|0.9203143|0.6746411\r\nNeural Network Sig Model|-|0.9124579|0.7105263\r\nBoosted Tree Model|8-fold CV|0.8933782|0.7559809\r\nBoosted Tree Sig Model|8-fold CV|0.8922559|0.7368421\r\nKNN Best 8 Model|LOOCV|0.8787879|0.7559809\r\nKNN Best 6 Model|LOOCV|0.8754209|0.7200957\r\nKNN Best 7 Model|LOOCV|0.8754209|0.7344498\r\nKNN Best 9 Model|LOOCV|0.8742985|0.7631579\r\nEnsemble Means Model|-|0.8664000|0.7703000\r\nRandomForest Sig Model|-|0.8608305|0.7822967\r\nEnsemble Manual Model|-|0.8575000|0.7847000\r\nRandomForest Model|-|0.8574635|0.7846890\r\nKNN Full Model|LOOCV|0.8563412|0.7607656\r\nTree Sig Model|Pruning|0.8540965|0.7607656\r\nNeural Network Sig Model|10-fold CV|0.8529742|0.7751196\r\nTree Model|Pruning|0.8507295|0.7727273\r\nRandomForest Trimmed Model|-|0.8484848|0.7775120\r\nNeural Network Full Model|10-fold CV|0.8417508|0.7846890\r\nLogistic Regression Sig Model|-|0.8395062|0.7607656\r\nSVM Model|10-fold CV|0.8395062|0.7679426\r\nLasso Regression Sig Model|10-fold CV|0.8372615|0.7775120\r\nLDA Model|-|0.8372615|0.7727273\r\nLasso Regression Full Model|10-fold CV|0.8361392|0.7703349\r\nLDA Reduced Model|-|0.8338945|0.7727273\r\nLogistic Regression Full Model|-|0.8338945|0.7607656\r\nRidge Regression Sig Model|10-fold CV|0.8249158|0.7870813\r\nRidge Regression Full Model|10-fold CV|0.8226712|0.7846890\r\nQDAModel|-|0.8159371|0.7296651\r\nWomenAndChildrenFirstModel|-|0.7901235|0.7511962\r\ngender_submission|-|0.7867565|0.7631579\r\nQDASigModel|-|0.7845118|0.7272727\r\nNullModel|-|0.6161616|0.6244019\r\n\r\nNow then, how does one go about picking a model out of all of these?  Clearly, the training scores of some of the more complex models cannot be trusted as they\u0027re obviously overfit.  The bagged tree model has training accuracies of about 0.98!  But how would we know that they\u0027re overfit without peeking at the test scores?  The boosted tree cross-validation accuracies were about 0.89 - but these also proved to be vastly overfit.\r\n\r\nBut suppose all of these training results were more or less equal - some at 0.84 cross-validated training set accuracy, some at 0.83 cross-validated training set accuracy, and one at 0.85 cross-validated training set accuracy.  What then would be the criterion for choosing a model?  At first blush, it might be obvious to choose the one that that has the highest cross-validated training set accuracy of 0.85.  But what if that model is very complex with a lot of parameterization and there is a much simpler model that scored at 0.84.  Then would it be better to choose the simpler, but lower-scoring model?  Is the difference between 0.84 and 0.85 statistically significant or is the difference just due to random chance?  Is there a way to quantify the statistical significance?  I\u0027ve seen McNemar\u0027s test but it looks like it operates on the correct counts between only two different models, not when num_models \u003e 2.  And if we run McNemar\u0027s test over all pairs of models, then we\u0027re likely to see something \u0022significant\u0022 just through chance and need to somehow correct for it.\r\n\r\nIf I had to pick the \u0022best\u0022 model out of these, it would probably be the ridge regression model.  The squared regularization penalty helps to drive down the coefficients of some of the predictors better than the similar lasso regression model.  \r\n\r\n## Lessons Learned\r\n\r\nThis report is a little stream-of-consciousness as it is/was my first Kaggle competition and kernel.  But what a great introduction to overfitting with R.  The EDA portion was really enjoyable as was the feature engineering section.  I thought that time put in to the early part would pay off later on but in retrospect, I probably should have tried to regularize the data a little more with outlier detection and deletion.  But I was reluctant to delete any data that might help me eek out a few more percentage points!\r\n\r\nAlso, in retrospect, it is *much* easier to run all of the models through the caret framework.  In this model, a single feature set can be run through different models fairly seemlessly - and then even run as ensembles at a later step.\r\n\r\nThanks for reading along!","dateCreated":"2018-09-01T00:28:50.8416023Z"},"resources":null,"isolatorResults":"\u003cresults\u003e\u003cdisk_kb_free\u003e4814684\u003c/disk_kb_free\u003e\u003cdocker_image_digest\u003e52937f97db1d60cc30110f82402aec2d3dec26569f3ae4239adbe334f10492ad\u003c/docker_image_digest\u003e\u003cdocker_image_id\u003esha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a\u003c/docker_image_id\u003e\u003cdocker_image_name\u003egcr.io/kaggle-images/rstats\u003c/docker_image_name\u003e\u003cexit_code\u003e0\u003c/exit_code\u003e\u003cfailure_message /\u003e\u003cinvalid_path_errors\u003eFalse\u003c/invalid_path_errors\u003e\u003cout_of_memory\u003eFalse\u003c/out_of_memory\u003e\u003crun_time_seconds\u003e4783.986551859\u003c/run_time_seconds\u003e\u003csucceeded\u003eTrue\u003c/succeeded\u003e\u003ctimeout_exceeded\u003eFalse\u003c/timeout_exceeded\u003e\u003cused_all_space\u003eFalse\u003c/used_all_space\u003e\u003cwas_killed\u003eFalse\u003c/was_killed\u003e\u003c/results\u003e","runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageDigest":"52937f97db1d60cc30110f82402aec2d3dec26569f3ae4239adbe334f10492ad","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"kaggle/rstats","diskKbFree":4814684,"failureMessage":"","exitCode":0,"queuedSeconds":0,"outputSizeBytes":0,"runTimeSeconds":4783.986551859,"usedAllSpace":false,"timeoutExceeded":false,"isValidStatus":false,"wasGpuEnabled":false,"wasInternetEnabled":false,"outOfMemory":false,"invalidPathErrors":false,"succeeded":true,"wasKilled":false},"outputFilesTotalSizeBytes":7122526,"dockerImageVersionId":47,"usedCustomDockerImage":false},"author":{"id":226735,"displayName":"Neill White","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"neillwhite0","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/226735-kg.jpg","profileUrl":"/neillwhite0","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":1,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"baseUrl":"/neillwhite0/titanic-survival-prediction-with-r","collaborators":{"owner":{"userId":226735,"groupId":null,"groupMemberCount":null,"profileUrl":"/neillwhite0","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/226735-kg.jpg","name":"Neill White","slug":"neillwhite0","userTier":1,"joinDate":null,"type":"owner","isUser":true,"isGroup":false},"collaborators":[]},"initialTab":null,"log":"[{\n  \u0022data\u0022: \u0022\\n\\nprocessing file: script.Rmd\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.872084660999462\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |                                                                 |   0%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 5.023278017999473\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.                                                                |   1%\\nlabel: getpics (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 5.0550702679993265\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.2834521340009815\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.                                                                |   2%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.315928936999626\n},{\n  \u0022data\u0022: \u0022label: trainsample (with options) \\nList of 1\\n $ echo:\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.413268992000667\n},{\n  \u0022data\u0022: \u0022 logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.445506680000108\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..                                                               |   2%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..                                                               |   3%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.712555280000743\n},{\n  \u0022data\u0022: \u0022label: testsample (with options) \\nList of 2\\n $ echo   : logi FALSE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.744592524999462\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...                                                              |   4%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 7.193304378999528\n},{\n  \u0022data\u0022: \u0022label: eda1\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...                                                              |   5%\\nlabel: eda1table (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\\r  |                                                                       \\r  |....                                                             |   5%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....                                                             |   6%\\nlabel: eda2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 7.227295443999537\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....                                                             |   7%\\nlabel: eda2table (with options) \\nList of 2\\n $ echo   : logi FALSE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 7.258266116999948\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.....                                                            |   7%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....                                                            |   8%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 7.489057959001002\n},{\n  \u0022data\u0022: \u0022label: eda_pclass1\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 7.521602614000585\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......                                                           |   9%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.53553954400013\n},{\n  \u0022data\u0022: \u0022label: eda_pclass2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.568057989999943\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......                                                           |  10%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 9.921322797999892\n},{\n  \u0022data\u0022: \u0022label: eda_name\\n\\r  |                                                                       \\r  |.......                                                          |  10%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.......                                                          |  11%\\nlabel: eda_sex1\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 9.99041342000055\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........                                                         |  12%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 10.242822157000774\n},{\n  \u0022data\u0022: \u0022label: eda_sex2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 10.274594668000645\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........                                                         |  13%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 10.877930927999842\n},{\n  \u0022data\u0022: \u0022label: eda_age1\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 10.910791624999547\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........                                                        |  13%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.48589714300033\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........                                                        |  14%\\nlabel: eda_age2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.531334355000581\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........                                                       |  15%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.211946071000057\n},{\n  \u0022data\u0022: \u0022label: eda_sibsp1\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.246751119000692\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........                                                       |  16%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.590389774999494\n},{\n  \u0022data\u0022: \u0022label: eda_sibsp2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.627386238000327\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...........                                                      |  16%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...........                                                      |  17%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.982941075999406\n},{\n  \u0022data\u0022: \u0022label: eda_parch1\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 13.023912463999295\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............                                                     |  18%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 13.308367511001052\n},{\n  \u0022data\u0022: \u0022label: eda_parch2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 13.345158125001035\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............                                                     |  19%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 13.762469612000132\n},{\n  \u0022data\u0022: \u0022label: eda_ticket\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.............                                                    |  20%\\nlabel: eda_fare1\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 13.798038889000964\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.............                                                    |  21%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 14.658605480000915\n},{\n  \u0022data\u0022: \u0022label: eda_fare2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 14.691738446999807\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............                                                   |  21%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..............                                                   |  22%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 15.470462966999548\n},{\n  \u0022data\u0022: \u0022label: eda_cabin1\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............                                                  |  23%\\nlabel: eda_cabin2 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 15.50464217499939\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............                                                  |  24%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 16.32343986200067\n},{\n  \u0022data\u0022: \u0022label: eda_cabin3 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |................                                                 |  24%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................                                                 |  25%\\nlabel: eda_cabin4\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 16.359605719000683\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................                                                |  26%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.05454867499975\n},{\n  \u0022data\u0022: \u0022label: eda_embarked1 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................                                                |  27%\\nlabel: eda_embarked2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.088258690000657\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................                                               |  27%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..................                                               |  28%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.967713828000342\n},{\n  \u0022data\u0022: \u0022label: errors1 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................                                              |  29%\\nlabel: errors2 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.999866059999476\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................                                              |  30%\\nlabel: errors3 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\\r  |                                                                       \\r  |....................                                             |  30%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....................                                             |  31%\\nlabel: errors4 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.034587878999446\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....................                                             |  32%\\nlabel: feature1\\n\\r  |                                                                       \\r  |.....................                                            |  32%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....................                                            |  33%\\nlabel: impute_embarked\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.06595862100039\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......................                                           |  33%\\nlabel: impute_fare1 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\\r  |                                                                       \\r  |......................                                           |  34%\\n  ordinary text without R code\\n\\n\\nlabel: impute_fare2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.09700011300083\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.......................                                          |  35%\\n  ordinary text without R code\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.342028891000155\n},{\n  \u0022data\u0022: \u0022label: impute_age1 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.375927126000533\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.......................                                          |  36%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........................                                         |  36%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.50648754100075\n},{\n  \u0022data\u0022: \u0022label: impute_age2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.5398016550007\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |........................                                         |  37%\\n  ordinary text without R code\\n\\n\\nlabel: feature2\\n\\r  |                                                                       \\r  |.........................                                        |  38%\\n  ordinary text without R code\\n\\n\\nlabel: feature3\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.57037017500079\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................                                        |  39%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........................                                       |  39%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.677281374000813\n},{\n  \u0022data\u0022: \u0022label: feature4\\n\\r  |                                                                       \\r  |..........................                                       |  40%\\n  ordinary text without R code\\n\\n\\nlabel: feature5\\n\\r  |                                                                       \\r  |...........................                                      |  41%\\n  ordinary text without R code\\n\\n\\nlabel: feature6\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.708913755999674\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...........................                                      |  42%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................                                     |  42%\\nlabel: feature7\\n\\r  |                                                                       \\r  |............................                                     |  43%\\n  ordinary text without R code\\n\\n\\nlabel: feature8\\n\\r  |                                                                       \\r  |............................                                     |  44%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.............................                                    |  44%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.740470929000367\n},{\n  \u0022data\u0022: \u0022label: feature9\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.77157088400054\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................                                    |  45%\\n  ordinary text without R code\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 19.970148627000526\n},{\n  \u0022data\u0022: \u0022label: feature10\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 20.00139563800076\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............................                                   |  46%\\n  ordinary text without R code\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.30323554200004\n},{\n  \u0022data\u0022: \u0022label: feature11\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.34190453500014\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............................                                   |  47%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............................                                  |  47%\\nlabel: setup (with options) \\nList of 1\\n $ include: logi FALSE\\n\\n\\r  |                                                                       \\r  |...............................                                  |  48%\\n  ordinary text without R code\\n\\n\\nlabel: unnamed-chunk-1 (with options) \\nList of 1\\n $ include: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.37548892599989\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |................................                                 |  49%\\n  ordinary text without R code\\n\\n\\nlabel: null1 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.406959580999683\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |................................                                 |  50%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................................                                |  50%\\nlabel: null2 (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\\r  |                                                                       \\r  |.................................                                |  51%\\n  ordinary text without R code\\n\\n\\nlabel: genderSubmissionModel (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\\r  |                                                                       \\r  |..................................                               |  52%\\n  ordinary text without R code\\n\\n\\nlabel: womenAndChildrenFirstTrain (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 22.075118775999726\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................                               |  53%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................................                              |  53%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 24.607750887000293\n},{\n  \u0022data\u0022: \u0022label: womenAndChildrenFirstTest (with options) \\nList of 1\\n $ echo: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 24.63909694799986\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...................................                              |  54%\\n  ordinary text without R code\\n\\n\\nlabel: lda1 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 24.670163885000875\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |....................................                             |  55%\\n  ordinary text without R code\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.04900891000034\n},{\n  \u0022data\u0022: \u0022label: lda2 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.082253161999688\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |....................................                             |  56%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....................................                            |  56%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.501642975999857\n},{\n  \u0022data\u0022: \u0022label: qda1 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.534106966000763\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.....................................                            |  57%\\n  ordinary text without R code\\n\\n\\nlabel: qda2 (with options) \\nList of 2\\n $ echo   :\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.584270321000076\n},{\n  \u0022data\u0022: \u0022 logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.618471253999815\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.....................................                            |  58%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.6542870079993\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |......................................                           |  58%\\nlabel: logisticRegression1 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.69036469999992\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.754358690001027\n},{\n  \u0022data\u0022: \u0022\\r  |......................................                           |  59%\\n  ordinary text without R code\\n\\n\\nlabel: logisticRegression2 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |.......................................                          |  60%\\n  ordinary text without R code\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.787233613000353\n},{\n  \u0022data\u0022: \u0022label: logisticRegression3 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |.......................................                          |  61%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........................................                         |  61%\\nlabel: logisticRegression4 (with options) \\nList of 2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.821338204999847\n},{\n  \u0022data\u0022: \u0022 $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 25.852800463000676\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |........................................                         |  62%\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 94.72782515499966\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\nlabel: logisticRegression5 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 94.76363148600103\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................................                        |  63%\\n  ordinary text without R code\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 162.1321635660006\n},{\n  \u0022data\u0022: \u0022label: logisticRegression6 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 162.16561375700076\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................................                        |  64%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 162.21508514200104\n},{\n  \u0022data\u0022: \u0022  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........................................                       |  64%\\nlabel: ridgeRegression1 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 162.2547983759996\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..........................................                       |  65%\\n  ordinary text without R code\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 163.07039483900007\n},{\n  \u0022data\u0022: \u0022label: ridgeRegression1b (with options) \\nList of 1\\n $ echo: logi TRUE\\n\\n\\r  |                                                                       \\r  |...........................................                      |  66%\\n  ordinary text without R code\\n\\n\\nlabel: ridgeRegression2 (with options) \\nList of 1\\n $ echo: logi TRUE\\n\\n\\r  |                                                                       \\r  |...........................................                      |  67%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................................                     |  67%\\nlabel: ridgeRegression3 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning:\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 163.1022744180009\n},{\n  \u0022data\u0022: \u0022 logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 163.13340322400109\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................................                     |  68%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 163.66856393200032\n},{\n  \u0022data\u0022: \u0022label: ridgeRegression4 (with options) \\nList of 1\\n $ echo: logi TRUE\\n\\n\\r  |                                                                       \\r  |.............................................                    |  68%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.............................................                    |  69%\\nlabel: lassoRegression (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 163.70007869499932\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 164.01903624499937\n},{\n  \u0022data\u0022: \u0022\\r  |.............................................                    |  70%\\nlabel: lassoRegression2 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |..............................................                   |  70%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..............................................                   |  71%\\nlabel: lassoRegression3 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............................................                  |  72%\\nlabel: lassoRegression4 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 164.05335366700092\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............................................                  |  73%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 164.378083435\n},{\n  \u0022data\u0022: \u0022label: lassoRegression5 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |................................................                 |  73%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................................................                 |  74%\\nlabel: lassoRegression6 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................................................                |  75%\\nlabel: lassoRegression7 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 164.41556677600056\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................................................                |  76%\\nlabel: knn1 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 164.44867931000044\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................................               |  76%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 367.0889351229998\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................................               |  77%\\nlabel: knn2 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 367.1200716919993\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................................................              |  78%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 550.2955969670002\n},{\n  \u0022data\u0022: \u0022label: knn3 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 550.327132587001\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................................................              |  79%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 1708.2391931740003\n},{\n  \u0022data\u0022: \u0022label: knn4 (with options) \\nList of 3\\n $ echo   : logi TRUE\\n $ eval   : logi FALSE\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |....................................................             |  79%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....................................................             |  80%\\nlabel: knn5 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 1708.2753567149994\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....................................................            |  81%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 1870.3476097069997\n},{\n  \u0022data\u0022: \u0022label: knn6 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 1870.3835316269997\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....................................................            |  82%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2024.1714881330008\n},{\n  \u0022data\u0022: \u0022label: knn7 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2024.207770033001\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2166.731137543\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |......................................................           |  83%\\nlabel: knn8 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2166.7675099530006\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......................................................           |  84%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2303.1408693520007\n},{\n  \u0022data\u0022: \u0022label: treeModel1 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2303.1817181490005\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.......................................................          |  84%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.......................................................          |  85%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2303.7341609550003\n},{\n  \u0022data\u0022: \u0022label: treeModel2 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2303.770802766001\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2304.1362919740004\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |........................................................         |  86%\\nlabel: baggingModel1 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2304.1725406179994\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........................................................         |  87%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2306.184625479\n},{\n  \u0022data\u0022: \u0022label: baggingModel2 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2306.2189313929994\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................................................        |  87%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2307.843713448001\n},{\n  \u0022data\u0022: \u0022\\r  |.........................................................        |  88%\\nlabel: rfModel1 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2307.8807955890006\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2308.937664067\n},{\n  \u0022data\u0022: \u0022\\r  |..........................................................       |  89%\\nlabel: rfModel2 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2308.9759609840003\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........................................................       |  90%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2309.693341794\n},{\n  \u0022data\u0022: \u0022label: rfModel3 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2309.7320012\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...........................................................      |  90%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2310.445485352\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...........................................................      |  91%\\nlabel: boostedModel0 (with options) \\nList of 3\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n $ eval   : logi FALSE\\n\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................................................     |  92%\\nlabel: boostedModel1 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2310.50474624\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 3394.661783087\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |............................................................     |  93%\\nlabel: boostedModel2 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 3394.6972791240005\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................................................    |  93%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4329.596806275\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................................................    |  94%\\nlabel: svm1 (with options) \\nList of 2\\n $ echo   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4329.633628973001\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.............................................................    |  95%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4440.621860103\n},{\n  \u0022data\u0022: \u0022label: nnModel1 (with options) \\nList of 3\\n $ echo   : logi TRUE\\n $ eval   : logi FALSE\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |..............................................................   |  95%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..............................................................   |  96%\\nlabel: nnModel2 (with options) \\nList of 3\\n $ echo   : logi TRUE\\n $ eval   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4440.6580973380005\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4779.3727410209995\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...............................................................  |  97%\\nlabel: ensemble (with options) \\nList of 3\\n $ echo   : logi TRUE\\n $ eval   : logi TRUE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4779.4075585049995\n},{\n  \u0022data\u0022: \u0022\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............................................................  |  98%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4782.684154648001\n},{\n  \u0022data\u0022: \u0022label: cheatModel1 (with options) \\nList of 3\\n $ echo   : logi TRUE\\n $ eval   : logi FALSE\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |................................................................ |  98%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................................................................ |  99%\\nlabel: cheatModel2 (with options) \\nList of 3\\n $ echo   : logi TRUE\\n $ eval   : logi FALSE\\n $ warning: logi FALSE\\n\\n\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................................................................| 100%\\nlabel: summary (with options) \\nList of 2\\n $ eval: logi FALSE\\n $ echo: logi FALSE\\n\\n\\n  ordinary text without R code\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4782.719775836\n},{\n  \u0022data\u0022: \u0022output file: /kaggle/working/script.knit.md\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4782.736775043\n},{\n  \u0022data\u0022: \u0022/usr/local/bin/pandoc +RTS -K512m -RTS /kaggle/working/script.utf8.md --to html --from markdown+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash --output /kaggle/working/__results__.html --smart --email-obfuscation none --standalone --section-divs --table-of-contents --toc-depth 4 --template /usr/local/lib/R/site-library/rmarkdown/rmd/h/default.html --no-highlight --variable highlightjs=1 --variable \u0027theme:bootstrap\u0027 --include-in-header /tmp/Rtmp23nzVS/rmarkdown-str1cbe154b.html --mathjax --variable \u0027mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0027 \\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 4782.912409832999\n},{\n  \u0022data\u0022: \u0022\\nOutput created: __results__.html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4783.636536694999\n},{\n  \u0022data\u0022: \u0022There were 50 or more warnings (use warnings() to see the first 50)\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4783.672294254\n}]","outputFiles":[{"ownerInfo":null,"kernelVersionOutputFileId":14769804,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TreeModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TreeModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..NWK74ae0S1PMbKgCy_rQIA.0MKY-zhgRF2g44bZFLaIVXmwigZfkyGVhGveS5Z4q5yiB7MUMFbv5zFckPp5Hhe1OvD6H4WH1EfHrRHVldLguNMY1yfyDXmrKPLeRoDAszb7baydi5q3MOeVbaJGk074KIfvo8Lej0rd6c11C8BYQpUju0HuneiptTsY43nr7eM.xR6wW4dnlL-VPq5FESS3LA/TreeModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TreeModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769833,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TreeSigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TreeSigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nb3KxDb61glM-YDaFYf6Yg.6VqYPoDmnm4SngAoLIEqTvTgFy5yFTjI7ExvwFimVFI6AKpizbtFfaUGouqv59XyhejEFqloR-H8EF_2x88IBYTSfyv6E47RiUaymFI3YnIWkWelJxQR5VkkQ7EV3sLGP_lvVjnCYXQYfC5St20KdslVdf3J6gu1VNJQYKJjEHg.a0t1EP0qeyMeJGjyaKPYHg/TreeSigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TreeSigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769834,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TreeSigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TreeSigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..XEYly-DSpp73ZRlPY2C4xg.mrXKos84Wa4N16BtoyyiD-NB1tnouATQdwdmJJDTYIDbtcxmDgQaXSxyu0zPK5Q8dLGqb90xxhvqH7GE8Eg1J-sYboDDdO6e_Xo-a0GGc7h1laUTTrAdviRXqRvjw5yKzf0byo4v9kxwtuDe6sAH3pnbApbHb7uH4ESSaGNjmtQ.8BB2IQ4_onEfWs4pPzPsBw/TreeSigModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TreeSigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769835,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TreeSigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TreeSigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..dSSjPIwgp8X4qtEoLeKuDg.a9yOncA0CnKE3cQNSPQQp97RYqDa7ZccejxGmMFw-67vEoWBhJHZc_jqbH-IR1DYnA_3mGfaPdPh7aF27YMo-SImSZaWRJqYQyFmqcxGTujvXyjc2rtbaUJuFohevDW__6ub939IYxvPx1Ali3Cb-GXnueRlNaJ2IZzaFUDCCH4.naqjrRCLS74uTeV8wkMqZg/TreeSigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TreeSigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769836,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TreeSigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TreeSigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..MlvbD0Kz2rK6o5sFCFjvmA.uBLA4WRCAtCFSx6eMWnyKSseHOr8M2nF9tYUcfp9w0hSOrw5lSk0lKJwj_TdsQ5cDboROdEZTzZnyN7SW9bU35MQDufV5ekZ6CBrXhhAv_LVOXaCYVKQL6BRsRFkcWnEwkJdAe9YsJe0dVSEdON0Aa1BfGAhesptWE5oq5PvsxQ.eUDYQJOfXvYvpwAyfuRM1g/TreeSigModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TreeSigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769837,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"WCModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/WCModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..I7s9lE85k4DHxbE1KPcvxg.QIuT8TtybYK7HJNy6EJcSemy4QowKXqVO5ohj0m59xUxhU-xOSMDzMKhXrVwhfXntsKiLbIke-rZ0h8ohFzPgQ6v4OjYx7LbH2AYER7hbKhsAIE6_3L3ZGoMV3g5_hujCjKngWTKWttz7VTbmnIwlw0gGK7bY-v4Skmb1xEe-SI.MnEzSoikK9j0nKdBGpBHTQ/WCModelProbs.csv","fileType":".csv","contentLength":9527,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"WCModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769838,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"WCModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/WCModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..RO3-siHr2D_ty8xoPYRPLg.hwlmHV6ZCJFfvGOR4g1t5FRQW6bNJPi0MZLB7N2Q3kfx85hF1stzgycYOliMwg1q1jwSqpJCvmFD0Ml8AB9ihRN52XdnL0_XDzDpMaT-jJyfPYDoO_X31BqnPWr1JA7o4ap5AB09wtFupbdZPcvIoz_xIY3ULKd4swmkYbw8CnU.LNu5adtlK2TDW-82tC-YzQ/WCModelTrainProbs.csv","fileType":".csv","contentLength":19515,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"WCModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769839,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"WomenAndChildrenFirstModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/WomenAndChildrenFirstModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..fRwx77pzToXyFOBxOnbY3g.jlmX1ValydFaQVceLf8iDheJ0qaRleTwTXVyJ3nzsxxV_JTDEDGddVN-RtYNqkiAbf59p_5MkHelm2s1ikHTYHriYvrIwUQn-LEfWXrLq04d2ARNc1f_kc6eG_q3GV4MZDY5yOSeQlrrMjpxbaF99yqGHPG-PXp6rC3kgfqKYQk.elxzZOIAlk0P8YjUhN2P9w/WomenAndChildrenFirstModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"WomenAndChildrenFirstModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769840,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"WomenAndChildrenFirstModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/WomenAndChildrenFirstModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..T0Qv1mNqtbexQlgekXGIRg.8QcoiSHJgHI86yQ8JQc9IapN2BOQ-PkXDpen68MYqXRu3MhHCyKDoy_vqzoBibxpeb2Ddmcb4gc8oqpEjxdQdReKwwdvP6X8yxgj85oYzNmTaUSEUhbn6sXbrSKmiwRRkciFRxKgAj4EeyBbs5Gx8UEwGzeE6pW123CiRGK3kUw.1xV8hWD4lCuN24ZAP3v75Q/WomenAndChildrenFirstModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"WomenAndChildrenFirstModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769888,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TreeModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TreeModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..9UaHyyUlF8Q9bRJNIPgarg.W-Rcew0X6Y7zpo_IbKll_llbGyOwsb1P-Ii76vTr2PjQu1MfKrXfbKl5QYEmOaF7gcnH9HSrSHHEdIgnKv_2S8QehYkYmVNQaHHlkS20Bf9ShFEXAzX73dtNPLCRAAtz5PQbHs2-0DPWxOfjWlaBrrdvQ-O5kioE6OWeWJg-Y9k.7r1FsAEU1Tcs_SM26Nor1Q/TreeModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TreeModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769917,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TreeModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TreeModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Pq_7pkbeaAXyra1vLoTfCA.e_j32d8kSPyEmTn0zrBg3K2tGKKxozrvUIJH9xYV6SuxO7-_SpVhHDgpmrS2zfIBuaruLgk-lDtjgzT6uqycGlGLhsqDulED3WhZ45ezzy3Kt1Guyq3ob-eyH9Lmg37GHmdYh9Shpwa6A5X-p_CejJJhL0GLa72pLqTN7lCkqZs.swab9tIdhHmxXWYiTVWFRA/TreeModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TreeModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769918,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"titanic.jpg","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/titanic.jpg","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..0H_2gmtOE6rc_EfHuXQPnQ.hqVLhwV4yXibICMFsJAi8g_xMwGuv0wqRMRpDVDVAFytcSmmimLAxc3dpKngwASKSXovzYm7EBBT-S0W2RG6VSA4XaiuIftpH83g_Sv3MN_qHdGb6qlDW3vRwE8VvPyYLevF2sxS_LMyLC6KK1i-3fA7JJ5QXpUO0mUIJytokFk.BsE66fFD2GmCDKAZB73aWw/titanic.jpg","fileType":".jpg","contentLength":75162,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"titanic.jpg","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769919,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TreeModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TreeModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..7beEv8MCvp9F-61A4bW5cA.KacV3VK2HBtshdVwMB_ZYgE4HIwSOENVH1A5NJ_2qthrgIiU8VmhxIN3k3cRaeGzxF6JGjCpd4HllwDGLMLh7ybY4k0q0jMvIdmcAS794P-OsDKn38GO-GtkZ5t81m6W79rbBL2gvdbhzYob6aqAHMqulx9Ia1ajOr9c7rJH1-M.laOToLORPTctYpq-kSvyXQ/TreeModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TreeModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769920,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest7ModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest7ModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..bDif0hwUBf9CYrnBo5BIEg.fd3Rg4tN7zGWRB6F8XisExdzuJCag45uIBPbut79lfO_Yk8sdMoMVwu6VoLgN4BAgN_oWQoYDFhj2st5P6nsM3Z5_G1KY5garPFNPMhlWLRk9HN1Sanoh97lcOu9s3Ym7OOwyZdB7GF4K1blXDI8hdzh1XjylHXfBrebPSfV0jI.6W1uk5fIXiDmAxOECV5qrw/KNNBest7ModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest7ModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769921,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest8Model.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest8Model.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..eYPlM3FZ2NvzF0TvtH2FdA.HsxnEkzx_2gVaOA_vRCAX8BDOeZiupZsXi4Q_9jxyfEfhisxQf5UBL3M0015nNT2rq7PSuvUQODnAcbgL4rftb7o66-hUKc9SwgsLqGIkj-SHFrm3foUISAVy1zB1OQ6iwA2y_9oms07INR4IfCXPEK161mfbsDi1mqe8TWfBfU.9qhTysrGwvAK3zmz4g6j_A/KNNBest8Model.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest8Model.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769922,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest8ModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest8ModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..qwCdRs-Jw5gaVNY_f-Qipg.luMv-SiFB0PWBbmhhpdsEnM9tan5_XSlhcytR-SvSf40nrxPYUn4ZbnWXoZMKfGeJsq0zc7WAnL4rz24fcuOtOiWV4rR0TDah0LfG7aFmLpNaTe2jDjZjqToKwToUzGEM74fCvgpiT_2krZBV8F4xVdrn0p7X2kkXb1Q6tuzaEk.V_D1miOLxMN4xuBBbmNO1w/KNNBest8ModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest8ModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769923,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest8ModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest8ModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..jlBMGtNHJATnl_kwFi7f0A.zVTpSIXjSx0tHndE6hw8Xlhy6hMCwA4LmtbN-LE48tfspmo1hGtEmQ2XbbSZx10McdAFydvbUymJDC_v3nGo-Wff5xOIM_Gx0z1zQYP23HTZXNz0QIXdEyKvEhFdebJNwFFRr6CwmWFDxQoWDZj24QsklYZ-5IRpxtrV51l8g5I.gD5UGsnq_rHmhWQj3WRgcg/KNNBest8ModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest8ModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769924,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest8ModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest8ModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nNdcR0fjima3Ay95JB-qBw.0cKeja6ImKbmqZYJZI6VNvloB9UrSM0sSg70559f9Me0w2v5eam-50dfl40WotDY8ATMbbRxN_5LoW8Oc_V5mZO4jtb80uPg5bla8oVQnTFvNUdTHEjKSMhIdo2fIGOj5LZmKAGKxDVuICi6eIHfR_jgriISLa9R1cugTtoAZJs.ch464OJ9G_Je32q6srfocg/KNNBest8ModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest8ModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769925,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest9Model.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest9Model.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..F3U4b8eLeQkR7CAreDd28Q.OKrl6WdDY-YwhxNgGUPCvKzKajrdVh-e0mGR6Oat3TwhTVtWba_YZ7LDG3TYbNBuVizdX7JnNKA84u30G7-fDltAnXGyZSCWyvGrlUe_JoN7-fsJPp6gSU9fN0bqIXGhGx7GfjPPEjBJRtxLWAh077lN6z4IRyUUaMK3zmKHeT8.b6mczYqgCZAmH3oNGNbnMA/KNNBest9Model.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest9Model.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769926,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest9ModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest9ModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..BM-4fMskZDVdRiqXSAMj7w.D1w5d273I3PEGeMb3d9u0W3iZ0D432KPxgLVavQpnPaa4OJ9XTPwpMK5KH66J9w58KNCosTktY5310_teNG-pmS6aoMJisfODIQAkinDh6hPaSityRQMIXCNVYm_pznZDU3kJaNhp4RdkmugbIvkvxLlNhGVAC7b5v7gB2dh6-8.DH7HnNkwGLCzO8xPImbLhA/KNNBest9ModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest9ModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769927,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest9ModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest9ModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..PK93tEUNDgAkpP3BKPwnBw.4hJ7b_6gi2sR-4Yw7_Cy_Y2jzrfwui1xhmRZ-xKKkmQQCTMq8iFQTOrivagnm-PsCCBKPHxjaMHM7_vDLfDMhuw6k9fxiTjWBH_W7tiglCqFW2A9734FD_oZPtSH6eTcCcl_uGDnb1OL2TIcHVyIWxuCI6IgkAPh9hYiiJNpdrs.HnLXiysp9wNvaHPDv1ceGA/KNNBest9ModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest9ModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769928,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest9ModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest9ModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..RPnS5_Kx4AC4bAnIH76vIw.MntwjqhTH1f0cAjI-YGIBtcPYVFRCJapcNM5ly04FDF0c-5QykjHCHDfOZWYpQqJ9ZhKzHDMnrIskQ9wNGaJ_0gk5R2vvz_h31py2mqLW_m99gUWT-6nXwKQPuTdya1DPWwIviNGvVJq5VjKEHEgv98RSAWkgmH-0ynzb0DpSqM.AL5vLVJzjsSFLkQM05AsQg/KNNBest9ModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest9ModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769929,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNFullModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNFullModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..1nD-CBO1hoOJlzUtsglkFQ.Sa0l_7ZlgiFwYn8uKfXeiYC3vldMMLQ3GPnVEE5G3uFUYou45pyIblmt4V4QojZrUF1dKe3KqF7YK2DLj2YvNgkHiYqAaGzh4OQUNwv1Kp9RhCVwN2-Ol5ee4u9ErjItSybLPwwhoJVaY0ay86aMcmHQxCMtm1mvOzvzt02Z3hs.ANG-GIuzFpczsD4Yj4MK0w/KNNFullModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNFullModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769930,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNFullModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNFullModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..-_pJHsmi_GVbbZ1DmGO5OA.zE13JkzMkFV2sHiC9sp2bMA9r4QvRuxWph9DBPcCBFayYZgvNmhxhFyOqHnRLj26rPFg4bdSiiE3WNyV-bgqjFEknmoCzAwYjXsKOucfUm4RBNJaDg0EqPDCPzHLrtf_stnSjMJ2j3ocLD3tSjAnKu2CbSRIVQhdxWhHzGQIJHM.sF6RuuMOQP1Z-omAyNpY-Q/KNNFullModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNFullModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769931,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest7ModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest7ModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..QkuaOlK576U3BHbz0T-N2w.pUNQuyrr-fZJfUlIb6cYBghhBiAVQb-dAs0mAGWf5TzTpdORdmTVGX0LJv0rAc9lLRRGwOu4F39kCkcS2232m_GSJ_Oq17W0qpwOdlgGLn-Bt50LA-DgUmfpFfeZ3H2FqQHR6TD9xA8SjZ9y_PFnWceA_nkDkUgIBklIctYFVxA.2LidrzPtturrY7lM4JaPPA/KNNBest7ModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest7ModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769932,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNFullModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNFullModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JLl1GDEqQk3Pu2jYK6ndHQ.U8DeybrYSPMFkXrfnBINNZhcFIYb3eZtmR7xcJF3GzmwAVQIWreP9ghG_2QCp6Evaoe83sV4s3LVscXMVB7LPCEthR1x-5C_UVioDjGxFFsrea6hud-kIn1qv6YdnCBkanYuia_xd66TGyAN1RxxiKuYAmut1Vwdgdn7dBE0O-s.e_3z1DJhBUj0dfnbx-NMUg/KNNFullModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNFullModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769933,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LDAModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LDAModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..I4UrRKR75pblRtygDG--mg.hEffVNCwYWs7mYP6IF0OAP9dUKDAKrKp9ypV7SvFHa6Zle52VeW8ZEX6K4CG_PZrX-p43FG97ZBUaDsd4hgeAThUyToc6rNu69XLx2LqONSmHyck8bRPlrMZnv9J2Ihztsa_GiQT-Qlae0gGgJc9JI95QnD1_aS2LqpwFnXl3eY.1FbzKNCrpvFu7NR5yHns_Q/LDAModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LDAModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769934,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LDAModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LDAModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..tG35J-K8kJevoqQpkWHtnA.5inMJNs5ik8J1ktFVs6TSQWRy4mfQVQ-0yEK2nWPG6l_xvSmQGuS0fxd7Pk3MLuAlRP6DjxtZHpFpLN-6GbnTuzMjVsl5fK7hMEBJmSHwdd_aY7rtHve-zsg951FVJEzydyPi3feBqisccVYAwXjNgbX0pK4QoqBAn8dXzYRbqU.WminwBqIQHEUEEA8ejTYNQ/LDAModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LDAModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769935,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LDAModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LDAModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..H8FTGCxrsNJjrDZiCbUvAA.73PhI_8dJvhZ4n7vq4Z9XT2p-v6x_jsEjuNzDWInCNe0afdTdllUcN0EjOQxcB3O9lSQ-RHqykD6VPswakq6pPcn_5CzEYNcxYGh_rVDXtqZTu7I8bRTvbiUrtdMNfEueze1d01Wz4ciLJYZRZl0ZqRecek4xVM4zmqJhnM2vXM.Nn45BX1XK-cDAxbjgXj9Hw/LDAModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LDAModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769936,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LDAModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LDAModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..cPrpMzl6Sj-e8LbQ71aCLg.BXCLaKa187lQYVVZuJDvNHmqGX0DOvVN2KMhjqok6rieDksq-nASnPnw8B1QW47wvkSm9_CbFLSFBlWkTjfZuGhNGe0NlbZV11DM3D8W2RW1t-_MOBq1LcRqduV7P7PCw23DDYueTQvxBTgFfsKH5bqpIziSljISy8IEItVgmwI.TDc1oLGXfuyIAqse7uugDw/LDAModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LDAModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769937,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LDAReducedModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LDAReducedModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..-fB2NcJcu6wC1xnhvFBYIw.vtEmBEOsyTMBzHiqcKxP0a5uNE3PVyGHzNu4N5fOYm7diIL76a9jRvod48Ow9wXYoVEcRmYECi6XrfHYyqPyPn3GL0q_UNvM0n_sOMlkHb1BkSgsQ-_QaEyuVqr_xgn5jHJCcctDT57n3kL9lk3GmhpCTVBSfOMujtRo5Ndt3mY.pKe5N4aE6xrRd1ml1D8rag/LDAReducedModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LDAReducedModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769938,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LDAReducedModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LDAReducedModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..1YsUhxOJpd65HL5CloqToQ.hyNHq8LYG25IYSJaWlqzKQMQzZtpxoOgLrXQRQUE1Y0WT23BDsrl0mnybtDtSwKeYGdJ9Jkd5kPXFl6nFMoHXS_6R19nBQeWQkSr1LZNxja95eVciRNiU7WDHpn5fYrMwQSXTtlHcImGNq9et1NOYUfPuWMKgtNmnGTWObdsH5M.OMlxOAVfWM16_m5JKjgQCQ/LDAReducedModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LDAReducedModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769939,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LDAReducedModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LDAReducedModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..r2Rh2R46empYl78wBHEW_Q.RVvn5WmLFnsh2Yxz8tTuJGZvZyb2FBcbP3hu_10Krul4hZ5EulfzGhMrIibel6RWCD1t41nAWZMbote7EAx7bKqQiiV8HCeF85Yq7xgCnGE3SAVODOamqxUSOezx5Evmt3ofUXFFtrsq8d85qXRdnpDuQgYepWVuHaGfOdwphx0.TU6UvE0gcSFp7Ug7f2Ie-g/LDAReducedModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LDAReducedModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769940,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LDAReducedModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LDAReducedModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..f27BHbohQ-hl8rqDUwN_8w.0UShZxOOwvNu_jT4JR9PoPzacuEviXYc8FKVlOrW2QNFXCj08h5ycEPFw4Z1nVq9rW6J9iEZuz_WxtlrQwu6p5923vsvICq6MeSC3lztKtn8jvNlo13se3Sq0JGE5USKSOZWwXGWwLa3aDCoYVF8PupCRG95GGllnAPQ_LN3Sa4.yj-HR3hA7zm21cJ6v51bUQ/LDAReducedModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LDAReducedModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769941,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LassoRegressionFullModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LassoRegressionFullModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..vsrSUFfnPKRhi7dObHbBiw.URomFXPpc2tJqSZHpSTaMfeNBsCUBcvF5t7z-HQt-70sJRFrTQo5irkUwMrTAIfMlk124Hkhg4e-4Y_79WTngcpKUvsyC8vBZ7RqRolU1TYHuJ5eGpEx90HRfHAobBVECrQmd_BIkSKMPpFUNx6Kt7hXPoLp9_ftFQg6EKyQuow.FxPnxOw1P9Br4saXIcWAlg/LassoRegressionFullModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LassoRegressionFullModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769942,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LassoRegressionFullModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LassoRegressionFullModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..ieR-7yG-qZENg8Saj9vBIA.3338LYTgutlevcgE7fqi7TSvmydTuxpfR3rutRKTZyjIespZmr2IWQjZjKziDFECSN7sMGpyVhDaCfwALsFLvnr6qHVQWz7CGxgwi43RbxF5VguAsCBxJ-BBd7wXkSHZpp-bf-ioB2Kv34XbnmDO2bMZ_rNX-qHjxIf3wJUvtUM.A_0V1k5tfW9KZzq9mQKmYw/LassoRegressionFullModelProbs.csv","fileType":".csv","contentLength":9539,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LassoRegressionFullModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769943,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LassoRegressionFullModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LassoRegressionFullModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..duTexfl-I8lOmhwwpOXEmw.lZ3duwocEZMjiZ8wpTOEa1kKolflaJ2eXU0DYVMNqm17mLxlYsY1F20_Ate5u309lfb2E0oCqrso79cpQbP9RiOrpqPys1-00rHDkpd_9ZRCFIjWq1ssj7PLQEpOcspoUYxew8QGiOfUacdco7gTbNj8uy3us4z7Lp4M1kuv-aM.hmmYhRndKRa5Fd7phvLqOg/LassoRegressionFullModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LassoRegressionFullModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769944,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNFullModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNFullModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..VG8EdVJ1uOep02USkK0DzA.t3lCfMJustERVX3XSxRQ_dos7JgEu-vF_OzyE_zeR1HoZBsmYFCjenoLGjDI8FPWJZQiZLB0hR4W-YJsoz_67UBUzVF2ReUYKSvoEhkQTX06_SSOBYIV8aY7tIFsQEnovty7s0Lah6gNas6UBxGXa-w3XLllvq6AmgmvauhJ0-0.Elejo_yk6nFsJLAnSBhBZg/KNNFullModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNFullModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769945,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest7ModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest7ModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..IqSGF7q2Y1fBT52KTVUxjA.MQqp2cHXmUbnf0iwOfgK85Akije5vfMDTfE8JR2XQucicTJxufcPMs5lndzbAbtl2j7SuQX-xTACvpg5OWMrs8PChU7jc2MIgcrMDXwwo5vZKk30QDpct49l7GLcN0oR6TMotcy9D08CM8jh1hjSbA_ecP9eCn8jxESpkLg0nL4.1PtXJcYkk7O9vccPE3enoA/KNNBest7ModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest7ModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769946,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest7Model.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest7Model.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..UCB09kLVd7ZbcsXVHc8Ozw.yBuzN93QSJTTkjH1TjcDKYO4YjgSwYf5VMJbDjGc3PJBJmJqzowMf595CD3DMMAu5kIo4j9B53-HoTODfJPYWnPFhxwkKS3mdIgeUqYvS85DkYmrrQMKuhq2XIXbrAWfgGbplYBipnVdWgVCUWZaZxp8cxPx4BDowSInnxlwBjg.1_cyM3upcDn_8sc6UBEIAw/KNNBest7Model.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest7Model.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769947,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest6ModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest6ModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..QOsM3o8TpFZ70sU75idc3A.QF-jcua0gHW_bIKtVbj7uYgv9eZu_1ludoYgldeBkGmtAwSrjEtS04f9fLRdHWzqxW9aruZBlyBLwCr344mLSOFZx-GaBp9RMl7iIFT4r2oKmMjXECsTE7su1RR9x9iWJFx_sAMKcAyNSSvamXxIb7BbAsmxm8g67ISos7fiank.it9H0jrv-sx-fRzf-57BJA/KNNBest6ModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest6ModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769948,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BagModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BagModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..VC4Y_pXYGxuHokiXyR8zFg.1xfwD-PqzyKTdjTgmIPuVUEVPzrd4MPg6pjkPhRYWiE6AJFvo9LVUk_uHEBzONEMyPS50qIdnzE4a2Frtx0YnqpxgxkNwTb2WpS8K8ICmWzMTGZ4CNA_GhpJA8Iz5Opk8Lxllld09ShtCASZmZJHVfretdBgvHQq0xi7485HKuA.9wNPPRP3IQhNDfuC495F6g/BagModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BagModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769949,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BagModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BagModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..5XjTriP8FIt5kb327dy1EQ.Prxw5tVtO2LolD7IUIYuQ5jWW7_5KaTifdZCHeLKpqe1_99MQNRbH4GFrzK-RRY-B1BdXvFM1uIKy19OHUTNwfmICRUxKSB9TO8nCS7QTTaBOGN4pES3H9YlR3wDh6UdVn6I71tuf_iY5Iq72BBdxUw0jtEBMWYGhV_mKVtd1WU.zeU4imqZJ4_sR2-OmagOLg/BagModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BagModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769950,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BagModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BagModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..7T9c8aHhi6QCMBwcfKvd4Q.hvpQi-AwsUwYA1owHH7mh9umroiVoaaUtmdI2_o2NIQiARaZAIg4-GjPCQkEyoS0fm-lN_gj1Q4lFzHHjOXTKF7fwdt1lqZEcCConOXPcFki2kavSzYVhVme53L-H9yHM2htMjm2sFQAwuCMT2xFm4JiY8yWHSiCqeW1LoUcehk.XvTXsUHysoZ8lbp8ufuIMQ/BagModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BagModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769951,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BagModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BagModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..41c5woff3XEb9CTyqDl2OA.cttP94oeqUnYwd1q-oHb13Hl29iB81-GaAlVV_TQOj2a0t8_0Vm9OhBbbB0q-qbW8pCUqEP3eed5udtaK5Jfj7Ujyfdwu0erjKlEPBVTarGWK0WYoN4wdl2a5HKBGR8Z5qz_1769SxGFBwUZA9K6LkuorkMAG-kNa6GETZtNnoY.Q9leHUbu3GIwxCORaQqi2g/BagModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BagModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769952,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BagSigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BagSigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..yBQIfvNYM6U-_iIqz6VNyw.QMvddt-TCkBKUK_yUKT16ldMHdANQthkpAkCdKsQAgIhEY1Fy6ZSTFfc5yBUUY20aOmcg0D04ZJlm4i_qdLjjB8BhYfC9r1D00jJrY4Ag0S7itkht_bSEBuWJmt93Q3wkddISCoYZds9Yo3crAaZeDX0EUWj4W-_fM22iKjdZaQ.atI3nI7Mr_mpdtqkCyR4WQ/BagSigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BagSigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769953,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BagSigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BagSigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..676zBd4QpNRn-IAtqPc_Wg.qQwR8Q9IOmeIlpzkhAkPDeLVKX38G6hRfkBM466GtO7JWHsE4GDr4EvV8B9K6pfI8ZCzP3XxpRPFutrVdHokjDcyfCq7qIJF38QQi1r4V0OAUyxgK2iNKRIy1jnVSlR8tXgq2wR4reYHcigCjNLy5nJS5A3h9o8nQrO1P6PG61Y.-kWmCgA8tQTG7kKH4lh1KA/BagSigModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BagSigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769954,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BagSigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BagSigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..stwOXaR3W14sZreYiJGy5Q.KFnIm4PIDgTkhrfUoZh9HrQUr5tvtA-snCWzWZmaa-fnscyZMG0Y2sHR8BJekyVwthX0WvG-vK-_dWjozcyHNuWg7KlyX5Rm9if_RirrnFEGeW0iX9Zeh0DET3Rf_X6lfvLLiqsx60fjdR8jJKwwE_sW9JX7pwWpoy0ok_rEUg4.8TOvsRIICX80UwlZxWaaIg/BagSigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BagSigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769955,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BagSigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BagSigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..TcZ6pkOptGFjWeAAgjddYg.GW8vboaJxqaH9YKfxh4MmxHO2DZTZQs9_6WHEXTe3EO2WfaNSTOEqV1yOdqwr9L_wwdT2rOePjouC08CeNYtwKlWlH2xZb4jWdpLVZfxFcg64aS4GzIlHE_swhHA-CVWpDafo7VnTs8Etx6E_Y5ynsFUbhdwFohunsGPeH-Iuvw.yuMJ1KCjqw7yfUtFvSiiAQ/BagSigModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BagSigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769956,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BoostedTreeModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BoostedTreeModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..hkCssHIWkhmna6gNNStBGg.gP8m0PKrPNi9YaaxGsTGhwh0ameqmdWnPhVXZdgwhjcGw_lAj1K0twXdwDkuQlInHiR4-TKdKtMzkohdlUOPynKCk85wH2rgdtPmpNJX6c0udQxol9DdRCLtgX_Dmddyo08GaQztbmFMIPeabqG9RziFtILv81DyY-V27GOaeh0.RnBkvJZlo80LgmYERBNKiw/BoostedTreeModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BoostedTreeModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769957,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BoostedTreeModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BoostedTreeModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..9Dq_nf7Yp3Nu5QFbtgdDzQ.Xtr8AUD7EFXDvkKteiuc0-OIL5IGVt0GVV8GbJcw0-qG-g0CkRcUxIU_hap6drijG0YYWKWse28qKmwK0i-cYcv0xVNzCrnF6a3nF_Dxv_MUuXR-hWA-lnWRNEibEoF_YpJSDfZLJBTm69M0lQEnxC0qp9ymJaR8pXGLtc5rlm4.yEOVl5jVsmWUgKSQW-7FOg/BoostedTreeModelProbs.csv","fileType":".csv","contentLength":9608,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BoostedTreeModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769958,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BoostedTreeModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BoostedTreeModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Ro4DdHTDrX1gBsRuWMbbzw.zx5oOAUOC6gqBVCIaAJx4vgsNUjD8vY3XKiTlBGP23PS6l-X7RiXbPH9abCj6EiEj7ycgZA9kigcW05Cof3wv3yReQxB6yo2VyJNprzZOLWsZ68fGSi2T1OCAU1uMyv4l5yK_FRhQLmZEUR_vnNUUspKDWGxI3H8sGBReFWOO6A.qV44aHFFuuIJuSJ3F_hBvg/BoostedTreeModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BoostedTreeModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769959,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BoostedTreeModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BoostedTreeModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..j9U2qNvJgqe4XmAMlzNdIw.-3VEUft8hOMJOF9Z-8eDeNYnDxL6c-DAX-KSVHSPdN4GFa3qQhFglJgbsIP9uCrDpkz5kpU24HqDCOkm70ty72QHZSkVBC1_qSvFUN36AGbTVVBdJgmcObce-lf3ai5Cz_V5c-E4ycGVkT6S15uXeVgBOLram5RsXHSxY1yU4YQ.fbT5hZvc0bGw_B2mSgmo_g/BoostedTreeModelTrainProbs.csv","fileType":".csv","contentLength":19696,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BoostedTreeModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769960,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BoostedTreeSigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BoostedTreeSigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..WtDwWAqWewi9TN-CVgU_qQ.LXYw-hXBA0SQba2kLALnM56oNDJOA7KbdY111MACJX4GKTcTpQkEg_C1KMxyJSQleEgm9inYebWyasHuR6Vy3cWQy5sF_0yzQKx2kiI8L175YL6xSychyYjCqT9P4Gdc6d0La3aa5wZMI65S4i6NOo24dzdTG6fJhGCPf2lxRuc.3RQuhFvodrHSDsxtBvnOMw/BoostedTreeSigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BoostedTreeSigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769961,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BoostedTreeSigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BoostedTreeSigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..4hQrUr_b6xTmVPVxoiwsnw.-GuJGK-AbXR-VYcruvY7RgyQXhGLFaUQnnbOekuE0ePecq2dlKsdlCEcK0gL5IKYOJCLXQW_ToktSPMkSJSh9orMPYGcPdSPmWLP6s0YkfYVDJBAYAOQqw_Ar_Z7E_pR-fJy7WPJOSD1RYD9xVXv327kLQcKRPY2MKPo3yl5A4U.x_uhTnPImwX3mDrGolaqig/BoostedTreeSigModelProbs.csv","fileType":".csv","contentLength":9606,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BoostedTreeSigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769962,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BoostedTreeSigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BoostedTreeSigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..xTxA7h8lqrBHEedD-2ORMw.3LGryt8B0cum_Y_fy63IVxD1yPx0aPH_Fw0zSz5y25Fqw6gXKi1pDtDefdiT9puBwPZb8j7GCX8eQY642bKKnNLJHDblWMn75bGCSFGv4cSjfIzqmOBIEX95ItBVme1JjSdLfCcm2dM7pZdSB3rpm5gxC7NNcFTHjPF-GybcbQc.bgW4htx3I485IQK6t6GTog/BoostedTreeSigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BoostedTreeSigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769963,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"BoostedTreeSigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/BoostedTreeSigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..d-jy4OIiQeXGFjWFNNLlEQ.VXeV6kguO3YIdTTU3DcXfJ6HrVQVo-3gPoZaTWQzbvSn4aPwr--bsDuhGHM9R9WC5x0YF3beUNNbPPHFCv_fsBZDVf_M4gAGl-MBcKX3oHE5F-E7tl46QeesSTVkEGt0I_ahpkQ2gaCg2f_kkTVcD7mBUOEhCG6-GhioGPdR6fs.f8hDnexs_SlLie3Rb4DnLQ/BoostedTreeSigModelTrainProbs.csv","fileType":".csv","contentLength":19709,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"BoostedTreeSigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769964,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"EnsembleModelManual.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/EnsembleModelManual.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..KYSTNhM-svM0TuH8DtW7PQ.Oiu-KT9nbtFfmbB8YInuXs4NtIxBcI7iQON3pwjo9QBmEX3e_vCZl0-dA21G0oz1VnGxhyD8UpYW-VapsAFom_siBybrCt-NjSSh1swv5pJPeApIuxHjL_ZvW8KVZJ_Q0zwMLY30L5337ddcQWA5UJ-OR6tLx33CmRCRLPGOSCE.g35vU03q4sF7_ZTxnG-4Nw/EnsembleModelManual.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"EnsembleModelManual.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769965,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"EnsembleModelManualTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/EnsembleModelManualTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..KWnSoJx_i77cpN767fQF6A.zMZO7cMLp4G-t9f7TAD7w82XV3ht3L96nN6w-9Up5CjSwKFMOAlTy0JRe58yYzVtXrZd8i9NVXUhVmos3Bvd5JQY4iFlc-5gDtLcM3NioB5Xy_42QD4QqrdXEVKmQt591Cx_EkOqsgM9PPtzhwyAku1gaHdaKqGt2IlsQd96Iv0.D-8LG0MMBI6_nIMCDEo3vg/EnsembleModelManualTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"EnsembleModelManualTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769966,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"EnsembleModelMeans.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/EnsembleModelMeans.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..feFp8eJhRC6Rz_sB0JEA3g._1pYEi9WP_mq27phOLe5KYjE2lCC70RECRP-UvRC1x8zeBLC-2ju_n3G-lgDp7O_bZ39KgOG1Sbc_IG2iA1WU6WL8Bc-jvg_7qBGjLcjcNuWoEzEZXTmX3Up8Q6zk81SzT926Xe5FcjAN9fJAWA63iG2T1QjcpkAMnPNgaXeZv4.DbKPszo_8CLEJmQRKEOPPw/EnsembleModelMeans.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"EnsembleModelMeans.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769967,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"EnsembleModelMeansTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/EnsembleModelMeansTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..J4VnxRkMsn-Qb-igXcgWDQ._dkjy9lkIC7kH6Sme31o89vtCu3U7mQwgCRlsDU83jxULvZJanxHRtCvfqjp2RPFgC6E29RHHDv6UNITLLLYpTCLRmJDnvsv9WMDFEOaDmM_5337mELyVBMJqv11ni6oyr0LPppomjIu83uAoxgGvvo65ea7D-jNjcGWNGx1fw8.BPlbesQT1w_72jWhed7h0g/EnsembleModelMeansTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"EnsembleModelMeansTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769968,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"GenderModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/GenderModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..ppQGeoULTadvPbM3OP1XXA.i0nd_-86S4i3GcuLoXw7uB-6vTn5LkRDLYkvWfXyobPl_ctQ4DEs5oRMIucyPheOmRqErpBcXBL6gvypRAuT8ZCwgZp8C4hjtdXopkeoNK7HMzoiWVKYIo0WqL8b_tpR_RZVj5c7I1qsMTfODmbjgtU7LunuF44So6a1Q4bqxIY.WryQu0biWzKRAFt908nWxw/GenderModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"GenderModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769969,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"GenderModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/GenderModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..-Tm2P2szqREzaJuWD-uVBg.e38Q-VZuqgLmCLs3HwFwhHY-H88KG4MD7KdTxLJyLxaN3I1gBdKUtgMUuxa80DPQT9HPj50LrZyEwykkHIZldWzUWo0_Tw3B4gNJrFJQOhVeG00PQj9O6BUhrDcBxqhkn6GQtCkx3Sy14SV-sIos4ekNYaNMT3frLRuuYYp2xfM.sRVQ3O7uo1STYGcKKybHag/GenderModelTrainProbs.csv","fileType":".csv","contentLength":19201,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"GenderModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769970,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest6Model.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest6Model.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..PsszhX4_C7Tf5nVIAepjvQ.Y0lXcCui4_OdbEkE_fPXsWmnMBMjhQ28iriS_mfn7aklquh-BlKgUpSaNmpjTXi9Bil3Rkk6_mxhnPljgSkgSKQygsrcsnyGVUtu_B8LLRDTsGyydFsGr_z8tEq7R3Zyn7SixDhYiRhmyS6wb4UeA5p6zkOdDZcB39M_UyAR0IU.y9WunY24PJlYB_m0zNGHnA/KNNBest6Model.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest6Model.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769971,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest6ModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest6ModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..SZfABLqiZP-XVQY5uU08Hw.XNsvaK7M4HpIl_mOECN4lZyc_K3oFK4uw0XzmBNaJhwZ7UX5dh-GQE_OIMb_NmZ6p3I-PIV3oWu1vDTuvurj8UTcw8NOisih4CcCyTFyUUYP_6DKPCV4pCnx1dmRH2We9NSV0SxUX9L3nlcFEAy_L6pSe6A7fLw3cUnnIuUYwkE.qWGzW7BuJzWVtXZ2yl3-gQ/KNNBest6ModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest6ModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769972,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"KNNBest6ModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/KNNBest6ModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Gp_UPT0peegiK99-TyI5ww.T16YKPK93Kl7fcKnSs4GM2ydnAYop7tFPH6zgY-8gW7T5dIw4NltlAPlSbaTCwkV8_KgJBrshOnrcH0NgIsCWbsey-ghV5_LNIJuh1LvpwmPEWj0VnbfrK6EaBMTccvW6lJ9mP4Fn4402SHzcM5vQT1fl1I58QSfESqHkvVdpjY.qPj2JH-TnpJP7STPDRu2gg/KNNBest6ModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"KNNBest6ModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769973,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LassoRegressionFullModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LassoRegressionFullModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..ULe469hMVqXs3Ai_06FpLA.xjd519U0jY1oahUga234-3Fix_Z-70vb2nxpHU0AUdvR-MTeZTbnffvaS0eKFbd_s_LboZ54_wln63CkzMAAMl_gcKOOTl6FrApEKQnePtn0zzkv_u9cqApRXjt7eWGXvBgTMU6DwtpPkaoF1kxQJK5Oi9wlLWajjp0y4HkhG2E.PFFoyCA3MJgQ3wQ_jWzq-w/LassoRegressionFullModelTrainProbs.csv","fileType":".csv","contentLength":19610,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LassoRegressionFullModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769975,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LassoRegressionSigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LassoRegressionSigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..lL2C0qcY5_dIyaBNgY7M9g.HztmDczSMGD5IYJLrMJ3vwy78q-VCSnr7lw31EULO5tPy8ylbS4Nh_uzH5sp5KcsBBIDc9RdQLmHksFlzDd3t37dOyY46SGMyi663s-KBXIR9tgb4oUj8VUL5d3GaTdvdJBDsIwptaf4KloPys3zF_QUZjERTfUcMbjazDO7ewU.bCjDMQl-6qHKZYdMd_eZVw/LassoRegressionSigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LassoRegressionSigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769976,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LassoRegressionSigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LassoRegressionSigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..4AKvbW3isk23LqIQQfqXEg.fXdej4Kd8TWmbM3QDIATsrRvmRnk8Tbno5MoTHA4gyidhdqN78gTNxZ__VE1UIyRsOFkIj6BiYIvFnOQU4iMVQGSdjNC-BZ2UTSQjsVl1_Om7Zvjv-oSgVg2sfT2dT4JtmuYnEh8w4FHKnAhX_8ca2efBtuHINItOPF75-lV7ck.cPmxot18k8KEFaLiXN5C3w/LassoRegressionSigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LassoRegressionSigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769977,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..YoQj3tHgBJ5okK4U0gX1HQ.FcLFP6vguKAvz6XoDnNGF2tLjSZuPJmmcQ5-yqlUy5GuCxNrQlin2Fb6so6IjktgLEiqFYxFEWjHqjMZTFeLwCT9ir9WAqvpBu9kMOeetiAmbxmqnApeBvnBe-vLelhSZDFYq4A4vkVtKkvilzjLPzhZ7d0iJOFgrer5Y1Wgh0o.vIxDoE4DD6KKcF5UqDLczw/RandomForestModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769978,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..J1o8zp1AXkS1KxidlqoflQ.T3Ct_hB59477ockYUz9LxVNpRUWMYaDkIyuOqTaixxLAnnD2pMT0_SRIQGUThSFTZklhs_IsZuMYY31w9gISMs-sO58K1t4hGRYX389eIl7aRQHzU_oRjFAqwThkXenqry9dN4Tp8bDi9F7S50PXhtMo2BLuE6lTdFFyJGUBh8c.JmqIzVisrQTyb8D7ogf0OA/RandomForestModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769979,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nVzKgY5B73GZiPhBfmxywA.I5y_MQ37qmcj7naebUrNJzOv_Y5D1vH-BgWDlDeagF1Ji2FSrZSu84EEyPfyqs5ZM6MrWppPaLx_Dpr_KFDhne_Sx-uWvkU6XYhqnuERCNVLEtiL1Td2dm_qUMbQ5rZX5_jZAMfmmVEVKTzWwIBQqYfJpnaIm2k7oKcH8bQSCGY.dgVdwvxI-ax5JykxiZJlHA/RandomForestModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769980,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..EWpUM6dHumXplFpK1HkP3Q.9ucLfoQhJ0vVEWMfc38hPxzxgIaVX8a2bluY0zFNqvtpelm-spDF07hN3Z6fHiny0F_KYzfkbPpHEtYakOSid0DltP7-xlUPgrnX86KU79Isr5vCOHGLO0qSSgD75E_sW1pD1l1lh8QCwCqFD6vdjG3mhzN66MHmK27o3z7pVEg.nsdo_tyHW14ZZLSbI8bc_g/RandomForestModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769981,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestSigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestSigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..SwqgCH1UMhL2RWJwxPpm8g.cRq4TE3NNu7D7aWOv3pvKAKp1FEy_UL-6U1Ebb0Or6tNRFCWpdTiPAfe4hprfMT3nsaiCoEPXdpGmVWJ43HDtameUSbqk6vkqxcUC-sB3LvuMFpYAanOLLNK3ka5AJFrN44MYCJYeKJnBA08NpTd65VvmqLTQUKDvhedX8HbcHQ.1w2q6HwrK1V9JgsjAGDIgg/RandomForestSigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestSigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769982,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestSigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestSigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nQKKYg02VS01GOsw3OaqPQ.ja6ZIn6Uqwmw8W9WOCExMJ2086d5wfkwhE3DlMHkzFYnYrcJ0x-9K8xs604C_batdLYCfYXGRi9nvgXMr_HflceGwPeKE7oPNGO1R6mz3EEn6R0FWoM_xVgHEZZAERM-ve0vd7bqor1EMwRYGrZ4OdHFbMjecczl8qMws5kXzxM.TSBtSDYS5oIsC0JjNXgrjw/RandomForestSigModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestSigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769983,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestSigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestSigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..aTDiHi2DrynKZBd6lz0b8w.Fb2FfDlHvlFcO16xrcT_UUk2F6HlnMupJfFHQQ1WwPc6pycNA7pvUsBI1GNFAMGEPyQQn0XNd_-OfVsJlmUUyoljSSXI_KbA1IBAqZbH-6aWmpicX2yZFC-5z0qVfz_iYUYXfiqEIelUbOcDnjDVfZhTGESW6fDXIjivsuUDI6A.c5L72kQgxgIPlL7ckG_ZDw/RandomForestSigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestSigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769984,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestSigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestSigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..ref4XQy7BjCFZ2pWBlbQNw.AVY2ZPrYyzq_ppPjnEMydp5q2IoiH8zPW9CD69FOwCh33nL4KUll53hcfx3pgvEMHg2wQcFNq7IiCZjwI2vAD58Jo1JfykDMXd_rpKSSjAjQU8_p-_QKYejtSXVoo6E9ppL351rZVvOX4YLu9aHfbK-4RtsDjPOoE1Z7iV9AdE8.cezOF9t70ZjSRiMtaP4OFQ/RandomForestSigModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestSigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769985,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestTrimmedModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestTrimmedModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..pEbR8-ryeJi9vJTGkpRX6g.IgvfdkeUsGcrsb3d9m77a9QSPrjH9ru1x9dOIUv0hBQn266otFj7rxF9-chj0wqN0Ie_-mVDq9bGZwcRrD_aL7dE68rrPqHm3wGuoDuACVaupq1K-xU6fn5eA9eDladmU3eA8YzYmg5-8ZorS8GmizAlVQS6YhK5XSMYm9OGnuI.qo-6W1l8T_tyjfJsIW9jCw/RandomForestTrimmedModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestTrimmedModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769986,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestTrimmedModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestTrimmedModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..7Zu7cFuLE47OsWGGTgjX1Q.ZtTz7rQk3m81fKU4dKpXGkO2tiDE1P5ULApH-NjiGP9i24sl72Q3UKGe13F7GYYG-dzA-lubab0SNFto1USdU1yS84_Izp7yQmX7JwvLvTOghTlLea3SgBkm3iJR8q7vPDKqJiUCDdisVYbntfaTS1Jp6ck6O3HQ0ulLabQ3FVU.lQx7MQJrHt-WJG7UwN5hrw/RandomForestTrimmedModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestTrimmedModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769987,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestTrimmedModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestTrimmedModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..XCSH3CznZ4EHwHpVqmifhA.QQUouUDC9iIk-2Z0fFaUF-M6lQofLdHnM_O1717GmPa7k0qZycv81eErS2NaeWazrQvzCYTu4HvmbpJNF28cZztU4p8H3IxhApE4Hi_Da3sjbSMXERV9HJzNqblQAadGa4rP-btxCpP5qpXdUc-B7Ydn2a1lftjpCL5X-OzhMEQ.lQKwQ79rPfj-r_VcYQ-xQA/RandomForestTrimmedModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestTrimmedModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769988,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RandomForestTrimmedModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RandomForestTrimmedModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..h1ppxcL0UdT9njwtVU-avA.2B-2Nhk4G87B6o2ikBLOiCXAo7mCXbxzkKwc0fUNlxAkDa_CS4ZAJgfBpByPggG8SjNTz8RMUL3fLBMqSzKTKiPmwPdcglpRWntpi_jVlcjY6rFuI2foiOSy7eG69Wl4rm6DNGEwGqZQtVo77kPL_KR15DTt1XkEusuYi6pOoXQ.Xo43sljxWgp35bmCj5VTZA/RandomForestTrimmedModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RandomForestTrimmedModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769989,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RidgeRegressionFullModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RidgeRegressionFullModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..xcedJKDZ_LGdbxxrxmsdOA.UtCFeH-6VEU6D_vBZBInqY2Vi49RUNLm57JoM4bJKxnAyki39AQXNQJx1380uGhkUL_ZM9usKL4h-lsEb6Nch1SWJbonB7yDeVqHGiYe3smO98gpVwPyxWLE380ns3sOZ1bLelGOjXH915eSVyBKe2Rujf0lS1cDkQteTYAWdyY.By6zPwe2sSkC2_gtxQAcpA/RidgeRegressionFullModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RidgeRegressionFullModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769990,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RidgeRegressionFullModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RidgeRegressionFullModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..F4Y03Ay6KJqMjuc4akEt1g.Y5NK9ug0TSGnST2OKMfgMvE38y9rKis1ABjYzCdkwqLlEFJ7Wb3D2AipqHcKy0CoMTjGEy8ux942kkjzTet9b01yFs-r0jUxGfjbX7EcjJaFjb4DOATu_zMdt-EYW1_Op354dGov5fXxDqJ7xgmoVQsly-eEBZFIZbbKdHNqqHc.iGC5QRhWW2E44OJicdnWgQ/RidgeRegressionFullModelProbs.csv","fileType":".csv","contentLength":9532,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RidgeRegressionFullModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769991,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RidgeRegressionFullModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RidgeRegressionFullModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..keJDJuqU9mq7EFXhMkKs2w.evPt_fnipAYM30gEXvfNIl9TJ5m0y9L2PFNJn6_7PQo-hNBi1jAzOP3RM_MKu6vAzxnqFh7DNOdd6ESRuQ8scyLWmVsrZ_GO2xZgZpthCxfWxUNTk_U8eBQGSIfSZAQMl9vgkQf2g2g-rCbOYzB9beiQkGuhCeO8iKO_Dekz9Sw.T270B80JB-91FIuuuosQ9w/RidgeRegressionFullModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RidgeRegressionFullModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769992,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RidgeRegressionFullModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RidgeRegressionFullModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..-4K1GgR2deVxtzF_ZPN_nQ.lH0LKNKKdvSAVcnEa9mcjM5jN0eyjemXW6l_HUXgDqmtA2UnyT9XdSeJ_L45zpeq-Jfui4zWHGgnj8kZREWqEHgvVnC_7RUklFOt9RRwtkPO5reoTdRdJi5wRXqzka2UlaAZoNUA9iBCwO52aamqidGpX9emkFwTgn9Puvl6D28.INnQogGaDKmAdH_HqBO-3g/RidgeRegressionFullModelTrainProbs.csv","fileType":".csv","contentLength":19620,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RidgeRegressionFullModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769993,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RidgeRegressionSigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RidgeRegressionSigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..QXXCBMJ9mvJVInc2I8Oaow.DwQSg2O4gpI5KeqzJWWU9kKGZG5ioN8YE2a_0K9eN-e9U6I0YAEjM02sJW57cv3dUhqxjyeaTiXMSxuNzphrmQxnoWG8D999tG9iq58FFDN29UysQRuDOED5xXIXSzC20YFsAL-U4w8QDRF4UEjzDhcrz5zeiH3Xx7XIw0ELhwA.5Db7cNpd18ZcmFDbixF9qw/RidgeRegressionSigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RidgeRegressionSigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769994,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RidgeRegressionSigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RidgeRegressionSigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..sUS78J8-wwDMoiZADKwLHA.PGRHnrgRKW4n9vyTsXSWo4mVShIWY5Y4eg7zcUBcY1KuMQtP9fwYM3YobPQassK-v9cpEDOyYflhSk_u5ndLg4v4ldHb51D5OKRDEF4MaTeHExukS4gjT2J3tMQ-eUKBIpsYfUU0WSPbFbYD5eBL7rKoN3bzuwZ5FSlBuo7d3ck.mhTlakwGIvkauceX47cLFQ/RidgeRegressionSigModelProbs.csv","fileType":".csv","contentLength":9565,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RidgeRegressionSigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769995,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RidgeRegressionSigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RidgeRegressionSigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..XQoEflfpYUNdnC8OyHxOhw.MkTZKtxVKIO9jGekXK-2vTa3306MoU7xPRRMX7UfI1aUC1ls4nlHTfnDDWl6M9_DMDSDU3b5itf31WdykF8rwLhDwEMU3PZUz1wzLlci8jrcDZeDiBj6QHukf5eA2DC8rqC0V19NI0ztb92oJYafQeFuI_rfEmJWKu8ncmJP1P4.pD892FuXMoE4C11ENfaaTQ/RidgeRegressionSigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RidgeRegressionSigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769996,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"RidgeRegressionSigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/RidgeRegressionSigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..LHgG-SFK5QxCFX3o9FKisA._RSHQySbP6l7kEDDs3Lz9iHGKwG5FOatD3QEVqZFeS-I4RIoEkemWRJpZz7nOiPbxPXrFXMSBrq2duWtN3n5hiTaM_pDf1vb26E3rMKXW0fde2wdm1oGuTgXJFlFPBgi8b46H6_c1004tFdxbEgKmeHc01dlgptCaii_wdPkgqQ.O9alDmqx-2yacUC1DJnQuw/RidgeRegressionSigModelTrainProbs.csv","fileType":".csv","contentLength":19632,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"RidgeRegressionSigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769997,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"SVMModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/SVMModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..TMaSXuyJLjFlN_G5VK7TWg.Yci-ofhGFEZRVFhAamQ8RRsqQe0qR_WbPRWgiN6N42xFXDBJSA0q0Nd3xBu1xpuc5PBhmwhBE9vJXaLtpWYjqXK7o5Ml6H2wKBu22HSybtour8gOMFL_SZhQEz5YNn7Jg3bg2yz8_f_LBI1Uk6IYkABhflu1fdaZS1CR8ifeUZ4.QeCv24yscfzymAAW-5f9aA/SVMModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"SVMModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769998,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"SVMModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/SVMModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..0qtXAutpHvBR3KZ3emGAsA.DSNhdxCeo5KMoqLRhWqiM2TgxG8oyif2WvYkIlcOcJ-51Q0sy5LQHsbM_1vZI8QIbA5n-1wE4h3b-c0-zuvT8NvSvmEvj3P6ou7u8Gmr9fGhZAh0zenyizIg4yR_MUZkKPpImMDcaAGJWKKl2G5m-rEc0Nirce4uSQTCrcBKcio.omn2tPU-oBrYwjsCVz0mIg/SVMModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"SVMModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14769999,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"SVMModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/SVMModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..ilJnKEMQRsiY77wu08fxeQ.uldwq82PfqYgu2Uyn0pxyCtAuozRZLCU01Vp5-LxjjmxgMSGKlX4z0E5kUNrKsPqc1g9CmtmUYVyNCZfmAW_J8wnTQZzIfnJ-iScEw21c_-UrHlhuiCTvQnAGfq2CpvbJ-sfkVYLO7ps9c1RzKnAUVIbyvncBZtL0tQ3olV7-eo.QUFsp2R_xrSo52UPxGEhwQ/SVMModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"SVMModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770000,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"SVMModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/SVMModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..SayTwqtId0ik7Q9HOxNIMA.PuwFTSaXYvC6BStfh-0MJf2LMCPBLcZGY56pSfd4ABVVQXqshP1a20L7rxjg3ya9xOyKYuCCHJ1gPWw1odD1Kv6gaOpOzGNytSXO2zpuiqaolLoXzKoCMUVGA8spjWWD_KWk5H0inSf8fQ4rzhuPW--gYDKwjIA5L2PaoFPyAn0.kCeJWHH38u6QJEawbbg8dA/SVMModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"SVMModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770001,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"TitanicModelSummary.png","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/TitanicModelSummary.png","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..-ztbDZMfHJ_xaHwAs3j9mg.4jgTrePVymQPsnl4aoOc83yUVmMYLsjlnPJRN8ZtW-jsyuIEJJAfGAxw1LbQaHWozjmfmaD3qPZHGY1wqKrQ2fWmDS8UWrNh37eMyH1LPeUobXfyZltN6X-umcjP3aRCPVmfZDoOQCyiG5rNCSxszv0HHvY7fBGShV4-z3NhB6c.cLhEJvvkVRgQ6Q2TgN460w/TitanicModelSummary.png","fileType":".png","contentLength":75863,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"TitanicModelSummary.png","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770002,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"QDASigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/QDASigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..GW5PLtHFC7zCH7Vybsmi5Q.rcmPhT5Dd9M0Ktn9GQuzNXyqmQ4xscAeVzNRkuojVmpnlJOkV2-s2eQ1E7RlBne3ryX79nzWyiTecn9fZ2z6U1vm5z-bTndAGCmk3et0Rousj8HuiccGJVozZ-Vz8oP-NmffWFA3PKcQZjNFlTkwAdBO-3ehoTx9GCqzPSqIHGA.MtrRdrScptFui-0gQsmMWw/QDASigModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"QDASigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770003,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LassoRegressionSigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LassoRegressionSigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..DpZ0vxXcVZ4-GzI1iAsi1Q.JryMgtq8wnP7Sb20x_qxPjWK2_eCOusRwVml6Ca485DPRG6cPAKYmovetsdI3FCWE7pQ_P2c-sDDfxMn7GfA8N3QiMbsUuzCnw9lePA0N6G4gKKPDUexPjFKHYxGwDS52WMPKRQFwriKexG4B5U4FPAVwSFBnlBh60xrQ5BgZvE.KkoqyFGdszI0KPEmpG8Yeg/LassoRegressionSigModelProbs.csv","fileType":".csv","contentLength":9540,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LassoRegressionSigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770004,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"QDASigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/QDASigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..KoC3Y5pKXoL_s75UpoTSaA.HKRdm3l8-QpI29-xSa2zIhebtmntniNsQYxnXEfotwzn8SHy2eOLw5iEfqKjXYLae-4dDyVYSk33O2_U89-iH2JTf3Y-hZB6_v1yF9Ptt6Zv_oh49LYSAIFRiTKIHLh8NMeEsB9fXakrVxF-TMvTWz95cM5Ao-Zy-wDKiopslWs.uWZodxEZ0hKKswhiXwGj3g/QDASigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"QDASigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770005,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"QDASigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/QDASigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..-n0_Z5Da0ETKrj1UyL1NEg.Ko39Y0HClxFkXRsLHpETCbt-LcxLPozAwMZNsfpJx7shVKOAKXwNC-rx16BCnMFewIE8k003RVB932nEh1J3RUo0-xRDKjNXTY3dAMWGjB8MP4WVNsOWBwBDYrqK3eEMsUouP6TAxC0Lneyg-yrC3qiaigHUmzB-jSeuQd5iU1s.cTuNCFkNW--dMkJK4XfODg/QDASigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"QDASigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770006,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LassoRegressionSigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LassoRegressionSigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JPf7XVVgeC9nvM3Ut2sTTA.K_vuP1Rs44Rm9w2u3DIxLmvXJx8wdDdY8lVBQOD3n_acszgpQba5gtLmli-UQ3SGjhLrdJIiuAYToIaAQGu4OyVo-1HKdr0ZZxzEY6ByBKG-tSmKVCe08DWGZ2IEjmezfe1GP0ij3PnR2iyS4xo4fEG4nxiXiLXkA8YxpS4eK1I.ypX0q18zCa3mnzhSzVrrFg/LassoRegressionSigModelTrainProbs.csv","fileType":".csv","contentLength":19591,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LassoRegressionSigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770007,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LogisticRegressionFullModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LogisticRegressionFullModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..EJaT8QuapMyjmUKCuX2yjA.M9D7SaYovh2ysMbc7l1hUfLFLujGD6OOM5FEucG7ODpwhxqJgpmcUje9M4T3vJivh4qs4mnLED8DPK-G-LjZ-qc5CyU5uyCvvUpJ0mmpk_gtmpVYKCCS4R1iiBGwz_UjPZPCbfLCU4LEpuMt0l6mEtquL3Oj3mkmxGKC6PWMhqU.G0iOaLm0jd4x4C4I93n2wg/LogisticRegressionFullModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LogisticRegressionFullModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770008,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LogisticRegressionFullModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LogisticRegressionFullModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JcoJYyGwn1c9fwR6fydpyw.F1Z4n2fmFhEaMKOuMDNCx5xEN84aSOSqBK5X6b3ejQyOWZFenUJOaQ2U4jgTgMSnUhInBG1RGyHrc6WRbL7mVVDHU5Zqp1-nx_0anDi22_McZcMtaKFk_ImE3OONV7HPBLYtuTPq4VgaNizVdTWKslovBedkTlSySiM2P6OrgMI.kJOs0fkU0fg9rCeRbL5_xA/LogisticRegressionFullModelProbs.csv","fileType":".csv","contentLength":9601,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LogisticRegressionFullModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770009,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LogisticRegressionFullModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LogisticRegressionFullModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Hq9FlNs11lb_qGEpz8DsyA.q6A6rjo_ow9o3VaKiFWxbMANMPikL836kcdodQj6eFuxzFEUdIXRi4AMEyUoYPQIqMPLeG9hjKkdbfS3ilmJOIQvSs5ld-BOngOtfTZUnu0al4oNvGQ6HGzsJPl9KOWuJlQbdy8tNlQqnInGCBuJjMCy14VDDJDUvgismZFQYEE.g06_UeKPEjKUJ-cQ6X2-7w/LogisticRegressionFullModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LogisticRegressionFullModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770010,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LogisticRegressionFullModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LogisticRegressionFullModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..aH1Jv01c3dcp4U14fF-dzA.xwS_ICmXsryDsSOWQe3y9JVlk3tbaWdol-w2m63yY33F-eRvsU9JHSucT_9F7vM78i2dUoyOJGArjQj1YEW_p1_ZMG7gCgqyuh4ZYbjt5vvQ-aBdgS6CLpcAq9klW1_rfVvb12bpIRmgY8t-XuJC-6LLM3OoUyewLQubsXpuSJI.zt1lCH7UhVmMsozL7qyjhQ/LogisticRegressionFullModelTrainProbs.csv","fileType":".csv","contentLength":19726,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LogisticRegressionFullModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770011,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LogisticRegressionSigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LogisticRegressionSigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JQKdy0ILg_v2XrkihE_tGA.5WiR69_qE3yRVnRYrnSJ6GFIS_e7WX3mMLWF3Y6pYgZ9Z-OgwzmZMRJOiiiGu3QOPyXuN8HwFy5Dpq19KGJG96XUtFSB1KQ36D1Vv0Q3Rs0pQsD6fuh2IuoXUu8uasZ3JdGecwwz5J3tFGsgaone9r2C3H8go7jIM8ae4JxK5TQ.Gg3E5qNhLzMBJTE7AHcNYw/LogisticRegressionSigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LogisticRegressionSigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770012,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LogisticRegressionSigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LogisticRegressionSigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..kX187SrIOXqzsYcuobhL2w.nARSIpjP7nbuW3b3lc9kaAT0iKTvZkUwWV9xvjGZ615JAVPzs9uxECjR61MF8BG31Lg-YdrZA7WxqiLLtszHfugTTQuIYnJXvGbtZFRi4v9Fd5vw5vijYFpeXxh4h-LltZK9cVY8Xln90QKc_qfmMMZ1S4XbKaWxoHnZkMnR-F0.GybK4wCL2urhckQI2Me9Zw/LogisticRegressionSigModelProbs.csv","fileType":".csv","contentLength":9612,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LogisticRegressionSigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770013,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LogisticRegressionSigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LogisticRegressionSigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..lL6vN-1xEsK-iK9XgpSUoA.AD7YVlK8HvoJDtei2YwRI2mOF_DRARTpcB75bDuCje701-SZ8YX5j-BavXJWZy_r_C-dOrnCtqFFBBJtIyjvN1zENInObh8cYUtSLPemNyYGSQA6xBAIf0p_OsRl_ux1jeV83OyarY4AjXofVtD5RRultGfjZRViNu8XXe4Ii9M.5I4gPG0pl-xFgBIYph9Prw/LogisticRegressionSigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LogisticRegressionSigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770014,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"LogisticRegressionSigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/LogisticRegressionSigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..IK8hGDoN_CMYMoRpeAJb6w.nQRY7Em8PRiytfjJyj-xtggCyIaFyVwraj_n0hhx7nnjwYRjL38wtp6GgyfLEPkaemHE5aPzCyAEidCC6si9XVxLoXjdg3X72AZFCr_ptL5FVDldpTpDPVmHzFWycjvoMJ3WLyvT0Mbx1piSicdnMWtsldrIQgOdkBDTQzzGpWM.T43Id3RIlDeU12CPYH1JEg/LogisticRegressionSigModelTrainProbs.csv","fileType":".csv","contentLength":19723,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"LogisticRegressionSigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770015,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NeuralNetworkSigModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NeuralNetworkSigModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..CLDzznUXRCVPxocl_qoU6g.6P0adva_oPVpi72VG0JzKTHzzpeDJnzyLbmYdZ0l-90BmpRECPLRP70rbIgLUSkGjONX7-shouAlj9sT7TTizSzUi0wxjS03NWlkkbbPEbIYca98D-YnuxR1qO-yqG7nNyJgQZbTt4hOdnLbG8jkShEFYPIkGLWpUz1cwE-isOw.7B5sRjo-cjYB7aNbbcCSVA/NeuralNetworkSigModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NeuralNetworkSigModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770016,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NeuralNetworkSigModelCV.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NeuralNetworkSigModelCV.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..zecVi2CUgX8mvDFe5smA1A.Rf5mlvsTtrsQUpHPqgubwMzGkVwKaySQ00RtsInErTy9pD8c4dGc3wJ_cekSkywrlNNpprc7D7YHEV-ijJ2OgiE3pNusrHwTDoaTgW_x13YoacC-ZJo-03FXgtqyFAGWiT_KvJ1AqmHMlfeoVwxU2PFgn776H5zB2XtmDWU3vtE._1gfC3dfGfZbAogfNg6UFw/NeuralNetworkSigModelCV.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NeuralNetworkSigModelCV.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770017,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"QDASigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/QDASigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..UDwzlp_5oz3rqnnZTDnRig.OsBm5TQDX7Lu9836GQefMIpWJYRKMl5C7DMy6Q-dsXorrc4-lSQLbOY4TqHXoR9Z6JQQGKO-Pal4hkDS2lH3nzl8fkq99Pu2Kb_hxKl16WV1PqMZkBSmpcOuCqkQK2_tRkqn_PEQggNK5FTAIf5Wt2ek26_Is6s1-T4SMR3aTVA.5dq1r_uy5_SGvhCbHq3q_Q/QDASigModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"QDASigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770018,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NeuralNetworkSigModelCVProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NeuralNetworkSigModelCVProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..hBv1pFIMor4WLdZiu9haIw.6e64f4rGerQnxxqEM6xAA8elaLghT65Eii84Hw3v-bob7wRiSo-CNAroYMI_FKJI0dWPrvhA_hP7pufTwMa2mVlyXCFxNMwDm58REonsEzHWRlgIhkEHZoMRmLX5XxgQ5R5pJneKKS7P0Aw9AbVhh-cbjefe-jnoXrCEik5sF9I.fRQiDsQtjzqQBzo7JlLlaA/NeuralNetworkSigModelCVProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NeuralNetworkSigModelCVProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770019,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NeuralNetworkSigModelCVTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NeuralNetworkSigModelCVTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..9FxuA9Um-JNpE6FhffpzxA.WWYrvv9DHwopbYffymcQw2_qY_r8yFq6qhUyaa1-tGXLnunT7sKglHLFsxtBRzeFvKwWMQaT9SzJwTlhAxvSUlVcFJ5tMU2aWRfVkOsib-Kx13MELFJS9z3D1ltpGtY2BB_I9tuYHEgahEIhxhljadWgHXiIh0wO4QoOLZBV5qk.3x0i9c86DrtSsI3DiIzzyg/NeuralNetworkSigModelCVTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NeuralNetworkSigModelCVTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770020,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NeuralNetworkSigModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NeuralNetworkSigModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..6gqHPbnfQ-YTkQz0jE8Cbw.m1_R8inWSpbG6Ws6uWFTVufdvZKfdmPGlfqX1Agg-VQBY8w28lVFbrP4LbyX_DEypO8Iycx_JUOxPfijsI_zm23YuVotRks7fySs6v6oY7sPqH7ODldxo3oRTn4ZcROJ7Z1tHmYsb2OghsWCl6pyl6nzEn_dRPFfQOq1S35YppM._reuZrAtdIF1wKv8LdFFSw/NeuralNetworkSigModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NeuralNetworkSigModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770021,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NeuralNetworkSigModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NeuralNetworkSigModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..TTik3_h2K2Lyc0s3XHMP5Q.WSXNBXDQa0tXTqE1ayclak5bHrwhV6XGSLwuxjYeV4dD5GcZ2rlU2j7IjbyygnHyPC-NzjgHzpfKIcCQdCi53PrmjnPGMiWJzQAl1pnot9Inj60KDpu3I4Hx8vPazQymH29LAqudWRhPgoR9xY7geUIPYzPcipFOFdsIoy_Y4Es.qFjtqxsenL9qRE-X2clkRA/NeuralNetworkSigModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NeuralNetworkSigModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770022,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NeuralNetworkSigModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NeuralNetworkSigModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..oj4vZT7br9sJ_NBzHMsJ0g.MZP0Zmm173b5rvD6NJgjxdJgBuxJadhb6h-I9ZwK1-YDfEby0xkdrk4q4F--p8bo0QNcXv_QGQ-Tm7Aybn-lDSxVITbShPIz7jxim5eWjziYUp-NE-HiAaKtV5CD1iE7f8LSMM0Xv9ynhVL1RUn2ure5dhNk5tB2XtyoZTLLu60.x0_Fp1FLX1oe934-0BxbEw/NeuralNetworkSigModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NeuralNetworkSigModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770023,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NullModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NullModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..PdtmkKN05hHyBGpBP5gSVQ.Z9sA1lkN0axc64gJXaAlJRdcwRfuLkS6kbdvb2cWS7ExSkDVO8dgtbsaLIQo4vCXipk69fr-sY93pLT1pEEMV80daL2ZcGOGndPiwaKHVPU8LTQPZ6IX3In-FwSNbV96CXugRT3iOTTwpbMeGpnziO2zeB_yysECcDZBU3W5xv0.lpuS6VJ2RHUXN1sgdRVlTg/NullModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NullModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770024,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NullModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NullModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..sqejKbVQATJVEe0iGS_Vow.Fjecn6VAuY8bEWcsltGRDGIDmqcKhVSHD0HIJZMJKCCto_ylcAalk5IiMZEyJ0J4Y0Qghck5XiAm--BH4BzbdHIIAQPI9uZE5l9gHEZicfnMCmUhy7UOI-rNdxiGZbmdTH4NsC6v8uqShEgyKf6Kh_-9v61eLIg7LApc6LamxOE.oYuv4MzuUEHjn-IM3sGbRA/NullModelProbs.csv","fileType":".csv","contentLength":9527,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NullModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770025,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NullModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NullModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..9TOJaxzPO_s79qOS8HmkfA.6PyZ4DA9SLEQHZ3oklHeB230rIAmlIX8Wug-1_JaJZYrmktZ6Vb1zXBCetaI58vNKkQsp-VlfLM6vzU5iDc1Wb6qdzyfRMTvg6jD0YlM0uzBkMF2qjoHhgDgkwbjE8vn-hqjKkuBnhmw__uHESx0eR_ITLDJzVj-qX_xrEUddBc.FKOubS7MHcBWntnzpdy3aQ/NullModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NullModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770026,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NullModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NullModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..7w1vytI1dBLsSn3HgyrxhQ.Bm-s1cm-V2o3EDsxY3v9C-YKwM64DTBBDE598Q_WTVseGVAIr7c2EnZCJrmyxwzwF1aNOLF30ASdF0aDsX3pRuRdYF0pOEp4Bb6_FBUiMtOBjZeA5SxudCUaBm6MM0zXQxKmNKbwn3t-afjCRu5vQYIdAacSkwVi9y4Ix0PDRYg.tMMvckNbd-83WX4V2MzAOA/NullModelTrainProbs.csv","fileType":".csv","contentLength":19515,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NullModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770027,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"QDAModel.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/QDAModel.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..FL3CpdLWzSQSHg1aDF11Qw.qlCDO0d1IkARacBKpqNYljlDlcdIYKnNxpdBG7lECGLZXAB1LT9d2QIZJygn7aARGF318fNq7a1tkq-C2LYsE1kXdaO1gYLh0pih1HAwCWSeAvePGxc0V6F9hd4Y_aqKu7raw_aqOoKD3TGXyz-N-SmC3LrFatWZ_c5oaBq9f8c.cW5DAXsjuwzFxBhPfnt9eA/QDAModel.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"QDAModel.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770028,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"QDAModelProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/QDAModelProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..6HnOcEuhBfVs_n0lueU04g.CaC4fzUWBxo54KTrADB5yV0JitCuKq2llqe5JJDQbb_mfM5Z2dusJXflkvoH7n_v7zDKR63JrSM7pwAAVWs0-UNf84sM7J0Kmo1D5lxgr9j5doCM9h0sIjjt_1RoVYMRIZtc3_03T17UJuAKR_Z3NeEtzZf8FQ_PoilNInlnvkk.DZEv78uODHbEIL2G8S7fNw/QDAModelProbs.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"QDAModelProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770029,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"QDAModelTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/QDAModelTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..xWgQkZdZIy59QnjBDlbf9A.jGcla5Lw7fVaXqsYFhNYRpJlBFCCMqFN2JPqh08aAw9W1pFcF2Mqdb3ERVilCSCU8xPdQA-oOh0FPa-unRvzplzHPSWDfpogQD3HyeXW5UVghMQK9nF8gxwgSlmEkaJlWkCLlu2VACeb5OiSYj-RYKZs1bWC_8BndOilSYm_Shg.9WPVSEXrI4ooZZOZP8NfWw/QDAModelTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"QDAModelTrain.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770030,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"QDAModelTrainProbs.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/QDAModelTrainProbs.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..IxowhyrvzPS8DSorDC32vg.OFaMha6grhopdrEJTTi3qifoqofZxmHzGUYSVMmO3UdRYaHKz7dXqtHjgBTz85jP9PwshjMbCtuvlJrHgTe4eks8tAplZX6S8mE1Ufh1To43PGm-BcV-OTsRMP5vaHzFK6V7SLH54Xt-M6ELG3mKsAFUU4nA38G7AeqSrl2OJKU.BkwzCCYVwQLBnDcngCwLSA/QDAModelTrainProbs.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"QDAModelTrainProbs.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":14770031,"kernelVersionId":5439677,"kernelId":1549984,"size":0,"fullPath":"NeuralNetworkSigModelCVTrain.csv","previewUrl":"/kernels/preview.json/5439677/40d39498-53c6-cb8f-c555-f14e6d614558/NeuralNetworkSigModelCVTrain.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/5439677/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..197qmVIQP70T7bCnby6csA.vJLM7pcXj7xlYoU9wKyZjDVf7kP0CdODNEJzceti0Ef16Y5drGrDB9Mf2RDnZM3frDDeeqr3bY4GZeTl-8jHw_zqWNGD_UjdR7aBtkU6RxrpJNxn3x8s6RGTSQ1kPjAx6ZUSyDFL7yhR2haS_u61501rrdvxlHF01mZuOYCLm-k.OsbwP1i26jsQLZMX64KDbg/NeuralNetworkSigModelCVTrain.csv","fileType":".csv","contentLength":5259,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"NeuralNetworkSigModelCVTrain.csv","description":null}],"outputFilesCropped":false,"ouputFilesOwnerInfo":{"databundleVersionId":0,"dataset":null,"competition":null,"kernel":{"kernelId":1549984,"kernelVersionId":5439677,"dataviewToken":"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..xcPDtMVWtIsXNHDCG-wqww.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.KzwpEZCYmpR086fPM0amNA","scope":"neillwhite0/titanic-survival-prediction-with-r"},"previewsDisabled":false},"pageMessages":[],"dataSources":[{"imageUrl":"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png","sourceUrl":"/c/titanic","slug":"titanic","lastUpdated":"2012-09-28T21:13:33.55Z","overview":"Start here! Predict survival on the Titanic and get familiar with ML basics","sourceType":"competition","sourceVersionType":null,"sourceId":3136,"sourceVersionNumber":null,"maxVersionNumber":null,"descriptionMimeType":"text/html","deleted":false,"private":false,"privateButVisible":false,"ownerInfo":{"databundleVersionId":26502,"dataset":null,"competition":{"competitionId":3136,"dataviewToken":null,"scope":"c/titanic"},"kernel":null,"previewsDisabled":true},"type":"dataSource","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"id":63842,"blobFileId":37991,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/gender_submission.csv","creationDate":"2017-02-01T01:49:18Z","isDummy":false,"size":3258,"fullPath":"../input/gender_submission.csv","previewUrl":"kernels/competition-preview/3136?relativePath=gender_submission.csv","downloadUrl":"/c/titanic/download/gender_submission.csv","fileType":".csv","contentLength":3258,"contentType":"text/csv","contentMD5":"MNEHO5ZKXYFUMexgOg3jUw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":418},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n"},{"order":1,"originalType":"","type":"boolean","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Survived","description":"0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"gender_submission.csv","description":"892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,0\n899,0\n900,0\n901,0\n902,0\n903,0\n904,1\n905,0a\n906,1\n907,0\n908,0\n909,0\n910,0\n911,0\n912,1\n913,0\n914,0\n915,0\n916,1\n917,0\n918,0\n919,0\n920,0\n921,0\n922,0\n923,0\n924,0\n925,0\n926,1\n927,0\n928,0\n929,0\n930,0\n931,1\n932,0\n933,0\n934,0\n935,0\n936,1\n937,0\n938,0\n939,0\n940,1\n941,0\n942,1\n943,0\n944,0\n945,1\n946,0\n947,0\n948,0\n949,0\n950,0\n951,1\n952,0\n953,0\n954,0\n955,0\n956,1\n957,0\n958,0\n959,0\n960,0\n961,1\n962,\n963,0\n964,0\n965,0\n966,1\n967,1\n968,0\n969,0\n970,0\n971,0\n972,0\n973,1\n974,0\n975,0\n976,0\n977,0\n978,0\n979,0\n980,0\n981,0\n982,0\n983,0\n984,0\n985,0\n986,0\n987,0\n988,1\n989,0\n990,0\n991,0\n992,1\n993,0\n994,0\n995,0\n996,0\n997,0\n998,0\n999,0\n1000,0\n1001,0\n1002,0\n1003,0\n1004,0\n1005,0\n1006,1\n1007,0\n1008,0\n1009,0\n1010,1\n1011,0\n1012,0\n1013,0\n1014,1\n1015,0\n1016,0\n1017,0\n1018,0\n1019,0\n1020,0\n1021,0\n1022,0\n1023,0\n1024,0\n1025,0\n1026,0\n1027,0\n1028,0\n1029,0\n1030,0\n1031,0\n1032,0\n1033,1\n1034,1\n1035,0\n1036,0\n1037,0\n1038,1\n1039,0\n1040,0\n1041,0\n1042,1\n1043,0\n1044,0\n1045,0\n1046,0\n1047,0\n1048,1\n1049,0\n1050,0\n1051,0\n1052,0\n1053,0\n1054,0\n1055,0\n1056,0\n1057,0\n1058,1\n1059,0\n1060,0\n1061,0\n1062,0\n1063,0\n1064,0\n1065,0\n1066,0\n1067,0\n1068,0\n1069,1\n1070,0\n1071,1\n1072,0\n1073,1\n1074,1\n1075,0\n1076,1\n1077,0\n1078,0\n1079,0\n1080,1\n1081,0\n1082,0\n1083,0\n1084,0\n1085,0\n1086,0\n1087,0\n1088,1\n1089,0\n1090,0\n1091,0\n1092,0\n1093,0\n1094,1\n1095,0\n1096,0\n1097,0\n1098,0\n1099,0\n1100,0\n1101,0\n1102,0\n1103,0\n1104,1\n1105,0\n1106,0\n1107,0\n1108,0\n1109,1\n1110,1\n1111,0\n1112,0\n1113,0\n1114,0\n1115,0\n1116,0\n1117,0\n1118,0\n1119,0\n1120,0\n1121,0\n1122,1\n1123,0\n1124,0\n1125,0\n1126,1\n1127,0\n1128,1\n1129,0\n1130,0\n1131,1\n1132,0\n1133,0\n1134,1\n1135,0\n1136,0\n1137,1\n1138,0\n1139,0\n1140,0\n1141,0\n1142,0\n1143,0\n1144,1\n1145,0\n1146,0\n1147,0\n1148,0\n1149,0\n1150,0\n1151,0\n1152,0\n1153,0\n1154,0\n1155,0\n1156,0\n1157,0\n1158,0\n1159,0\n1160,0\n1161,0\n1162,1\n1163,0\n1164,1\n1165,0\n1166,0\n1167,0\n1168,0\n1169,0\n1170,0\n1171,0\n1172,0\n1173,0\n1174,0\n1175,0\n1176,0\n1177,0\n1178,0\n1179,1\n1180,0\n1181,0\n1182,0\n1183,0\n1184,0\n1185,1\n1186,0\n1187,0\n1188,0\n1189,0\n1190,1\n1191,0\n1192,0\n1193,0\n1194,0\n1195,0\n1196,0\n1197,0\n1198,1\n1199,0\n1200,1\n1201,0\n1202,0\n1203,0\n1204,0\n1205,0\n1206,1\n1207,0\n1208,1\n1209,0\n1210,0\n1211,0\n1212,0\n1213,0\n1214,0\n1215,0\n1216,1\n1217,0\n1218,0\n1219,1\n1220,0\n1221,0\n1222,0\n1223,0\n1224,0\n1225,0\n1226,0\n1227,0\n1228,0\n1229,0\n1230,0\n1231,0\n1232,0\n1233,0\n1234,1\n1235,1\n1236,0\n1237,0\n1238,0\n1239,0\n1240,0\n1241,0\n1242,1\n1243,0\n1244,1\n1245,1\n1246,0\n1247,0\n1248,1\n1249,0\n1250,0\n1251,0\n1252,1\n1253,0\n1254,0\n1255,0\n1256,1\n1257,1\n1258,0\n1259,0\n1260,0\n1261,0\n1262,0\n1263,1\n1264,0\n1265,0\n1266,1\n1267,1\n1268,0\n1269,0\n1270,1\n1271,0\n1272,0\n1273,0\n1274,0\n1275,0\n1276,0\n1277,1\n1278,0\n1279,0\n1280,0\n1281,0\n1282,1\n1283,0\n1284,0\n1285,0\n1286,0\n1287,1\n1288,0\n1289,1\n1290,0\n1291,0\n1292,1\n1293,0\n1294,0\n1295,1\n1296,0\n1297,0\n1298,0\n1299,1\n1300,0\n1301,0\n1302,0\n1303,1\n1304,0\n1305,0\n1306,1\n1307,0\n1308,0\n1309,0"},{"id":63841,"blobFileId":2613,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/test.csv","creationDate":"2013-06-28T13:40:24.227Z","isDummy":false,"size":28629,"fullPath":"../input/test.csv","previewUrl":"kernels/competition-preview/3136?relativePath=test.csv","downloadUrl":"/c/titanic/download/test.csv","fileType":".csv","contentLength":28629,"contentType":"text/csv","contentMD5":"dTO4Lq5LWCYQy9aKpjawFw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":418},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"1"},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Pclass","description":"1"},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Name","description":"the name of the passenger"},{"order":3,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Sex","description":null},{"order":4,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Age","description":null},{"order":5,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"SibSp","description":"of siblings / spouses aboard the Titanic"},{"order":6,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Parch","description":"of parents / children aboard the Titanic"},{"order":7,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Ticket","description":"Ticket number"},{"order":8,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Fare","description":"Passenger fare"},{"order":9,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Cabin","description":"Cabin number"},{"order":10,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Embarked","description":"Port of Embarkation"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"test.csv","description":"test data to check the accuracy of the model created\n"},{"id":63840,"blobFileId":2307,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/train.csv","creationDate":"2013-06-28T13:40:25.23Z","isDummy":false,"size":61194,"fullPath":"../input/train.csv","previewUrl":"kernels/competition-preview/3136?relativePath=train.csv","downloadUrl":"/c/titanic/download/train.csv","fileType":".csv","contentLength":61194,"contentType":"text/csv","contentMD5":"IwnMXwR4Ltm7YBbZ9OOBzw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":891},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"type should be integers"},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Survived","description":"Survived or Not "},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Pclass","description":"Class of Travel"},{"order":3,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Name","description":"Name of Passenger"},{"order":4,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Sex","description":"Gender"},{"order":5,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Age","description":"Age of Passengers"},{"order":6,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"SibSp","description":"Number of Sibling/Spouse aboard"},{"order":7,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Parch","description":"Number of Parent/Child aboard"},{"order":8,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Ticket","description":null},{"order":9,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Fare","description":null},{"order":10,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Cabin","description":null},{"order":11,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Embarked","description":"The port in which a passenger has embarked. C - Cherbourg, S - Southampton, Q = Queenstown"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"train.csv","description":"contains data \n"}],"name":"Titanic: Machine Learning from Disaster","description":"\u003ch3\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe data has been split into two groups:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etraining set (train.csv)\u003c/li\u003e\n\u003cli\u003etest set (test.csv)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003e The training set \u003c/b\u003eshould be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use \u003ca href=\u0022https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\u0022 target=\u0022_blank\u0022\u003e feature engineering \u003c/a\u003eto create new features.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eThe test set \u003c/b\u003eshould be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\u003c/p\u003e\n\u003cp\u003eWe also include \u003cb\u003egender_submission.csv\u003c/b\u003e, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\u003c/p\u003e\n\u003ch3\u003eData Dictionary\u003c/h3\u003e\n\u003ctable style=\u0022width: 100%;\u0022\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\u003cth\u003e\u003cb\u003eVariable\u003c/b\u003e\u003c/th\u003e\u003cth\u003e\u003cb\u003eDefinition\u003c/b\u003e\u003c/th\u003e\u003cth\u003e\u003cb\u003eKey\u003c/b\u003e\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esurvival\u003c/td\u003e\n\u003ctd\u003eSurvival\u003c/td\u003e\n\u003ctd\u003e0 = No, 1 = Yes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epclass\u003c/td\u003e\n\u003ctd\u003eTicket class\u003c/td\u003e\n\u003ctd\u003e1 = 1st, 2 = 2nd, 3 = 3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esex\u003c/td\u003e\n\u003ctd\u003eSex\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAge\u003c/td\u003e\n\u003ctd\u003eAge in years\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esibsp\u003c/td\u003e\n\u003ctd\u003e# of siblings / spouses aboard the Titanic\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eparch\u003c/td\u003e\n\u003ctd\u003e# of parents / children aboard the Titanic\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eticket\u003c/td\u003e\n\u003ctd\u003eTicket number\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003efare\u003c/td\u003e\n\u003ctd\u003ePassenger fare\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecabin\u003c/td\u003e\n\u003ctd\u003eCabin number\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eembarked\u003c/td\u003e\n\u003ctd\u003ePort of Embarkation\u003c/td\u003e\n\u003ctd\u003eC = Cherbourg, Q = Queenstown, S = Southampton\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eVariable Notes\u003c/h3\u003e\n\u003cp\u003e\u003cb\u003epclass\u003c/b\u003e: A proxy for socio-economic status (SES)\u003cbr /\u003e 1st = Upper\u003cbr /\u003e 2nd = Middle\u003cbr /\u003e 3rd = Lower\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003eage\u003c/b\u003e: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003esibsp\u003c/b\u003e: The dataset defines family relations in this way...\u003cbr /\u003e Sibling = brother, sister, stepbrother, stepsister\u003cbr /\u003e Spouse = husband, wife (mistresses and fiancés were ignored)\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003eparch\u003c/b\u003e: The dataset defines family relations in this way...\u003cbr /\u003e Parent = mother, father\u003cbr /\u003e Child = daughter, son, stepdaughter, stepson\u003cbr /\u003e Some children travelled only with a nanny, therefore parch=0 for them.\u003c/p\u003e"}],"versions":[{"id":5439677,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2018-09-01T00:28:52.653Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":1,"linesInsertedFromPrevious":2,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":4783.986551859,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic Survival Prediction with R","url":"/neillwhite0/titanic-survival-prediction-with-r?scriptVersionId=5439677","versionNumber":6,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":5438929,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2018-08-31T22:46:45.93Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":8,"linesInsertedFromPrevious":139,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":5294.060889039,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic Survival Prediction with R","url":"/neillwhite0/titanic-survival-prediction-with-r?scriptVersionId=5438929","versionNumber":5,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":5434813,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2018-08-31T16:40:45.42Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":82,"linesInsertedFromPrevious":0,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":5051.251221658,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic Survival Prediction with R","url":"/neillwhite0/titanic-survival-prediction-with-r?scriptVersionId=5434813","versionNumber":4,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":5432785,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2018-08-31T14:23:02.937Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":0,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":1,"failureMessage":"The kernel returned an unsuccessful exit code (1).","isValidStatus":true,"runTimeSeconds":5029.564117523,"succeeded":false,"timeoutExceeded":false,"usedAllSpace":false},"status":"error","title":"Titanic Survival Prediction with R","url":"/neillwhite0/titanic-survival-prediction-with-r?scriptVersionId=5432785","versionNumber":3,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":5427090,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2018-08-31T08:11:00.43Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":33,"linesInsertedFromPrevious":47,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":1,"failureMessage":"The kernel returned an unsuccessful exit code (1).","isValidStatus":true,"runTimeSeconds":4752.758956349,"succeeded":false,"timeoutExceeded":false,"usedAllSpace":false},"status":"error","title":"Titanic Survival Prediction with R","url":"/neillwhite0/titanic-survival-prediction-with-r?scriptVersionId=5427090","versionNumber":2,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":5424562,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2018-08-31T05:34:21.417Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":2815,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":1,"failureMessage":"The kernel returned an unsuccessful exit code (1).","isValidStatus":true,"runTimeSeconds":5016.237120375,"succeeded":false,"timeoutExceeded":false,"usedAllSpace":false},"status":"error","title":"Journey through Titanic Disaster with R","url":"/neillwhite0/titanic-survival-prediction-with-r?scriptVersionId=5424562","versionNumber":1,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null}],"categories":{"categories":[],"type":"script"},"submitToCompetitionInfo":null,"downloadAllFilesUrl":"/kernels/svzip/5439677","submission":null,"menuLinks":[{"href":"/neillwhite0/titanic-survival-prediction-with-r/notebook","text":"Report","title":"Notebook","tab":"report","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/neillwhite0/titanic-survival-prediction-with-r/code","text":"Code","title":"Code","tab":"code","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/neillwhite0/titanic-survival-prediction-with-r/data","text":"Data","title":"Data","tab":"data","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/neillwhite0/titanic-survival-prediction-with-r/output","text":"Output","title":"Output","tab":"output","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/neillwhite0/titanic-survival-prediction-with-r/comments","text":"Comments","title":"Comments","tab":"comments","count":0,"showZeroCountExplicitly":true,"reportEventCategory":null,"reportEventType":null},{"href":"/neillwhite0/titanic-survival-prediction-with-r/log","text":"Log","title":"Log","tab":"log","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/neillwhite0/titanic-survival-prediction-with-r/versions","text":"Versions","title":"Versions","tab":"versions","count":6,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/neillwhite0/titanic-survival-prediction-with-r/forks","text":"Forks","title":"Forks","tab":"forks","count":0,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null}],"rightMenuLinks":[],"callToAction":{"href":"/kernels/fork-version/5439677","text":"Fork Script","title":"Fork Script","tab":null,"count":null,"showZeroCountExplicitly":false,"reportEventCategory":"kernels","reportEventType":"anonymousKernelForkCreation"},"voteButton":{"totalVotes":3,"hasAlreadyVotedUp":false,"hasAlreadyVotedDown":false,"canUpvote":true,"canDownvote":false,"voteUpUrl":"/kernels/vote?id=1549984","voteDownUrl":null,"voters":[{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Ahmed Sherif","profileUrl":"/ahmedsherif011","tier":"Novice","tierInt":0,"userId":1342670,"userName":"ahmedsherif011"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Zeitlin Xu","profileUrl":"/linzexu","tier":"Contributor","tierInt":1,"userId":1497955,"userName":"linzexu"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2034734-gr.jpg","displayName":"RogerX","profileUrl":"/takmanish","tier":"Novice","tierInt":0,"userId":2034734,"userName":"takmanish"}],"currentUserInfo":null,"showVoters":true,"alwaysShowVoters":true},"parentDataSource":null,"parentName":"Titanic: Machine Learning from Disaster","parentUrl":"/c/titanic","thumbnailImageUrl":"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png","canWrite":false,"canAdminister":false,"datasetHidden":false,"forkParentIsRedacted":false,"forkDiffLinesChanged":0,"forkDiffLinesDeleted":0,"forkDiffLinesInserted":0,"forkDiffUrl":null,"forkParentAuthorDisplayName":null,"forkParentAuthorUrl":null,"forkParentTitle":null,"forkParentUrl":null,"canSeeDataExplorerV2":true,"canSeeRevampedViewer":true,"canSeeInnerTableOfContents":true,"canSeeCopyAndEditText":true,"simplifiedViewer":false,"kernelOutputDataset":null});performance && performance.mark && performance.mark("KernelViewer.componentCouldBootstrap");</script>

<form action="/neillwhite0/titanic-survival-prediction-with-r" id="__AjaxAntiForgeryForm" method="post"><input name="X-XSRF-TOKEN" type="hidden" value="CfDJ8LdUzqlsSWBPr4Ce3rb9VL-gcCZL_Dn4dfC3xFL6ttUspzLqFqrve0CUNudm4yg8zWtSnFIOAHRVqR_i5wLXhQRvwYc2tvH5m5oW6euJQBYeab3eI0Td85U6isGUmBjHq1jt4khbweaHNk0sLalvMsQ" /></form>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["STIX", "TeX"],
            linebreaks: {
                automatic: true
            },
            EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
        },
        tex2jax: {
            inlineMath: [["\\(", "\\)"], ["\\\\(", "\\\\)"]],
            displayMath: [["$$", "$$"], ["\\[", "\\]"]],
            processEscapes: true,
            ignoreClass: "tex2jax_ignore|dno"
        },
        TeX: {
            noUndefined: {
                attributes: {
                    mathcolor: "red",
                    mathbackground: "#FFEEEE",
                    mathsize: "90%"
                }
            }
        },
        Macros: {
            href: "{}"
        },
        skipStartupTypeset: true,
        messageStyle: "none"
    });
</script>
<script type="text/javascript" async crossorigin="anonymous" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



    </div>

        <div class="site-layout__footer">
            <footer class="site-footer">
    <div class="site-footer__content">
        <div class="site-footer__copyright">
            <span>&copy; 2019 Kaggle Inc</span>
        </div>
        <nav class="site-footer__nav">
            <a href="/team">Our Team</a>
            <a href="/terms">Terms</a>
            <a href="/privacy">Privacy</a>
            <a href="/contact">Contact/Support</a>
        </nav>
        <nav class="site-footer__social">
            <div data-component-name="SocialIcons" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push();performance && performance.mark && performance.mark("SocialIcons.componentCouldBootstrap");</script>
        </nav>
    </div>
</footer>

        </div>
</div>




    </main>
</body>
</html>
