<!DOCTYPE html>
<html lang="en">
<head>
    <title>Titanic &quot;Spam&quot; Filter | Kaggle</title>
    <meta charset="utf-8" />
    <meta name="robots" content="index, follow" />
    <meta name="turbolinks-cache-control" content="no-cache" />
                <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">    <meta name="theme-color" content="#008ABC" />
    <script type="text/javascript">
        window["initialPageLoadStartTime"] = new Date().getTime();
    </script>
    <link rel="dns-prefetch" href="https://www.google-analytics.com" /><link rel="dns-prefetch" href="https://stats.g.doubleclick.net" /><link rel="dns-prefetch" href="https://js.intercomcdn.com" /><link rel="dns-prefetch" href="https://storage.googleapis.com/" />
    <link href="/static/images/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <link rel="manifest" href="/static/json/manifest.json">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:400,300,300italic,400italic,600,600italic,700,700italic" rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet" type='text/css'/>
        <link rel="canonical" href="/cdeotte/titanic-spam-filter" />                    <link rel="stylesheet" type="text/css" href="/static/assets/vendor.css?v=632d145d8598" />
        <link rel="stylesheet" type="text/css" href="/static/assets/app.css?v=9626e4919bfe" />
    
    
 
        <script>
        try{(function(a,s,y,n,c,h,i,d,e){d=s.createElement("style");
        d.appendChild(s.createTextNode(""));s.head.appendChild(d);d=d.sheet;
        y=y.map(x => d.insertRule(x + "{ opacity: 0 !important }"));
        h.start=1*new Date;h.end=i=function(){y.forEach(x => d.deleteRule(x))};
        (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;
        })(window,document,['.site-header-react__nav'],'dataLayer',2000,{'GTM-52LNT9S':true});}catch{}
    </script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-12629138-1', {
            'optimize_id': 'GTM-52LNT9S',
            'displayFeaturesTask': null,
            'send_page_view': false
        });
    </script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12629138-1"></script>

    
<script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
            n.callMethod.apply(n,arguments):n.queue.push(arguments)};
        if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
        n.queue=[];t=b.createElement(e);t.async=!0;
        t.src=v;s=b.getElementsByTagName(e)[0];
        s.parentNode.insertBefore(t,s)}(window,document,'script',
        'https://connect.facebook.net/en_US/fbevents.js');
    fbq("set", "autoConfig", "false", "136809193586742");
    fbq('init', '136809193586742'); 
    fbq('track', 'PageView');
</script>
<noscript>
    <img height="1" width="1" src="https://www.facebook.com/tr?id=136809193586742&ev=PageView&noscript=1"/>
</noscript>

<script>window.intercomSettings = {"app_id":"koj6gxx6"};</script>        <script>(function () { var w = window; var ic = w.Intercom; if (typeof ic === "function") { ic('reattach_activator'); ic('update', intercomSettings); } else { var d = document; var i = function () { i.c(arguments) }; i.q = []; i.c = function (args) { i.q.push(args) }; w.Intercom = i; function l() { var s = d.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'https://widget.intercom.io/widget/koj6gxx6'; var x = d.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x); } if (w.attachEvent) { w.attachEvent('onload', l); } else { w.addEventListener('load', l, false); } } })()</script>
    
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@kaggledatasets" />
    <meta name="og:url" content="https://kaggle.com/cdeotte/titanic-spam-filter" />
    <meta name="og:title" content="Titanic &quot;Spam&quot; Filter" />
    <meta name="og:description" content="Using data from Titanic: Machine Learning from Disaster" />
    <meta name="og:image" content="https://storage.googleapis.com/kaggle-avatars/thumbnails/1723677-kg.jpg" />


    
    

    
    
    
<script type="text/javascript">
    var Kaggle = Kaggle || {};

    Kaggle.Current = {
        antiForgeryToken: 'CfDJ8LdUzqlsSWBPr4Ce3rb9VL-xSoSCfV2i2Vel_IYRmqfAcEY_-LPVSwkZAlwBmXZ4_QC_cCRKACHq7qnLHE1qGuADOedpOTg7rMd9NHgl1gqowu4HbIp8awbI4pocJL0lQwiVml0iJflPbDzcxwokPMo',
        isAnonymous: true,
        analyticsToken: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NjAxNTk0NjIsIlVzZXJJZCI6MH0.F1RQ1UyBBIc84y2sL-abAg-w7M2zFZJ8gr7lpMzpxVQ',
        analyticsTokenExpiry: 15,
        internetKernelsEnabled: false,
        
        
        
        
        
        
        
        
        
        
        
    }
        Kaggle.Current.log = function(){};
        Kaggle.Current.warn = function(){};

    var decodeUserDisplayName = function () {
        var escapedUserDisplayName = Kaggle.Current.userDisplayNameEscaped || "";
        try {
            var textVersion = new DOMParser().parseFromString(escapedUserDisplayName, "text/html").documentElement.textContent;
            if (textVersion) {
                return textVersion;
            }
        } catch(ex) {}
        return escapedUserDisplayName;
    }
    Kaggle.Current.userDisplayName = decodeUserDisplayName();
</script>

    

<script type="text/javascript">
    var Kaggle = Kaggle || {};
    Kaggle.PageMessages = [];
</script>

    
<script type="text/javascript">
/* <![CDATA[ */
goog_snippet_vars = function() {
    var w = window;
    w.google_conversion_id = 955616553;
    w.google_conversion_label = "QSjvCKDksHMQqZrWxwM";
    w.google_conversion_value = 0.00;
    w.google_conversion_currency = "USD";
    w.google_remarketing_only = false;
    w.google_conversion_language = "en";
    w.google_conversion_format = "3";
    w.google_conversion_color = "ffffff";
}
// DO NOT CHANGE THE CODE BELOW.
goog_report_conversion = function(url) {
    goog_snippet_vars();
    window.google_conversion_format = "3";
    var opt = new Object();
    opt.onload_callback = function() {
        if (typeof(url) != 'undefined') {
            window.location = url;
        }
    }
    var conv_handler = window['google_trackConversion'];
    if (typeof(conv_handler) == 'function') {
        conv_handler(opt);
    }
}
/* ]]> */
</script>
<script type="text/javascript"
src="//www.googleadservices.com/pagead/conversion_async.js">
</script>



        <script>window['useKaggleAnalytics'] = true;</script>

    <script src="/static/assets/vendor.js?v=4721d2c14786" data-turbolinks-track="reload"></script>
    <script src="/static/assets/app.js?v=1d32f39aa6aa" data-turbolinks-track="reload"></script>
        <script>
            (function() {
                if ('serviceWorker' in navigator) {
                    navigator.serviceWorker.register("/static/assets/service-worker.js").then(function(reg) {
                        reg.onupdatefound = function() {
                            var installingWorker = reg.installing;
                            installingWorker.onstatechange = function() {
                                switch (installingWorker.state) {
                                case 'installed':
                                    if (navigator.serviceWorker.controller) {
                                        console.log('New or updated content is available.');
                                    } else {
                                        console.log('Content is now available offline!');
                                    }
                                    break;
                                case 'redundant':
                                    console.error('The installing service worker became redundant.');
                                    break;
                                }
                            };
                        };
                    }).catch(function(e) {
                      console.error('Error during service worker registration:', e);
                    });
                }
            })();
        </script>
    <script>
        function handleClientLoad() {
            try {
                gapi.load('client:auth2');
            } catch (e) {
                // In Opera, readystatechange is an unreliable detection of script load, causing
                // this function to be called before gapi exists on the window. The onload callback
                // is still called at the correct time, so the feature works as expected - it's
                // just generating noisy errors.
            }
        }
    </script>
    <script async defer src="https://apis.google.com/js/api.js"
            onload="this.googleApiOnLoad=function(){};handleClientLoad()"
            onreadystatechange="if (this.readyState === 'complete') this.googleApiOnLoad()">
    </script>
</head>
<body data-turbolinks="true">
    






<div class="site-layout">
        <div class="site-layout__header">
            <div data-component-name="SiteHeaderContainer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({});performance && performance.mark && performance.mark("SiteHeaderContainer.componentCouldBootstrap");</script>
        </div>

    <div class="site-layout__main-content">
        

<div data-component-name="KernelViewer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({"kernel":{"id":1029920,"title":"Titanic \u0022Spam\u0022 Filter","forkParent":null,"currentRunId":3955427,"mostRecentRunId":3955427,"url":"/cdeotte/titanic-spam-filter","tags":[{"name":"tutorial","slug":"tutorial","url":"/tags/tutorial"},{"name":"data visualization","slug":"data-visualization","url":"/tags/data-visualization"},{"name":"feature engineering","slug":"feature-engineering","url":"/tags/feature-engineering"},{"name":"pca","slug":"pca","url":"/tags/pca"}],"commentCount":0,"upvoteCount":13,"viewCount":517,"forkCount":1,"bestPublicScore":null,"author":{"id":1723677,"displayName":"Chris Deotte","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"cdeotte","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1723677-kg.jpg","profileUrl":"/cdeotte","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":4,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"isPrivate":false,"updatedTime":"2018-06-08T21:36:02.83Z","selfLink":"/kernels/1029920","pinnedDockerImageVersionId":null,"isLanguageTemplate":false,"medal":"bronze","topicId":75835,"readGroupId":null,"writeGroupId":null,"slug":"titanic-spam-filter"},"kernelBlob":{"id":8742620,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"Competition","sourceId":3136,"databundleVersionId":null}],"sourceType":"notebook","language":"r","isGpuEnabled":false,"isInternetEnabled":false},"source":"{\u0022cells\u0022:[{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022c9807f6c7ae92f549a00bf07368b16f4470c2a6c\u0022,\u0022_execution_state\u0022:\u0022idle\u0022,\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022# Titanic Name \\\u0022Spam\\\u0022 Filter:\\n\\n![spam](http://playagricola.com/Kaggle/spam2.jpg)\\n\\n# Introduction\\n\\nThank you [Oscar Takeshita][1] for postulating an interesting question [here][2]. Oscar asks, how well can we predict Titanic passengers\u0027 survival\\nusing only the words in the Name column and ignoring semantic information (i.e. don\u0027t acknowledge and prioritize title, last name, etc). What a fun idea. We must detect which emails (passengers) are spam (stay alive) by analyzing words.\\n\\nThis problem particularly interests me because I previously made a kernel that only uses the Name column [here][3] that scored an impressive 82% leaderboard score and cross validated at 84%. Therefore we should be able to build a great model using the Words within the Names. It would be great if a purely machine learning algorithm can find unsupervised the patterns that I did by reasoning. Furthermore Oscar\u0027s formulation provides an opportunity to practice techniques that didn\u0027t come up when tackling Titanic in the usual way.  We must work in high dimensional space and use techniques involved in spam classification and dimension reduction. \\n\\nIn this notebook, we\u0027ll tackle this problem first using PCA plus kNN, then second we\u0027ll employ a variant of naive Bayes.\\n\\n# Build dictionary and vectorize passengers\\nLet\u0027s begin. Our first task is to build a dictionary of all the words used in the Name column.\\n\\n[1]:https://www.kaggle.com/pliptor\\n[2]:https://www.kaggle.com/pliptor/name-only-study-with-interactive-3d-plot\\n[3]:https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u00228a4bf521d7258eb7d6b2f907b2c92157e9a45a71\u0022},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022train \u003c- read.csv(\\\u0022../input/train.csv\\\u0022,stringsAsFactors=F)\\ntest \u003c- read.csv(\\\u0022../input/test.csv\\\u0022,stringsAsFactors=F)\\ntest$Survived \u003c- NA\\ndata \u003c- rbind(train,test)\\n\\n# create dictionary from training names\\nwords = paste(data$Name[1:891],collapse=\u0027 \u0027)\\nwords = gsub(\u0027[.,\\\u0022()/]\u0027,\u0027\u0027,words)\\nwords = gsub(\u0027 . |  \u0027,\u0027 \u0027,words)\\nwords = strsplit(words,\u0027 \u0027)[[1]]\\nfreq = ave(1:length(words),words,FUN=length)\\ndictionary = data.frame(Words=words,Freq=freq,stringsAsFactors=F)\\ndictionary = dictionary[!duplicated(dictionary$Words) \u0026 dictionary$Freq\u003e1,]\\ndictionary \u003c- dictionary[order(-dictionary$Freq),]\\n\\n# create vectorization from dictionary\\ndata2 \u003c- data.frame(Mr=rep(0,1309))\\nfor (i in dictionary$Words) data2[,i] \u003c- 0\\nfor (i in 1:nrow(data)){\\n    n = gsub(\u0027[.,\\\u0022()/]\u0027,\u0027\u0027,data$Name[i])\\n    n = gsub(\u0027 . |  \u0027,\u0027 \u0027,n)\\n    n = strsplit(n,\u0027 \u0027)[[1]]\\n    for (j in n){\\n        if (j %in% dictionary$Words){\\n            data2[i,j] = 1\\n        }\\n    }\\n}\u0022,\u0022execution_count\u0022:2,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u002256742dc2c20041b78d7609f0743b310c981a21b9\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The dictionary Words are now in a data frame named _dictionary_. It contains all words found in the training dataset names 2 or more times. And we one-hot-encoded all the passengers using these 415 words. This vectorization of passengers is stored in the variable named _data2_. It has dimension 1309 rows and 415 columns. (Note the variable _data_, contains the usual Titanic spreadsheet of all training and test passengers.)\\n## Calculate the probability of survival given word\\nNext let\u0027s calculate what the probability of survival is given that a passenger\u0027s Name contains the specific word. We won\u0027t need this to implement PCA with kNN, but we\u0027ll use this when we implement a variant of naive Bayes below.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u002227966b342d98c383352c385879406ad030015435\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# calculate survival probability\\ns=1:891\\ndictionary$Survival \u003c- NA\\nfor (i in 1:length(dictionary$Words)){\\n    x = intersect(which(data2[,dictionary$Words[i]]\u003e0),s)\\n    dictionary$Survival[i] \u003c- mean(data$Survive[x],na.rm=T)\\n}\\n# display words and survival\\nrow.names(dictionary) \u003c- 1:nrow(dictionary)\\ncat(\\\u002215 most frequent Words and their survival rate:\\\\n\\\u0022)\\nhead(data.frame(n=1:nrow(dictionary),dictionary[order(-dictionary$Freq),]),15)\\ncat(sprintf(\u0027%d Words in dictionary\\\\n\u0027,nrow(dictionary)))\\ndictionary$Words[order(dictionary$Words)]\u0022,\u0022execution_count\u0022:7,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022fa3bfdbd5c5be35e78ff6b4c568bd8b32d7e9324\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022# Principal Component Analysis\\n## Calculate the components\\nNow let\u0027s use PCA plus kNN to classify passengers\u0027 survival. First, let\u0027s calcuate the principal components. In the process, let\u0027s determine which words are correlated with which other words. Our variable _data2_ is \\\\\\\\(X\\\\\\\\). It contains the 1309 passengers each as 1 row. The 413 columns indicate whether Word \\\\\\\\(k\\\\\\\\) is present in that passenger\u0027s Name. You can think of each passenger as a vector of zeros and ones of length 413. \\n\\n$$ \\\\Sigma= \\\\text{cov}(X)= \\\\frac{1}{n} X^TX + m^Tm  \\\\quad\\\\text{ where } m \\\\in R^{1 \\\\times d} \\\\text{ are } X \\\\text{ column means} \\\\quad \\\\text{and }X\\\\in R^{n\\\\times d}$$\\n$$ \\\\text{Principal components}=P \\\\in R^{d \\\\times d} \\\\quad \\\\text{where } \\\\Sigma = PDP^T \\\\quad \\\\text{with } P \\\\text{ orthogonal} \\\\quad \\\\text{and } D \\\\text{ diagonal}$$\\n$$ \\\\text{corr}(x_i,x_j) = \\\\frac {\\\\Sigma_{i,j}}{\\\\sigma_i \\\\sigma_j} \\\\quad \\\\text{where } \\\\sigma_i = \\\\text{standard deviation of }x_i$$\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022868a0a276e64fe52279487f0cf76d71a0cfefca2\u0022,\u0022scrolled\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022data3 \u003c- as.matrix(data2)\\n# calculate mean and sd\\nm = rep(0,ncol(data3))\\ns = rep(0,ncol(data3))\\nfor (i in 1:ncol(data3)){\\n    m[i] = mean(data3[,i])\\n    s[i] = sd(data3[,i])\\n}\\n# calculate covariance\\ndata4 = (1/1309)*t(data3) %*% data3 - m %*% t(m)\\n# calculate correlation\\ndata5 = data4 / (s %*% t(s))\\n# calculate principal components\\nev \u003c- eigen(data4,symmetric=T)\\nev2 \u003c- data.frame(values=ev[[1]],vectors=ev[[2]])\\ncat(\u0027Principal components and correlations have been calculated.\\\\n\\\\n\u0027)\\n# find word pairs with frequency\u003e=4 and high correlation\\ncat(\\\u0022Correlations above 0.3 of Words with freq\u003e=4 are:\\\\n\\\u0022)\\nfor (i in 1:length(which(dictionary$Freq\u003e=4)))\\nfor (j in (i+1):length(which(dictionary$Freq\u003e=4))){\\n    if (i!=j \u0026 abs(data5[i,j])\u003e=0.3)\\n        cat(sprintf(\\\u0022%s and %s have correlation r = %f\\\\n\\\u0022,row.names(data5)[i],row.names(data5)[j],data5[i,j]))\\t\\n}\\n#cat(\\\u0022\\\\nCorrelations above 0.5 of Words with freq\u003c=3\\\\n\\\u0022)\\n#for (i in length(which(dictionary$Freq\u003e=4)):nrow(data5)-1)\\n#for (j in (i+1):nrow(data5)-1){\\n#    if (i!=j \u0026 abs(data5[i,j])\u003e=0.5)\\n#        cat(sprintf(\\\u0022%s and %s have correlation r = %f\\\\n\\\u0022,row.names(data5)[i],row.names(data5)[j],data5[i,j]))\\n#}\u0022,\u0022execution_count\u0022:13,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u00226770c55c0fe11d33728f67ebb59d1b56b791353a\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Wow, among the 115 Words whose frequency is 4 or larger, there isn\u0027t much correlation. This surprises me. I know that we\u0027re approaching this problem without semantic information. (We\u0027re pretending that we don\u0027t know what titles, first names, and last names are.) But let me add a comment here. I expected male first names to correlate with male titles and I expected PCA to cluster all these names and titles into only a few principal components. This didn\u0027t happen. After thinking about, it makes mathematical sense. Yes, male names are associated with male titles but the relationship isn\u0027t the mathematical one defined as correlation. Here is a plot of the relationship between Charles and Mr that shows why.\u0022},{\u0022metadata\u0022:{\u0022_kg_hide-output\u0022:true,\u0022trusted\u0022:true,\u0022scrolled\u0022:true,\u0022_uuid\u0022:\u00229ffa2ae6cbdf0895debdc9210fa1b077e95f78c7\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022library(ggplot2)\\nlibrary(gridExtra)\\n\\n#ggplot(data=data2, aes(x=data2$Charles,y=data2$Mr)) + \\n#    geom_jitter(width=0.1,height=0.1) +\\n#    geom_smooth(method=\u0027lm\u0027) +\\n#    labs(x=\u0027Charles\u0027,y=\u0027Mr\u0027,title=\u0027Corr(Charles,Mr) = 0.0828.\\\\nLinear regression R^2 = 0.00686\u0027)\u0022,\u0022execution_count\u0022:12,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u00225490b451eb7d4436efcb6daa692b771a2bbb4c1c\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022![Charles](http://playagricola.com/Kaggle/Charles.png)\\n  \\nIt is true that when Charles is present, Mr is more likely to be present but the reason that Charles and Mr don\u0027t correlate is because when Charles is absent, it isn\u0027t the case that Mr is more likely to be absent.\\n\\n## Transform passengers into principal component weights\\nLet\u0027s convert all 1309 passengers into vectors of principal component weights.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022c1107f3c430e9f067ff2db8d0f4f5d4110590be2\u0022,\u0022_kg_hide-input\u0022:false},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022data3Transformed = (t(ev[[2]]))[,1:nrow(ev[[2]])] %*% t(data3)\\ndataPC6 = data.frame(n=1:1309,Survived=data$Survived,t(data3Transformed)[,1:6])\\ncolnames(dataPC6) \u003c- c(\u0027PassengerId\u0027,\u0027Survived\u0027,paste(\u0027PC\u0027,1:6,\u0027 wgt\u0027,sep=\u0027\u0027))\\nrownames(dataPC6) \u003c- 1:1309\\nhead(data[,c(\u0027PassengerId\u0027,\u0027Name\u0027,\u0027Sex\u0027,\u0027Age\u0027)])\\ncat(\u0027First 6 principal component weights for the first 6 passengers:\\\\n\u0027)\\nhead(dataPC6[,-2])\u0022,\u0022execution_count\u0022:9,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022b38eb0007795918d26488e32b7a268a9d8289bfb\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Examining the first 6 principal components (which are not displayed above) we learn they reference the following words:  \\n* PC1  is 82% Mr,   -48% Miss,   -28 %Mrs,   0% other words  \\n* PC2 is -64% Miss,   72.9% Mrs,   0% other words  \\n* PC3 is 68% William,   56% Master,   0% other words  \\n* PC4 is 67% William,   -59% Master,   0% other words  \\n* PC5 is 96% John,   0% other words   \\n* PC6 is 90% Henry,   0% other words    \\n  \\nTherefore when Mr. Owen Braund, has principal components weights (0.8, -0.1, -0.2, 0.2, 0.0, 0.2) that means he has 0.8 of Mr and nothing positive for Mrs, Miss, William, John, or Master. The second passenger, Mrs. John Cumings has weights (-0.25, 0.8, -0.15, 0.3, 1.1, -0.1). This says that she is mainly John and Mrs. The third passenger, Miss Laina Heikkinen has weights (-0.5, -0.6, -0.1, 0.3, 0.1, 0.0). When multiplied by the principal components, she becomes mostly Miss. The other 3 make sense too.\\n\\n## Model and cross validate\\nWe will use the passengers\u0027 principal components to classify unknown passengers using kNN. First let\u0027s cross validate and perform a grid search to find the optimal dimension _d_ of principal components and optimal _k_ for kNN\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022b6becb679ed0a20c71f13960eccc53b99767ac0d\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022library(caret)\\naccuracy \u003c- matrix(nrow=5,ncol=5)\\nrownames(accuracy) \u003c- paste(\u0027d=\u0027,3:7,sep=\u0027\u0027)\\ncolnames(accuracy) \u003c- paste(\u0027k=\u0027,c(7,9,11,13,15),sep=\u0027\u0027)\\nfor (d in 1:5){\\n    xt = (t(ev[[2]]))[1:(d+2),1:nrow(ev[[2]])] %*% t(data3)\\n    dataPC = data.frame(Survived=data$Survived,t(xt))\\n    for (k in 1:5){\\n        trials = 100\\n        total = 0\\n        for (i in 1:trials){\\n            s = sample(1:891,802)\\n            s2 = (1:891)[-s]\\n            model \u003c- knn3(factor(Survived) ~ .,dataPC[s,],k=2*k+5)\\n            p \u003c- predict(model,newdata=dataPC[s2,])\\n            p \u003c- ifelse(p[,2]\u003e=0.5,1,0)\\n            # calculate one minus misclassification rate\\n            x = 1-sum(abs(dataPC$Survived[s2]-p))/length(s2)\\n            #if (i%%10==0) cat(sprintf(\\\u0022Trial %d has CV accuracy %f\\\\n\\\u0022,i,x))\\n            total = total + x\\n    }\\n        #cat(sprintf(\\\u0022For d=%d, k=%d, average CV accuracy of %d trials is %f\\\\n\\\u0022\\n        #    ,d+2,2*k+5,trials,total/trials))\\n    accuracy[d,k] \u003c- total/trials\\n    }\\n}\\ncat(\u0027Cross validation accuracy using 10-fold CV:\\\\n\u0027)\\naccuracy\u0022,\u0022execution_count\u0022:10,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u00227f6c43f8d455d8dbb88beca38637ea870e5003be\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022From our search, we see that d=5 and k=11 are the best combination achieving CV = 80.5% which is better than the gender model\u0027s cross validation of 78.6%. (Results may vary due to the random natural of cross validation k-folds.) So, PCA is detecting more than just gender, cool. Let\u0027s use those hyperparameters to make a submission to Kaggle\\n## Submission to Kaggle\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022e2ca2255816ef7d3c2c1e5481e44f91b537ccda1\u0022,\u0022_kg_hide-input\u0022:false},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022d=5; k=11;\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% t(data3)\\ndataPC = data.frame(Survived=data$Survived,t(xt))\\nmodel \u003c- knn3(factor(Survived) ~ .,dataPC[1:891,],k=k)\\np \u003c- predict(model,newdata=dataPC[892:1309,])\\np \u003c- ifelse(p[,2]\u003e=0.5,1,0)\\nsubmit = data.frame(PassengerId=892:1309,Survived=p)\\nwrite.csv(submit,\u0027PCA5kNN11.csv\u0027,row.names=F)\u0022,\u0022execution_count\u0022:19,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022caefc934de5217ffecb3bb4ecfdef822bab28135\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022![result](http://playagricola.com/Kaggle/resultPCA5kNN9.png)\\nThis isn\u0027t a high score but we did achieve the accuracy of the gender model by implemented an interesting and complex model. Let\u0027s explore our PCA a bit and then implement a variant of naive Bayes.\u0022},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022406a667e01b47bda85f6390dab8003e27a8199d0\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## PCA Exploration\\nThe overall lack of correlation among Words with frequency 4 or larger means that our principal components weren\u0027t able to compress these 115 Words into a dimension much less than 115. Let\u0027s plot the first 12 principal components. And then let\u0027s try to compress the first 115 Words. It appears the next 298 = 413 - 115 Words have some correlation and will be able to be compressed. However that won\u0027t help much because we\u0027d prefer to use the first 115 frequent Words and discard the next 298 anyway. We expect each principal component to represent more or less a single word. So the principal components should be mostly flat with a single spike.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u00220b474af2da930dcf98be97f13f672544f845ce2a\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022pimage \u003c- function(x){\\n    return(ggplot(data=ev2) + \\n           geom_line(aes(x=1:length(ev2[[x+1]]),y=ev2[[x+1]])) + \\n           labs(x=\u0027\u0027,y=\u0027\u0027,title=paste(\u0027PC\u0027,x,sep=\u0027\u0027)))\\n}\\nx=0\\ngrid.arrange(pimage(x+1),pimage(x+2),pimage(x+3),pimage(x+4),pimage(x+5),pimage(x+6),\\n    pimage(x+7),pimage(x+8),pimage(x+9),pimage(x+10),pimage(x+11),pimage(x+12),as.table=F)\u0022,\u0022execution_count\u0022:13,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022884877feef0c3bee5aeec01b09718ad94670eb14\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022As expected, these principal components only involve a single word more or less because the most frequent Words aren\u0027t correlated with other Words. The less frequent Words did correlate with other Words, so as we plot more principal components, we expect them to involve multiple words. For example, here are principal components 100 thru 112.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u002263c00e3e3c0de72e95235ec9f859532d43bcf77a\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022x=99\\ngrid.arrange(pimage(x+1),pimage(x+2),pimage(x+3),pimage(x+4),pimage(x+5),pimage(x+6),\\n    pimage(x+7),pimage(x+8),pimage(x+9),pimage(x+10),pimage(x+11),pimage(x+12),as.table=F)\u0022,\u0022execution_count\u0022:40,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022cf294924c056b6a5689522082485110b26f3b38b\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We can test PCA\u0027s ability to compress Words by one-hot-encoding our entire dictionary and transforming it into PC space only using k principal components. Then we can transform it back into regular Word space and see how many Words we get back. It would be fantastic if we could transform 413 Words into PC space using 20 principal components and then transform the result back and still have something like 100 Words. That would be a 5 to 1 compression.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022679522d076a164c5ee3a1ba20c5813c2d627115d\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022prep \u003c- function(data,s){\\n    x=c(); y=c(); z=c()\\n    for (i in 1:nrow(data))\\n    for (j in 1:ncol(data)){\\n        x=c(x,s*j)\\n        y=c(y,s*i)\\n        z=c(z,data[i,j])\\n    }\\n    return (data.frame(x=x,y=y,z=z))\\n}\\n\\nx=diag(nrow(data5))\\ng1 = ggplot(prep( x[4*1:100,4*1:100] ,4),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(400,0) +\\n labs(title=\u0027Words in dictionary\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd = 5\\nx=diag(nrow(data5))\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng2 = ggplot(prep( x2[1:10,1:10] ,1),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(10,0) +\\n labs(title=\u0027Words recreated from 5 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=10\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng3 = ggplot(prep( x2[1:20,1:20] ,1),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(20,0) +\\n labs(title=\u0027Words recreated from 10 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=25\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng4 = ggplot(prep( x2[1:50,1:50] ,1),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(50,0) +\\n labs(title=\u0027Words recreated from 25 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\ngrid.arrange(g1,g2,g3,g4,nrow=2,ncol=2)\u0022,\u0022execution_count\u0022:41,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022197ecde74f124ada9ac53cf885468bd5bdf3f64f\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022d=50\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng5 = ggplot(prep( x2[1:100,1:100] ,1),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(100,0) +\\n labs(title=\u0027Words recreated from 50 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=100\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng6 = ggplot(prep( x2[2*1:100,2*1:100] ,2),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(200,0) +\\n labs(title=\u0027Words recreated from 100 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=200\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng7 = ggplot(prep( x2[4*1:100,4*1:100] ,4),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(400,0) +\\n labs(title=\u0027Words recreated from 200 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=300\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng8 = ggplot(prep( x2[4*1:100,4*1:100] ,4),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(400,0) +\\n labs(title=\u0027Words recreated from 300 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\ngrid.arrange(g5,g6,g7,g8,nrow=2,ncol=2)\u0022,\u0022execution_count\u0022:42,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u002220b9f855ff159ffcde281ebe7b258de02f37816b\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022As expected, we don\u0027t see much compression. When we use 5 principal components, we can only recreate 6 Words from the dictionary. Then 10 PC recreates 11 Words. Then 25 recreates 25. At 50, we begin to see some compression power. Using 50 PC, we can represent more than 50 Words. Next 100, 200, and 300 can represent more Words than 100, 200, 300 too. It appears that 200 and 300 PC can basically represent the entire dictionary of 413 Words.\\n\\n# Naive Bayes variant (Oscar\u0027s SLogL)\\n\\nSince we only really want to use Words with freq\u003e=4 which are the first 115 Words and PCA isn\u0027t doing a great job compressing those 115, let\u0027s leave the Words in their original space. Eventually we can perform a dimension reduction by discarding less frequent Words thus mimicking PCA. Since the Words are so uncorrelated, the Words themselves when ordered by frequency are basically equivilent to principal components. Let\u0027s plot each Word\u0027s predictive power.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u00220b8cf76a7de59077388197f46722e186161b98fe\u0022,\u0022scrolled\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# returns mode of a set of numbers\\ngetmode \u003c- function(v) {\\n   uniqv \u003c- unique(v)\\n   uniqv[which.max(tabulate(match(v, uniqv)))]\\n}\\n\\n# calculate average (mode) position of each word\\nPosition \u003c- vector(\\\u0022list\\\u0022,length=nrow(dictionary))\\nfor (i in 1:nrow(data)){\\n    n = gsub(\u0027[.,\\\u0022()/]\u0027,\u0027\u0027,data$Name[i])\\n    n = gsub(\u0027 . |  \u0027,\u0027 \u0027,n)\\n    n = strsplit(n,\u0027 \u0027)[[1]]\\n    for (j in 1:length(n)){\\n        k = which(dictionary$Words==n[j])[1]\\n        if (!is.na(k)) Position[[k]] = c(Position[[k]],j)\\n    }\\n}\\ndictionary$Position \u003c- NULL\\nfor (i in 1:nrow(dictionary)) dictionary$Position[i] \u003c- getmode(Position[[i]])\\ndictionary$Position \u003c- pmin(4,dictionary$Position)\\n\\nggplot(data=dictionary) +\\ngeom_jitter(height=0.005,aes(x=1:nrow(dictionary),y=dictionary$Survival,color=factor(dictionary$Position))) +\\nscale_colour_manual(values = c(\\\u0022red\\\u0022, \\\u0022black\\\u0022, \\\u0022blue\\\u0022,\\\u0022white\\\u0022),labels=c(\u00271\u0027,\u00272\u0027,\u00273\u0027,\u00274+\u0027)) +\\ngeom_hline(yintercept=0.384, linetype=\u0027dotted\u0027) +\\ngeom_vline(xintercept=45) +\\ngeom_vline(xintercept=57) +\\ngeom_vline(xintercept=80) +\\ngeom_vline(xintercept=115) +\\ngeom_vline(xintercept=187) +\\ngeom_vline(xintercept=6) +\\nannotate(\u0027text\u0027,x=-15,y=0.87,label=\u0027n\u003c=6\\\\nfreq\\\\n\u003e=35\u0027) +\\nannotate(\u0027text\u0027,x=98,y=0.87,label=\u0027freq\\\\n\u003e=4\u0027,color=\u0027gray70\u0027) +\\n#annotate(\u0027text\u0027,x=68,y=0.87,label=\u0027freq\\\\n\u003e=5\u0027,color=\u0027gray70\u0027) +\\nannotate(\u0027text\u0027,x=25,y=0.87,label=\u0027freq\\\\n\u003e=7\u0027,color=\u0027gray70\u0027) +\\nannotate(\u0027text\u0027,x=143,y=0.87,label=\u0027n\u003e=116\\\\nfreq\u003c=3\u0027) +\\nannotate(\u0027text\u0027,x=215,y=0.87,label=\u0027n\u003e=188\\\\nfreq\u003c=2\u0027) +\\nannotate(\u0027text\u0027,x=305,y=0.36,label=\u0027p = 0.384 = probability of survival\u0027) +\\nlabs(x=\u0027Words order by frequency\u0027) +\\nlabs(title=\u0027Each dot represents a Word in the dictionary.\u0027) +\\nlabs(y=\u0027Probability Survival given Word\u0027,color=\u0027word\\\\\u0027s\\\\nposition\\\\nin name\u0027)\u0022,\u0022execution_count\u0022:43,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u002201a1ec4e8edd68aa1e64fef489fbeb188a067425\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022This plot is very interesting. The Words whose frequencies are 35 or higher are mostly black dots. The 6 most frequent words are contained in the first 5 principal components we saw above. These Words are Mr (518 occurences), Miss (182), Mrs (129), William (63), John (44), and Master (40). Our key shows us that black dots are Words that are found in the second postion in a Name. (We\u0027re pretending this is a foreign language, so I can\u0027t say what these are, but we know these to be titles.) Next the Words with frequency between 7 and 34 are blue dots. These are Words found in position 3 (first names). Next we have a mixture of colors which include the red dots (last names).\\n\\n## Model and cross validate\\n\\nLet\u0027s use a variant of naive Bayes presented by [Oscar Takeshita][2] called [Divide and Conquer][1] to predict passenger survival.\\n\\n$$P(y|w_1,w_2,...,w_n) = \\\\text{logit}^{-1}\\\\Bigg(  \\\\sum_{k=1}^n{  \\\\log \\\\bigg( \\\\frac{ P(y|w_k)}{ 1-P(y|w_k) } \\\\bigg)} +  \\\\sum_{k=1}^{n-1}{  \\\\log \\\\bigg( \\\\frac{ 1-P(y)}{ P(y) } \\\\bigg)}  \\\\Bigg)$$\\n\\n$$ \\\\text{where} \\\\quad \\\\text{logit}^{-1}(z) = \\\\frac{1}{1 + e^{-z}} \\\\quad \\\\text{and} \\\\quad P(y|w_k) = \\\\text{probability of survival given word } k $$\\n[1]: https://www.kaggle.com/pliptor/divide-and-conquer-0-82296\\n[2]:https://www.kaggle.com/pliptor\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022scrolled\u0022:true,\u0022_uuid\u0022:\u00225d8e8c0361dcb981fa011bff0949f385b943e2ce\u0022},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# log odds\\nlogl \u003c- function(p){\\n    e = 0.001\\n    q = 1 - p\\n    return(log(max(p,e))-log(max(q,e)))\\n}\\n# inverse logit\\nilogit \u003c- function(z){\\n    return (1/(1+exp(-z)))\\n}\\n# cross validation trials\\ntrials = 100\\ntotal = 0\\nfor (k in 1:trials){\\nif (k%%25==0) cat(sprintf(\\\u0022Begin trial %d\\\\n completed\\\u0022,k))\\ns = sample(1:891,802)\\ns2 = (1:891)[-s]\\n\\n# calculate the Tfreq and Survival within our training subset\\ndictionary$Survival \u003c- NA\\ndictionary$Tfreq \u003c- 0\\nfor (i in 1:length(dictionary$Words)){\\n    x = intersect(which(data2[,dictionary$Words[i]]\u003e0),s)\\n    dictionary$Survival[i] \u003c- mean(data$Survive[x],na.rm=T)\\n    dictionary$Tfreq[i] \u003c- length(x)\\n}\\ndictionary$Survival[is.na(dictionary$Survival)] \u003c- NA\\ndictionary$Tfreq[is.na(dictionary$Tfreq)] \u003c- NA\\ndictionary$Logl \u003c- sapply(dictionary$Survival,FUN=logl)\\n\\n# calculate bias term for Oscar\u0027s naive Bayes\\nps = sum(data$Survived[s])/length(s)\\nbias = logl(1-ps)\\n\\n# the following line mimics PCA dimension reduction\\ndictionary2 \u003c- dictionary[dictionary$Tfreq\u003e=4,]\\np = rep(0,891-length(s))\\nfor (j in 1:length(s2)){\\n    c = 0\\n    slogl = 0\\n    # perform Oscar\u0027s naive Bayes\\n    for (i in 1:nrow(dictionary2)){\\n        if (data2[s2[j],dictionary2$Words[i]]\u003e0 \u0026 !is.na(dictionary2$Survival[i])){\\n            slogl = slogl + dictionary2$Logl[i]\\n            if (c\u003e0) slogl = slogl + bias\\n            c = 1\\n        }\\n    }\\n    if (c!=0) p[j] = ilogit(slogl)\\n    else p[j] = ps\\n    if (k%%25==0 \u0026 j%%10==0) cat(sprintf(\\\u0022 j=%d \\\u0022,j))\\n}\\np \u003c- ifelse(p\u003e=0.5,1,0)\\n# calculate one minus misclassification rate\\nx = 1-sum(abs(data$Survived[s2]-p))/length(s2)\\nif (k%%25==0) cat(sprintf(\\\u0022\\\\n Trial %d has CV accuracy %f\\\\n\\\u0022,k,x))\\ntotal = total + x\\n}\\n\\ncat(sprintf(\\\u0022Average CV accuracy of %d trials is %f\\\\n\\\u0022,trials,total/trials))\\n\u0022,\u0022execution_count\u0022:45,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u002210688262e3e5beacea51061ec3fc1c73f9240fce\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Cross validation shows that when our model uses the first 115 dictionary Words which are all the Words with frequency 4 or more, then we can expect prediction accuracy of 80%! That\u0027s better than the gender model that cross validates at 78.6%, so we\u0027re extracting gender and something more from these words.\\n\\nI also ran cross validation using the first 6, 23, 42, 57, 80, 115, 187, 413 Words, (frequency \u003e= 35, 12, 7, 6, 5, 4, 3, 2), which achieved results of 79%, 78%, 78%, 79%, 79%, 80%, 80%, 80% respectively. What\u0027s interesting is when we transistion from using the first 6 \\\u0022principal components\\\u0022 to using the next 34, our CV drops. Then our CV improves when we add more \\\u0022principal components\\\u0022. This experimental result can be explained by the plot of Words above. The first 6 \\\u0022principal components\\\u0022 are black dots (titles), the next 38 are blue dots (first names), and after that are red dots (last names). If we use every Word (approximately 600 Words) including those with frequency equal 1 (that were removed from our dictionary), then the CV drops to 76%.\\n## Submission to Kaggle\\n![resultNB](http://playagricola.com/Kaggle/resultNaiveBayes.png)\\nAgain, not a high score. But it did achieve the gender model accuracy which is nice. Also, this kernel serves as a template to build upon in the future.\\n# Conclusion\\nCross validation indicates that the black dots and red dots contain the most predictive power. And the blue dots hurt our CV. I checked and our CV increases to 80.5% by removing the blue and white dots. But we still aren\u0027t close to the Name only kernel posted [here][1] which has an 84% CV. Therefore there is still room to improve our model. In the future, I plan to use all the insights gained from the above investigations to improve PCA and naive Bayes operating on the words within the names.\\n\\nThank you everyone for reading my notebook. I look forward to your comments.\\n\\n# References\\n\\n* [Oscar Takeshita][2]: Postulates this interesting problem and provides a solution using PCA and kNN\\n* [Chris Deotte][3]: Provides an example of PCA\\n* [Chris Deotte][4]: Provides an example of SVD compression\\n* [Cathy O\u0027Neil \u0026 Rachel Schutt][5]: Doing Data Science explains naive Bayes\\n* [Oscar Takeshita][6]: Presents a variant of naive Bayes called SLogL in his kernel Divide and Conquer\\n\\n[1]: https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818\\n[2]: https://www.kaggle.com/pliptor/name-only-study-with-interactive-3d-plot\\n[3]:http://www.ccom.ucsd.edu/~cdeotte/papers/PrincipleComponents.pdf\\n[4]:http://www.ccom.ucsd.edu/~cdeotte/papers/SVD.pdf\\n[5]: https://www.amazon.com/Doing-Data-Science-Straight-Frontline/dp/1449358659\\n[6]:https://www.kaggle.com/pliptor/divide-and-conquer-0-82296\\n\u0022}],\u0022metadata\u0022:{\u0022kernelspec\u0022:{\u0022display_name\u0022:\u0022R\u0022,\u0022language\u0022:\u0022R\u0022,\u0022name\u0022:\u0022ir\u0022},\u0022language_info\u0022:{\u0022mimetype\u0022:\u0022text/x-r-source\u0022,\u0022name\u0022:\u0022R\u0022,\u0022pygments_lexer\u0022:\u0022r\u0022,\u0022version\u0022:\u00223.4.2\u0022,\u0022file_extension\u0022:\u0022.r\u0022,\u0022codemirror_mode\u0022:\u0022r\u0022}},\u0022nbformat\u0022:4,\u0022nbformat_minor\u0022:1}","dateCreated":"2018-06-08T21:36:01.1859654Z"},"kernelRun":{"id":3955427,"kernelId":1029920,"status":"complete","type":"batch","sourceType":"notebook","language":"r","title":"Titanic \u0022Spam\u0022 Filter","dateCreated":"2018-06-08T21:36:01.537Z","dateEvaluated":"2018-06-08T21:36:02.83Z","workerContainerPort":null,"workerUptimeSeconds":96808,"workerIPAddress":"172.16.0.4     ","scriptLanguageId":12,"scriptLanguageName":"R Notebook HTML","renderedOutputUrl":"https://www.kaggleusercontent.com/kf/3955427/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..agI8yTXlmDgKC-igBAUH6A.xX22AcE2H28zGHB86PfnXIRhQT6RML1u0pZelUdj5YvHaSDPcYKE0GbG5hKf9BJ3-vaEF0ZPofMZg_SkBF5sxpsinxrlmUIaOYXL62Mp0JjB2NZVZ75RTV7WeKPW7ctY3MnyQ6bkuNOi1DdS0jGqQw.CjLXqdUm0pxP66JAwof8Rg/__results__.html","commit":{"id":8742620,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"Competition","sourceId":3136,"databundleVersionId":null}],"sourceType":"notebook","language":"r","isGpuEnabled":false,"isInternetEnabled":false},"source":"{\u0022cells\u0022:[{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022c9807f6c7ae92f549a00bf07368b16f4470c2a6c\u0022,\u0022_execution_state\u0022:\u0022idle\u0022,\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022# Titanic Name \\\u0022Spam\\\u0022 Filter:\\n\\n![spam](http://playagricola.com/Kaggle/spam2.jpg)\\n\\n# Introduction\\n\\nThank you [Oscar Takeshita][1] for postulating an interesting question [here][2]. Oscar asks, how well can we predict Titanic passengers\u0027 survival\\nusing only the words in the Name column and ignoring semantic information (i.e. don\u0027t acknowledge and prioritize title, last name, etc). What a fun idea. We must detect which emails (passengers) are spam (stay alive) by analyzing words.\\n\\nThis problem particularly interests me because I previously made a kernel that only uses the Name column [here][3] that scored an impressive 82% leaderboard score and cross validated at 84%. Therefore we should be able to build a great model using the Words within the Names. It would be great if a purely machine learning algorithm can find unsupervised the patterns that I did by reasoning. Furthermore Oscar\u0027s formulation provides an opportunity to practice techniques that didn\u0027t come up when tackling Titanic in the usual way.  We must work in high dimensional space and use techniques involved in spam classification and dimension reduction. \\n\\nIn this notebook, we\u0027ll tackle this problem first using PCA plus kNN, then second we\u0027ll employ a variant of naive Bayes.\\n\\n# Build dictionary and vectorize passengers\\nLet\u0027s begin. Our first task is to build a dictionary of all the words used in the Name column.\\n\\n[1]:https://www.kaggle.com/pliptor\\n[2]:https://www.kaggle.com/pliptor/name-only-study-with-interactive-3d-plot\\n[3]:https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u00228a4bf521d7258eb7d6b2f907b2c92157e9a45a71\u0022},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022train \u003c- read.csv(\\\u0022../input/train.csv\\\u0022,stringsAsFactors=F)\\ntest \u003c- read.csv(\\\u0022../input/test.csv\\\u0022,stringsAsFactors=F)\\ntest$Survived \u003c- NA\\ndata \u003c- rbind(train,test)\\n\\n# create dictionary from training names\\nwords = paste(data$Name[1:891],collapse=\u0027 \u0027)\\nwords = gsub(\u0027[.,\\\u0022()/]\u0027,\u0027\u0027,words)\\nwords = gsub(\u0027 . |  \u0027,\u0027 \u0027,words)\\nwords = strsplit(words,\u0027 \u0027)[[1]]\\nfreq = ave(1:length(words),words,FUN=length)\\ndictionary = data.frame(Words=words,Freq=freq,stringsAsFactors=F)\\ndictionary = dictionary[!duplicated(dictionary$Words) \u0026 dictionary$Freq\u003e1,]\\ndictionary \u003c- dictionary[order(-dictionary$Freq),]\\n\\n# create vectorization from dictionary\\ndata2 \u003c- data.frame(Mr=rep(0,1309))\\nfor (i in dictionary$Words) data2[,i] \u003c- 0\\nfor (i in 1:nrow(data)){\\n    n = gsub(\u0027[.,\\\u0022()/]\u0027,\u0027\u0027,data$Name[i])\\n    n = gsub(\u0027 . |  \u0027,\u0027 \u0027,n)\\n    n = strsplit(n,\u0027 \u0027)[[1]]\\n    for (j in n){\\n        if (j %in% dictionary$Words){\\n            data2[i,j] = 1\\n        }\\n    }\\n}\u0022,\u0022execution_count\u0022:2,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u002256742dc2c20041b78d7609f0743b310c981a21b9\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The dictionary Words are now in a data frame named _dictionary_. It contains all words found in the training dataset names 2 or more times. And we one-hot-encoded all the passengers using these 415 words. This vectorization of passengers is stored in the variable named _data2_. It has dimension 1309 rows and 415 columns. (Note the variable _data_, contains the usual Titanic spreadsheet of all training and test passengers.)\\n## Calculate the probability of survival given word\\nNext let\u0027s calculate what the probability of survival is given that a passenger\u0027s Name contains the specific word. We won\u0027t need this to implement PCA with kNN, but we\u0027ll use this when we implement a variant of naive Bayes below.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u002227966b342d98c383352c385879406ad030015435\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# calculate survival probability\\ns=1:891\\ndictionary$Survival \u003c- NA\\nfor (i in 1:length(dictionary$Words)){\\n    x = intersect(which(data2[,dictionary$Words[i]]\u003e0),s)\\n    dictionary$Survival[i] \u003c- mean(data$Survive[x],na.rm=T)\\n}\\n# display words and survival\\nrow.names(dictionary) \u003c- 1:nrow(dictionary)\\ncat(\\\u002215 most frequent Words and their survival rate:\\\\n\\\u0022)\\nhead(data.frame(n=1:nrow(dictionary),dictionary[order(-dictionary$Freq),]),15)\\ncat(sprintf(\u0027%d Words in dictionary\\\\n\u0027,nrow(dictionary)))\\ndictionary$Words[order(dictionary$Words)]\u0022,\u0022execution_count\u0022:7,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022fa3bfdbd5c5be35e78ff6b4c568bd8b32d7e9324\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022# Principal Component Analysis\\n## Calculate the components\\nNow let\u0027s use PCA plus kNN to classify passengers\u0027 survival. First, let\u0027s calcuate the principal components. In the process, let\u0027s determine which words are correlated with which other words. Our variable _data2_ is \\\\\\\\(X\\\\\\\\). It contains the 1309 passengers each as 1 row. The 413 columns indicate whether Word \\\\\\\\(k\\\\\\\\) is present in that passenger\u0027s Name. You can think of each passenger as a vector of zeros and ones of length 413. \\n\\n$$ \\\\Sigma= \\\\text{cov}(X)= \\\\frac{1}{n} X^TX + m^Tm  \\\\quad\\\\text{ where } m \\\\in R^{1 \\\\times d} \\\\text{ are } X \\\\text{ column means} \\\\quad \\\\text{and }X\\\\in R^{n\\\\times d}$$\\n$$ \\\\text{Principal components}=P \\\\in R^{d \\\\times d} \\\\quad \\\\text{where } \\\\Sigma = PDP^T \\\\quad \\\\text{with } P \\\\text{ orthogonal} \\\\quad \\\\text{and } D \\\\text{ diagonal}$$\\n$$ \\\\text{corr}(x_i,x_j) = \\\\frac {\\\\Sigma_{i,j}}{\\\\sigma_i \\\\sigma_j} \\\\quad \\\\text{where } \\\\sigma_i = \\\\text{standard deviation of }x_i$$\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022868a0a276e64fe52279487f0cf76d71a0cfefca2\u0022,\u0022scrolled\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022data3 \u003c- as.matrix(data2)\\n# calculate mean and sd\\nm = rep(0,ncol(data3))\\ns = rep(0,ncol(data3))\\nfor (i in 1:ncol(data3)){\\n    m[i] = mean(data3[,i])\\n    s[i] = sd(data3[,i])\\n}\\n# calculate covariance\\ndata4 = (1/1309)*t(data3) %*% data3 - m %*% t(m)\\n# calculate correlation\\ndata5 = data4 / (s %*% t(s))\\n# calculate principal components\\nev \u003c- eigen(data4,symmetric=T)\\nev2 \u003c- data.frame(values=ev[[1]],vectors=ev[[2]])\\ncat(\u0027Principal components and correlations have been calculated.\\\\n\\\\n\u0027)\\n# find word pairs with frequency\u003e=4 and high correlation\\ncat(\\\u0022Correlations above 0.3 of Words with freq\u003e=4 are:\\\\n\\\u0022)\\nfor (i in 1:length(which(dictionary$Freq\u003e=4)))\\nfor (j in (i+1):length(which(dictionary$Freq\u003e=4))){\\n    if (i!=j \u0026 abs(data5[i,j])\u003e=0.3)\\n        cat(sprintf(\\\u0022%s and %s have correlation r = %f\\\\n\\\u0022,row.names(data5)[i],row.names(data5)[j],data5[i,j]))\\t\\n}\\n#cat(\\\u0022\\\\nCorrelations above 0.5 of Words with freq\u003c=3\\\\n\\\u0022)\\n#for (i in length(which(dictionary$Freq\u003e=4)):nrow(data5)-1)\\n#for (j in (i+1):nrow(data5)-1){\\n#    if (i!=j \u0026 abs(data5[i,j])\u003e=0.5)\\n#        cat(sprintf(\\\u0022%s and %s have correlation r = %f\\\\n\\\u0022,row.names(data5)[i],row.names(data5)[j],data5[i,j]))\\n#}\u0022,\u0022execution_count\u0022:13,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u00226770c55c0fe11d33728f67ebb59d1b56b791353a\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Wow, among the 115 Words whose frequency is 4 or larger, there isn\u0027t much correlation. This surprises me. I know that we\u0027re approaching this problem without semantic information. (We\u0027re pretending that we don\u0027t know what titles, first names, and last names are.) But let me add a comment here. I expected male first names to correlate with male titles and I expected PCA to cluster all these names and titles into only a few principal components. This didn\u0027t happen. After thinking about, it makes mathematical sense. Yes, male names are associated with male titles but the relationship isn\u0027t the mathematical one defined as correlation. Here is a plot of the relationship between Charles and Mr that shows why.\u0022},{\u0022metadata\u0022:{\u0022_kg_hide-output\u0022:true,\u0022trusted\u0022:true,\u0022scrolled\u0022:true,\u0022_uuid\u0022:\u00229ffa2ae6cbdf0895debdc9210fa1b077e95f78c7\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022library(ggplot2)\\nlibrary(gridExtra)\\n\\n#ggplot(data=data2, aes(x=data2$Charles,y=data2$Mr)) + \\n#    geom_jitter(width=0.1,height=0.1) +\\n#    geom_smooth(method=\u0027lm\u0027) +\\n#    labs(x=\u0027Charles\u0027,y=\u0027Mr\u0027,title=\u0027Corr(Charles,Mr) = 0.0828.\\\\nLinear regression R^2 = 0.00686\u0027)\u0022,\u0022execution_count\u0022:12,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u00225490b451eb7d4436efcb6daa692b771a2bbb4c1c\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022![Charles](http://playagricola.com/Kaggle/Charles.png)\\n  \\nIt is true that when Charles is present, Mr is more likely to be present but the reason that Charles and Mr don\u0027t correlate is because when Charles is absent, it isn\u0027t the case that Mr is more likely to be absent.\\n\\n## Transform passengers into principal component weights\\nLet\u0027s convert all 1309 passengers into vectors of principal component weights.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022c1107f3c430e9f067ff2db8d0f4f5d4110590be2\u0022,\u0022_kg_hide-input\u0022:false},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022data3Transformed = (t(ev[[2]]))[,1:nrow(ev[[2]])] %*% t(data3)\\ndataPC6 = data.frame(n=1:1309,Survived=data$Survived,t(data3Transformed)[,1:6])\\ncolnames(dataPC6) \u003c- c(\u0027PassengerId\u0027,\u0027Survived\u0027,paste(\u0027PC\u0027,1:6,\u0027 wgt\u0027,sep=\u0027\u0027))\\nrownames(dataPC6) \u003c- 1:1309\\nhead(data[,c(\u0027PassengerId\u0027,\u0027Name\u0027,\u0027Sex\u0027,\u0027Age\u0027)])\\ncat(\u0027First 6 principal component weights for the first 6 passengers:\\\\n\u0027)\\nhead(dataPC6[,-2])\u0022,\u0022execution_count\u0022:9,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022b38eb0007795918d26488e32b7a268a9d8289bfb\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Examining the first 6 principal components (which are not displayed above) we learn they reference the following words:  \\n* PC1  is 82% Mr,   -48% Miss,   -28 %Mrs,   0% other words  \\n* PC2 is -64% Miss,   72.9% Mrs,   0% other words  \\n* PC3 is 68% William,   56% Master,   0% other words  \\n* PC4 is 67% William,   -59% Master,   0% other words  \\n* PC5 is 96% John,   0% other words   \\n* PC6 is 90% Henry,   0% other words    \\n  \\nTherefore when Mr. Owen Braund, has principal components weights (0.8, -0.1, -0.2, 0.2, 0.0, 0.2) that means he has 0.8 of Mr and nothing positive for Mrs, Miss, William, John, or Master. The second passenger, Mrs. John Cumings has weights (-0.25, 0.8, -0.15, 0.3, 1.1, -0.1). This says that she is mainly John and Mrs. The third passenger, Miss Laina Heikkinen has weights (-0.5, -0.6, -0.1, 0.3, 0.1, 0.0). When multiplied by the principal components, she becomes mostly Miss. The other 3 make sense too.\\n\\n## Model and cross validate\\nWe will use the passengers\u0027 principal components to classify unknown passengers using kNN. First let\u0027s cross validate and perform a grid search to find the optimal dimension _d_ of principal components and optimal _k_ for kNN\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022b6becb679ed0a20c71f13960eccc53b99767ac0d\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022library(caret)\\naccuracy \u003c- matrix(nrow=5,ncol=5)\\nrownames(accuracy) \u003c- paste(\u0027d=\u0027,3:7,sep=\u0027\u0027)\\ncolnames(accuracy) \u003c- paste(\u0027k=\u0027,c(7,9,11,13,15),sep=\u0027\u0027)\\nfor (d in 1:5){\\n    xt = (t(ev[[2]]))[1:(d+2),1:nrow(ev[[2]])] %*% t(data3)\\n    dataPC = data.frame(Survived=data$Survived,t(xt))\\n    for (k in 1:5){\\n        trials = 100\\n        total = 0\\n        for (i in 1:trials){\\n            s = sample(1:891,802)\\n            s2 = (1:891)[-s]\\n            model \u003c- knn3(factor(Survived) ~ .,dataPC[s,],k=2*k+5)\\n            p \u003c- predict(model,newdata=dataPC[s2,])\\n            p \u003c- ifelse(p[,2]\u003e=0.5,1,0)\\n            # calculate one minus misclassification rate\\n            x = 1-sum(abs(dataPC$Survived[s2]-p))/length(s2)\\n            #if (i%%10==0) cat(sprintf(\\\u0022Trial %d has CV accuracy %f\\\\n\\\u0022,i,x))\\n            total = total + x\\n    }\\n        #cat(sprintf(\\\u0022For d=%d, k=%d, average CV accuracy of %d trials is %f\\\\n\\\u0022\\n        #    ,d+2,2*k+5,trials,total/trials))\\n    accuracy[d,k] \u003c- total/trials\\n    }\\n}\\ncat(\u0027Cross validation accuracy using 10-fold CV:\\\\n\u0027)\\naccuracy\u0022,\u0022execution_count\u0022:10,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u00227f6c43f8d455d8dbb88beca38637ea870e5003be\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022From our search, we see that d=5 and k=11 are the best combination achieving CV = 80.5% which is better than the gender model\u0027s cross validation of 78.6%. (Results may vary due to the random natural of cross validation k-folds.) So, PCA is detecting more than just gender, cool. Let\u0027s use those hyperparameters to make a submission to Kaggle\\n## Submission to Kaggle\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022e2ca2255816ef7d3c2c1e5481e44f91b537ccda1\u0022,\u0022_kg_hide-input\u0022:false},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022d=5; k=11;\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% t(data3)\\ndataPC = data.frame(Survived=data$Survived,t(xt))\\nmodel \u003c- knn3(factor(Survived) ~ .,dataPC[1:891,],k=k)\\np \u003c- predict(model,newdata=dataPC[892:1309,])\\np \u003c- ifelse(p[,2]\u003e=0.5,1,0)\\nsubmit = data.frame(PassengerId=892:1309,Survived=p)\\nwrite.csv(submit,\u0027PCA5kNN11.csv\u0027,row.names=F)\u0022,\u0022execution_count\u0022:19,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022caefc934de5217ffecb3bb4ecfdef822bab28135\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022![result](http://playagricola.com/Kaggle/resultPCA5kNN9.png)\\nThis isn\u0027t a high score but we did achieve the accuracy of the gender model by implemented an interesting and complex model. Let\u0027s explore our PCA a bit and then implement a variant of naive Bayes.\u0022},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022406a667e01b47bda85f6390dab8003e27a8199d0\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## PCA Exploration\\nThe overall lack of correlation among Words with frequency 4 or larger means that our principal components weren\u0027t able to compress these 115 Words into a dimension much less than 115. Let\u0027s plot the first 12 principal components. And then let\u0027s try to compress the first 115 Words. It appears the next 298 = 413 - 115 Words have some correlation and will be able to be compressed. However that won\u0027t help much because we\u0027d prefer to use the first 115 frequent Words and discard the next 298 anyway. We expect each principal component to represent more or less a single word. So the principal components should be mostly flat with a single spike.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u00220b474af2da930dcf98be97f13f672544f845ce2a\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022pimage \u003c- function(x){\\n    return(ggplot(data=ev2) + \\n           geom_line(aes(x=1:length(ev2[[x+1]]),y=ev2[[x+1]])) + \\n           labs(x=\u0027\u0027,y=\u0027\u0027,title=paste(\u0027PC\u0027,x,sep=\u0027\u0027)))\\n}\\nx=0\\ngrid.arrange(pimage(x+1),pimage(x+2),pimage(x+3),pimage(x+4),pimage(x+5),pimage(x+6),\\n    pimage(x+7),pimage(x+8),pimage(x+9),pimage(x+10),pimage(x+11),pimage(x+12),as.table=F)\u0022,\u0022execution_count\u0022:13,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022884877feef0c3bee5aeec01b09718ad94670eb14\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022As expected, these principal components only involve a single word more or less because the most frequent Words aren\u0027t correlated with other Words. The less frequent Words did correlate with other Words, so as we plot more principal components, we expect them to involve multiple words. For example, here are principal components 100 thru 112.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u002263c00e3e3c0de72e95235ec9f859532d43bcf77a\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022x=99\\ngrid.arrange(pimage(x+1),pimage(x+2),pimage(x+3),pimage(x+4),pimage(x+5),pimage(x+6),\\n    pimage(x+7),pimage(x+8),pimage(x+9),pimage(x+10),pimage(x+11),pimage(x+12),as.table=F)\u0022,\u0022execution_count\u0022:40,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u0022cf294924c056b6a5689522082485110b26f3b38b\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We can test PCA\u0027s ability to compress Words by one-hot-encoding our entire dictionary and transforming it into PC space only using k principal components. Then we can transform it back into regular Word space and see how many Words we get back. It would be fantastic if we could transform 413 Words into PC space using 20 principal components and then transform the result back and still have something like 100 Words. That would be a 5 to 1 compression.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022679522d076a164c5ee3a1ba20c5813c2d627115d\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022prep \u003c- function(data,s){\\n    x=c(); y=c(); z=c()\\n    for (i in 1:nrow(data))\\n    for (j in 1:ncol(data)){\\n        x=c(x,s*j)\\n        y=c(y,s*i)\\n        z=c(z,data[i,j])\\n    }\\n    return (data.frame(x=x,y=y,z=z))\\n}\\n\\nx=diag(nrow(data5))\\ng1 = ggplot(prep( x[4*1:100,4*1:100] ,4),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(400,0) +\\n labs(title=\u0027Words in dictionary\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd = 5\\nx=diag(nrow(data5))\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng2 = ggplot(prep( x2[1:10,1:10] ,1),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(10,0) +\\n labs(title=\u0027Words recreated from 5 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=10\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng3 = ggplot(prep( x2[1:20,1:20] ,1),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(20,0) +\\n labs(title=\u0027Words recreated from 10 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=25\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng4 = ggplot(prep( x2[1:50,1:50] ,1),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(50,0) +\\n labs(title=\u0027Words recreated from 25 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\ngrid.arrange(g1,g2,g3,g4,nrow=2,ncol=2)\u0022,\u0022execution_count\u0022:41,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u0022197ecde74f124ada9ac53cf885468bd5bdf3f64f\u0022,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022d=50\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng5 = ggplot(prep( x2[1:100,1:100] ,1),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(100,0) +\\n labs(title=\u0027Words recreated from 50 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=100\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng6 = ggplot(prep( x2[2*1:100,2*1:100] ,2),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(200,0) +\\n labs(title=\u0027Words recreated from 100 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=200\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng7 = ggplot(prep( x2[4*1:100,4*1:100] ,4),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(400,0) +\\n labs(title=\u0027Words recreated from 200 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\nd=300\\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\\ng8 = ggplot(prep( x2[4*1:100,4*1:100] ,4),aes(x,y)) +\\n geom_raster(aes(fill=z)) +\\n ylim(400,0) +\\n labs(title=\u0027Words recreated from 300 PC\u0027,x=\u0027\u0027,y=\u0027\u0027)\\ngrid.arrange(g5,g6,g7,g8,nrow=2,ncol=2)\u0022,\u0022execution_count\u0022:42,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u002220b9f855ff159ffcde281ebe7b258de02f37816b\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022As expected, we don\u0027t see much compression. When we use 5 principal components, we can only recreate 6 Words from the dictionary. Then 10 PC recreates 11 Words. Then 25 recreates 25. At 50, we begin to see some compression power. Using 50 PC, we can represent more than 50 Words. Next 100, 200, and 300 can represent more Words than 100, 200, 300 too. It appears that 200 and 300 PC can basically represent the entire dictionary of 413 Words.\\n\\n# Naive Bayes variant (Oscar\u0027s SLogL)\\n\\nSince we only really want to use Words with freq\u003e=4 which are the first 115 Words and PCA isn\u0027t doing a great job compressing those 115, let\u0027s leave the Words in their original space. Eventually we can perform a dimension reduction by discarding less frequent Words thus mimicking PCA. Since the Words are so uncorrelated, the Words themselves when ordered by frequency are basically equivilent to principal components. Let\u0027s plot each Word\u0027s predictive power.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u00220b8cf76a7de59077388197f46722e186161b98fe\u0022,\u0022scrolled\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# returns mode of a set of numbers\\ngetmode \u003c- function(v) {\\n   uniqv \u003c- unique(v)\\n   uniqv[which.max(tabulate(match(v, uniqv)))]\\n}\\n\\n# calculate average (mode) position of each word\\nPosition \u003c- vector(\\\u0022list\\\u0022,length=nrow(dictionary))\\nfor (i in 1:nrow(data)){\\n    n = gsub(\u0027[.,\\\u0022()/]\u0027,\u0027\u0027,data$Name[i])\\n    n = gsub(\u0027 . |  \u0027,\u0027 \u0027,n)\\n    n = strsplit(n,\u0027 \u0027)[[1]]\\n    for (j in 1:length(n)){\\n        k = which(dictionary$Words==n[j])[1]\\n        if (!is.na(k)) Position[[k]] = c(Position[[k]],j)\\n    }\\n}\\ndictionary$Position \u003c- NULL\\nfor (i in 1:nrow(dictionary)) dictionary$Position[i] \u003c- getmode(Position[[i]])\\ndictionary$Position \u003c- pmin(4,dictionary$Position)\\n\\nggplot(data=dictionary) +\\ngeom_jitter(height=0.005,aes(x=1:nrow(dictionary),y=dictionary$Survival,color=factor(dictionary$Position))) +\\nscale_colour_manual(values = c(\\\u0022red\\\u0022, \\\u0022black\\\u0022, \\\u0022blue\\\u0022,\\\u0022white\\\u0022),labels=c(\u00271\u0027,\u00272\u0027,\u00273\u0027,\u00274+\u0027)) +\\ngeom_hline(yintercept=0.384, linetype=\u0027dotted\u0027) +\\ngeom_vline(xintercept=45) +\\ngeom_vline(xintercept=57) +\\ngeom_vline(xintercept=80) +\\ngeom_vline(xintercept=115) +\\ngeom_vline(xintercept=187) +\\ngeom_vline(xintercept=6) +\\nannotate(\u0027text\u0027,x=-15,y=0.87,label=\u0027n\u003c=6\\\\nfreq\\\\n\u003e=35\u0027) +\\nannotate(\u0027text\u0027,x=98,y=0.87,label=\u0027freq\\\\n\u003e=4\u0027,color=\u0027gray70\u0027) +\\n#annotate(\u0027text\u0027,x=68,y=0.87,label=\u0027freq\\\\n\u003e=5\u0027,color=\u0027gray70\u0027) +\\nannotate(\u0027text\u0027,x=25,y=0.87,label=\u0027freq\\\\n\u003e=7\u0027,color=\u0027gray70\u0027) +\\nannotate(\u0027text\u0027,x=143,y=0.87,label=\u0027n\u003e=116\\\\nfreq\u003c=3\u0027) +\\nannotate(\u0027text\u0027,x=215,y=0.87,label=\u0027n\u003e=188\\\\nfreq\u003c=2\u0027) +\\nannotate(\u0027text\u0027,x=305,y=0.36,label=\u0027p = 0.384 = probability of survival\u0027) +\\nlabs(x=\u0027Words order by frequency\u0027) +\\nlabs(title=\u0027Each dot represents a Word in the dictionary.\u0027) +\\nlabs(y=\u0027Probability Survival given Word\u0027,color=\u0027word\\\\\u0027s\\\\nposition\\\\nin name\u0027)\u0022,\u0022execution_count\u0022:43,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_uuid\u0022:\u002201a1ec4e8edd68aa1e64fef489fbeb188a067425\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022This plot is very interesting. The Words whose frequencies are 35 or higher are mostly black dots. The 6 most frequent words are contained in the first 5 principal components we saw above. These Words are Mr (518 occurences), Miss (182), Mrs (129), William (63), John (44), and Master (40). Our key shows us that black dots are Words that are found in the second postion in a Name. (We\u0027re pretending this is a foreign language, so I can\u0027t say what these are, but we know these to be titles.) Next the Words with frequency between 7 and 34 are blue dots. These are Words found in position 3 (first names). Next we have a mixture of colors which include the red dots (last names).\\n\\n## Model and cross validate\\n\\nLet\u0027s use a variant of naive Bayes presented by [Oscar Takeshita][2] called [Divide and Conquer][1] to predict passenger survival.\\n\\n$$P(y|w_1,w_2,...,w_n) = \\\\text{logit}^{-1}\\\\Bigg(  \\\\sum_{k=1}^n{  \\\\log \\\\bigg( \\\\frac{ P(y|w_k)}{ 1-P(y|w_k) } \\\\bigg)} +  \\\\sum_{k=1}^{n-1}{  \\\\log \\\\bigg( \\\\frac{ 1-P(y)}{ P(y) } \\\\bigg)}  \\\\Bigg)$$\\n\\n$$ \\\\text{where} \\\\quad \\\\text{logit}^{-1}(z) = \\\\frac{1}{1 + e^{-z}} \\\\quad \\\\text{and} \\\\quad P(y|w_k) = \\\\text{probability of survival given word } k $$\\n[1]: https://www.kaggle.com/pliptor/divide-and-conquer-0-82296\\n[2]:https://www.kaggle.com/pliptor\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022scrolled\u0022:true,\u0022_uuid\u0022:\u00225d8e8c0361dcb981fa011bff0949f385b943e2ce\u0022},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# log odds\\nlogl \u003c- function(p){\\n    e = 0.001\\n    q = 1 - p\\n    return(log(max(p,e))-log(max(q,e)))\\n}\\n# inverse logit\\nilogit \u003c- function(z){\\n    return (1/(1+exp(-z)))\\n}\\n# cross validation trials\\ntrials = 100\\ntotal = 0\\nfor (k in 1:trials){\\nif (k%%25==0) cat(sprintf(\\\u0022Begin trial %d\\\\n completed\\\u0022,k))\\ns = sample(1:891,802)\\ns2 = (1:891)[-s]\\n\\n# calculate the Tfreq and Survival within our training subset\\ndictionary$Survival \u003c- NA\\ndictionary$Tfreq \u003c- 0\\nfor (i in 1:length(dictionary$Words)){\\n    x = intersect(which(data2[,dictionary$Words[i]]\u003e0),s)\\n    dictionary$Survival[i] \u003c- mean(data$Survive[x],na.rm=T)\\n    dictionary$Tfreq[i] \u003c- length(x)\\n}\\ndictionary$Survival[is.na(dictionary$Survival)] \u003c- NA\\ndictionary$Tfreq[is.na(dictionary$Tfreq)] \u003c- NA\\ndictionary$Logl \u003c- sapply(dictionary$Survival,FUN=logl)\\n\\n# calculate bias term for Oscar\u0027s naive Bayes\\nps = sum(data$Survived[s])/length(s)\\nbias = logl(1-ps)\\n\\n# the following line mimics PCA dimension reduction\\ndictionary2 \u003c- dictionary[dictionary$Tfreq\u003e=4,]\\np = rep(0,891-length(s))\\nfor (j in 1:length(s2)){\\n    c = 0\\n    slogl = 0\\n    # perform Oscar\u0027s naive Bayes\\n    for (i in 1:nrow(dictionary2)){\\n        if (data2[s2[j],dictionary2$Words[i]]\u003e0 \u0026 !is.na(dictionary2$Survival[i])){\\n            slogl = slogl + dictionary2$Logl[i]\\n            if (c\u003e0) slogl = slogl + bias\\n            c = 1\\n        }\\n    }\\n    if (c!=0) p[j] = ilogit(slogl)\\n    else p[j] = ps\\n    if (k%%25==0 \u0026 j%%10==0) cat(sprintf(\\\u0022 j=%d \\\u0022,j))\\n}\\np \u003c- ifelse(p\u003e=0.5,1,0)\\n# calculate one minus misclassification rate\\nx = 1-sum(abs(data$Survived[s2]-p))/length(s2)\\nif (k%%25==0) cat(sprintf(\\\u0022\\\\n Trial %d has CV accuracy %f\\\\n\\\u0022,k,x))\\ntotal = total + x\\n}\\n\\ncat(sprintf(\\\u0022Average CV accuracy of %d trials is %f\\\\n\\\u0022,trials,total/trials))\\n\u0022,\u0022execution_count\u0022:45,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022_uuid\u0022:\u002210688262e3e5beacea51061ec3fc1c73f9240fce\u0022},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Cross validation shows that when our model uses the first 115 dictionary Words which are all the Words with frequency 4 or more, then we can expect prediction accuracy of 80%! That\u0027s better than the gender model that cross validates at 78.6%, so we\u0027re extracting gender and something more from these words.\\n\\nI also ran cross validation using the first 6, 23, 42, 57, 80, 115, 187, 413 Words, (frequency \u003e= 35, 12, 7, 6, 5, 4, 3, 2), which achieved results of 79%, 78%, 78%, 79%, 79%, 80%, 80%, 80% respectively. What\u0027s interesting is when we transistion from using the first 6 \\\u0022principal components\\\u0022 to using the next 34, our CV drops. Then our CV improves when we add more \\\u0022principal components\\\u0022. This experimental result can be explained by the plot of Words above. The first 6 \\\u0022principal components\\\u0022 are black dots (titles), the next 38 are blue dots (first names), and after that are red dots (last names). If we use every Word (approximately 600 Words) including those with frequency equal 1 (that were removed from our dictionary), then the CV drops to 76%.\\n## Submission to Kaggle\\n![resultNB](http://playagricola.com/Kaggle/resultNaiveBayes.png)\\nAgain, not a high score. But it did achieve the gender model accuracy which is nice. Also, this kernel serves as a template to build upon in the future.\\n# Conclusion\\nCross validation indicates that the black dots and red dots contain the most predictive power. And the blue dots hurt our CV. I checked and our CV increases to 80.5% by removing the blue and white dots. But we still aren\u0027t close to the Name only kernel posted [here][1] which has an 84% CV. Therefore there is still room to improve our model. In the future, I plan to use all the insights gained from the above investigations to improve PCA and naive Bayes operating on the words within the names.\\n\\nThank you everyone for reading my notebook. I look forward to your comments.\\n\\n# References\\n\\n* [Oscar Takeshita][2]: Postulates this interesting problem and provides a solution using PCA and kNN\\n* [Chris Deotte][3]: Provides an example of PCA\\n* [Chris Deotte][4]: Provides an example of SVD compression\\n* [Cathy O\u0027Neil \u0026 Rachel Schutt][5]: Doing Data Science explains naive Bayes\\n* [Oscar Takeshita][6]: Presents a variant of naive Bayes called SLogL in his kernel Divide and Conquer\\n\\n[1]: https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818\\n[2]: https://www.kaggle.com/pliptor/name-only-study-with-interactive-3d-plot\\n[3]:http://www.ccom.ucsd.edu/~cdeotte/papers/PrincipleComponents.pdf\\n[4]:http://www.ccom.ucsd.edu/~cdeotte/papers/SVD.pdf\\n[5]: https://www.amazon.com/Doing-Data-Science-Straight-Frontline/dp/1449358659\\n[6]:https://www.kaggle.com/pliptor/divide-and-conquer-0-82296\\n\u0022}],\u0022metadata\u0022:{\u0022kernelspec\u0022:{\u0022display_name\u0022:\u0022R\u0022,\u0022language\u0022:\u0022R\u0022,\u0022name\u0022:\u0022ir\u0022},\u0022language_info\u0022:{\u0022mimetype\u0022:\u0022text/x-r-source\u0022,\u0022name\u0022:\u0022R\u0022,\u0022pygments_lexer\u0022:\u0022r\u0022,\u0022version\u0022:\u00223.4.2\u0022,\u0022file_extension\u0022:\u0022.r\u0022,\u0022codemirror_mode\u0022:\u0022r\u0022}},\u0022nbformat\u0022:4,\u0022nbformat_minor\u0022:1}","dateCreated":"2018-06-08T21:36:01.1859654Z"},"resources":null,"isolatorResults":"\u003cresults\u003e\u003cdisk_kb_free\u003e944076\u003c/disk_kb_free\u003e\u003cdocker_image_digest\u003e52937f97db1d60cc30110f82402aec2d3dec26569f3ae4239adbe334f10492ad\u003c/docker_image_digest\u003e\u003cdocker_image_id\u003esha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a\u003c/docker_image_id\u003e\u003cdocker_image_name\u003egcr.io/kaggle-images/rstats\u003c/docker_image_name\u003e\u003cexit_code\u003e0\u003c/exit_code\u003e\u003cfailure_message /\u003e\u003cinvalid_path_errors\u003eFalse\u003c/invalid_path_errors\u003e\u003cout_of_memory\u003eFalse\u003c/out_of_memory\u003e\u003crun_time_seconds\u003e90.168918313997\u003c/run_time_seconds\u003e\u003csucceeded\u003eTrue\u003c/succeeded\u003e\u003ctimeout_exceeded\u003eFalse\u003c/timeout_exceeded\u003e\u003cused_all_space\u003eFalse\u003c/used_all_space\u003e\u003cwas_killed\u003eFalse\u003c/was_killed\u003e\u003c/results\u003e","runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageDigest":"52937f97db1d60cc30110f82402aec2d3dec26569f3ae4239adbe334f10492ad","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"kaggle/rstats","diskKbFree":944076,"failureMessage":"","exitCode":0,"queuedSeconds":0,"outputSizeBytes":0,"runTimeSeconds":90.168918313997,"usedAllSpace":false,"timeoutExceeded":false,"isValidStatus":false,"wasGpuEnabled":false,"wasInternetEnabled":false,"outOfMemory":false,"invalidPathErrors":false,"succeeded":true,"wasKilled":false},"outputFilesTotalSizeBytes":889469,"dockerImageVersionId":47,"usedCustomDockerImage":false},"author":{"id":1723677,"displayName":"Chris Deotte","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"cdeotte","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1723677-kg.jpg","profileUrl":"/cdeotte","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":4,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"baseUrl":"/cdeotte/titanic-spam-filter","collaborators":{"owner":{"userId":1723677,"groupId":null,"groupMemberCount":null,"profileUrl":"/cdeotte","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1723677-kg.jpg","name":"Chris Deotte","slug":"cdeotte","userTier":4,"joinDate":null,"type":"owner","isUser":true,"isGroup":false},"collaborators":[]},"initialTab":null,"log":"[{\n  \u0022data\u0022: \u0022[NbConvertApp] Converting notebook script.irnb to html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 0.8303662700054701\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Executing notebook with kernel: ir\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 0.878325539000798\n},{\n  \u0022data\u0022: \u0022/usr/local/lib/python2.7/dist-packages/nbconvert/filters/datatypefilter.py:41: UserWarning: Your element with mimetype(s) [] is not able to be represented.\\n  mimetypes=output.keys())\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 89.79740366000624\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Support files will be in __results___files/\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Writing 352643 bytes to __results__.html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 89.83514828600164\n}]","outputFiles":[{"ownerInfo":null,"kernelVersionOutputFileId":12344209,"kernelVersionId":3955427,"kernelId":1029920,"size":0,"fullPath":"PCA5kNN11.csv","previewUrl":"/kernels/preview.json/3955427/24620936-cb1c-829a-d137-e380d0889936/PCA5kNN11.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/3955427/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mvRSEX07uoRJxv1lOv3nxg.BJd-2QtvPJFweOXP-QwTj0ofixxgS-qvgdJ0IpOeCxpI1GzgKhCmd8gi7UKeXlzXfYruscUOnR6w1gxa4OBD5gZjFSXg3ArmZJ55rwC4mjEJrt5LnuSA7WuItwdIlVlZQYkePbfZghkaZHD3vOferQ.8Bt0WmOE_ovG8m3naIcP2A/PCA5kNN11.csv","fileType":".csv","contentLength":2843,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"PCA5kNN11.csv","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":12344210,"kernelVersionId":3955427,"kernelId":1029920,"size":0,"fullPath":"Rplot001.png","previewUrl":"/kernels/preview.json/3955427/24620936-cb1c-829a-d137-e380d0889936/Rplot001.png","downloadUrl":"https://www.kaggleusercontent.com/kf/3955427/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..4Yun_Q08Fgi6YjVRQJ08tQ.8yw3kj9AdBvf9XN11kxgSX2fEz-FREWuIMzA65VYKgby6gpAl9mVzfVEX7weR3TULeY-R01KDet0lNrkJq5iPbUOFADrLGPUYTS4wSl4j3qkgnsCbzwRmoWIi_NFNBu2V8tCYROcHzNCoRgtX0JyIg.0_U68iHDFIcg2KlEpSHsug/Rplot001.png","fileType":".png","contentLength":48914,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"Rplot001.png","description":null}],"outputFilesCropped":false,"ouputFilesOwnerInfo":{"databundleVersionId":0,"dataset":null,"competition":null,"kernel":{"kernelId":1029920,"kernelVersionId":3955427,"dataviewToken":"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..xpG3EoJ60PQAuc6q1DGw7w.5BOD7t8dl5vSZwQ1zIaR1TsK4_eGqIwYkwj5xcN_5HkR7L9y893ju24QbGg2Tig9Z834J5MonVICaDxjsi2ahZjo9zE-KYeQ1B3w03qYiQxxlE-HA9D-W8p59KFQCNEsIFRnKfHOCXM-64T3EP20W7I7m6e_qRIWyRhFEn8IGmTkCitW5vKyulPNAspt4pJVkqn4KtTaqi80NbDKI8jozhdRInmZMq7QJhneRQecDNPsrM135Qm27b1VRB2kZhiyVZjf3yCVxF3aATh1CDmdhIT9UF0LZ114vsMsk_UeJgZPVX85No6h0UqO4YQ-tgWilNhM2zKGd99ZZqNkHIvj0rMw5RiZZ2RjNAh2ChZawAF0o7vL13gb7NJw8SweSzQVoohWgE1wXvYBqi25NXxg5iVqniNtVbUxAUX44qyaRemmND8Xcxuv7_JxdtlxCOv7r_DurhpFFXHtYL66QWOAns14MwdknN6mB8pqbDW_eA18gLgZq3OR8sSsis9nxMimPgfa-1DdEjpSMewE1O_LmA.Z9QyimR4r94QHyLI3KTmcA","scope":"cdeotte/titanic-spam-filter"},"previewsDisabled":false},"pageMessages":[],"dataSources":[{"imageUrl":"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png","sourceUrl":"/c/titanic","slug":"titanic","lastUpdated":"2012-09-28T21:13:33.55Z","overview":"Start here! Predict survival on the Titanic and get familiar with ML basics","sourceType":"competition","sourceVersionType":null,"sourceId":3136,"sourceVersionNumber":null,"maxVersionNumber":null,"descriptionMimeType":"text/html","deleted":false,"private":false,"privateButVisible":false,"ownerInfo":{"databundleVersionId":26502,"dataset":null,"competition":{"competitionId":3136,"dataviewToken":null,"scope":"c/titanic"},"kernel":null,"previewsDisabled":true},"type":"dataSource","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"id":63842,"blobFileId":37991,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/gender_submission.csv","creationDate":"2017-02-01T01:49:18Z","isDummy":false,"size":3258,"fullPath":"../input/gender_submission.csv","previewUrl":"kernels/competition-preview/3136?relativePath=gender_submission.csv","downloadUrl":"/c/titanic/download/gender_submission.csv","fileType":".csv","contentLength":3258,"contentType":"text/csv","contentMD5":"MNEHO5ZKXYFUMexgOg3jUw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":418},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n"},{"order":1,"originalType":"","type":"boolean","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Survived","description":"0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"gender_submission.csv","description":"892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,0\n899,0\n900,0\n901,0\n902,0\n903,0\n904,1\n905,0a\n906,1\n907,0\n908,0\n909,0\n910,0\n911,0\n912,1\n913,0\n914,0\n915,0\n916,1\n917,0\n918,0\n919,0\n920,0\n921,0\n922,0\n923,0\n924,0\n925,0\n926,1\n927,0\n928,0\n929,0\n930,0\n931,1\n932,0\n933,0\n934,0\n935,0\n936,1\n937,0\n938,0\n939,0\n940,1\n941,0\n942,1\n943,0\n944,0\n945,1\n946,0\n947,0\n948,0\n949,0\n950,0\n951,1\n952,0\n953,0\n954,0\n955,0\n956,1\n957,0\n958,0\n959,0\n960,0\n961,1\n962,\n963,0\n964,0\n965,0\n966,1\n967,1\n968,0\n969,0\n970,0\n971,0\n972,0\n973,1\n974,0\n975,0\n976,0\n977,0\n978,0\n979,0\n980,0\n981,0\n982,0\n983,0\n984,0\n985,0\n986,0\n987,0\n988,1\n989,0\n990,0\n991,0\n992,1\n993,0\n994,0\n995,0\n996,0\n997,0\n998,0\n999,0\n1000,0\n1001,0\n1002,0\n1003,0\n1004,0\n1005,0\n1006,1\n1007,0\n1008,0\n1009,0\n1010,1\n1011,0\n1012,0\n1013,0\n1014,1\n1015,0\n1016,0\n1017,0\n1018,0\n1019,0\n1020,0\n1021,0\n1022,0\n1023,0\n1024,0\n1025,0\n1026,0\n1027,0\n1028,0\n1029,0\n1030,0\n1031,0\n1032,0\n1033,1\n1034,1\n1035,0\n1036,0\n1037,0\n1038,1\n1039,0\n1040,0\n1041,0\n1042,1\n1043,0\n1044,0\n1045,0\n1046,0\n1047,0\n1048,1\n1049,0\n1050,0\n1051,0\n1052,0\n1053,0\n1054,0\n1055,0\n1056,0\n1057,0\n1058,1\n1059,0\n1060,0\n1061,0\n1062,0\n1063,0\n1064,0\n1065,0\n1066,0\n1067,0\n1068,0\n1069,1\n1070,0\n1071,1\n1072,0\n1073,1\n1074,1\n1075,0\n1076,1\n1077,0\n1078,0\n1079,0\n1080,1\n1081,0\n1082,0\n1083,0\n1084,0\n1085,0\n1086,0\n1087,0\n1088,1\n1089,0\n1090,0\n1091,0\n1092,0\n1093,0\n1094,1\n1095,0\n1096,0\n1097,0\n1098,0\n1099,0\n1100,0\n1101,0\n1102,0\n1103,0\n1104,1\n1105,0\n1106,0\n1107,0\n1108,0\n1109,1\n1110,1\n1111,0\n1112,0\n1113,0\n1114,0\n1115,0\n1116,0\n1117,0\n1118,0\n1119,0\n1120,0\n1121,0\n1122,1\n1123,0\n1124,0\n1125,0\n1126,1\n1127,0\n1128,1\n1129,0\n1130,0\n1131,1\n1132,0\n1133,0\n1134,1\n1135,0\n1136,0\n1137,1\n1138,0\n1139,0\n1140,0\n1141,0\n1142,0\n1143,0\n1144,1\n1145,0\n1146,0\n1147,0\n1148,0\n1149,0\n1150,0\n1151,0\n1152,0\n1153,0\n1154,0\n1155,0\n1156,0\n1157,0\n1158,0\n1159,0\n1160,0\n1161,0\n1162,1\n1163,0\n1164,1\n1165,0\n1166,0\n1167,0\n1168,0\n1169,0\n1170,0\n1171,0\n1172,0\n1173,0\n1174,0\n1175,0\n1176,0\n1177,0\n1178,0\n1179,1\n1180,0\n1181,0\n1182,0\n1183,0\n1184,0\n1185,1\n1186,0\n1187,0\n1188,0\n1189,0\n1190,1\n1191,0\n1192,0\n1193,0\n1194,0\n1195,0\n1196,0\n1197,0\n1198,1\n1199,0\n1200,1\n1201,0\n1202,0\n1203,0\n1204,0\n1205,0\n1206,1\n1207,0\n1208,1\n1209,0\n1210,0\n1211,0\n1212,0\n1213,0\n1214,0\n1215,0\n1216,1\n1217,0\n1218,0\n1219,1\n1220,0\n1221,0\n1222,0\n1223,0\n1224,0\n1225,0\n1226,0\n1227,0\n1228,0\n1229,0\n1230,0\n1231,0\n1232,0\n1233,0\n1234,1\n1235,1\n1236,0\n1237,0\n1238,0\n1239,0\n1240,0\n1241,0\n1242,1\n1243,0\n1244,1\n1245,1\n1246,0\n1247,0\n1248,1\n1249,0\n1250,0\n1251,0\n1252,1\n1253,0\n1254,0\n1255,0\n1256,1\n1257,1\n1258,0\n1259,0\n1260,0\n1261,0\n1262,0\n1263,1\n1264,0\n1265,0\n1266,1\n1267,1\n1268,0\n1269,0\n1270,1\n1271,0\n1272,0\n1273,0\n1274,0\n1275,0\n1276,0\n1277,1\n1278,0\n1279,0\n1280,0\n1281,0\n1282,1\n1283,0\n1284,0\n1285,0\n1286,0\n1287,1\n1288,0\n1289,1\n1290,0\n1291,0\n1292,1\n1293,0\n1294,0\n1295,1\n1296,0\n1297,0\n1298,0\n1299,1\n1300,0\n1301,0\n1302,0\n1303,1\n1304,0\n1305,0\n1306,1\n1307,0\n1308,0\n1309,0"},{"id":63841,"blobFileId":2613,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/test.csv","creationDate":"2013-06-28T13:40:24.227Z","isDummy":false,"size":28629,"fullPath":"../input/test.csv","previewUrl":"kernels/competition-preview/3136?relativePath=test.csv","downloadUrl":"/c/titanic/download/test.csv","fileType":".csv","contentLength":28629,"contentType":"text/csv","contentMD5":"dTO4Lq5LWCYQy9aKpjawFw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":418},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"1"},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Pclass","description":"1"},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Name","description":"the name of the passenger"},{"order":3,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Sex","description":null},{"order":4,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Age","description":null},{"order":5,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"SibSp","description":"of siblings / spouses aboard the Titanic"},{"order":6,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Parch","description":"of parents / children aboard the Titanic"},{"order":7,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Ticket","description":"Ticket number"},{"order":8,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Fare","description":"Passenger fare"},{"order":9,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Cabin","description":"Cabin number"},{"order":10,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Embarked","description":"Port of Embarkation"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"test.csv","description":"test data to check the accuracy of the model created\n"},{"id":63840,"blobFileId":2307,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/train.csv","creationDate":"2013-06-28T13:40:25.23Z","isDummy":false,"size":61194,"fullPath":"../input/train.csv","previewUrl":"kernels/competition-preview/3136?relativePath=train.csv","downloadUrl":"/c/titanic/download/train.csv","fileType":".csv","contentLength":61194,"contentType":"text/csv","contentMD5":"IwnMXwR4Ltm7YBbZ9OOBzw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":891},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"type should be integers"},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Survived","description":"Survived or Not "},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Pclass","description":"Class of Travel"},{"order":3,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Name","description":"Name of Passenger"},{"order":4,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Sex","description":"Gender"},{"order":5,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Age","description":"Age of Passengers"},{"order":6,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"SibSp","description":"Number of Sibling/Spouse aboard"},{"order":7,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Parch","description":"Number of Parent/Child aboard"},{"order":8,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Ticket","description":null},{"order":9,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Fare","description":null},{"order":10,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Cabin","description":null},{"order":11,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Embarked","description":"The port in which a passenger has embarked. C - Cherbourg, S - Southampton, Q = Queenstown"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"train.csv","description":"contains data \n"}],"name":"Titanic: Machine Learning from Disaster","description":"\u003ch3\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe data has been split into two groups:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etraining set (train.csv)\u003c/li\u003e\n\u003cli\u003etest set (test.csv)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003e The training set \u003c/b\u003eshould be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use \u003ca href=\u0022https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\u0022 target=\u0022_blank\u0022\u003e feature engineering \u003c/a\u003eto create new features.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eThe test set \u003c/b\u003eshould be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\u003c/p\u003e\n\u003cp\u003eWe also include \u003cb\u003egender_submission.csv\u003c/b\u003e, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\u003c/p\u003e\n\u003ch3\u003eData Dictionary\u003c/h3\u003e\n\u003ctable style=\u0022width: 100%;\u0022\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\u003cth\u003e\u003cb\u003eVariable\u003c/b\u003e\u003c/th\u003e\u003cth\u003e\u003cb\u003eDefinition\u003c/b\u003e\u003c/th\u003e\u003cth\u003e\u003cb\u003eKey\u003c/b\u003e\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esurvival\u003c/td\u003e\n\u003ctd\u003eSurvival\u003c/td\u003e\n\u003ctd\u003e0 = No, 1 = Yes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epclass\u003c/td\u003e\n\u003ctd\u003eTicket class\u003c/td\u003e\n\u003ctd\u003e1 = 1st, 2 = 2nd, 3 = 3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esex\u003c/td\u003e\n\u003ctd\u003eSex\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAge\u003c/td\u003e\n\u003ctd\u003eAge in years\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esibsp\u003c/td\u003e\n\u003ctd\u003e# of siblings / spouses aboard the Titanic\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eparch\u003c/td\u003e\n\u003ctd\u003e# of parents / children aboard the Titanic\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eticket\u003c/td\u003e\n\u003ctd\u003eTicket number\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003efare\u003c/td\u003e\n\u003ctd\u003ePassenger fare\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecabin\u003c/td\u003e\n\u003ctd\u003eCabin number\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eembarked\u003c/td\u003e\n\u003ctd\u003ePort of Embarkation\u003c/td\u003e\n\u003ctd\u003eC = Cherbourg, Q = Queenstown, S = Southampton\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eVariable Notes\u003c/h3\u003e\n\u003cp\u003e\u003cb\u003epclass\u003c/b\u003e: A proxy for socio-economic status (SES)\u003cbr /\u003e 1st = Upper\u003cbr /\u003e 2nd = Middle\u003cbr /\u003e 3rd = Lower\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003eage\u003c/b\u003e: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003esibsp\u003c/b\u003e: The dataset defines family relations in this way...\u003cbr /\u003e Sibling = brother, sister, stepbrother, stepsister\u003cbr /\u003e Spouse = husband, wife (mistresses and fiancés were ignored)\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003eparch\u003c/b\u003e: The dataset defines family relations in this way...\u003cbr /\u003e Parent = mother, father\u003cbr /\u003e Child = daughter, son, stepdaughter, stepson\u003cbr /\u003e Some children travelled only with a nanny, therefore parch=0 for them.\u003c/p\u003e"}],"versions":[{"id":3955427,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-06-08T21:36:01.537Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":22,"linesInsertedFromPrevious":18,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":90.168918313997,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic \u0022Spam\u0022 Filter","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3955427","versionNumber":11,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3953616,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-06-08T17:59:39.58Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":69,"linesInsertedFromPrevious":116,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":86.1657414641231,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic \u0022Spam\u0022 Filter","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3953616","versionNumber":10,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3829711,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-30T16:00:19.057Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":2,"linesInsertedFromPrevious":2,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":78.6064944961108,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic \u0022Spam\u0022 Filter","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3829711","versionNumber":9,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3804361,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-28T19:58:15.033Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":3,"linesInsertedFromPrevious":2,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":86.3353239999851,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic \u0022Spam\u0022 Filter","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3804361","versionNumber":8,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3804142,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-28T19:33:32.6Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":31,"linesInsertedFromPrevious":53,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":92.3896920820698,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic \u0022Spam\u0022 Filter","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3804142","versionNumber":7,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3780041,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-27T03:39:11.37Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":0,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":67.0290020969696,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic \u0022Spam\u0022 Filter","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3780041","versionNumber":6,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3780012,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-27T03:34:20.17Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":1,"linesInsertedFromPrevious":1,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":79.2757398129907,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic \u0022Spam\u0022 Filter","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3780012","versionNumber":5,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3779982,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-27T03:29:44.497Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":10,"linesInsertedFromPrevious":15,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":86.2631329919677,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic \u0022Spam\u0022 Filter","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3779982","versionNumber":4,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3778619,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-26T23:04:42.547Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":4,"linesInsertedFromPrevious":4,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":84.1923651730176,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic Spam Classifier","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3778619","versionNumber":3,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3778285,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-26T22:02:09.15Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":39,"linesInsertedFromPrevious":53,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":34.3942250900436,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic Spam Classifier","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3778285","versionNumber":2,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":3777591,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"R","lastRunTime":"2018-05-26T20:09:28.603Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":342,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":41.5679918520618,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic Spam Classifier","url":"/cdeotte/titanic-spam-filter?scriptVersionId=3777591","versionNumber":1,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null}],"categories":{"categories":[{"id":13208,"name":"data visualization","displayName":"data visualization","fullPath":"analysis \u003e data visualization","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27data visualization%27","tagUrl":"/tags/data-visualization","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":130,"competitionCount":0,"scriptCount":6381,"totalCount":6511},{"id":13101,"name":"tutorial","displayName":"tutorial","fullPath":"audience \u003e tutorial","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27tutorial%27","tagUrl":"/tags/tutorial","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":27,"competitionCount":1,"scriptCount":3260,"totalCount":3288},{"id":13306,"name":"feature engineering","displayName":"feature engineering","fullPath":"machine learning \u003e feature engineering","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27feature engineering%27","tagUrl":"/tags/feature-engineering","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":44,"competitionCount":0,"scriptCount":1971,"totalCount":2015},{"id":13409,"name":"pca","displayName":"PCA","fullPath":"algorithms \u003e pca","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27pca%27","tagUrl":"/tags/pca","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":4,"competitionCount":0,"scriptCount":170,"totalCount":174}],"type":"script"},"submitToCompetitionInfo":null,"downloadAllFilesUrl":"/kernels/svzip/3955427","submission":null,"menuLinks":[{"href":"/cdeotte/titanic-spam-filter/notebook","text":"Notebook","title":"Notebook","tab":"notebook","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/cdeotte/titanic-spam-filter/code","text":"Code","title":"Code","tab":"code","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/cdeotte/titanic-spam-filter/data","text":"Data","title":"Data","tab":"data","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/cdeotte/titanic-spam-filter/output","text":"Output","title":"Output","tab":"output","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/cdeotte/titanic-spam-filter/comments","text":"Comments","title":"Comments","tab":"comments","count":2,"showZeroCountExplicitly":true,"reportEventCategory":null,"reportEventType":null},{"href":"/cdeotte/titanic-spam-filter/log","text":"Log","title":"Log","tab":"log","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/cdeotte/titanic-spam-filter/versions","text":"Versions","title":"Versions","tab":"versions","count":11,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/cdeotte/titanic-spam-filter/forks","text":"Forks","title":"Forks","tab":"forks","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null}],"rightMenuLinks":[],"callToAction":{"href":"/kernels/fork-version/3955427","text":"Fork Notebook","title":"Fork Notebook","tab":null,"count":null,"showZeroCountExplicitly":false,"reportEventCategory":"kernels","reportEventType":"anonymousKernelForkCreation"},"voteButton":{"totalVotes":13,"hasAlreadyVotedUp":false,"hasAlreadyVotedDown":false,"canUpvote":true,"canDownvote":false,"voteUpUrl":"/kernels/vote?id=1029920","voteDownUrl":null,"voters":[{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/419579-fb.jpg","displayName":"Ranjeet Jain","profileUrl":"/ranjeetjain3","tier":"Expert","tierInt":2,"userId":419579,"userName":"ranjeetjain3"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/682667-kg.jpg","displayName":"Reinhard","profileUrl":"/reisel","tier":"Contributor","tierInt":1,"userId":682667,"userName":"reisel"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/769452-kg.jpg","displayName":"Gabriel Preda","profileUrl":"/gpreda","tier":"Grandmaster","tierInt":4,"userId":769452,"userName":"gpreda"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1020983-kg.png","displayName":"Oscar Takeshita","profileUrl":"/pliptor","tier":"Expert","tierInt":2,"userId":1020983,"userName":"pliptor"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1155353-kg.jpg","displayName":"YouHan Lee","profileUrl":"/youhanlee","tier":"Expert","tierInt":2,"userId":1155353,"userName":"youhanlee"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1525668-kg.jpg","displayName":"Mykhailo Nedodai","profileUrl":"/sarmat","tier":"Contributor","tierInt":1,"userId":1525668,"userName":"sarmat"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1546000-kg.JPG","displayName":"YLT0609","profileUrl":"/ylt0609","tier":"Novice","tierInt":0,"userId":1546000,"userName":"ylt0609"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1767362-kg.jpg","displayName":"bhb2572","profileUrl":"/bhb2572","tier":"Contributor","tierInt":1,"userId":1767362,"userName":"bhb2572"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1873860-kg.jpeg","displayName":"Rupa Lahiri","profileUrl":"/rblcoder","tier":"Contributor","tierInt":1,"userId":1873860,"userName":"rblcoder"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1939212-gr.jpg","displayName":"Mia Iversen","profileUrl":"/chillfox","tier":"Novice","tierInt":0,"userId":1939212,"userName":"chillfox"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2208565-gr.jpg","displayName":"Pavlo Fesenko","profileUrl":"/pavlofesenko","tier":"Expert","tierInt":2,"userId":2208565,"userName":"pavlofesenko"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2726739-kg.png","displayName":"jiegeng","profileUrl":"/jiegeng94","tier":"Contributor","tierInt":1,"userId":2726739,"userName":"jiegeng94"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Ishani Chakrobarty","profileUrl":"/ishu789","tier":"Novice","tierInt":0,"userId":3198021,"userName":"ishu789"}],"currentUserInfo":null,"showVoters":true,"alwaysShowVoters":true},"parentDataSource":null,"parentName":"Titanic: Machine Learning from Disaster","parentUrl":"/c/titanic","thumbnailImageUrl":"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png","canWrite":false,"canAdminister":false,"datasetHidden":false,"forkParentIsRedacted":false,"forkDiffLinesChanged":0,"forkDiffLinesDeleted":0,"forkDiffLinesInserted":0,"forkDiffUrl":null,"forkParentAuthorDisplayName":null,"forkParentAuthorUrl":null,"forkParentTitle":null,"forkParentUrl":null,"canSeeDataExplorerV2":true,"canSeeRevampedViewer":true,"canSeeInnerTableOfContents":true,"simplifiedViewer":false,"kernelOutputDataset":null});performance && performance.mark && performance.mark("KernelViewer.componentCouldBootstrap");</script>

<form action="/cdeotte/titanic-spam-filter" id="__AjaxAntiForgeryForm" method="post"><input name="X-XSRF-TOKEN" type="hidden" value="CfDJ8LdUzqlsSWBPr4Ce3rb9VL-xSoSCfV2i2Vel_IYRmqfAcEY_-LPVSwkZAlwBmXZ4_QC_cCRKACHq7qnLHE1qGuADOedpOTg7rMd9NHgl1gqowu4HbIp8awbI4pocJL0lQwiVml0iJflPbDzcxwokPMo" /></form>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["STIX", "TeX"],
            linebreaks: {
                automatic: true
            },
            EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
        },
        tex2jax: {
            inlineMath: [["\\(", "\\)"], ["\\\\(", "\\\\)"]],
            displayMath: [["$$", "$$"], ["\\[", "\\]"]],
            processEscapes: true,
            ignoreClass: "tex2jax_ignore|dno"
        },
        TeX: {
            noUndefined: {
                attributes: {
                    mathcolor: "red",
                    mathbackground: "#FFEEEE",
                    mathsize: "90%"
                }
            }
        },
        Macros: {
            href: "{}"
        },
        skipStartupTypeset: true,
        messageStyle: "none"
    });
</script>
<script type="text/javascript" async crossorigin="anonymous" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



    </div>

        <div class="site-layout__footer">
            <footer class="site-footer">
    <div class="site-footer__content">
        <div class="site-footer__copyright">
            <span>&copy; 2019 Kaggle Inc</span>
        </div>
        <nav class="site-footer__nav">
            <a href="/team">Our Team</a>
            <a href="/terms">Terms</a>
            <a href="/privacy">Privacy</a>
            <a href="/contact">Contact/Support</a>
        </nav>
        <nav class="site-footer__social">
            <div data-component-name="SocialIcons" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push();performance && performance.mark && performance.mark("SocialIcons.componentCouldBootstrap");</script>
        </nav>
    </div>
</footer>

        </div>
</div>




</body>
</html>
