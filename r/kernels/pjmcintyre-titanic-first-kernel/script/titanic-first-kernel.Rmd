---
title: "titanic"
output: 
    html_document:
        number_sections: true
        toc: true
---

# My first kernel
With inspiration from many sources, including:

* https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic
* https://www.kaggle.com/jasonm/large-families-not-good-for-survival

I hope it's useful / interesting - or at least not too boring! :)

# Load libraries

```{r , warning=FALSE,message=FALSE}
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
library(data.table)
library(scales)
library(pander)
library(rpart)
library(pROC)
library(plyr)
require(randomForest)
```


# read files

```{r}
#list.files("../input")
trainData = read.csv(file='../input/train.csv', stringsAsFactors = F)
trainData = setDT(trainData)
summary(trainData)
testData = read.csv(file='../input/test.csv',stringsAsFactors = F)
testData = setDT(testData)
summary(testData)
trainData$sample = 'training'
testData$sample = 'testing'
trainData = rbind.fill(trainData,testData)
trainData = setDT(trainData)
```

# data prep

## Pclass
Change to factor
```{r}
trainData[,Pclass:=as.factor(Pclass),]
```

## names
Name comes in format: surname, title. firstname1 firstname2 ... firstnameN
```{r}
trainData$surname = gsub(',.*','',trainData$Name,perl=TRUE)
trainData$title = gsub('.*,\\s|\\..*','',trainData$Name) # (anything, comma, white space) or (full stop, anything) gets replaced with blank

head(trainData[,c('Name','surname','title')])
```

## cabin

Appears to be structured as letterNumber. Some people have several - make a note of it. Likewise, create a dummy flag for those with a cabin vs those without.

```{r}
trainData$cabinLetter = gsub('(\\d*)|\\s+','',trainData$Cabin)
trainData$cabinNumCabins = nchar(trainData$cabinLetter)
trainData$cabinLetter = substring(trainData$cabinLetter,1,1)
trainData$cabinMain = sapply(strsplit(trainData$Cabin,' '),function(x) {if (length(x)>0){x[[1]]}else{''}})
trainData$cabinNumber = as.numeric(gsub('\\D*','',trainData$cabinMain))
trainData$hasCabin = ifelse(trainData$cabinLetter == '',0,1)

head(trainData[,c('Cabin','cabinLetter','cabinNumber','hasCabin')])
```

## ticket
```{r}
head(trainData[,c('Ticket')])
```
Looks odd! Two styles, a number (ticket number?) and either text or text and number followed by ticket number.
```{r}
trainData$ticketNumber = as.numeric(gsub('^.* ','',trainData$Ticket))
trainData$ticketPrefix = gsub('\\d*$','',trainData$Ticket)

head(trainData[,c('Ticket','ticketNumber','ticketPrefix')])
```

## embarkation
```{r}
head(trainData[,c('Embarked')])
```

## family size
```{r familysize}
trainData$familySize = trainData$SibSp + trainData$Parch + 1
trainData[,numPplSameTicket := .N,.(ticketNumber)]
trainData[,travelGroupSize:=pmax(familySize, numPplSameTicket),]
trainData[,travellingSolo:=ifelse(travelGroupSize==1,1,0),]
```


# Missing values

## Useful data quality function
```{r}

checkColumn = function(df,colname){
  
  testData = df[[colname]]
  numMissing = max(sum(is.na(testData)|is.nan(testData)|testData==''),0)

  
  if (class(testData) == 'numeric' | class(testData) == 'Date' | class(testData) == 'difftime' | class(testData) == 'integer'){
    list('col' = colname,'class' = class(testData), 'num' = length(testData) - numMissing, 'numMissing' = numMissing, 'numInfinite' = sum(is.infinite(testData)), 'avgVal' = mean(testData,na.rm=TRUE), 'minVal' = round(min(testData,na.rm = TRUE)), 'maxVal' = round(max(testData,na.rm = TRUE)))
  } else{
    list('col' = colname,'class' = class(testData), 'num' = length(testData) - numMissing, 'numMissing' = numMissing, 'numInfinite' = NA,  'avgVal' = NA, 'minVal' = NA, 'maxVal' = NA)
  }
  
}
checkAllCols = function(df){
  resDF = data.frame()
  for (colName in names(df)){
    resDF = rbind(resDF,as.data.frame(checkColumn(df=df,colname=colName)))
  }
  resDF
}
```

## Review the data quality
```{r results='asis'}
pandoc.table(checkAllCols(trainData))
```

Looks like we need to deal with the following missing data:

* Age
* Cabin (and derived vars)
* Embarked
* Ticket number and ticket prefix
* Fare

## Age

Try to impute missing ages using a simple decision tree. First we'll create some useful dummy variables based on simple logical guesses in case it helps.

* If (number of spouses or siblings > 1) then it's probably a child
* If (number of parents or children > 2) then it's probably an adult

```{r}
trainData[,likelyChild:=ifelse(SibSp > 1,1,
                        ifelse(Parch > 2,0,
                                        3)),] #Use '3' to denote 'no clues'
```

Now build the model:

```{r}
ageTree = rpart(Age ~ Fare + Pclass + Embarked + title + SibSp + Parch + travelGroupSize + likelyChild,data = trainData)
rpart.plot::rpart.plot(ageTree)
trainData$predAge = predict(ageTree,trainData)
ggplot(data=trainData,aes(x=Age,y=predAge)) + geom_point() + geom_smooth()
```

And impute the missing values and tidy up after ourselves:

```{r}
trainData[is.na(Age),Age := predAge,]
trainData[,predAge := NULL,]
trainData[,likelyChild := NULL,]
summary(trainData$Age)
```

Age sorted!

(TODO: explore whether average age of passengers in same ticketNumber and with same Parch and SibSp is useful. It ought to be since logically the average age of one's partner or siblings is a good predictor for your own age)

## Cabin

Not everyone has a Cabin - but that sounds reasonable? Presumably third class missed out on some of the champagne lifestyle of the first class passengers.
```{r}
trainData[Cabin=='',c('Cabin','cabinMain','cabinLetter','cabinNumCabins','cabinNumber','hasCabin'):=.('U','U','U',0,0,0)]
trainData[is.na(cabinNumber),cabinNumber:=0,]
```

## Embarked

Class is a good predictor of Embarked:
```{r echo=FALSE}
ggplot(data=trainData,aes(x=Embarked)) + geom_bar() + facet_grid(Pclass ~ .,scales='free_y')
```

Only two people are missing Embarked, both on the same ticket. Unfortunately no-one else is travelling on that Ticket or we would just use that Embarked point.
```{r}
trainData[Embarked == '',,] # travelling on same ticket
trainData[ticketNumber == 113572,,] # anyone else on same ticket?
#No, bother
```

Build a simple model to predict Embarked point for First Class passengers based on Fare. They're both in first class, so restrict the model to first class passengers only.
```{r, results='asis'}
embarked1stTree = rpart(Embarked ~ Fare,data = trainData[Pclass==1,,])
rpart.plot::rpart.plot(embarked1stTree)
trainData$predictedEmbarkationPoint = predict(embarked1stTree,trainData,type="class")
pandoc.table(table(trainData[Pclass==1,.(Embarked,predictedEmbarkationPoint),]))
```

And update the missing variables

```{r}
#trainData[Embarked=='',Embarked:=predict(embarked1stTree,.SD,type='class')]
trainData[Embarked=='',Embarked:=predictedEmbarkationPoint,]
trainData[,predictedEmbarkationPoint:=NULL,]
```

## Tickets

Simple notes of missing info.
```{r}
trainData[is.na(ticketNumber),ticketNumber:=0,]
trainData[ticketPrefix == '',ticketPrefix:='Unk',]
```

## Fare

One fare is missing and it's in our testing dataset:
```{r}
trainData[is.na(Fare),.N,.(sample)]
```

There are also some 0 fares that suggest a data error (I suppose there could've been a 'golden ticket' style lottery, but let's assume it's a data issue for now).
```{r}
trainData[Fare==0,.N,.(sample)]
```

So let's build a simple model to fix it.

```{r}
fareTree = rpart(Fare ~ Age + Pclass + Embarked + title + SibSp + Parch + travelGroupSize,data = trainData)
rpart.plot::rpart.plot(fareTree)
trainData$predFare = predict(fareTree,trainData)
ggplot(data=trainData,aes(x=Fare,y=predFare)) + geom_point() + geom_smooth()
```

And impute the missing values and tidy up after ourselves:

```{r}
trainData[is.na(Fare) | Fare==0,Fare := predFare,]
trainData[,predFare := NULL,]
summary(trainData$Fare)
```

## Check we've fixed everything
```{r results='asis'}
pandoc.table(checkAllCols(trainData))
```


# data exploration

## useful functions
```{r}

#logit
logit <- function(pr) {
  #pr = pmax(pmin(pr,0.99999999999),0.00000000001)
  log(pr/(1-pr)) 
} 

#format graph
formatGraph = function(x,addYPctFmt=FALSE,ymin=NA,ymax=NA){
  x = x + 
    theme(panel.grid.minor = element_blank(), 
          panel.grid.major = element_line(color='gray50',linetype = 'dashed'),
          panel.grid.major.x = element_blank()) +
    theme(panel.background = element_blank())
  
  if (addYPctFmt==TRUE){
    #x = x + 
    #  scale_y_continuous(limits=c(0,1), labels=percent) 
    if (!is.na(ymin) | !is.na(ymax)){
      x = x +
        scale_y_continuous(labels=percent,limits=c(ymin,ymax))
    } else{
       x = x +
           scale_y_continuous(labels=percent)
    }
  } else {
    if (!is.na(ymin) | !is.na(ymax)){
      x = x +
        scale_y_continuous(limits=c(ymin,ymax))
    } 
  }
  
  x
}

#plot a barplot
barPlot = function(x,y,data){
    #pandoc.header(paste0('\n logitBar plotting ',x,'\n'),3)
    pandoc.p(' \n Colour and labels show percentage of population in each category \n')
    
    plotData = data[!is.na(get(y)),.(meanY = mean(get(y),na.rm=TRUE),num=.N),.(get(x))]
    names(plotData) = c(x,y,'num')
    plotData[,totNum:=sum(num),]
    plotData[,pct:=round(num/totNum,2),]
    plotData[,logitY:=logit(get(y)),]
    
    ggp1 = ggplot(data=plotData,aes(x=get(x),y=get(y),colour=pct,fill=pct)) + geom_bar(stat='identity') + labs(x=x,y=y) + geom_label(aes(label=pct),fill='white')
    
    print(formatGraph(ggp1))
}

#plot a graph of logit odds against density
logitloess <- function(x, y, data,plotDensity=TRUE) { 
  
  #pandoc.p(paste0(' \n Plotting ',y,' against ',x))
  
  xvals = data[[x]]
  yvals = data[[y]]

  # data fixes
  xvals = xvals[!is.na(yvals)]
  yvals = yvals[!is.na(yvals)]
  

  if (length(unique(xvals))>20){
  
  
    #message('setting quantiles')
    xquantiles = quantile(xvals,prob=c(0.05,0.95),na.rm=TRUE)
  
    #message('loessfit - building local regression model')
    loessfit <- predict(loess(yvals~xvals,span = 0.7)) 
    loessfit2 = predict(loess(yvals~xvals,span = 0.2))
    pi <- pmax(pmin(loessfit,0.99999999),0.0000001) 
    pi2 = pmax(pmin(loessfit2,0.99999999),0.0000001)
    logitfitted <- logit(pi) 
    logitfitted2 = logit(pi2)
  
  
    yquantiles = quantile(logitfitted2,prob=c(0.05,0.95),na.rm=TRUE)
    t=data.frame(xaxis=xvals[!is.na(xvals)],yaxis=logitfitted,yaxis2=yvals[!is.na(xvals)],yaxis3=logitfitted2)
    t=t[t$xaxis>xquantiles[1] & t$xaxis < xquantiles[2],]
    #message('printing graph')
    ggp1 = ggplot(data=t,aes(x=xaxis,y=yaxis)) + geom_line() + ylab(paste("logodds(",y,")")) + xlab(x) + geom_line(aes(y=yaxis3),linetype='dashed',alpha=0.3) + geom_point(aes(y=yaxis3),alpha=0.3,colour='red') + geom_hline(yintercept = 0)  
    ggp2 = ggplot(data=t,aes(x=xaxis,y=yaxis)) + ylab(paste("density(",y,")")) + xlab(x) + geom_density(colour = 'red',fill='red',alpha=0.05,aes(y=..scaled..),show.legend =FALSE) 
  
    print(formatGraph(ggp1,ymin = yquantiles[1],ymax=yquantiles[2]))
    if (plotDensity){
        print(formatGraph(ggp2,addYPctFmt=TRUE))
    }
  }else{
    barPlot(x,y,data)
  }
    
  
}

```


## exploration

For this section I'm just going to run some automated code over all the available characteristics to see if I can get a feel for how it works and where some data transformations might be needed. As a result of the code being automated I'm not going to be able to comment on each characteristic individually, so pick up the commentary again in the next section.

```{r dataexploration, results='asis', error=FALSE, warning=FALSE, message=FALSE}
for (n in names(trainData)){
    #message('examining ',n,' for plotting')
    pandoc.header(n,3)
    if (class(trainData[[n]]) == 'numeric' | class(trainData[[n]]) == 'integer'){
        logitloess(n,'Survived',trainData)
    } else {
        barPlot(n,'Survived',trainData)
    }
    pandoc.p(' \n ')
}
```

# data prep based on data exploration

## Fare

Fare looks like it could benefit from a log transformation.

Before:

```{r fare1, echo=FALSE,warning=FALSE}
logitloess('Fare','Survived',trainData,plotDensity=FALSE)
```

After:

```{r fare2, results='asis',warning=FALSE}
trainData[,fareLn := log(pmax(0.01,Fare)),]
logitloess('fareLn','Survived',trainData)
```

Looks a bit better. Maybe bucketing might also help here.

```{r fare3, results='asis'}
trainData$fareQuantile = cut(x=trainData$Fare,quantile(x=trainData$Fare,na.rm=TRUE),include.lowest=TRUE) #withotut include.lowest we end up with NAs
logitloess('fareQuantile','Survived',trainData)
```


## Age

Age isn't linear in survival probability. This will cause problems for the logistic regression. Instead, let's bucket ages.

Before:

```{r, echo=FALSE, results='asis',warning=FALSE}
logitloess('Age','Survived',trainData,plotDensity=FALSE)
```


After:


```{r age3, results='asis'}
trainData$ageBucket = ifelse(trainData$Age < 16, '<16',
                        ifelse(trainData$Age < 22, '16 - 22',
                        ifelse(trainData$Age < 35, '21 - 35',
                        ifelse(trainData$Age < 50, '35 - 50',
                        ifelse(trainData$Age >= 50, '> 50','Unk')))))


#change order for plotting
trainData$ageBucketFactor = factor(trainData$ageBucket, levels = unique(trainData$ageBucket[order(trainData$Age)]))
logitloess('ageBucketFactor','Survived',trainData)
#delete factor to avoid model picking it up accidentally - instead it'll get the raw ageBucket
trainData[,ageBucketFactor:=NULL,]
```

## Title

Title looks promising, but could do with some consolidation:

```{r titleconsol,echo=FALSE, results='asis'}
barPlot('title','Survived',trainData)
```

First let's build a list of titles sorted by Sex and Prob(survival). This will help us choose which titles to combine.

```{r}

t = trainData[,.(num = .N, pctSurvived =sum(Survived)/sum(!is.na(Survived))),.(title,Sex)] 
setkeyv(t,c('Sex','pctSurvived')) 
t

```

Okay, so taking the ladies first. There are two main titles ('Mrs' and 'Miss') and a bunch of less common ones. Let's try to combined them as best we can.

```{r}
trainData[title %in% c('Mme','Ms','Lady','the Countess','Dr','Dona') & Sex=='female',title:='Mrs',]
trainData[title %in% c('Mlle') & Sex=='female',title:='Miss',]
```

What does the picture now look like?

```{r, echo=FALSE}
t = trainData[,.(num = .N, pctSurvived =sum(Survived)/sum(!is.na(Survived))),.(title,Sex)] 
setkeyv(t,c('Sex','pctSurvived')) 
t
```

Ah, much better on female titles. Only two, with a noticable difference in survival chance. On to the men.

Again, we have two main titles ('Mr' and 'Master'), typically differentiated by age. The other titles are rare enough I don't trust their surival probabilities to be accurate (if there's a 'Sir' in the test sample I'm not *certain* that he would survive) so I'm going to merge them all up to the two main titles.

In fact, the other titles all look like professional or inherited titles, which suggests that the owners are more likely to be 'Mr' than 'Master', so let's update to that.

```{r}
trainData[!(title %in% c('Mr','Master')) & Sex == 'male',title:='Mr',]
```
```{r, echo=FALSE}
t = trainData[!is.na(Survived),.(num = .N, pctSurvived =sum(Survived)/sum(!is.na(Survived))),.(title,Sex)] 
setkeyv(t,c('Sex','pctSurvived')) 
t
```

Looks much happier!

```{r, echo=FALSE, results='asis'}
barPlot('title','Survived',trainData)
```


# Explore the data

## Layout of the Titanic

I wonder whether there is a trend in Cabin Number against the Survival chance? Perhaps cabins closer to escape routes did better?

```{r cabinPlot, warning=FALSE, error=FALSE}
ggplot(data = trainData,aes(x=cabinNumber,y=Survived,colour=Fare,fill=Fare,alpha=Fare)) + geom_point() + geom_smooth(colour='forestgreen') + facet_grid(cabinLetter ~ .) + scale_y_continuous(limits=c(0,1))
```

And does that change when split by passenger class?

```{r cabinPlot2, warning=FALSE, error=FALSE}
ggplot(data = trainData,aes(x=cabinNumber,y=Survived,colour=Fare,fill=Fare,alpha=Fare)) + geom_point() + geom_smooth(colour='forestgreen') + facet_grid(cabinLetter ~ factor(Pclass)) + scale_y_continuous(limits=c(0,1))
```

Interesting... but not particularly helpful. Why are there cabins in third class? Where did the second class passengers sleep?

## Interaction of class and gender

The interaction of class and gender looks interesting:
```{r}
t = trainData[!is.na(Survived),.(num = .N, survivedPct = mean(Survived)),.(Pclass,Sex)]
ggplot(data = t, aes(x=Pclass,y=survivedPct,fill=num,colour=num)) + geom_bar(stat='identity') + facet_grid(Sex ~ .)

```

If you were female then the key transition appears to be from 2nd to 3rd class. If male, then from 1st to 2nd.

Let's use this and give the model a dummy variable to pick this interaction up.

```{r, results='asis'}
trainData[Pclass %in% c('1','2') & Sex == 'female',iSexClass := 'femaleUpper',]
trainData[Pclass == '3' & Sex == 'female',iSexClass := 'femaleLower',]
trainData[Pclass == '1' & Sex == 'male',iSexClass := 'maleUpper',]
trainData[Pclass %in% c('2','3') & Sex == 'male',iSexClass := 'maleLower',]
barPlot('iSexClass','Survived',trainData)
```

## Interaction of travelGroupSize and class

Travel group size is predictive of survival:
```{r echo=FALSE,results='asis'}
barPlot('travelGroupSize','Survived',trainData)
```

What does it look like if we control for class? Is it possible that large travel groups (families) are just a proxy for class?

```{r}
t = trainData[!is.na(Survived),.(survProp=mean(Survived,na.rm=TRUE),meanFare=mean(Fare,na.rm=TRUE)),.(travelGroupSize,Pclass)]
ggplot(data = t,aes(x=travelGroupSize,y=survProp,colour=meanFare,fill=meanFare,alpha=meanFare)) + geom_bar(stat='identity') + facet_grid(factor(Pclass) ~ .) 

```

Well... that's interesting. Why would Fare *increase* with family size? Unless Fare is what was paid for the entire party? 

Let's check:

```{r}
trainData[travelGroupSize > 3 & Pclass == 1 & substr(Name,1,6)=='Carter',.(Name,Sex,Age,Ticket,Fare),]
```

Yep! Every member travelling on the same ticket has the same Fare size. Let's try adjusting Fare for that and seeing how it looks before / after vs Pclass.

```{r}
trainData[,fareAdj:=Fare / numPplSameTicket,]
trainData[,fareAdjLn:=log(fareAdj+1),] #+1 to avoid 0 fares causing -Infinities
ggplot(data=trainData[Fare < quantile(trainData$Fare[trainData$Pclass ==1])[4],,],aes(x=Fare,colour=Pclass,fill=Pclass)) + geom_density() + facet_grid(Pclass ~ .) + scale_y_continuous(limits=c(0,1))
ggplot(data=trainData[fareAdj < quantile(trainData$Fare[trainData$Pclass ==1])[4],,],aes(x=fareAdj,colour=Pclass,fill=Pclass)) + geom_density() + facet_grid(Pclass ~ .) + scale_y_continuous(limits=c(0,1))
```

Third class now looks a look more sensible - everyone paying the same Fare. The first / second class distribution might be associated with different cabin qualities?

Does embarkation point explain remaining difference?

```{r}
ggplot(data=trainData[fareAdj < quantile(trainData$Fare[trainData$Pclass ==1])[4],,],aes(x=fareAdj,colour=Pclass,fill=Pclass)) + geom_density() + facet_grid(Pclass ~ Embarked,scales='free_y') 
#trainData[,.(avg=mean(fareAdj),sd=sd(fareAdj)) ,keyby=.(Pclass,Embarked,ageBucket)]
```

Starting to look pretty good.

One final check: how does the new fare look against survival?

```{r,results='asis',warning=FALSE}
logitloess('fareAdj','Survived',trainData)
logitloess('fareAdjLn','Survived',trainData) #-Inf in there
```

Very low fares appear to survive better than expected - this may be because they have the lowest average age (and therefore may have been priortised for lifeboats)?

```{r,results='asis'}
barPlot('ageBucket','fareAdj',trainData[Pclass==3,,])
```



# Re-split the data
```{r}
testData = trainData[sample=='testing',,]
trainData = trainData[sample=='training',,]
```

# Final checks

## Check we've not broken anything

Have we introduced any missing, NA, NaNs? These may foul up the modelling process.

```{r results='asis',warning=FALSE}
pandoc.table(checkAllCols(trainData))
```

## Compare train and test datasets

Unusually for modelling exercises we know what the test data looks like. If it is skewed differently to the development dataset we should consider re-weighting the development dataset to reflect the test population:

```{r results='asis',warning=FALSE}
convertToFactors = function(dt){
  for (c in names(dt)){
    if (class(dt[,get(c),]) == 'character'){
      dt[,c(c):=as.factor(get(c)),]
    }
  }
}
profileByGraph = function(trainData,testData){
  bothDT = as.data.table(rbind.fill(trainData,testData))
  varlist = names(bothDT)
  varlist = varlist[!(varlist %in% c('Name','Ticket','Cabin','surname','cabinMain','ticketPrefix'))]
  bothDT[,sample:=ifelse(is.na(Survived),'Dev','Test'),]
  convertToFactors(bothDT)
  for (v in varlist){
    #message(v)
    if (length(unique(bothDT[[v]])) < 30){
      print(formatGraph(ggplot(data=bothDT,aes(x=get(v))) + geom_histogram(stat='count') + facet_grid(sample ~ .,scales = 'free_y') + ggtitle(v)))
    }else{
      print(formatGraph(ggplot(data=bothDT,aes(x=get(v))) + geom_histogram() + facet_grid(sample ~ .,scales = 'free_y') + ggtitle(v)))
    }
  }
}
profileByGraph(trainData,testData)
```

PassengerId is clearly something we don't want in any model!

# Build models

## Useful functions
```{r}

getPredictiveVars = function(df,addExcludedVars=c()){
  
  excludedVars = c('sample',
                   'Survived',
                   'Name',
                   'Ticket',
                   'Cabin',
                   'cabinMain',
                   'surname',
                   'ticketPrefix',
                   'ticketNumber',
                   'passengerId',
                   addExcludedVars
                   )
  
  allVars = names(df)
  
  predictorVars = allVars[!(allVars %in% excludedVars)]
  
  predictorVars
  
}

getPredictionFormula = function(df,targetVar,addExcludedVars=c()){
  varlist = getPredictiveVars(df=df,addExcludedVars)
  
  f = paste(targetVar,'~')
  for (v in varlist){
    f = paste(f,v,'+')
  }
  f = substr(x=f,start = 0,stop = nchar(f)-1)
  as.formula(f)
}
```

## The 'kitchen sink' model (aka throw everything at it)
```{r}
logisticSurvivalModel = glm(getPredictionFormula(trainData,'Survived'),data=trainData,family='binomial')
summary(logisticSurvivalModel)
roc(response = logisticSurvivalModel$y, predictor = logisticSurvivalModel$fitted.values,plot=TRUE, auc.polygon=TRUE, grid=TRUE,print.auc=TRUE)
```

## Reducing over-fitting
```{r}
logisticSurvivalModelReduced = step(logisticSurvivalModel,direction='backward')
summary(logisticSurvivalModelReduced)
roc(response = logisticSurvivalModelReduced$y, predictor = logisticSurvivalModelReduced$fitted.values,plot=TRUE, auc.polygon=TRUE, grid=TRUE,print.auc=TRUE)
```

Well that didn't seem to be very successful. We still have Fare, fareLn, fareAdjLn and fareAdj in the model. Neither has much power, but that might just be  because they were all included. With just one I'd expect to see it being more powerful.

Interestingly, we also have numPplSameTicket and travelGroupSize both in, and both pulling in different directions (-1.707497 and 1.235890). One is indicating that larger travel groups indicate a greater survival chance and the other that larger groups predict a lower survival rate. This suggests strongly to me that the model is over-fit. Although there is a chance that there is an effect captured in the difference between the two definitions I think it unlikely that it is extremely powerful.

Our final issue is that one of our derived variables (iSexClass) is a linear combination of other variables. This is why its parameter value cannot be estimated:
```{r}
alias(logisticSurvivalModelReduced)
```

## manual removal of correlated vars

I'm going to remove the original Fare and fareLn variables, and the numPplSameTicket and see if that improves matters.

I'm also going to remove title (!) because I think it's causing issues with our dummy variable iSexClass.

```{r}
logisticSurvivalModelManual = glm(getPredictionFormula(trainData,'Survived',addExcludedVars=c('Fare',
                                                                                            'fareLn',
                                                                                            'fareAdj',
                                                                                            'Embarked',
                                                                                            'title',
                                                                                            'Sex',
                                                                                            'numPplSameTicket',
                                                                                            'familySize',
                                                                                            'SibSp',
                                                                                            'Age',
                                                                                            'Parch')),
                                                                data=trainData,
                                                                family='binomial')
logisticSurvivalModelManual = step( logisticSurvivalModelManual,direction='backward')
summary(logisticSurvivalModelManual)
roc(response = logisticSurvivalModelManual$y, predictor = logisticSurvivalModelManual$fitted.values,plot=TRUE, auc.polygon=TRUE, grid=TRUE,print.auc=TRUE)

```

How accurate is it on our development dataset?

```{r ,results='asis'}
predSurvived = ifelse(logisticSurvivalModelManual$fitted.values > 0.5,'predSurvived','predDied')
pandoc.table(table(logisticSurvivalModelManual$y,predSurvived))
```

# Prediction time

```{r}
prediction <- predict(logisticSurvivalModelManual, testData,type='response')
predictionB = ifelse(prediction > 0.5,1,0)
solution <- data.frame(PassengerID = testData$PassengerId, Survived = predictionB)
write.csv(solution, file = 'logisticSurvivalModelManual_Prediction.csv', row.names = F)
```

0.76077.

On to a random forest!

# Random Forest

## Dev

Random Forests need factors as arguments:
```{r}
bothDT = as.data.table(rbind.fill(trainData,testData))
convertToFactors(bothDT)
testData = bothDT[is.na(Survived),,]
trainData = bothDT[!is.na(Survived),,]
```

```{r}
myPredForm = getPredictionFormula(trainData,'as.factor(Survived)')
myrandomForest = randomForest(myPredForm,
                      data=trainData, 
                      importance=TRUE, 
                      ntree=200)
varImpPlot(myrandomForest)
predictions  = predict(myrandomForest, trainData,type= 'prob')[,2]
roc(response = trainData$Survived, predictor = predictions, plot=TRUE, auc.polygon=TRUE, grid=TRUE,print.auc=TRUE)
```

This looks... too good to be true. No way will the model perform that well on unseen data.

## Prediction

Now make our predictions:
```{r}
prediction <- predict(myrandomForest, testData,type='response')
solution <- data.frame(PassengerID = testData$PassengerId, Survived = prediction)
write.csv(solution, file = 'randomForestModel_Prediction.csv', row.names = F)

```

0.78947. An improvement on logistic regression.

## Manually tune the random Forest

How about we do some manual tuning of the random Forest? From above we can definitely see some cross-correlations...

* fareAdj, fareAdjLn, fareLn
* numPplSameTicket with familySize and travelGroupSize
* PassengerId (no logical reason this should be predictive as far as know, treat as overfitting)

```{r}
myPredForm = getPredictionFormula(trainData,'as.factor(Survived)',addExcludedVars=c('Fare',                                                                                   'fareLn',
                                                                                    'fareAdj',
                                                                                    'fareLn',
                                                                                    'Sex',
                                                                                    'Pclass',
                                                                                    'title',
                                                                                    'familySize',
                                                                                    'Parch',
                                                                                    'SibSp',
                                                                                    'numPplSameTicket',                                                                               'familySize',                                                                                    'SibSp',                                                                                     'Parch',
                                                                                    'PassengerId'))
myTunedrandomForest = randomForest(myPredForm,
                      data=trainData, 
                      importance=TRUE, 
                      ntree=200)
plot(myTunedrandomForest)
varImpPlot(myTunedrandomForest)
Tunedpredictions  = predict(myTunedrandomForest, trainData,type= 'prob')[,2]
roc(response = trainData$Survived, predictor = Tunedpredictions, plot=TRUE, auc.polygon=TRUE, grid=TRUE,print.auc=TRUE)
prediction <- predict(myTunedrandomForest, testData,type='response')
solution <- data.frame(PassengerID = testData$PassengerId, Survived = prediction)
write.csv(solution, file = 'randomForestTunedModel_Prediction.csv', row.names = F)

```

0.76555. Still not an improvement on the logistic regression!

# random forest with re-weighting of the survivorship

Maybe the model will work better if we re-sample the data to balance the classes out?
````{r}
trainData[,.N,Survived]
library(caret)
set.seed(42)
trainData2 = upSample(as.data.frame(trainData)[,getPredictiveVars(trainData)],
                      as.factor(trainData$Survived))
table(trainData2$Class)
trainData2$Survived = trainData2$Class
myPredForm = getPredictionFormula(trainData,'as.factor(Survived)',addExcludedVars=c('Fare',                                                                                   'fareLn',
                                                                                    'fareAdj',
                                                                                    'fareLn',
                                                                                    'Sex',
                                                                                    'Pclass',
                                                                                    'fareQuantile',
                                                                                    'numPplSameTicket',                                                                               'familySize',                                                                                    'SibSp',                                                                                     'Parch',
                                                                                    'PassengerId'))
myTunedrandomForest = randomForest(myPredForm,
                      data=trainData2, 
                      importance=TRUE, 
                      ntree=200)
plot(myTunedrandomForest)
varImpPlot(myTunedrandomForest)
Tunedpredictions  = predict(myTunedrandomForest, trainData2,type= 'prob')[,2]
roc(response = trainData2$Survived, predictor = Tunedpredictions, plot=TRUE, auc.polygon=TRUE, grid=TRUE,print.auc=TRUE)
prediction <- predict(myTunedrandomForest, testData,type='response')
solution <- data.frame(PassengerID = testData$PassengerId, Survived = prediction)
write.csv(solution, file = 'randomForestResampledModel_Prediction.csv', row.names = F)


```
0.76555.

Logistic regression:
```{r logisticregressionwtihresampling}
logisticSurvivalModelManual = glm(getPredictionFormula(trainData,'Survived',addExcludedVars=c('Fare',
                                                                                            'fareLn',
                                                                                            'iSexClass',
                                                                                            'numPplSameTicket',
                                                                                            'familySize',
                                                                                            'SibSp',
                                                                                            'Parch',
                                                                                            'iSexClass')),
                                                                data=trainData2,
                                                                family='binomial')
logisticSurvivalModelManual = step( logisticSurvivalModelManual,direction='backward')
summary(logisticSurvivalModelManual)
roc(response = logisticSurvivalModelManual$y, predictor = logisticSurvivalModelManual$fitted.values,plot=TRUE, auc.polygon=TRUE, grid=TRUE,print.auc=TRUE)
prediction <- predict(logisticSurvivalModelManual, testData,type='response')
predictionB = ifelse(prediction > 0.5,1,0)
solution <- data.frame(PassengerID = testData$PassengerId, Survived = predictionB)
write.csv(solution, file = 'logisticSurvivalModelManualResampled_Prediction.csv', row.names = F)
```

0.74641. Ho-hum.

# Final performances by model type

```{r finalmodelperformances,echo=FALSE,results='asis'}
modelPerf = as.data.table(list(c('manual logistic','random forest', 'manual random forest','resampled RF','resampled logistic'),c(0.76077,0.78947,0.76555,0.76555, 0.74641)))
names(modelPerf) = c('modelName','accuracyOnUnseenData')
#ggplot(data=modelPerf,aes(x=modelName,y=accuracyOnUnseenData,fill=modelName,colour=modelName)) + geom_bar(stat='identity') + scale_y_continuous(limits=c(0,1))
pandoc.table(modelPerf)
```

I guess the 'best' model is the random forest, but what the convergence of scores says to me is that we're at or near the limits of the available predictors. There are 418 test cases in total, so 209 in the public test set (and 209 in the private one). So to get a roughly 1% improvement in accuracy means flipping two people. This starts to get very close to optimising for noise. I've seen people get scores > 80% by constructing new data (e.g. what the average surivival rate of the travel groups were) but that feels a little like 'cheating' to me, and pretty close to 'overfitting' (it only has power because of 'leakage' from the seen training data to the unseen test data).

I'm probably going to leave this one here and start looking at the digit recogniser and try to teach myself something about neural networks. I hope some of this was useful and/or interesting!

Thanks for reading and have fun! :)


# ANNEX: excessive optimisation
Ok, I couldn't resist :)

Be warned, these new chars are all almost certain to over-fit to the data. Sigh.


## Reassemble datasets
```{r}
trainData$type = 'train'
testData$type = 'test'
trainData = as.data.table(rbind.fill(trainData,testData))
```

## Do neighbouring cabins tell us anything helpful?

Let's add a feature to the dataset that tries to predict surival based on the cabin number. The theory is that some cabins are nearer the escape routes and so easier to escape from.

```{r}
trainData[,cabinLetterClean:=cabinLetter,]
trainData[cabinLetter %in% c('G','T'),cabinLetterClean:='U',]
for (cl in unique(trainData$cabinLetterClean)){
  #message(cl)
  cabinSurvTree = rpart(Survived ~ cabinNumber ,data = trainData[cabinLetterClean==cl,,])
  #rpart.plot::rpart.plot(cabinSurvTree)
  predictions = predict(cabinSurvTree,trainData[cabinLetterClean==cl,,])
  trainData[cabinLetterClean==cl,cabinSurvProb:=predictions,]
}
trainData[,cabinLetterClean:=NULL,]
```

## Does information about how other passengers on the same ticket did help us?

```{r}
trainData[,ticketSurvProb:=mean(Survived,na.rm=TRUE),.(ticketNumber)]

trainData[is.nan(ticketSurvProb),ticketSurvProb:=mean(trainData$Survived,na.rm=TRUE),] #fix for those without a ticket in the training set
```

## And make our prediction

### Re-split the data
```{r}
testData=trainData[type=='test',,]
trainData=trainData[type=='train',,]
testData[,type:=NULL,]
trainData[,type:=NULL,]
```


### Make the prediction

I'm excluding our ticketSurvProb because with it in the model *really* overfits (accuracy drops to 70.8%).

```{r}
convertToFactors(trainData)
myPredForm = getPredictionFormula(trainData,'as.factor(Survived)',addExcludedVars=c('Fare',                                                                                          'fareLn',
                                                                                    'fareAdj',
                                                                                    'iSexClass',
                                                                                    'title',
                                                                                    'familySize',
                                                                                    'ticketSurvProb',
                                                                                    'Parch',
                                                                                    'SibSp',
                                                                                 'numPplSameTicket',                                                               
                                                                                    'PassengerId'))
myOverFittedRF = randomForest(myPredForm,
                      data=trainData, 
                      importance=TRUE, 
                      ntree=200)
plot(myOverFittedRF)
varImpPlot(myOverFittedRF)
OverFittedpredictions  = predict(myOverFittedRF, trainData,type= 'prob')[,2]
roc(response = trainData$Survived, predictor = OverFittedpredictions, plot=TRUE, auc.polygon=TRUE, grid=TRUE,print.auc=TRUE)
prediction <- predict(myOverFittedRF, testData,type='response')
solution <- data.frame(PassengerID = testData$PassengerId, Survived = prediction)
write.csv(solution, file = 'overfittedRF_Prediction.csv', row.names = F)

```

