---
title: "Titanic Dataset: EDA, Feature Engineering & Random Forests"
author: 'Liam Morgan'
date: '01 Apr 2019'
output:
  html_document:
    number_sections: true
    toc: true
    theme: readable
    highlight: haddock
---

# **Introduction**

## **Some Background on Me**
I'm a 23-year-old Maths graduate, living & working in the UK. I started my first and current position as an Insight Analyst ~19 months ago as of writing this and have primarily worked in SAS, SQL & Excel. [You can find me on linkedin here.](https://www.linkedin.com/in/liam-morgan-5168b413a/)  
About 9 months ago I decided I would start learning R and to expand my skills more into Machine Learning, so I worked through the 'Data Scientist with R' career track on DataCamp.  
I now do a lot of my day-to-day work in R, but this is my first independent ML project, and it's also my first Kaggle competition & kernel.  
Hopefully someone else can find this useful and perhaps give me some feedback!  


## **Methodology**
Since the dataset dimensions are pretty small, I started by getting an **overview of the data** and visualizing individual features using the `ggplot2` package, gathering ideas of new features I could create from existing ones.  
I go on to do some **feature engineering**, creating new variables in an attempt to improve the predictive power of the data available. I thought about simply using the `missForest` package to successively **impute missing data**, but for practice I decided to try a few different approaches depending on the variable.  

For all my predictions (both for imputation & final model), I used the `caret` package to build and tune a **random forest**, an ensemble method that is a collection of simple decision trees, built on different samples (with replacement) of the training data, and selecting random samples of the predictor variables. At each decision node in every tree, the variable from the random subset of variables that provides the best split of the data is chosen.  

<center><img src="https://s3.amazonaws.com/cdn.freshdesk.com/data/helpdesk/attachments/production/6018025962/original/RF.png?1458020765" style="width: 800px;"/></center>

For **prediction**, each test observation is passed through every decision tree in the forest. The trees **'vote'** on an outcome (Survived & Died in this case), and the votes of all the trees are collated. The outcome with the most votes is our prediction for that observation.  

Random forests can deal with outliers and skewed data quite well, are quick to train & tune and will generally out-perform traditional methods such as a grown & pruned decision tree, whilst also being harder to over-fit, but what they gain in predictive power they lose in interpretability (they are very "black box").  


## **Goals of the analysis**

*	**Import, combine and explore** the full dataset (single variable plots, missing values, variable interactions)
*	**Feature Engineering & Missing Value Imputation**: Creating new variables from existing variables, imputing missing variables with different methods.
*	Build and tune a **Random Forest** for 'Survived' on the training data, make our predictions and choose a final model to submit for the hidden dataset.

# **Import, Combine and Explore**

## **Importing, Combining, Data Overview**

```{r message = FALSE, warning = FALSE}
library(readr) # import
library(dplyr) # data manipulation
library(ggplot2) # visualizations
library(gridExtra) # visualizations
library(tictoc) # timing models
library(caret) # tuning & cross-validation

theme_set(theme_light())

train_titanic <- read_csv("../input/train.csv") %>%
  mutate(Test_Data = 0)

test_titanic <- read_csv("../input/test.csv") %>%
  mutate(Test_Data = 1)

titanic_full <- bind_rows(train_titanic, test_titanic) 
```

```{r}
glimpse(titanic_full)
```

There are 10 variables present in the combined dataframe `titanic_full` that are potentially useful. I define the function `missing_vars`, 
which I can use to get an overview of what proportion of each variable is missing, and re-use it later if I need to.

```{r}
missing_vars <- function(x) {
  var <- 0
  missing <- 0
  missing_prop <- 0
  for (i in 1:length(names(x))) {
    var[i] <- names(x)[i]
    missing[i] <- sum(is.na(x[, i]))
    missing_prop[i] <- missing[i] / nrow(x)
  }
  (missing_data <- data.frame(var = var, missing = missing, missing_prop = missing_prop) %>% 
      arrange(desc(missing_prop)))
}
```
```{r}
missing_vars(titanic_full)
```

`Cabin` & `Age` have some significant proportion of missing values, whereas `Embarked` & `Fare` are far less of a problem. 
`Survived` is the target variable and is only missing in the test portion of our dataframe, so this is not an issue.  

Below I create a quick correlation plot of the numeric variables to get an idea of how they might relate to one another. 

```{r echo = FALSE}
titanic_full %>%
  select(Survived, Age, Fare, Pclass, SibSp, Parch) %>%
  cor(use="pairwise.complete.obs") %>%
  corrplot::corrplot.mixed(upper = "circle", tl.col = "black")
```

We can see the strongest correlation is between `Fare` and `Pclass`, where as `Pclass` increases (1st -> 3rd), `Fare` decreases.

## **Exploring existing variables**

### **PassengerId**

```{r}
str(titanic_full$PassengerId)
```

This is just the ID of the passenger, all unique, 1:891 = train data, 892:1309 = test data.  
This appears to just be a row id - I could remove it, but I'll need it later to submit my predictions.



### **Survived**

1 = Survived, 0 = Died, NA = Test Data.

```{r}
prop.table(table(train_titanic$Survived))
```

```{r echo = FALSE}
ggplot(train_titanic, aes(x = factor(Survived), fill = factor(Survived))) + 
  geom_bar() + 
  scale_y_continuous(breaks = seq(0, 600, 100)) + 
  labs(x = "Survived", y = "Count") + 
  theme(legend.position = "none") + 
  ggtitle("Train Dataset - Survived")
```

This is just for the training dataset (we don't have the information on whether the test observations survived or died).
If we guessed that everyone died, we would obtain an accuracy of 61.6%.



### **Pclass**

The passengers ticket class, 1 = 1st, 2 = 2nd, 3 = 3rd.

```{r}
prop.table(table(titanic_full$Pclass))
```

We can see the overall distribution across the classes, with third class being the most common.

```{r echo = FALSE}
p1 <- ggplot(titanic_full, aes(x = factor(Pclass))) + 
  geom_bar(fill = "deepskyblue4") + 
  scale_y_continuous(breaks = seq(0, 1000, 100)) +
  labs(x = "Class", y = "Count") + 
  ggtitle("Full Dataset - Class") +
  theme(plot.title = element_text(size = 10, face = "bold"))

p2 <- ggplot(train_titanic, aes(x = factor(Pclass), fill = factor(Survived))) + 
  geom_bar(position = "dodge") + 
  scale_y_continuous(breaks = seq(0, 400, 50)) + 
  scale_fill_discrete(name = "Survived") +
  labs(x = "Class", y = "Count") + 
  ggtitle("Train Dataset - Class (Proportion Survived)") +
  theme(plot.title = element_text(size = 10, face = "bold"), 
        legend.position = "bottom")

grid.arrange(p1, p2, ncol = 2)
```

The majority of tickets were third class (54%), and third class passengers were (by far) the least likely to survive.  
First and Second class tickets were of a similar frequency (24.7%, 21.2% respectively).



### **Name**

The name of the passenger. Two names are 'duplicted', but these appear to be different people:

```{r}
length(unique(titanic_full$Name)) < nrow(titanic_full)

titanic_full %>% 
  filter(Name == "Connolly, Miss. Kate" | Name == "Kelly, Mr. James") %>%
  select(PassengerId, Survived, Name, Age, Ticket)
```


I will extract title into a new variable. There could maybe be potential to link people together into families by extracting surname and other data (Parch, SibSp), but this would be considerably more difficult.



### **Sex**

The sex of the passenger (male, female).

```{r}
prop.table(table(titanic_full$Sex))
```

```{r echo = FALSE}
g3 <- ggplot(titanic_full, aes(x = factor(Sex))) + 
  geom_bar(fill = "deepskyblue4") + 
  scale_y_continuous(breaks = seq(0, 900, 100)) +
  labs(x = "Sex", y = "Count") + 
  ggtitle("Full Dataset - Sex") +
  theme(plot.title = element_text(size = 10, face = "bold"))

g4 <- ggplot(train_titanic, aes(x = factor(Sex), fill = factor(Survived))) + 
  geom_bar(position = "dodge") + 
  scale_y_continuous(breaks = seq(0, 500, 50)) + 
  scale_fill_discrete(name = "Survived") +
  labs(x = "Sex", y = "Count") + 
  ggtitle("Train Dataset - Sex (Proportion Survived)") +
  theme(plot.title = element_text(size = 10, face = "bold"), 
        legend.position = "bottom")

grid.arrange(g3, g4, ncol = 2)
```

The passengers were approximately 2:1, male:female.
Females also appear to have been far more likely to survive.



### **Age**

The age of the passenger.

Age has quite a problem with missing data (20.1%) - we should bear this in mind when looking at these visualisations.

```{r message = FALSE, warning = FALSE, echo = FALSE}
g5 <- ggplot(titanic_full, aes(x = Age)) + 
  geom_histogram(fill = "deepskyblue4") + 
  scale_y_continuous(breaks = seq(0, 150, 10)) + 
  labs(x = "Age", y = "Count") + 
  ggtitle("Full Dataset - Age") +
  theme(plot.title = element_text(size = 10, face = "bold"))

g6 <- ggplot(train_titanic, aes(x = Age, fill = factor(Survived))) + 
  geom_histogram() + 
  scale_y_continuous(breaks = seq(0, 150, 10)) + 
  scale_x_continuous(breaks = seq(0, 80, 10)) +
  scale_fill_discrete(name = "Survived") + 
  labs(x = "Age", y = "Count") + 
  ggtitle("Train Dataset - Age (Proportion Survived)") +
  theme(plot.title = element_text(size = 10, face = "bold"), 
        legend.position = "bottom")

grid.arrange(g5, g6, ncol = 2)
```

`Age` appears fairly normally distributed.  
Children appeared most likely to survive, with those in their 20's and 30's seeming least likely. 
There appears to be decimal ages, both for children < 1 and also for a few people above this age.
Reading the data documentation these appear to be for infants & predicted ages respectively.



### **SibSp**

The number of siblings/spouses also aboard the titanic.

```{r message = FALSE, warning = FALSE, echo = FALSE}
g7 <- ggplot(titanic_full, aes(x = SibSp)) + 
  geom_histogram(fill = "deepskyblue4") + 
  scale_y_continuous(breaks = seq(0, 1000, 100)) + 
  scale_x_continuous(breaks = seq(0, 10)) +
  labs(x = "SibSp", y = "Count") + 
  ggtitle("Full Dataset - SibSp") +
  theme(plot.title = element_text(size = 10, face = "bold"))

g8 <- ggplot(train_titanic, aes(x = SibSp, fill = factor(Survived))) + 
  geom_histogram() + 
  scale_y_continuous(breaks = seq(0, 700, 100)) + 
  scale_x_continuous(breaks = seq(0, 10)) +
  scale_fill_discrete(name = "Survived") + 
  labs(x = "SibSp", y = "Count") + 
  ggtitle("Train Dataset - SibSp (Proportion Survived)") +
  theme(plot.title = element_text(size = 10, face = "bold"), 
        legend.position = "bottom")

grid.arrange(g7, g8, ncol = 2)
```



### **Parch**

The number of parents/children also aboard the titanic.

```{r message = FALSE, warning = FALSE, echo = FALSE}
g9 <- ggplot(titanic_full, aes(x = Parch)) + 
  geom_histogram(fill = "deepskyblue4") + 
  scale_y_continuous(breaks = seq(0, 1000, 100)) + 
  scale_x_continuous(breaks = seq(0, 10)) +
  labs(x = "Parch", y = "Count") + 
  ggtitle("Full Dataset - Parch") +
  theme(plot.title = element_text(size = 10, face = "bold"))

g10 <- ggplot(train_titanic, aes(x = Parch, fill = factor(Survived))) + 
  geom_histogram() + 
  scale_y_continuous(breaks = seq(0, 700, 100)) + 
  scale_x_continuous(breaks = seq(0, 10)) +
  scale_fill_discrete(name = "Survived") + 
  labs(x = "Parch", y = "Count") + 
  ggtitle("Train Dataset - Parch (Proportion Survived)") +
  theme(plot.title = element_text(size = 10, face = "bold"), 
        legend.position = "bottom")

grid.arrange(g9, g10, ncol = 2)
```

This would make the most sense when combined with `SibSp`, giving the total number of family members also on-board.



### **Ticket**

Ticket number.

```{r}
length(unique(titanic_full$Ticket))
```

```{r}
titanic_full %>%
  group_by(Ticket) %>%
  count() %>%
  arrange(desc(n))
```

Some passengers are sharing the same ticket number (maybe living together/family ticket/group ticket, something like that).

```{r}
titanic_full %>%
  group_by(Ticket, Fare) %>%
  count() %>%
  arrange(desc(n))
```

People with the same ticket appear to have the same fare (with the only exception being ticket '7534'):

```{r}
titanic_full %>%
  filter(Ticket == '7534')
```


The format Appears to mostly be 'Letters Numbers' or just 'Numbers' - it could be worth extracting this information.  
It seems likely that these people shared some sort of group ticket - I will probably divide the fare by the number in the group to get a more accurate representation of the fare per person.



### **Fare**

The passenger fare.

`Fare` appears to be very right-skewed, and has some pretty big outliers at > 500.

```{r warning = FALSE, echo = FALSE}
ggplot(titanic_full, aes(x = 1, y = Fare)) + 
  geom_boxplot() + 
  theme(axis.ticks.y = element_blank(), 
        axis.text.y=element_blank(), 
        axis.title.y=element_blank()) + 
  scale_y_continuous(breaks = seq(0, 600, 50)) + 
  coord_flip() + 
  ggtitle("Full Dataset - Fare")
```

Restricting the visualizations range from 0 to 300 and splitting by survival shows that those that survived generally paid more for their tickets.
```{r warning = FALSE, echo = FALSE}
ggplot(train_titanic, aes(x = 1, y = Fare)) + 
  geom_boxplot() + 
  theme(axis.ticks.y = element_blank(), 
        axis.text.y=element_blank(), 
        axis.title.y=element_blank()) + 
  scale_y_continuous(limits = c(0, 300), breaks = seq(0, 300, 50)) + 
  facet_grid(Survived ~ ., labeller = label_both) + 
  coord_flip() + 
  ggtitle("Train Dataset - Fare (Distributions, Survived vs Died)")
```

Even when controlling for `Pclass`, those that survived in first and second class seem to have paid more than those that died (not true for third class passengers).

```{r warning = FALSE, echo = FALSE}
ggplot(train_titanic, aes(x = 1, y = Fare)) + 
  geom_boxplot() + 
  theme(axis.ticks.y = element_blank(), 
        axis.text.y=element_blank(), 
        axis.title.y=element_blank()) + 
  scale_y_continuous(limits = c(0, 150), breaks = seq(0, 300, 50)) + 
  facet_grid(Survived ~ Pclass, labeller = label_both) + 
  coord_flip() + 
  ggtitle("Train Dataset - Fare, Controlling for Class (Survived vs Died)")
```

Based on the group ticket theory from earlier, this could just be because these people were families and so their group ticket cost more.



### **Cabin**

The cabin number. 

*Lots* of NA's (only 295 are populated (187 unique cabins), 1014 NA's).

```{r}
sum(is.na(titanic_full$Cabin))
```

When the variable is populated, the format appears to be: 'LetterNumber', sometimes multiple e.g. 'LetterNumber LetterNumber' etc.

```{r}
head(titanic_full$Cabin[!is.na(titanic_full$Cabin)])
```
This probably won't be useful in it's present state, but extracting the letter might be.
Perhaps the fact that the cabin information is missing could be useful - there could be a reason that only a small proportion of passengers have cabin data. I could simply create a flag for missing/present data.


### **Embarked**

The port that the passenger embarked from (C = Cherbourg, Q = Queenstown, S = Southampton). 

We can see that almost 70% departed from Southampton.
```{r}
prop.table(table(titanic_full$Embarked))
```
```{r echo = FALSE}
g11 <- titanic_full %>%
  filter(!is.na(Embarked)) %>%
ggplot(aes(x = factor(Embarked))) + 
  geom_bar(fill = "deepskyblue4") + 
  scale_y_continuous(breaks = seq(0, 1000, 100)) + 
  labs(x = "Embarked", y = "Count") + 
  ggtitle("Full Dataset - Embarked") +
  theme(plot.title = element_text(size = 10, face = "bold"))


g12 <- train_titanic %>%
  filter(!is.na(Embarked)) %>%
ggplot(aes(x = factor(Embarked), fill = factor(Survived))) + 
  geom_bar(position = "dodge") + 
  scale_y_continuous(breaks = seq(0, 500, 50)) + 
  scale_fill_discrete(name = "Survived") +
  labs(x = "Embarked", y = "Count") + 
  ggtitle("Train Dataset - Embarked (Proportion Survived)") +
  theme(plot.title = element_text(size = 10, face = "bold"), 
        legend.position = "bottom")

grid.arrange(g11, g12, ncol = 2)
```

Passengers departing from Cherbourg are the only passengers with a survival rate above 50%.

I anticipate that this will have a strong relationship with fare - it would make sense that those travelling further distances to have paid more for their ticket.


# **Feature Engineering & Missing Value Imputation**

## **Feature Engineering**

### **Title**

This is the title of the passenger, which can be extracted from the `Name` variable using a regular expression.

```{r}
titanic_full$Title_FE <- gsub('(.*, )|(\\..*)', '', titanic_full$Name)

titanic_full %>%
  group_by(Title_FE) %>%
  count() %>%
  arrange(desc(n))
```

I'll group those less common title's into an 'Other' category. I'd guess this would be better than manually putting them into the other groups, 
as most of the less common titles seem relatively upper-class and so have a shared characteristic.

```{r}
titanic_full$Title_FE <- ifelse(titanic_full$Title_FE %in% c("Mr", "Miss", "Mrs", "Master"), 
                                titanic_full$Title_FE, 
                                "Other")

table(titanic_full$Title_FE, titanic_full$Sex)
```

Checking the table of `Title` vs `Sex` shows nothing anomolous.
A stacked bar graph of the newly created variable suggests it could be quite useful - the difference in survival between 'Master' and 'Mr' will be something that isn't captured by the `Sex` variable too.

```{r echo = FALSE}
titanic_full %>%
  filter(!is.na(Survived)) %>%
ggplot(aes(x = factor(Title_FE), fill = factor(Survived))) + 
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Title", y = "") + 
  ggtitle("Title_FE - Engineered Variable (Proportion Survived)")
```



### **Cabin data**

The `Cabin` variable is 77.5% missing, so I don't realistically think that extracting the first letter will be particularly useful, as i'll still end up creating one level for 77.5% of the data, or trying to impute which seems like a bad idea.  
For this reason, i'm simply going to create a binary variable to indicate whether the data was available or not - there could be a good reason why so little data is available, and this could potentially help predict survival.

```{r}
titanic_full$Cabin_data_FE <- ifelse(!is.na(titanic_full$Cabin), 1, 0)

prop.table(table(titanic_full$Cabin_data_FE))
```

The graph below actually shows a significantly higher survival rate for those without missing data.

```{r echo = FALSE}
titanic_full %>%
  filter(!is.na(Survived)) %>%
ggplot(aes(x = factor(Cabin_data_FE), fill = factor(Survived))) + 
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Cabin Data Presence (0 = Missing, 1 = Available)", y = "") + 
  ggtitle("Cabin_data_FE - Engineered Variable (Proportion Survived)")
```

If this variable turns out to be useful, I might reconsider my approach and try extracting the letter.



### **Fare (per person)**

I'm creating a 'fare per person' variable, as I think this might be more useful than `Fare`. 
The reason behind this is that all Fare really does at the moment is represent the size of the group that the person purchased the ticket with, along with the class of the person.  
My hope is that Fare per person makes more sense to me as a predictor and might be generally be a less noisy variable in that a high Fare per person might represent wealthier people (instead of just larger groups, which we can create as a seperate variable anyway).

```{r}
(titanic_fare_pp <- titanic_full %>%
  group_by(Ticket, Fare) %>%
  summarize(Group_size_FE = n()) %>%
  mutate(Fare_pp_FE = Fare / Group_size_FE))

titanic_full <- left_join(titanic_full, titanic_fare_pp, by = c("Ticket", "Fare"))
```

Whilst the group size alone doesn't explain much of the variance in Fare, Group Size & Class together show a very strong relationship 
(see below, i've added some random jitter & reduced the alpha level to the plots, since Group Size is discrete and the points would overlap).

```{r message = FALSE, warning = FALSE, echo = FALSE}
g13 <- ggplot(titanic_full, aes(x = Group_size_FE, y = Fare)) + 
  geom_jitter(alpha = 0.2) + 
  geom_smooth(method = "lm", se = F) + 
  scale_y_continuous(limits = c(0, 300)) + 
  labs(x = "Group Size", y = "Fare") + 
  theme(legend.position = "none")

g14 <- ggplot(titanic_full, aes(x = Group_size_FE, y = Fare, col = factor(Pclass))) + 
  geom_jitter(alpha = 0.2) + 
  geom_smooth(method = "lm", se = F) + 
  facet_grid(Pclass ~ ., labeller = label_both) + 
  scale_y_continuous(limits = c(0, 300)) + 
  labs(x = "Group Size", y = "Fare") + 
  theme(legend.position = "none")

grid.arrange(g13, g14, ncol = 2)
```

I could justify this further with some quick linear regressions - if we fit a model predicting `Fare` with `Group_size_FE`, our R^2 should be relatively low.
```{r}
summary(lm(Fare ~ Group_size_FE, titanic_full))$r.square
```

Accounting for `Pclass` in the model will allow the regression line to 'shift' up for upper-class passengers and down for lower-class passengers, explaining more of the variance in Fare, so our R^2 should improve.
```{r}
summary(lm(Fare ~ Group_size_FE + factor(Pclass), titanic_full))$r.square
```

Accounting for an interaction effect between `Group_size_FE` and `Pclass` should give us a really good R^2, as this will allow for differing slopes (the effect of Group Size on Fare is different for different classes), which can be observed in the faceted plot.
```{r}
summary(lm(Fare ~ Group_size_FE + factor(Pclass) + factor(Pclass)*Group_size_FE, titanic_full))$r.square
```

I'm relatively new to random forests, so i'm not completely confident that this new variable will be better, but hopefully it will be useful.

A plot of the new variable can be seen below.

```{r echo = FALSE}
titanic_full %>%
  filter(!is.na(Survived)) %>%
ggplot(aes(x = Fare_pp_FE, fill = factor(Survived))) + 
  geom_histogram(binwidth = 2) + 
  scale_y_continuous(breaks = seq(0, 500, 50)) + 
  scale_x_continuous(breaks = seq(0, 200, 20)) +
  scale_fill_discrete(name = "Survived") + 
  labs(x = "Fare (per person)", y = "Count") + 
  ggtitle("Fare_pp_FE - Engineered Variable (Proportion Survived)")
```


### **Total group size**

I first create a 'total family size' variable, `Family_size_FE`, which is the number of parents/children/siblings/spouses onboard the titanic with the passenger (+1 for the passenger themselves).

I then combine this with the previous engineered variable, `Group_size_FE`, taking the maximum of the two.  
This will hopefully give the best estimate of the total number of people the passenger was with on the boat, 
which I expect will have a large impact on their probability of survival (groups might stick together, for better or worse).

```{r}
titanic_full$Family_size_FE <- titanic_full$SibSp + titanic_full$Parch + 1

titanic_full$Total_group_size_FE <- pmax(titanic_full$Family_size_FE, titanic_full$Group_size_FE)
```

The below plot appears to suggest that most people travelled alone, with small & large groups having the least chance of survival. Medium-sized groups seemed to have the best odds of living.

```{r message = FALSE, warning = FALSE, echo = FALSE}
titanic_full %>%
  filter(!is.na(Survived)) %>%
ggplot(aes(x = Total_group_size_FE, fill = factor(Survived))) + 
  geom_histogram() + 
  scale_y_continuous(breaks = seq(0, 700, 100)) + 
  scale_x_continuous(breaks = seq(0, 10)) +
  scale_fill_discrete(name = "Survived") + 
  labs(x = "Group Size: max(Family Size, Group Size)", y = "Count") + 
  ggtitle("Total_group_size_FE - Engineered Variable (Proportion Survived)")
```



## **Missing Values**

I'll tidy up the dataframe a little bit, then re-apply my missing values function to the updated dataframe:

```{r}
titanic_full <- titanic_full %>%
  select(-c(Name, Ticket, Cabin, Group_size_FE, Family_size_FE, Fare))

missing_vars(titanic_full)
```


### **Fare (per person)**

Since there was one missing value in `Fare`, there is also one for my 'Fare per person' variable.  
We can see that this person travelled alone, so I can't impute with the value of those that shared the same ticket - i'll waste no further time and impute with the median, since `Fare_pp_FE` is right-skewed.

```{r}
titanic_full[is.na(titanic_full$Fare_pp_FE), ]
titanic_full$Fare_pp_FE[is.na(titanic_full$Fare_pp_FE)] <- median(titanic_full$Fare_pp_FE, na.rm = T)
```


### **Embarked**

Despite the fact that 70% of passengers embarked from 'S', i'm going to impute these missing values with 'C'. 

```{r}
titanic_full[is.na(titanic_full$Embarked), c("PassengerId", "Embarked", "Fare_pp_FE", "Total_group_size_FE")]
```

We can see that these passengers paid quite a lot (Â£40) for their ticket (which I would expect would have a strong relationship with where they departed from).  
Below we can see that this fare would make them a big outlier if they embarked from 'Q' or 'S', but only just into the upper quartile if they embarked from 'C'.

```{r echo = FALSE}
titanic_full %>%
  filter(!is.na(Embarked)) %>%
ggplot(aes(x = Embarked, y = Fare_pp_FE)) + 
  geom_boxplot() + 
  geom_hline(yintercept = 40, col = "deepskyblue4")
```

I impute with 'C' for these reasons.

```{r}
titanic_full$Embarked[is.na(titanic_full$Embarked)] <- "C"
```


### **Age**

20% missing is within the realms of usable, and could be a good candidate for imputation by a predictive model since a good proportion of the data is still present. I also think that the age of a passenger would be a good predictor of survival, so i'll be more careful with this variable.  
Since i'm using a random forest for the final prediction of 'Survived', i'll give that a go here too.  
First I want to re-format my variables - create nominal/ordinal factors, etc. so they can be treated appropriately by the model.

```{r}
# Nominal factors
titanic_full_nominal <- c('Survived', 'Sex', 'Embarked', 'Title_FE', 'Cabin_data_FE')
titanic_full[titanic_full_nominal] <- lapply(titanic_full[titanic_full_nominal], function(x){factor(x)})

# Ordinal factors
titanic_full$Pclass <- factor(titanic_full$Pclass, 
                              ordered = TRUE, 
                              levels = c(3, 2, 1), 
                              labels = c("Third", "Second", "First"))

# Tidying
titanic_full$Total_group_size_FE <- as.integer(titanic_full$Total_group_size_FE)
titanic_full$Test_Data <- as.integer(titanic_full$Test_Data)
titanic_full$Fare_pp_FE <- round(titanic_full$Fare_pp_FE, 2)

glimpse(titanic_full)
```

I now use `caret` to train a random forest to predict the missing ages, using 5-repeated 5-fold cross-validation for model selection over different values of `mtry` - the number of variables available for splitting at each node of each decision tree in the random forest.

```{r message = FALSE, warning = FALSE}
age_train <- titanic_full %>%
  filter(!is.na(Age))

set.seed(2307)

repeatedCV <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 5)

rf_age_grid <- expand.grid(mtry = c(2, 3, 4))

tic()
rf_age <- train(x = age_train %>% select(c(Pclass, Sex, SibSp, Parch, Embarked, Title_FE, Cabin_data_FE, Fare_pp_FE, Total_group_size_FE)),
                     y = age_train$Age,
                     method = "rf", 
                     trControl = repeatedCV, 
                     importance = TRUE, 
                     tuneGrid = rf_age_grid)
toc()

rf_age
varImp(rf_age)
```

It seems that `mtry = 2` gave the lowest RMSE, so Caret selected that for training the final model. As noted earlier, the engineered `Title_FE` variable was a good predictor of `Age`.  
I create a new variable `Age_IMP`, which contains the natural values for `Age` if the data was present, and my predictions if not. I then do a quick band-aid fix is the imputed value is high for those with a 'Master' title.

```{r}
age_predictions <- predict(rf_age, titanic_full)

# joining back onto dataframe

titanic_full <- as_tibble(cbind(titanic_full, age_predictions))

titanic_full$Age_IMP <- ifelse(!is.na(titanic_full$Age), 
                               titanic_full$Age, 
                               titanic_full$age_predictions)

titanic_full <- titanic_full %>%
  select(-c(Age, SibSp, Parch, age_predictions))

titanic_full$Age_IMP <- ifelse(titanic_full$Title_FE == 'Master' & titanic_full$Age_IMP > 13,
                               13, 
                               titanic_full$Age_IMP) 
                               # winsorizing those above the datasets 'Master' upper bound for age
```

Below is the new complete Age variable, `Age_IMP`, plotted for the training dataset to show the survived/died split. The distribution doesn't appear to have changed much.

```{r message = FALSE, warning = FALSE, echo = FALSE}
titanic_full %>%
  filter(!is.na(Survived)) %>%
ggplot(aes(x = Age_IMP, fill = factor(Survived))) + 
  geom_histogram() + 
  scale_y_continuous(breaks = seq(0, 150, 10)) + 
  scale_x_continuous(breaks = seq(0, 80, 10)) +
  scale_fill_discrete(name = "Survived") + 
  labs(x = "Age (with imputations)", y = "Count") + 
  ggtitle("Age including imputations - (Proportion Survived)")
```


### **Child**

I'll take this opportunity to create one more variable, `IsChild_FE`, now that Age is a complete variable - a binary indicator for whether the person was a child, since children would have often been prioritised for survival.

```{r}
titanic_full$IsChild_FE <- factor(ifelse(titanic_full$Age_IMP < 15, 1, 0))
```

```{r echo = FALSE}
titanic_full %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = IsChild_FE, fill = Survived)) + 
  geom_bar(position = "fill") + 
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Child Flag (Age < 15)", y = "") + 
  ggtitle("IsChild_FE - Engineered Variable (Proportion Survived)")
```

Looking at my dataset below, I have 9 variables which could potentially be predictive of survival.

```{r}
glimpse(titanic_full)
```

I'll now move on to final model building, selection & tuning.



# **Building and Tuning the Final Model**

I separate the test data from the full dataset, as i'll need this to make predictions. I'll create training datasets as I go, selecting different variable subsets where necessary.

```{r}
rf_test <- as_tibble(titanic_full) %>%
  filter(Test_Data == 1)
```

## **Model 1: All Variables**

For my first model i'm just going to use all of my predictors and get an idea for where i'm at with regards to cross-validation accuracy and accuracy on the public dataset.

```{r}
rf_train_1 <- titanic_full %>%
  filter(Test_Data == 0) %>%
  select(c(Survived, Pclass, Sex, Embarked, Title_FE, Cabin_data_FE, Fare_pp_FE, Total_group_size_FE, Age_IMP, IsChild_FE))
```

Since random forests don't require too much tuning, i'm just going to perform a full hyperparameter grid search over different values of `mtry`, as it doesn't take too long.
Below I define my validation method and the grid over which i'll perform my hyperparameter search, then I train my first model.

```{r message = FALSE, warning = FALSE}
set.seed(2307)

repeatedCV <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 5)

rf_grid <- expand.grid(mtry = seq(from = 2, to = ncol(rf_train_1) - 1, by = 1))

tic()
rf_model_1 <- train(x = rf_train_1[ ,-1],
                  y = rf_train_1$Survived,
                  method = "rf", 
                  trControl = repeatedCV, 
                  importance = TRUE, 
                  tuneGrid = rf_grid)
toc()


rf_model_1 
```

The model selected an `mtry` of 3, as this maximized the cross-validation accuracy.

We can see the variable importance plot below, which shows that `Sex` was the best predictor, closely followed by 3 engineered features!

```{r}
varImp(rf_model_1)
```

```{r}
paste("The maximum accuracy was", round(max(rf_model_1$results$Accuracy), 5))
```

Submitting this prediction to the public dataset gave a public score of **0.76555**, which is quite far off from my cross-validation accuracy of **0.83684** (this seems to be a common theme for others too on this dataset).  
To an extent, this could be caused by differences in the train & test datasets (it's a very small dataset), but I think my model is overfitting quite a bit.



## **Model 2: Removing the least useful variables:**

From the previous variable importance ranking, I can see that `Cabin_data_FE` and `IsChild_FE` are both almost tied for the least important variable. I'll try removing both of them and see if my cross-validation accuracy increases.

```{r}
rf_train_2 <- titanic_full %>%
  filter(Test_Data == 0) %>%
  select(c(Survived, Pclass, Sex, Embarked, Title_FE, Fare_pp_FE, Total_group_size_FE, Age_IMP))
```

```{r message = FALSE, warning = FALSE}
set.seed(2307)

repeatedCV <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 5)

rf_grid <- expand.grid(mtry = seq(from = 2, to = ncol(rf_train_2) - 1, by = 1))

tic()
rf_model_2 <- train(x = rf_train_2[ ,-1],
                    y = rf_train_2$Survived,
                    method = "rf", 
                    trControl = repeatedCV, 
                    importance = TRUE, 
                    tuneGrid = rf_grid)
toc()


rf_model_2
```

Tuning this smaller model now gives an optimal `mtry` of 2.

```{r}
varImp(rf_model_2)
```

The variable importance has shifted around a fair bit.

Despite removing 2 predictors from the model, the cross-validation accuracy has increased *slightly*, from **0.83684** to **0.83773**.

```{r}
paste("The maximum accuracy was", round(max(rf_model_2$results$Accuracy), 5))
```

Testing this second model gave a public dataset accuracy of **0.77511** - the model has improved overall, but a ~6% difference in cross-validation to public accuracy still seems like a lot for this challenge.  
Maybe I should try a simpler varsion of age instead of the full imputed variable, since a good portion of the variable was imputed - i'll substitute `Age_IMP` for `IsChild_FE`.
This basically means i'm using my full model, with the exception of `Cabin_data_FE` (this was a very speculative variable anyway) and `Age_IMP`, the imputed age variable.




## **Model 3: My Final Model:**

### **The Model**

For this model, I tried removing `Cabin_data_FE` & `Age_IMP` - the 2 predictors based around the variables with the largest proportion of missing data.


```{r}
rf_train_3 <- titanic_full %>%
  filter(Test_Data == 0) %>%
  select(c(Survived, Pclass, Sex, Embarked, Title_FE, Fare_pp_FE, Total_group_size_FE, IsChild_FE))
```

```{r message = FALSE, warning = FALSE}
set.seed(2307)

repeatedCV <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 5)

rf_grid <- expand.grid(mtry = seq(from = 2, to = ncol(rf_train_3) - 1, by = 1))

tic()
rf_model_3 <- train(x = rf_train_3[ ,-1],
                    y = rf_train_3$Survived,
                    method = "rf", 
                    trControl = repeatedCV, 
                    importance = TRUE, 
                    tuneGrid = rf_grid)
toc()


rf_model_3 
```


Once again, tuning this model gave an optimal `mtry` value of 2.

```{r echo = FALSE}
ggplot(rf_model_3) + 
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.002)) + 
  geom_vline(xintercept = rf_model_3$bestTune$mtry, col = "green")
```

I plot the relative variable importances below:

```{r echo = FALSE}
plot(varImp(rf_model_3))
```

```{r}
paste("The maximum accuracy was", round(max(rf_model_3$results$Accuracy), 5))
```

The cross-validation accuracy has actually decreased a bit here, from **0.83773** to **0.82829**, but when I played around with creating and adding new variables, or including original variables (without `Age_IMP` in the model) my variable importance ranking seemed to stay more stable.
Reading up about this online, there are quite a few potential causes, such as highly correlated features, but i'm not sure exactly what my issue was.  



### **Predictions**

```{r}
rf_pred_3 <- predict(rf_model_3, rf_test)

test_predictions_3 <- cbind(rf_test, rf_pred_3)

test_predictions_3 <- test_predictions_3 %>%
  select(PassengerId, rf_pred_3) %>%
  rename(Survived = rf_pred_3)

write.csv(test_predictions_3, 
          file = "RF_Titanic_3.csv", 
          row.names = FALSE, 
          quote = FALSE)
```

Submitting this model now gave me a public score accuracy of **0.80382** - a significant improvement. In this case i'll go with this as my final model - it would appear that removing `Age_IMP` seemed to fix whatever overfitting issues I was having.


### **Final Plots/Analysis & Review**

Below we can see that the Out-Of-Bag error stabilizes to ~**17.5%** after 100 or so trees are in my forest.

The graph also shows that my final model has a low error when predicting death, but a higher error when predicting survival, which is as expected in this competition.

```{r echo = FALSE}
Final_model_error <- as.data.frame(cbind(ntree = 1:500, rf_model_3$finalModel$err.rate)) %>%
  tidyr::gather(key = "Error_Type", value = "Error", -ntree)

ggplot(Final_model_error, aes(x = ntree, y = Error, col = factor(Error_Type))) + 
  geom_line() + 
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.05)) +
  labs(x = "Number of Trees", y = "Error Rate", colour = "Error Type") + 
  ggtitle("Final Model Error Rates - Died, Survived & Out-Of-Bag")
```

This is confirmed by the final model confusion matrix:

```{r}
rf_model_3$finalModel$confusion
```

Error rate when predicting death: **8.2%**  
Error rate when predicting survival: **32.5%**

This final model gives a public score in the **top 8%** of Kaggle competitors, which i'm very happy with!