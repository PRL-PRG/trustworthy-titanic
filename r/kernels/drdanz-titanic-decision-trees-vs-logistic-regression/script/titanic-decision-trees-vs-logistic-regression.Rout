
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ## ----setup, include=FALSE--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> knitr::opts_chunk$set(echo = TRUE)
> 
> 
> ## --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> # load packages and data
> library(rpart)
> library(rpart.plot)
> library(lme4)
Loading required package: Matrix
Warning message:
package ‘lme4’ was built under R version 3.6.2 
> library(magrittr)
> 
> setwd("../input") # for loading data in Kaggle
> data <- read.csv("train.csv")
> 
> # make output more clear later
> levels(data$Embarked) <- c("", "Cherbourg", "Queenstown", "Southampton")
> 
> 
> ## --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> # Index data for training/validation split
> n <- nrow(data)
> set.seed(10)
> idx.train <- sample(1:n, n * 0.8)
> idx.val <- sample((1:n)[-idx.train])
> 
> 
> ## ----1---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> tree_cols <- c(3, 5, 7, 8, 12)
> x_train <- data[idx.train, tree_cols]
> x_val <- data[idx.val, tree_cols]
> y_train <- data[idx.train, 2]
> y_val <- data[idx.val, 2]
> 
> x <- cbind(x_train, y_train)
> 
> # grow tree 
> fit <- rpart(y_train ~ ., data = x, method="class")
> 
> # summarize and plot tree
> print(fit)
n= 712 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 712 271 0 (0.61938202 0.38061798)  
   2) Sex=male 453  83 0 (0.81677704 0.18322296) *
   3) Sex=female 259  71 1 (0.27413127 0.72586873)  
     6) Pclass>=2.5 121  58 0 (0.52066116 0.47933884)  
      12) Embarked=Southampton 70  23 0 (0.67142857 0.32857143) *
      13) Embarked=Cherbourg,Queenstown 51  16 1 (0.31372549 0.68627451) *
     7) Pclass< 2.5 138   8 1 (0.05797101 0.94202899) *
> prp(fit, extra = 4)
> 
> 
> ## --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> predicted_dt <- predict(fit, x_val)
> predicted_dt <- ifelse(predicted_dt[,1] <= 0.5, 1, 0) # reformat to binary
> 
> # calculate accuracy of prediction
> acc_dt <- sum(predicted_dt == y_val, na.rm = 1) / length(y_val)
> cat(paste0("Validation accuracy for the decision tree is ",
+            round(acc_dt,3), 
+            "."))
Validation accuracy for the decision tree is 0.788.> 
> 
> ## ----2a--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> # fit logistic regression model
> fit_lr <- glm(y_train ~ ., data = x, family = binomial(link = "logit"))
> summary(fit_lr)

Call:
glm(formula = y_train ~ ., family = binomial(link = "logit"), 
    data = x)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.3918  -0.6677  -0.4310   0.6880   2.5223  

Coefficients:
                     Estimate Std. Error z value Pr(>|z|)    
(Intercept)          14.50832  535.41118   0.027   0.9784    
Pclass               -0.94225    0.12513  -7.530 5.07e-14 ***
Sexmale              -2.70787    0.22006 -12.305  < 2e-16 ***
SibSp                -0.17963    0.10574  -1.699   0.0893 .  
Parch                -0.04507    0.11860  -0.380   0.7039    
EmbarkedCherbourg   -10.76453  535.41126  -0.020   0.9840    
EmbarkedQueenstown  -11.01522  535.41133  -0.021   0.9836    
EmbarkedSouthampton -11.30352  535.41123  -0.021   0.9832    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 946.06  on 711  degrees of freedom
Residual deviance: 648.97  on 704  degrees of freedom
AIC: 664.97

Number of Fisher Scoring iterations: 12

> 
> 
> ## --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> predicted_lr = predict(fit_lr, x_val)
> predicted_lr <- ifelse(predicted_lr >= 0, 1, 0) # reformat to binary
> 
> # calculate accuracy of prediction
> acc_lr <- sum(predicted_lr == y_val, na.rm = 1) / length(y_val)
> cat(paste0("Validation accuracy for the category-only logistic regression is ",
+            round(acc_lr,3), 
+            "."))
Validation accuracy for the category-only logistic regression is 0.804.> 
> 
> ## ----2b--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> ## Logistic regression w. all data incl. continuous ----------------------------
> x_train_all <- data[idx.train, c(3, 5:8, 10, 12)] # still omits names and tickets
> x_val_all <- data[idx.val, c(3, 5:8, 10, 12)]
> 
> # complete gaps in  data (could remove rows with NA, but decided to complete w. dummy data)
> # function to replace NAs with column mean
> NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
> NA2mean_df <- function(df){
+   nums <- unlist(lapply(df, is.numeric)) # index numeric columns
+   done <- replace(df, nums, lapply(df[, nums], NA2mean))
+   return(done)
+ }
> 
> # function to fill empty character cells with most frequent factor
> fill_facs <- function(df) {
+   for (i in 1:ncol(df)) {
+     levels(df[, i])[levels(df[, i]) == ""] <-
+       names(which.max(table(df[, i])))
+   }
+   return(df)
+ }
> 
> # replace NAs with column mean and replace empty cells with most frequent factor
> x_train_all %<>% NA2mean_df() %>% fill_facs()
> x_val_all %<>% NA2mean_df() %>% fill_facs()
> 
> # paste together
> x_all <- cbind(x_train_all, y_train)
> 
> # fit model
> fit_lr_all <- glm(y_train ~ ., data = x_all, family = binomial(link = "logit"))
> summary(fit_lr_all)

Call:
glm(formula = y_train ~ ., family = binomial(link = "logit"), 
    data = x_all)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7332  -0.6161  -0.3997   0.6346   2.5429  

Coefficients:
                    Estimate Std. Error z value Pr(>|z|)    
(Intercept)         5.171500   0.610847   8.466  < 2e-16 ***
Pclass             -1.191276   0.164503  -7.242 4.43e-13 ***
Sexmale            -2.700119   0.225110 -11.995  < 2e-16 ***
Age                -0.046794   0.008949  -5.229 1.70e-07 ***
SibSp              -0.296604   0.118207  -2.509   0.0121 *  
Parch              -0.083027   0.129782  -0.640   0.5223    
Fare                0.001893   0.002587   0.732   0.4642    
EmbarkedCherbourg   0.481046   0.266869   1.803   0.0715 .  
EmbarkedQueenstown  0.432886   0.367646   1.177   0.2390    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 946.06  on 711  degrees of freedom
Residual deviance: 617.42  on 703  degrees of freedom
AIC: 635.42

Number of Fisher Scoring iterations: 5

> 
> 
> ## --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> predicted_lr_all = predict(fit_lr_all, x_val_all) 
> predicted_lr_all <- ifelse(predicted_lr_all >= 0, 1, 0) # reformat to binary
> 
> # calculate accuracy of prediction
> acc_lr_all <- sum(predicted_lr_all == y_val, na.rm = 1) / length(y_val)
> cat(paste0("Validation accuracy for the full logistic regression is ",
+            round(acc_lr_all,3), 
+            "."))
Validation accuracy for the full logistic regression is 0.771.> 
> 
> ## --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> # fit model with interactions
> fit_lr_fact <-
+   glm(
+   y_train ~ . + (Sex * Pclass) + (Sex * Embarked) + (Pclass * Embarked),
+   data = x_all,
+   family = binomial(link = "logit")
+   )
> 
> summary(fit_lr_fact)

Call:
glm(formula = y_train ~ . + (Sex * Pclass) + (Sex * Embarked) + 
    (Pclass * Embarked), family = binomial(link = "logit"), data = x_all)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.3636  -0.6360  -0.4182   0.4776   2.5048  

Coefficients:
                            Estimate Std. Error z value Pr(>|z|)    
(Intercept)                 8.200852   1.084763   7.560 4.03e-14 ***
Pclass                     -2.457747   0.369480  -6.652 2.89e-11 ***
Sexmale                    -6.397770   1.008397  -6.344 2.23e-10 ***
Age                        -0.047421   0.009482  -5.001 5.70e-07 ***
SibSp                      -0.245892   0.120002  -2.049   0.0405 *  
Parch                      -0.006732   0.136521  -0.049   0.9607    
Fare                        0.001746   0.002640   0.661   0.5085    
EmbarkedCherbourg           0.330383   1.044264   0.316   0.7517    
EmbarkedQueenstown          2.281374   8.718990   0.262   0.7936    
Pclass:Sexmale              1.603160   0.383172   4.184 2.87e-05 ***
Sexmale:EmbarkedCherbourg  -0.212467   0.646988  -0.328   0.7426    
Sexmale:EmbarkedQueenstown -1.811760   0.902378  -2.008   0.0447 *  
Pclass:EmbarkedCherbourg    0.200673   0.331438   0.605   0.5449    
Pclass:EmbarkedQueenstown  -0.310689   2.913838  -0.107   0.9151    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 946.06  on 711  degrees of freedom
Residual deviance: 592.24  on 698  degrees of freedom
AIC: 620.24

Number of Fisher Scoring iterations: 6

> 
> predicted_lr_fact = predict(fit_lr_fact, x_val_all) 
> predicted_lr_fact <- ifelse(predicted_lr_fact >= 0, 1, 0) # reformat to binary
> 
> # calculate accuracy of prediction
> acc_lr_fact <- sum(predicted_lr_fact == y_val, na.rm = 1) / length(y_val)
> cat(paste0("Validation accuracy for the full logistic regression is ",
+            round(acc_lr_fact, 3), 
+            "."))
Validation accuracy for the full logistic regression is 0.782.> 
> 
> proc.time()
   user  system elapsed 
  1.264   0.119   1.386 
