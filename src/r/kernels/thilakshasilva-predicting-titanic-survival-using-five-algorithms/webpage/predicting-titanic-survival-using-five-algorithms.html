<!DOCTYPE html>
<html lang="en">
<head>
    <title>Predicting Titanic Survival using Five Algorithms | Kaggle</title>
    <meta charset="utf-8" />
    <meta name="robots" content="index, follow" />
    <meta name="turbolinks-cache-control" content="no-cache" />
                <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">    <meta name="theme-color" content="#008ABC" />
    <script type="text/javascript">
        window["initialPageLoadStartTime"] = new Date().getTime();
    </script>
    <link rel="dns-prefetch" href="https://www.google-analytics.com" /><link rel="dns-prefetch" href="https://stats.g.doubleclick.net" /><link rel="dns-prefetch" href="https://js.intercomcdn.com" /><link rel="dns-prefetch" href="https://storage.googleapis.com/" />
    <link href="/static/images/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <link rel="manifest" href="/static/json/manifest.json">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:400,300,300italic,400italic,600,600italic,700,700italic" rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet" type='text/css'/>
        <link rel="canonical" href="/thilakshasilva/predicting-titanic-survival-using-five-algorithms" />                    <link rel="stylesheet" type="text/css" href="/static/assets/vendor.css?v=632d145d8598" />
        <link rel="stylesheet" type="text/css" href="/static/assets/app.css?v=1d00932a7505" />
    
    
 
        <script>
        try{(function(a,s,y,n,c,h,i,d,e){d=s.createElement("style");
        d.appendChild(s.createTextNode(""));s.head.appendChild(d);d=d.sheet;
        y=y.map(x => d.insertRule(x + "{ opacity: 0 !important }"));
        h.start=1*new Date;h.end=i=function(){y.forEach(x => d.deleteRule(x))};
        (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;
        })(window,document,['.site-header-react__nav'],'dataLayer',2000,{'GTM-52LNT9S':true});}catch{}
    </script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-12629138-1', {
            'optimize_id': 'GTM-52LNT9S',
            'displayFeaturesTask': null,
            'send_page_view': false
        });
    </script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12629138-1"></script>

    
<script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
            n.callMethod.apply(n,arguments):n.queue.push(arguments)};
        if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
        n.queue=[];t=b.createElement(e);t.async=!0;
        t.src=v;s=b.getElementsByTagName(e)[0];
        s.parentNode.insertBefore(t,s)}(window,document,'script',
        'https://connect.facebook.net/en_US/fbevents.js');
    fbq("set", "autoConfig", "false", "136809193586742");
    fbq('init', '136809193586742'); 
    fbq('track', 'PageView');
</script>
<noscript>
    <img height="1" width="1" src="https://www.facebook.com/tr?id=136809193586742&ev=PageView&noscript=1"/>
</noscript>

<script>window.intercomSettings = {"app_id":"koj6gxx6"};</script>        <script>(function () { var w = window; var ic = w.Intercom; if (typeof ic === "function") { ic('reattach_activator'); ic('update', intercomSettings); } else { var d = document; var i = function () { i.c(arguments) }; i.q = []; i.c = function (args) { i.q.push(args) }; w.Intercom = i; function l() { var s = d.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'https://widget.intercom.io/widget/koj6gxx6'; var x = d.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x); } if (w.attachEvent) { w.attachEvent('onload', l); } else { w.addEventListener('load', l, false); } } })()</script>
    
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@kaggledatasets" />
    <meta name="og:url" content="https://kaggle.com/thilakshasilva/predicting-titanic-survival-using-five-algorithms" />
    <meta name="og:title" content="Predicting Titanic Survival using Five Algorithms" />
    <meta name="og:description" content="Using data from Titanic: Machine Learning from Disaster" />
    <meta name="og:image" content="https://storage.googleapis.com/kaggle-avatars/thumbnails/1407771-kg.JPG" />


    
    

    
    
    
<script type="text/javascript">
    var Kaggle = Kaggle || {};

    Kaggle.Current = {
        antiForgeryToken: 'CfDJ8LdUzqlsSWBPr4Ce3rb9VL8jXM5sZPXXv5pD0uksDxEMixYYaAL09Hs6LfcevquS0c3ETHFpeyB2S9mmymMlxtq2ltVWxGXT038_UQmdTfno6G-h8qNOOepmn0bLyJzlwKTHM3szp3ipM3Mm-vU2jys',
        isAnonymous: true,
        analyticsToken: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NjAzMDk5OTYsIlVzZXJJZCI6MH0.Ndma2rfnFuinGaU2by68qeyRXttrAoHl4CdIT42vSsY',
        analyticsTokenExpiry: 15,
        internetKernelsEnabled: false,
        
        
        
        
        
        
        
        
        
        
        
    }
        Kaggle.Current.log = function(){};
        Kaggle.Current.warn = function(){};

    var decodeUserDisplayName = function () {
        var escapedUserDisplayName = Kaggle.Current.userDisplayNameEscaped || "";
        try {
            var textVersion = new DOMParser().parseFromString(escapedUserDisplayName, "text/html").documentElement.textContent;
            if (textVersion) {
                return textVersion;
            }
        } catch(ex) {}
        return escapedUserDisplayName;
    }
    Kaggle.Current.userDisplayName = decodeUserDisplayName();
</script>

    

<script type="text/javascript">
    var Kaggle = Kaggle || {};
    Kaggle.PageMessages = [];
</script>

    
<script type="text/javascript">
/* <![CDATA[ */
goog_snippet_vars = function() {
    var w = window;
    w.google_conversion_id = 955616553;
    w.google_conversion_label = "QSjvCKDksHMQqZrWxwM";
    w.google_conversion_value = 0.00;
    w.google_conversion_currency = "USD";
    w.google_remarketing_only = false;
    w.google_conversion_language = "en";
    w.google_conversion_format = "3";
    w.google_conversion_color = "ffffff";
}
// DO NOT CHANGE THE CODE BELOW.
goog_report_conversion = function(url) {
    goog_snippet_vars();
    window.google_conversion_format = "3";
    var opt = new Object();
    opt.onload_callback = function() {
        if (typeof(url) != 'undefined') {
            window.location = url;
        }
    }
    var conv_handler = window['google_trackConversion'];
    if (typeof(conv_handler) == 'function') {
        conv_handler(opt);
    }
}
/* ]]> */
</script>
<script type="text/javascript"
src="//www.googleadservices.com/pagead/conversion_async.js">
</script>



        <script>window['useKaggleAnalytics'] = true;</script>

    <script src="/static/assets/vendor.js?v=4721d2c14786" data-turbolinks-track="reload"></script>
    <script src="/static/assets/app.js?v=d39bf3a6aba5" data-turbolinks-track="reload"></script>
        <script>
            (function() {
                if ('serviceWorker' in navigator) {
                    navigator.serviceWorker.register("/static/assets/service-worker.js").then(function(reg) {
                        reg.onupdatefound = function() {
                            var installingWorker = reg.installing;
                            installingWorker.onstatechange = function() {
                                switch (installingWorker.state) {
                                case 'installed':
                                    if (navigator.serviceWorker.controller) {
                                        console.log('New or updated content is available.');
                                    } else {
                                        console.log('Content is now available offline!');
                                    }
                                    break;
                                case 'redundant':
                                    console.error('The installing service worker became redundant.');
                                    break;
                                }
                            };
                        };
                    }).catch(function(e) {
                      console.error('Error during service worker registration:', e);
                    });
                }
            })();
        </script>
    <script>
        function handleClientLoad() {
            try {
                gapi.load('client:auth2');
            } catch (e) {
                // In Opera, readystatechange is an unreliable detection of script load, causing
                // this function to be called before gapi exists on the window. The onload callback
                // is still called at the correct time, so the feature works as expected - it's
                // just generating noisy errors.
            }
        }
    </script>
    <script async defer src="https://apis.google.com/js/api.js"
            onload="this.googleApiOnLoad=function(){};handleClientLoad()"
            onreadystatechange="if (this.readyState === 'complete') this.googleApiOnLoad()">
    </script>
</head>
<body data-turbolinks="true">
    <main>
        






<div class="site-layout">
        <div class="site-layout__header">
            <div data-component-name="SiteHeaderContainer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({});performance && performance.mark && performance.mark("SiteHeaderContainer.componentCouldBootstrap");</script>
        </div>

    <div class="site-layout__main-content">
        

<div data-component-name="KernelViewer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({"kernel":{"id":455390,"title":"Predicting Titanic Survival using Five Algorithms","forkParent":null,"currentRunId":1940189,"mostRecentRunId":1940189,"url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms","tags":[{"name":"beginner","slug":"beginner","url":"/tags/beginner"},{"name":"random forest","slug":"random-forest","url":"/tags/random-forest"},{"name":"logistic regression","slug":"logistic-regression","url":"/tags/logistic-regression"},{"name":"naive bayes","slug":"naive-bayes","url":"/tags/naive-bayes"},{"name":"svm","slug":"svm","url":"/tags/svm"}],"commentCount":0,"upvoteCount":57,"viewCount":9818,"forkCount":51,"bestPublicScore":null,"author":{"id":1407771,"displayName":"Thilaksha Silva","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"thilakshasilva","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1407771-kg.JPG","profileUrl":"/thilakshasilva","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":1,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"isPrivate":false,"updatedTime":"2017-12-14T04:46:24.71Z","selfLink":"/kernels/455390","pinnedDockerImageVersionId":null,"isLanguageTemplate":false,"medal":"bronze","topicId":45647,"readGroupId":null,"writeGroupId":null,"slug":"predicting-titanic-survival-using-five-algorithms"},"kernelBlob":{"id":6543276,"settings":{"dockerImageVersionId":34,"dataSources":[{"sourceType":"Competition","sourceId":3136,"databundleVersionId":null}],"sourceType":"script","language":"rmarkdown","isGpuEnabled":false,"isInternetEnabled":false},"source":"---\r\ntitle: \u0022Predicting Titanic Survival using Five Algorithms\u0022\r\nauthor: \u0027Thilaksha Silva\u0027\r\ndate: \u002702 December 2017\u0027\r\noutput:\r\n  html_document:\r\n    toc: true\r\n    number_sections: true\r\n    theme: readable\r\n    highlight: haddock\r\n---\r\n\r\n# Introduction\r\n\r\nI am stepping into the Machine Learning world with my first Kaggle competition! This real world classification problem helped me to greatly practice some predictive analytics techniques I have studied. \r\n\r\nMy script consists of five sections:\r\n\r\n* Handling Missing Data\r\n\r\n* Feature Engineering\r\n\r\n* Exploratory Data Analysis\r\n\r\n* Prediction\r\n\r\n* Discussion\r\n\r\nI imputed sensible values for features **Fare**, **Embarked** and **Age**.\r\n\r\nI also incorporated feature enginneering to create two new features, passenger **Title** and **FamilySize**, extracted from existing features.\r\n\r\nOn exploratory data analysis, I genuinely spent much time to analyse data with visual methods to summarize the main characteristics.\r\n\r\nWith this titanic dataset, I explore five classification algorithms: Logistic Regression, Support Vector Machines (both linear and non-linear), Decision Tree, Random Forest, and Naive Bayes.\r\n\r\nI will guide you through my journey of exploring titanic survival. I am eager to learn more and develop skills on machine learning. Any feedback is very welcome!\r\n\r\n\r\n## The Data\r\n\r\n```{r, message = FALSE, warning=FALSE}\r\n# Load all the packages required for the analysis\r\nlibrary(dplyr) # Data Manipulation\r\nlibrary(Amelia) # Missing Data: Missings Map\r\nlibrary(ggplot2) # Visualization\r\nlibrary(scales) # Visualization\r\nlibrary(caTools) # Prediction: Splitting Data\r\nlibrary(car) # Prediction: Checking Multicollinearity\r\nlibrary(ROCR) # Prediction: ROC Curve\r\nlibrary(e1071) # Prediction: SVM, Naive Bayes, Parameter Tuning\r\nlibrary(rpart) # Prediction: Decision Tree\r\nlibrary(rpart.plot) # Prediction: Decision Tree\r\nlibrary(randomForest) # Prediction: Random Forest\r\nlibrary(caret) # Prediction: k-Fold Cross Validation\r\n```\r\n\r\n```{r, warning=FALSE}\r\ntitanic_train = read.csv(\u0027../input/train.csv\u0027)\r\ntitanic_test = read.csv(\u0027../input/test.csv\u0027)\r\n\r\n# Combining data\r\ntitanic \u003c- bind_rows(titanic_train, titanic_test)\r\n\r\n# Checking the structure of the data\r\nstr(titanic)\r\n```\r\n\r\n\r\n# Handling Missing Data\r\n\r\n## Checking Missing Data\r\n```{r}\r\n# Checking missing values (missing values or empty values)\r\ncolSums(is.na(titanic)|titanic==\u0027\u0027)\r\n```\r\nCabin has the most number of missing values, 1014 values. Age has 263 missing values while Embarked and Fare have two and one missing values, respectively.\r\n\r\n`missmap` allows us to explore how much missing data we have.\r\n```{r}\r\nmissmap(titanic, main=\u0022Titanic Data - Missings Map\u0022,\r\n        col=c(\u0022yellow\u0022, \u0022black\u0022), legend=FALSE)\r\n```\r\n\r\n`missmap` function considers \u0022NA\u0022 values as missing values but it does not consider empty values as missing values. The missings map plot shows some of the age data is missing (because having a quick glance at the dataset we realise the missing age data are stored as NA). However, the plot does not show cabin has missing values (because missing cabin data are stored as empty values not NA values).\r\n\r\nThe next step is to fill the missing data rows instead of just dropping them. \r\n\r\n\r\n## Missing Fare Data Imputation\r\n\r\n```{r}\r\n# Extract the row which contains the missing Fare\r\nfilter(titanic, is.na(Fare)==TRUE|Fare==\u0027\u0027)\r\n```\r\nThis male was from the third class and had embarked from Southampton port. Let\u0027s look at the distribution of third class passengers embarked from Southampton port.\r\n\r\n```{r, warning=FALSE}\r\nggplot(filter(titanic, Pclass==3 \u0026 Embarked==\u0022S\u0022), aes(Fare)) +                       \r\n  geom_density(fill=\u0022blue\u0022, alpha=0.5) +\r\n  geom_vline(aes(xintercept=median(Fare, na.rm=T)), colour=\u0027darkblue\u0027, linetype=\u0027dashed\u0027, size=2) +\r\n  geom_vline(aes(xintercept=mean(Fare, na.rm=T)), colour=\u0027red\u0027, linetype=\u0027dashed\u0027, size=2) +\r\n  ggtitle(\u0022Fare distribution of third class passengers \\n embarked from Southampton port\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nThe mean and median fares are not close. The proportion of passengers with fares around median is very high. On the oher hand, it is not that high around mean. So, I believe it is not a good idea to impute the missing fare by the mean of all fares. I would rather impute the missing fare by the median fare of third class passengers embarked from Southampton port.\r\n\r\n```{r}\r\n# Impute the missing Fare value by the median fare of third class passengers embarked from Southampton port\r\ntitanic$Fare[is.na(titanic$Fare)==TRUE] = median(filter(titanic, Pclass==3 \u0026 Embarked==\u0022S\u0022)$Fare, na.rm=TRUE)\r\n\r\n# Checking missing values\r\ncolSums(is.na(titanic)|titanic==\u0027\u0027)\r\n```\r\nThe missing fare has been replaced.\r\n\r\n\r\n## Missing Embarked Data Imputation\r\n\r\nWe have noticed that there are two missing values for the Embarked feature.\r\n\r\n```{r}\r\n# Extract the rows which contain the missing Embarked values\r\nfilter(titanic, is.na(Embarked)==TRUE|Embarked==\u0027\u0027)\r\n```\r\nBoth were females from the first class with $80 fare and had stayed at the same cabin B28. There is a high chance that both embarked from the same port. Let\u0027s look at the frequency of ports of embarkation of first class passengers. \r\n\r\n```{r}\r\n# Frequency of ports of embarkation of first class passengers\r\ntable(filter(titanic, Pclass==1)$Embarked)\r\n```\r\nThe Southampton port is the most frequent port of embarkation with 177 ports and it is followed by the Cherbourg port with 141. Wait! Yet, we cannot decide to impute two missing values by the most frequent port of embarkation which is the Southampton port.  \r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Embarked)==FALSE \u0026 Embarked!=\u0027\u0027 \u0026 Pclass==1), \r\n       aes(Embarked, Fare)) +     \r\n  geom_boxplot(aes(colour = Embarked)) +\r\n  geom_hline(aes(yintercept=80), colour=\u0027red\u0027, linetype=\u0027dashed\u0027, size=2) +\r\n  ggtitle(\u0022Fare distribution of first class passengers\u0022) +\r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nThe box plot depicts the median fare for Cherbourg port passengers and $80 fare paid by two embarkement-deficient passengers almost concide. Thus, I am going to impute the missing Embarked values by the Cherbourg port.\r\n\r\n```{r}\r\n# Impute the missing Embarked values by the Cherbourg port\r\ntitanic$Embarked[titanic$Embarked==\u0022\u0022] = \u0022C\u0022\r\n\r\n# Checking missing values\r\ncolSums(is.na(titanic)|titanic==\u0027\u0027)\r\n```\r\n\r\n\r\n## Missing Age Data Imputation\r\n\r\n```{r, warning=FALSE}\r\nggplot(titanic,aes(Pclass,Age)) +                                                  \r\n  geom_boxplot(aes(fill=factor(Pclass)),alpha=0.5) +\r\n  ggtitle(\u0022Age distribution based on Pclass\u0022)\r\n```\r\n\r\nIt can be clearly seen the median age among classes is not similar (virtually certain, average age among classes is not similar as well). Infact, the passengers in the higher classes tend to be older. Rather than just imputing missing age values by the overall average for age, I will use average age values of each class to impute missing age values based on Pclass.\r\n\r\n```{r}\r\n# Imputation of Age based on Pclass\r\nimpute.age \u003c- function(age,class){\r\n  vector \u003c- age\r\n  for (i in 1:length(age)){\r\n    if (is.na(age[i])){\r\n      if (class[i] == 1){\r\n        vector[i] \u003c- round(mean(filter(titanic,Pclass==1)$Age, na.rm=TRUE),0)\r\n      }else if (class[i] == 2){\r\n        vector[i] \u003c- round(mean(filter(titanic,Pclass==2)$Age, na.rm=TRUE),0)\r\n      }else{\r\n        vector[i] \u003c- round(mean(filter(titanic,Pclass==3)$Age, na.rm=TRUE),0)\r\n      }\r\n    }else{\r\n      vector[i]\u003c-age[i]\r\n    }\r\n  }\r\n  return(vector)\r\n}\r\nimputed.age \u003c- impute.age(titanic$Age,titanic$Pclass)\r\ntitanic$Age \u003c- imputed.age\r\n```\r\n\r\nLet\u0027s check if the above imputation method worked.\r\n```{r}\r\n# Checking missing values\r\ncolSums(is.na(titanic)|titanic==\u0027\u0027)\r\n```\r\nIt worked. Now we are left with only Cabin missing values. However, due to the high number of missing values of Cabin feature, I keep the Cabin feature as it is and stop here.\r\n\r\n\r\n\r\n# Feature Engineering\r\n\r\n## Passenger Title\r\n\r\nSince the title of the passengers is contained within the passenger name feature, let\u0027s do some feature engineering to create a new feature with passenger titles.\r\n\r\n```{r}\r\nhead(titanic$Name)\r\n\r\n# Grab passenger title from passenger name\r\ntitanic$Title \u003c- gsub(\u0022^.*, (.*?)\\\\..*$\u0022, \u0022\\\\1\u0022, titanic$Name)\r\n\r\n# Frequency of each title by sex\r\ntable(titanic$Sex, titanic$Title)\r\n\r\n# First, I reassign few categories \r\ntitanic$Title[titanic$Title == \u0027Mlle\u0027 | titanic$Title == \u0027Ms\u0027] \u003c- \u0027Miss\u0027 \r\ntitanic$Title[titanic$Title == \u0027Mme\u0027]  \u003c- \u0027Mrs\u0027 \r\n\r\n# Then, I create a new category with low frequency of titles\r\nOther \u003c- c(\u0027Dona\u0027, \u0027Dr\u0027, \u0027Lady\u0027, \u0027the Countess\u0027,\u0027Capt\u0027, \u0027Col\u0027, \u0027Don\u0027, \u0027Jonkheer\u0027, \u0027Major\u0027, \u0027Rev\u0027, \u0027Sir\u0027)\r\ntitanic$Title[titanic$Title %in% Other]  \u003c- \u0027Other\u0027\r\n\r\n# Let\u0027s see if it worked\r\ntable(titanic$Sex, titanic$Title)\r\n```\r\nThe title is down to five categories. We will do exploratory analysis based on title in the next section. \r\n\r\n\r\n## Family Size\r\n\r\nI have also noticed that a new feature on family size can be created using some existing features such as **SigSp** and **Parch**.\r\n\r\n```{r}\r\nFamilySize \u003c- titanic$SibSp + titanic$Parch + 1\r\n\r\ntable(FamilySize)\r\n```\r\nThere are nine family sizes: 1 to 8 and 11. As this is too many categories, let\u0027s collapse some categories as follows.\r\n\r\n```{r}\r\n# Create a family size feature with three categories\r\ntitanic$FamilySize \u003c- sapply(1:nrow(titanic), function(x) \r\n                          ifelse(FamilySize[x]==1, \u0022Single\u0022, \r\n                          ifelse(FamilySize[x]\u003e4, \u0022Large\u0022, \u0022Small\u0022)))\r\n\r\ntable(titanic$FamilySize)\r\n```\r\nIn the next section, we will do some exploratory anaysis based on family size.\r\n\r\n\r\n\r\n# Exploratory Data Analysis\r\n\r\n## Encoding the categorical features as factors\r\n\r\n```{r}\r\ntitanic$Survived = factor(titanic$Survived)\r\ntitanic$Pclass = factor(titanic$Pclass)\r\ntitanic$Sex = factor(titanic$Sex)\r\ntitanic$Embarked = factor(titanic$Embarked)\r\ntitanic$Title = factor(titanic$Title)\r\ntitanic$FamilySize = factor(titanic$FamilySize, levels=c(\u0022Single\u0022,\u0022Small\u0022,\u0022Large\u0022))\r\n\r\n#Checking the structure of the data\r\nstr(titanic)\r\n```\r\n\r\n\r\n## Exploratory Data Analysis on Pclass, Sex and Age\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Pclass, fill=Survived)) + \r\n  geom_bar(aes(y = (..count..)/sum(..count..)), alpha=0.9, position=\u0022dodge\u0022) +\r\n  scale_fill_brewer(palette = \u0022Dark2\u0022, direction = -1) +\r\n  scale_y_continuous(labels=percent, breaks=seq(0,0.6,0.05)) +\r\n  ylab(\u0022Percentage\u0022) + \r\n  ggtitle(\u0022Survival Rate based on Pclass\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nOf course as expected, the wealthier passengers in the first class had a higher survival rate, roughly 15%, than the second class and third class passengers.\r\n\r\nLet\u0027s continue on by visualising data of some of the features.\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Sex, fill=Survived)) + \r\n  geom_bar(aes(y = (..count..)/sum(..count..)), alpha=0.9) +\r\n  facet_wrap(~Pclass) + \r\n  scale_fill_brewer(palette = \u0022Dark2\u0022, direction = -1) +\r\n  scale_y_continuous(labels=percent, breaks=seq(0,0.4,0.05)) +\r\n  ylab(\u0022Percentage\u0022) + \r\n  ggtitle(\u0022Survival Rate based on Pclass and Sex\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n```\r\n\r\nIt can be seen that females had a higher survival rate than males in each class. This makes sense due to women and children first policy. \r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Pclass, Age)) + \r\n  geom_violin(aes(fill=Survived), alpha=0.9) +\r\n  facet_wrap(~Survived) + \r\n  scale_fill_brewer(palette = \u0022Dark2\u0022, direction = -1) +\r\n  ggtitle(\u0022Survival Rate based on Pclass and Age\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nOverall, the passengers in the higher classes tend to be older disregard to whether they survived or not.\r\n\r\n\r\n## Exploratory Data Analysis on Title and FamilySize\r\n\r\n```{r}\r\nmosaicplot(~ Title + Survived, data=titanic, main=\u0027Survival Rate based on Title\u0027, shade=TRUE)\r\n```\r\n\r\nGenerally, male \u0022Mr\u0022 passengers had the poorest survival rate.\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Title)) + \r\n  geom_bar(aes(fill=Survived), alpha=0.9, position=\u0022fill\u0022) +\r\n  facet_wrap(~Pclass) + \r\n  scale_fill_brewer(palette=\u0022Set1\u0022) +\r\n  scale_y_continuous(labels=percent, breaks=seq(0,1,0.1)) +\r\n  ylab(\u0022Percentage\u0022) + \r\n  ggtitle(\u0022Survival Rate based on Pclass and Title\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n```\r\n\r\nThe same information can be depicted from the above graph - the male \u0022Mr\u0022 passengers had the lowest survival rate amongst all the classes.\r\n\r\n```{r}\r\nmosaicplot(~ FamilySize + Survived, data=titanic, main=\u0027Survival Rate based on FamilySize\u0027, shade=TRUE)\r\n```\r\n\r\nLarge families had the worst survival rate than singletons and small families.\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Title)) + \r\n  geom_bar(aes(fill=Survived), alpha=0.9, position=\u0022fill\u0022) +\r\n  facet_wrap(~FamilySize) + \r\n  scale_fill_brewer(palette=\u0022Set1\u0022) +\r\n  scale_y_continuous(labels=percent, breaks=seq(0,1,0.1)) +\r\n  ylab(\u0022Percentage\u0022) + \r\n  ggtitle(\u0022Survival Rate based on FamilySize and Title\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n```\r\n\r\nThe filled bar chart illustrates that we can preserve our rule: large families had the worst survival rate than singletons and small families. Infact, each member of the large families - Master, Miss, Mr and Mrs - suffered the lowest survival rate than their counterparts in other types of families.\r\n\r\n\r\n## Exploratory Data Analysis on Fare and Embarked\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Embarked, Fare)) + \r\n  geom_boxplot(aes(fill=Survived), alpha=0.9) +\r\n  facet_wrap(~Survived) + \r\n  scale_fill_manual(values=c(\u0022#56B4E9\u0022, \u0022#CC79A7\u0022)) +\r\n  ggtitle(\u0022Survival Rate based on Embarked and Fare\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nInterestingly, there is a substantial variation of fares in the survived category, especially from Cherbourg and Southampton ports. \r\n\r\nVisual analysis of data concludes:\r\n\r\n* the wealthier passengers in the first class had a higher survival rate;\r\n\r\n* females had a higher survival rate than males in each class;\r\n\r\n* male \u0022Mr\u0022 passengers had the lowest survival rate amongst all the classes; and\r\n\r\n* large families had the worst survival rate than singletons and small families.\r\n\r\nWoo-ah! After rectifying the missing values and exploring data visually, finally, we are ready to predict whether or not each passenger in the test set survived the sinking of the Titanic. I identify the following features may contribute to the prediction of the survival and include them in the classification algorithms: **Pclass**, **Sex**, **Age**, **SibSp**, **Parch**, **Fare**, **Embarked**, **Title** and **FamilySize**.\r\n\r\n* I ignore the feature **Name** as I have created a new feature Title from it and I believe the title has more predictive power than just name.\r\n\r\n* I ignore the feature **Ticket** as I believe it does not preserve any predictive power on survival.\r\n\r\n* I ignore the feature **Cabin** since it has many missing values.\r\n\r\n\r\n\r\n# Prediction\r\n\r\n## Splitting the dataset into the Training set and Test set\r\n\r\nAfter rectifying the missing values and encoding the categorical features as factors, now we are good to split the dataset into the training and test sets.\r\n\r\n```{r}\r\n# Splitting the dataset into the Training set and Test set\r\ntrain_original \u003c- titanic[1:891, c(\u0022Survived\u0022,\u0022Pclass\u0022,\u0022Sex\u0022,\u0022Age\u0022,\u0022SibSp\u0022,\u0022Parch\u0022,\u0022Fare\u0022,\u0022Embarked\u0022,\u0022Title\u0022,\u0022FamilySize\u0022)]\r\ntest_original \u003c- titanic[892:1309, c(\u0022Pclass\u0022,\u0022Sex\u0022,\u0022Age\u0022,\u0022SibSp\u0022,\u0022Parch\u0022,\u0022Fare\u0022,\u0022Embarked\u0022,\u0022Title\u0022,\u0022FamilySize\u0022)]\r\n```\r\n\r\n## Splitting the training set into the Training set and Validation set\r\n\r\nNow I split the training set into the training set (80% of training data) and validation set (20% of training data) for the evaluation purposes of the fitted models.\r\n\r\n```{r}\r\n# Splitting the Training set into the Training set and Validation set\r\nset.seed(789)\r\nsplit = sample.split(train_original$Survived, SplitRatio = 0.8)\r\ntrain = subset(train_original, split == TRUE)\r\ntest = subset(train_original, split == FALSE)\r\n```\r\n\r\n\r\n## Logistic Regression\r\n\r\nThe first maching learning classification algorithm I use in the analysis is the most popular and the simplest Logistic Regression. But wait! we cannot simply blindly fit the Logistic Regression. This algorithm comes with a price. We need to CHECK THE ASSUMPTIONS!!! Let\u0027s check the Logistic Regression assumptions: features should be independent from each other and residuals are not autocorrelated.\r\n\r\n```{r}\r\n# Show the correlation of numeric features\r\ncor(train[,unlist(lapply(train,is.numeric))])\r\n```\r\nIn statistics, two variables are strongly correlated if the correlation coefficient is either greater than 0.75 (some say 0.70 and some even say 0.8) or less than -0.75. Having a glance at the correlation matrix, none of the numeric features are strongly correlated. Hence, the **Multicollinearity** (a given feature in the model can be approximated by a linear combination of the other features in the model) does not exist among numeric features.\r\n\r\n```{r, warning=FALSE}\r\n# Show the p-value of Chi Square tests\r\nps = chisq.test(train$Pclass, train$Sex)$p.value\r\npe = chisq.test(train$Pclass, train$Embarked)$p.value\r\npt = chisq.test(train$Pclass, train$Title)$p.value\r\npf = chisq.test(train$Pclass, train$FamilySize)$p.value\r\nse = chisq.test(train$Sex, train$Embarked)$p.value\r\nst = chisq.test(train$Sex, train$Title)$p.value\r\nsf = chisq.test(train$Sex, train$FamilySize)$p.value\r\net = chisq.test(train$Embarked, train$Title)$p.value\r\nef = chisq.test(train$Embarked, train$FamilySize)$p.value\r\ntf = chisq.test(train$Title, train$FamilySize)$p.value\r\ncormatrix = matrix(c(0, ps, pe, pt, pf,\r\n                     ps, 0, se, st, sf,\r\n                     pe, se, 0, et, ef,\r\n                     pt, st, et, 0, tf,\r\n                     pf, sf, ef, tf, 0), \r\n                   5, 5, byrow = TRUE)\r\nrow.names(cormatrix) = colnames(cormatrix) = c(\u0022Pclass\u0022, \u0022Sex\u0022, \u0022Embarked\u0022, \u0022Title\u0022, \u0022FamilySize\u0022)\r\ncormatrix\r\n```\r\nI use Chi Square test to test the independence of factors/categorical features. Since all the p-values \u003c 0.05, we reject each Ho:Two factors are independent at 5% significance level and indeed at any reasonable level of significance. This violates the independence assumption of features and can be confirmed that multicollinearity does exist among factors. I will deal with this issue down the road and now go ahead and fit the logistic regression model.\r\n\r\n```{r}\r\n# Fitting Logistic Regression to the Training set\r\nclassifier = glm(Survived ~ ., family = binomial(link=\u0027logit\u0027), data = train)\r\n\r\n# Choosing the best model by AIC in a Stepwise Algorithm\r\n# The step() function iteratively removes insignificant predictor variables from the model.\r\nclassifier \u003c- step(classifier)\r\nsummary(classifier)\r\n```\r\nMmm... The factor **Sex** is not statistically significant at any reasonable level of significance (p-value = 0.976960 \u003e 0.05 or 0.01 or even 0.1); however, it is still in the best model. Furthermore, notice the standard error of **Sexmale**, **TitleMiss** and **TitleMrs** are very large. This has something (No!!! everything) to do with multicollinearity. As an effect of multicollinearity, the standard error (hence, the variance) of model coefficients for Sexmale, TitleMiss and TitleMrs became very large. \r\n\r\n```{r}\r\nvif(classifier)\r\n```\r\nAh hah! `vif` function delivers very high Generalized Variable Inflation Factor (GVIF) for factors Sex and Title. This confirmes the multicollinearity between factors **Sex** and **Title**.\r\n\r\n**_How to handle multicollinearity?_**\r\n\r\n\r\nWe want to make our model robust. I omit the factor **Sex** from the logistic regression model because it exhibits a high degree of multicollinearity.\r\n\r\n```{r}\r\n# Fitting Logistic Regression to the Training set again without the factor Sex \r\nclassifier = glm(Survived ~ . -Sex, family = binomial(link=\u0027logit\u0027), data = train)\r\n\r\n# Choosing the best model by AIC in a Stepwise Algorithm\r\n# The step() function iteratively removes insignificant features from the model.\r\nclassifier \u003c- step(classifier)\r\nsummary(classifier)\r\nvif(classifier)\r\ndurbinWatsonTest(classifier)\r\n```\r\nThe model looks good. The standard errors are in a reasonable range. GVIF values are all less than 5. Furthermore, since Durbin-Watson test results with D-W Statistic 1.96 and p-value \u003e 0.05, we do not reject Ho:Residuals are not autocorrelated. Hence, we can conclude there is sufficient evidence to say residuals are not autocorrelated. Hooray! The assumptions are checked and they are passed. \r\n\r\nAccording to the best model, the features **Pclass**, **Age**, **Title** and **FamilySize** significantly contribute to the model in predicting survival. We will see how well the model predicts on new data in the validation set. \r\n\r\n```{r}\r\n# Predicting the Validation set results\r\nprob_pred = predict(classifier, type = \u0027response\u0027, newdata = test)\r\ny_pred = ifelse(prob_pred \u003e 0.5, 1, 0)\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred \u003e 0.5) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n\r\n# Use the predictions to build a ROC curve to assess the performance of our model\r\nfitpred = prediction(prob_pred, test$Survived)\r\nfitperf = performance(fitpred,\u0022tpr\u0022,\u0022fpr\u0022)\r\nplot(fitperf,col=\u0022green\u0022,lwd=2,main=\u0022ROC Curve\u0022)\r\nabline(a=0,b=1,lwd=2,lty=2,col=\u0022gray\u0022)\r\n```\r\n\r\nThe ROC (Receiver Operating Characteristics) curve is a graphical representation of the performnace of the classifier and it shows the performance of our model rises well above the diagonal line. This indicates that our logistic regression model performs better than just a random guess. The logistic regression model delivers a whooping 0.8539 accuracy interms of predicting the survival.\r\n\r\n\r\n\r\n## Support Vector Machines\r\n\r\nSecondly, I use Support Vector Machines (SVM) for classification. In order to use SVM, we need to remember to do one thing - Feature Scaling! Because the SVM classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters.\r\n\r\n```{r}\r\n# Checking the variance of numeric features\r\npaste(\u0027Age variance: \u0027,var(train$Age),\u0027, SibSp variance: \u0027,var(train$SibSp),\u0027, Parch variance: \u0027,var(train$Parch),\u0027, Fare variance: \u0027,var(train$Fare))\r\n```\r\nThe variances of **Age** and **Fare** seem very high. Let\u0027s do feature scaling to standardize these features so that all features are on the same scale and no feature is dominated by the other.\r\n\r\n```{r}\r\n# Feature Scaling - use scale() to standardize the feature columns\r\nstandardized.train = cbind(select(train, Survived, Pclass, Sex, SibSp, Parch, Embarked, Title, FamilySize), Age = scale(train$Age), Fare = scale(train$Fare))\r\npaste(\u0027Age variance: \u0027,var(standardized.train$Age),\u0027, Fare variance: \u0027,var(standardized.train$Fare))\r\n\r\nstandardized.test = cbind(select(test, Survived, Pclass, Sex, SibSp, Parch, Embarked, Title, FamilySize), Age = scale(test$Age), Fare = scale(test$Fare))\r\npaste(\u0027Age variance: \u0027,var(standardized.test$Age),\u0027, Fare variance: \u0027,var(standardized.test$Fare))\r\n```\r\nNow that we have done the feature scaling, we can fit SVM to predict survival. First I fit a linear SVM.\r\n\r\n```{r}\r\n# Fitting Linear SVM to the Training set\r\nclassifier = svm(Survived ~ .,\r\n                 data = standardized.train,\r\n                 type = \u0027C-classification\u0027,\r\n                 kernel = \u0027linear\u0027)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nWow! The linear model accuracy is 0.8764. That is great and I am happy with the result. Let\u0027s fit a non-linear radial kernel and see whether the accuracy will be improved.\r\n\r\n```{r}\r\n# Fitting Non-linear SVM to the Training set\r\nclassifier = svm(Survived ~ .,\r\n                 data = standardized.train,\r\n                 type = \u0027C-classification\u0027,\r\n                 kernel = \u0027radial\u0027)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nYes! The accuracy has been improved by 1% to 0.8820. Also, note that both linear SVM and non-linear SVM accuracies are higher than the accuracy for logistic regression model. \r\n\r\n**_How about Parameter Tuning?_**\r\n\r\n\r\nThe best non-linear SVM performance occurs with cost=1 and gamma=0.0625. Now I tune these parameters to attempt to improve our model (remember, our model is already a good model).\r\n\r\n```{r}\r\n# Tuning the model\r\n# Applying Grid Search to find the best parameters\r\ntune.results \u003c- tune(svm,\r\n                     Survived ~ .,\r\n                     data = standardized.train,\r\n                     kernel=\u0027radial\u0027,\r\n                     ranges=list(cost=2^(-2:2), gamma=2^(-6:-2)))\r\nsummary(tune.results)\r\n# The best non-linear SVM performance occurs with cost=4 and gamma=0.125\r\n\r\n# Fitting Non-linear SVM to the Training set\r\nclassifier = svm(Survived ~ .,\r\n                 data = standardized.train,\r\n                 type = \u0027C-classification\u0027,\r\n                 kernel = \u0027radial\u0027,\r\n                 cost = 4,\r\n                 gamma = 0.125)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nThe accuracy went down to 0.8315. We were not able to improve our model. Infact, the best non-linear SVM was already a good model with accuracy 0.8820. I retain that model as my best non-linear SVM model with cost=1 and gamma=0.0625.\r\n\r\n\r\n\r\n## Decision Tree\r\n\r\nAs we all know, Random Forest is a more powerful algorithm over just a single tree. However, the Decision Tree classification preserve the interpretability which the random forest algorithm lacks. \r\n\r\nThe Decision Tree does not require feature scaling. Let\u0027s fit a decision tree model to our training data.\r\n\r\n```{r}\r\n# Fitting Decision Tree Classification Model to the Training set\r\nclassifier = rpart(Survived ~ ., data = train, method = \u0027class\u0027)\r\n\r\n# Tree Visualization\r\nrpart.plot(classifier, extra=4)\r\n```\r\n\r\nThe single tree uses five features **Title**, **Pclass**, **Fare**, **Age** and **FamilySize** for classification. Let\u0027s see how well our model performs with the data in the validation set.\r\n\r\n```{r}\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = test[,-which(names(test)==\u0022Survived\u0022)], type=\u0027class\u0027)\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nAccuracy of a single tree is 0.8371. Overfitting can easily occur in Decision Tree classification. We can idenfity that evaluating the model using **k-Fold Cross Validation**. Or we might be able to improve the model. Let\u0027s do 10-fold cross validation to find out whether we could improve the model. \r\n\r\n```{r}\r\n# Applying k-Fold Cross Validation\r\nset.seed(789)\r\nfolds = createMultiFolds(train$Survived, k = 10, times = 5)\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, index = folds)\r\nclassifier_cv \u003c- train(Survived ~ ., data = train, method = \u0022rpart\u0022, trControl = control)\r\n\r\n# Tree Visualization\r\nrpart.plot(classifier_cv$finalModel, extra=4)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier_cv, newdata = test[,-which(names(test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nWe were able to improve the model after 10-fold cross validation. The accuracy has been improved to 0.8427 but note the improved model uses only three features **Title**, **Pclass** and **Fare** for classification.\r\n\r\n\r\n\r\n## Random Forests\r\n\r\nRandom forests improve predictive accuracy by generating a large number of bootstrapped trees (based on random samples of variables). Random Forest is a prowerful machine learning algorithm which holds a relatively high classification accuracy.\r\n\r\n```{r}\r\n# Fitting Random Forest Classification to the Training set\r\nset.seed(432)\r\nclassifier = randomForest(Survived ~ ., data = train)\r\n\r\n# Choosing the number of trees\r\nplot(classifier)\r\n```\r\n\r\nThe green, black and red lines represent error rate for death, overall and survival, respectively. The overall error rate converges to around 17%. Interestingly, our model predicts death better than survival. Since the overall error rate converges to a constant and does not seem to further decrease, our choice of default 500 trees in the `randomForest` function is a good choice.\r\n\r\n```{r}\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = test[,-which(names(test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nThe accuracy is 0.8427 and which is greater than the accuracy of just a single tree 0.8371 (without 10-fold cross validation). Let\u0027s see if 10-fold cross validation can improve our model as it did for the Decision Tree classification.\r\n\r\n```{r}\r\n# Applying k-Fold Cross Validation\r\nset.seed(651)\r\nfolds = createMultiFolds(train$Survived, k = 10)\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, index = folds)\r\nclassifier_cv \u003c- train(Survived ~ ., data = train, method = \u0022rf\u0022, trControl = control)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier_cv, newdata = test[,-which(names(test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nAccuracy went down to 0.8258. We were not able to improve the random forest model using 10-fold cross validation.\r\n\r\nAs mentioned previously in the Decision Tree section, the random Forest classification suffers in terms of interpretability. We are unable to visualize the 500 trees and identify important features of the model. However, we can assess the **Feature Importance** using the Gini index measure. Let\u0027s plot mean Gini index across all trees and identify important features.\r\n\r\n```{r}\r\n# Feature Importance\r\ngini = as.data.frame(importance(classifier))\r\ngini = data.frame(Feature = row.names(gini), \r\n                  MeanGini = round(gini[ ,\u0027MeanDecreaseGini\u0027], 2))\r\ngini = gini[order(-gini[,\u0022MeanGini\u0022]),]\r\n\r\nggplot(gini,aes(reorder(Feature,MeanGini), MeanGini, group=1)) + \r\n  geom_point(color=\u0027red\u0027,shape=17,size=2) + \r\n  geom_line(color=\u0027blue\u0027,size=1) +\r\n  scale_y_continuous(breaks=seq(0,60,10)) +\r\n  xlab(\u0022Feature\u0022) + \r\n  ggtitle(\u0022Mean Gini Index of Features\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n```\r\n\r\nThe feature **Title** has the highest mean gini index, hence the highest importance. **Fare** is also realtively high important and it is followed by **Age** of the passengers. \r\n\r\n\r\n\r\n## Naive Bayes\r\n\r\nLastly, I use Naive Bayes algorithm to predict survival. Naive Bayes classification is a simple but effective algorithm; it is faster compared to many other iterative algorithms; it does not need feature scaling; and its foundation is the Bayes Theorem. \r\n\r\nHowever, Naive Bayes is based on the assumption that conditional probability of each feature given the class is independent of all the other features. The assumption of independent conditional probabilities means the features are completely independent of each other. This assumption was already checked in the Logistic Regression section and we have found that numeric features are independent to each other, however, the categorical features are not. By assuming the idependence assumption of all the features, let\u0027s fit a naive bayes model to our training data.\r\n\r\n```{r}\r\n# Fitting Naive Bayes to the Training set\r\nclassifier = naiveBayes(Survived ~ ., data = train)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = test[,-which(names(test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nWow! Naive Bayes classification performs well for our validation data with an accuracy of 0.8427. Note that the accuracy is identical to the Random Forest classification accuracy (without 10-fold cross validation).\r\n\r\n\r\n\r\n# Discussion\r\n\r\n## Comparison of models\r\n\r\nLogistic Regression\r\n\r\n* The assumptions were met and the accuracy of the best model is 0.8539. \r\n\r\n\r\n* Four features, **Title**, **Pclass**, **Age** and **FamilySize**, significantly contribute to the model in predicting survival.\r\n\r\n\r\n* Confusion matix consists of 9 false positives and 17 false negatives.\r\n\r\nLinear SVM\r\n\r\n* Scaled some features and gained an accuracy of 0.8764.\r\n\r\n* Confusion matix consists of 4 false positives and 18 false negatives.\r\n\r\nNon-linear Radial SVM\r\n\r\n* Scaled some features and secured the highest accuracy of 0.8820.\r\n\r\n* Parameter tuning did not improve the model accuracy.\r\n\r\n* Confusion matix consists of 5 false positives and 16 false negatives.\r\n\r\nDecision Tree\r\n\r\n* Were able to improve the model using 10-fold cross validation. \r\n\r\n* The accuracy was improved from 0.8371 to 0.8427 and the improved model uses three features, **Title**, **Fare** and **Pclass**, for classification. \r\n\r\n* Confusion matix consists of 11 false positives and 17 false negatives.\r\n\r\nRandom Forest\r\n\r\n* Gained an accuracy of 0.8427 and 10-fold cross validation did not improve the model accuracy. \r\n\r\n*  The first five important features based on their importance (highest to lowest) are as follows: **Title**, **Fare**, **Age**, **Sex** and **Pclass**. \r\n\r\n*  Confusion matix consists of 10 false positives and 18 false negatives.\r\n\r\nNaive Bayes\r\n\r\n* Gained an accuracy of 0.8427 which is identical to the Random Forest accuracy. \r\n\r\n* The independence among features was assumed.\r\n\r\n* Confusion matix consists of 11 false positives and 17 false negatives.\r\n\r\n\r\n## Results\r\n\r\n```{r}\r\n# Predicting the Test set results\r\ny_pred = predict(classifier, newdata = test_original)\r\n\r\n# Save the results\r\nresults \u003c- data.frame(PassengerID = titanic[892:1309,\u0022PassengerId\u0022], Survived = y_pred)\r\n\r\n# Write the results to a csv file\r\nwrite.csv(results, file = \u0027PredictingTitanicSurvival.csv\u0027, row.names = FALSE, quote=FALSE)\r\n```\r\n\r\nOnce I submitted my results from above six models, suprisingly, the worst performance was given by our best performer of the Validation Set, Non-linear SVM. Logistic Regression, Linear SVM, Decision Tree and Random Forest executed roughly the same score and closely followed by the Naive Bayes score.\r\n\r\n\r\n## Conclusion\r\n\r\nI am super happy I have finished my first kaggle kernel. With this exploration, I have acquired a great deal of knowledge about Machine Learning algorithms and their pros and cons. Also, I have developed skills on handling missing data, feature engineering and exploratory data analysis.\r\n\r\nI thank the kaggle community for publicly sharing the scripts which greatly helps new machine learning folks like me to learn and explore the field further.\r\n\r\n**Thank you everyone for reading my first kaggle kernel. I would be very much appreciated if you could upvote if you found my kernel useful or you just liked it. That will keep me motivated :) Also, I welcome your comments and suggestions!**","dateCreated":"2017-12-14T04:46:25.873Z"},"kernelRun":{"id":1940189,"kernelId":455390,"status":"complete","type":"batch","sourceType":"script","language":"rmarkdown","title":"Predicting Titanic Survival using Five Algorithms","dateCreated":"2017-12-14T04:46:25.873Z","dateEvaluated":"2017-12-14T04:46:24.71Z","workerContainerPort":null,"workerUptimeSeconds":465957,"workerIPAddress":"172.16.1.11    ","scriptLanguageId":5,"scriptLanguageName":"RMarkdown","renderedOutputUrl":"https://www.kaggleusercontent.com/kf/1940189/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..XP8B8c-fncvMw3jpdyNBfw.V7LD1P2e5yRrFyqnBftWJcgTT1GLlp4nFnfzJe2_PmxIE40TDWUQvGC5klYVzp5zrzuE3nCK9XbowBlSZFTWMpHUSjOwBFXcKHoGbjtFWnEj11hllAaYcyIQqt0WKBKgDzzb0_wpLxupOIk4lGDzuzL8ccmvOsy2iZz-T3PT0Nlh9SOYEBxPyv5LudhO97jJ.eMa-Pvat3w0G09t9t9yPqA/__results__.html","commit":{"id":6543276,"settings":{"dockerImageVersionId":34,"dataSources":[{"sourceType":"Competition","sourceId":3136,"databundleVersionId":null}],"sourceType":"script","language":"rmarkdown","isGpuEnabled":false,"isInternetEnabled":false},"source":"---\r\ntitle: \u0022Predicting Titanic Survival using Five Algorithms\u0022\r\nauthor: \u0027Thilaksha Silva\u0027\r\ndate: \u002702 December 2017\u0027\r\noutput:\r\n  html_document:\r\n    toc: true\r\n    number_sections: true\r\n    theme: readable\r\n    highlight: haddock\r\n---\r\n\r\n# Introduction\r\n\r\nI am stepping into the Machine Learning world with my first Kaggle competition! This real world classification problem helped me to greatly practice some predictive analytics techniques I have studied. \r\n\r\nMy script consists of five sections:\r\n\r\n* Handling Missing Data\r\n\r\n* Feature Engineering\r\n\r\n* Exploratory Data Analysis\r\n\r\n* Prediction\r\n\r\n* Discussion\r\n\r\nI imputed sensible values for features **Fare**, **Embarked** and **Age**.\r\n\r\nI also incorporated feature enginneering to create two new features, passenger **Title** and **FamilySize**, extracted from existing features.\r\n\r\nOn exploratory data analysis, I genuinely spent much time to analyse data with visual methods to summarize the main characteristics.\r\n\r\nWith this titanic dataset, I explore five classification algorithms: Logistic Regression, Support Vector Machines (both linear and non-linear), Decision Tree, Random Forest, and Naive Bayes.\r\n\r\nI will guide you through my journey of exploring titanic survival. I am eager to learn more and develop skills on machine learning. Any feedback is very welcome!\r\n\r\n\r\n## The Data\r\n\r\n```{r, message = FALSE, warning=FALSE}\r\n# Load all the packages required for the analysis\r\nlibrary(dplyr) # Data Manipulation\r\nlibrary(Amelia) # Missing Data: Missings Map\r\nlibrary(ggplot2) # Visualization\r\nlibrary(scales) # Visualization\r\nlibrary(caTools) # Prediction: Splitting Data\r\nlibrary(car) # Prediction: Checking Multicollinearity\r\nlibrary(ROCR) # Prediction: ROC Curve\r\nlibrary(e1071) # Prediction: SVM, Naive Bayes, Parameter Tuning\r\nlibrary(rpart) # Prediction: Decision Tree\r\nlibrary(rpart.plot) # Prediction: Decision Tree\r\nlibrary(randomForest) # Prediction: Random Forest\r\nlibrary(caret) # Prediction: k-Fold Cross Validation\r\n```\r\n\r\n```{r, warning=FALSE}\r\ntitanic_train = read.csv(\u0027../input/train.csv\u0027)\r\ntitanic_test = read.csv(\u0027../input/test.csv\u0027)\r\n\r\n# Combining data\r\ntitanic \u003c- bind_rows(titanic_train, titanic_test)\r\n\r\n# Checking the structure of the data\r\nstr(titanic)\r\n```\r\n\r\n\r\n# Handling Missing Data\r\n\r\n## Checking Missing Data\r\n```{r}\r\n# Checking missing values (missing values or empty values)\r\ncolSums(is.na(titanic)|titanic==\u0027\u0027)\r\n```\r\nCabin has the most number of missing values, 1014 values. Age has 263 missing values while Embarked and Fare have two and one missing values, respectively.\r\n\r\n`missmap` allows us to explore how much missing data we have.\r\n```{r}\r\nmissmap(titanic, main=\u0022Titanic Data - Missings Map\u0022,\r\n        col=c(\u0022yellow\u0022, \u0022black\u0022), legend=FALSE)\r\n```\r\n\r\n`missmap` function considers \u0022NA\u0022 values as missing values but it does not consider empty values as missing values. The missings map plot shows some of the age data is missing (because having a quick glance at the dataset we realise the missing age data are stored as NA). However, the plot does not show cabin has missing values (because missing cabin data are stored as empty values not NA values).\r\n\r\nThe next step is to fill the missing data rows instead of just dropping them. \r\n\r\n\r\n## Missing Fare Data Imputation\r\n\r\n```{r}\r\n# Extract the row which contains the missing Fare\r\nfilter(titanic, is.na(Fare)==TRUE|Fare==\u0027\u0027)\r\n```\r\nThis male was from the third class and had embarked from Southampton port. Let\u0027s look at the distribution of third class passengers embarked from Southampton port.\r\n\r\n```{r, warning=FALSE}\r\nggplot(filter(titanic, Pclass==3 \u0026 Embarked==\u0022S\u0022), aes(Fare)) +                       \r\n  geom_density(fill=\u0022blue\u0022, alpha=0.5) +\r\n  geom_vline(aes(xintercept=median(Fare, na.rm=T)), colour=\u0027darkblue\u0027, linetype=\u0027dashed\u0027, size=2) +\r\n  geom_vline(aes(xintercept=mean(Fare, na.rm=T)), colour=\u0027red\u0027, linetype=\u0027dashed\u0027, size=2) +\r\n  ggtitle(\u0022Fare distribution of third class passengers \\n embarked from Southampton port\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nThe mean and median fares are not close. The proportion of passengers with fares around median is very high. On the oher hand, it is not that high around mean. So, I believe it is not a good idea to impute the missing fare by the mean of all fares. I would rather impute the missing fare by the median fare of third class passengers embarked from Southampton port.\r\n\r\n```{r}\r\n# Impute the missing Fare value by the median fare of third class passengers embarked from Southampton port\r\ntitanic$Fare[is.na(titanic$Fare)==TRUE] = median(filter(titanic, Pclass==3 \u0026 Embarked==\u0022S\u0022)$Fare, na.rm=TRUE)\r\n\r\n# Checking missing values\r\ncolSums(is.na(titanic)|titanic==\u0027\u0027)\r\n```\r\nThe missing fare has been replaced.\r\n\r\n\r\n## Missing Embarked Data Imputation\r\n\r\nWe have noticed that there are two missing values for the Embarked feature.\r\n\r\n```{r}\r\n# Extract the rows which contain the missing Embarked values\r\nfilter(titanic, is.na(Embarked)==TRUE|Embarked==\u0027\u0027)\r\n```\r\nBoth were females from the first class with $80 fare and had stayed at the same cabin B28. There is a high chance that both embarked from the same port. Let\u0027s look at the frequency of ports of embarkation of first class passengers. \r\n\r\n```{r}\r\n# Frequency of ports of embarkation of first class passengers\r\ntable(filter(titanic, Pclass==1)$Embarked)\r\n```\r\nThe Southampton port is the most frequent port of embarkation with 177 ports and it is followed by the Cherbourg port with 141. Wait! Yet, we cannot decide to impute two missing values by the most frequent port of embarkation which is the Southampton port.  \r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Embarked)==FALSE \u0026 Embarked!=\u0027\u0027 \u0026 Pclass==1), \r\n       aes(Embarked, Fare)) +     \r\n  geom_boxplot(aes(colour = Embarked)) +\r\n  geom_hline(aes(yintercept=80), colour=\u0027red\u0027, linetype=\u0027dashed\u0027, size=2) +\r\n  ggtitle(\u0022Fare distribution of first class passengers\u0022) +\r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nThe box plot depicts the median fare for Cherbourg port passengers and $80 fare paid by two embarkement-deficient passengers almost concide. Thus, I am going to impute the missing Embarked values by the Cherbourg port.\r\n\r\n```{r}\r\n# Impute the missing Embarked values by the Cherbourg port\r\ntitanic$Embarked[titanic$Embarked==\u0022\u0022] = \u0022C\u0022\r\n\r\n# Checking missing values\r\ncolSums(is.na(titanic)|titanic==\u0027\u0027)\r\n```\r\n\r\n\r\n## Missing Age Data Imputation\r\n\r\n```{r, warning=FALSE}\r\nggplot(titanic,aes(Pclass,Age)) +                                                  \r\n  geom_boxplot(aes(fill=factor(Pclass)),alpha=0.5) +\r\n  ggtitle(\u0022Age distribution based on Pclass\u0022)\r\n```\r\n\r\nIt can be clearly seen the median age among classes is not similar (virtually certain, average age among classes is not similar as well). Infact, the passengers in the higher classes tend to be older. Rather than just imputing missing age values by the overall average for age, I will use average age values of each class to impute missing age values based on Pclass.\r\n\r\n```{r}\r\n# Imputation of Age based on Pclass\r\nimpute.age \u003c- function(age,class){\r\n  vector \u003c- age\r\n  for (i in 1:length(age)){\r\n    if (is.na(age[i])){\r\n      if (class[i] == 1){\r\n        vector[i] \u003c- round(mean(filter(titanic,Pclass==1)$Age, na.rm=TRUE),0)\r\n      }else if (class[i] == 2){\r\n        vector[i] \u003c- round(mean(filter(titanic,Pclass==2)$Age, na.rm=TRUE),0)\r\n      }else{\r\n        vector[i] \u003c- round(mean(filter(titanic,Pclass==3)$Age, na.rm=TRUE),0)\r\n      }\r\n    }else{\r\n      vector[i]\u003c-age[i]\r\n    }\r\n  }\r\n  return(vector)\r\n}\r\nimputed.age \u003c- impute.age(titanic$Age,titanic$Pclass)\r\ntitanic$Age \u003c- imputed.age\r\n```\r\n\r\nLet\u0027s check if the above imputation method worked.\r\n```{r}\r\n# Checking missing values\r\ncolSums(is.na(titanic)|titanic==\u0027\u0027)\r\n```\r\nIt worked. Now we are left with only Cabin missing values. However, due to the high number of missing values of Cabin feature, I keep the Cabin feature as it is and stop here.\r\n\r\n\r\n\r\n# Feature Engineering\r\n\r\n## Passenger Title\r\n\r\nSince the title of the passengers is contained within the passenger name feature, let\u0027s do some feature engineering to create a new feature with passenger titles.\r\n\r\n```{r}\r\nhead(titanic$Name)\r\n\r\n# Grab passenger title from passenger name\r\ntitanic$Title \u003c- gsub(\u0022^.*, (.*?)\\\\..*$\u0022, \u0022\\\\1\u0022, titanic$Name)\r\n\r\n# Frequency of each title by sex\r\ntable(titanic$Sex, titanic$Title)\r\n\r\n# First, I reassign few categories \r\ntitanic$Title[titanic$Title == \u0027Mlle\u0027 | titanic$Title == \u0027Ms\u0027] \u003c- \u0027Miss\u0027 \r\ntitanic$Title[titanic$Title == \u0027Mme\u0027]  \u003c- \u0027Mrs\u0027 \r\n\r\n# Then, I create a new category with low frequency of titles\r\nOther \u003c- c(\u0027Dona\u0027, \u0027Dr\u0027, \u0027Lady\u0027, \u0027the Countess\u0027,\u0027Capt\u0027, \u0027Col\u0027, \u0027Don\u0027, \u0027Jonkheer\u0027, \u0027Major\u0027, \u0027Rev\u0027, \u0027Sir\u0027)\r\ntitanic$Title[titanic$Title %in% Other]  \u003c- \u0027Other\u0027\r\n\r\n# Let\u0027s see if it worked\r\ntable(titanic$Sex, titanic$Title)\r\n```\r\nThe title is down to five categories. We will do exploratory analysis based on title in the next section. \r\n\r\n\r\n## Family Size\r\n\r\nI have also noticed that a new feature on family size can be created using some existing features such as **SigSp** and **Parch**.\r\n\r\n```{r}\r\nFamilySize \u003c- titanic$SibSp + titanic$Parch + 1\r\n\r\ntable(FamilySize)\r\n```\r\nThere are nine family sizes: 1 to 8 and 11. As this is too many categories, let\u0027s collapse some categories as follows.\r\n\r\n```{r}\r\n# Create a family size feature with three categories\r\ntitanic$FamilySize \u003c- sapply(1:nrow(titanic), function(x) \r\n                          ifelse(FamilySize[x]==1, \u0022Single\u0022, \r\n                          ifelse(FamilySize[x]\u003e4, \u0022Large\u0022, \u0022Small\u0022)))\r\n\r\ntable(titanic$FamilySize)\r\n```\r\nIn the next section, we will do some exploratory anaysis based on family size.\r\n\r\n\r\n\r\n# Exploratory Data Analysis\r\n\r\n## Encoding the categorical features as factors\r\n\r\n```{r}\r\ntitanic$Survived = factor(titanic$Survived)\r\ntitanic$Pclass = factor(titanic$Pclass)\r\ntitanic$Sex = factor(titanic$Sex)\r\ntitanic$Embarked = factor(titanic$Embarked)\r\ntitanic$Title = factor(titanic$Title)\r\ntitanic$FamilySize = factor(titanic$FamilySize, levels=c(\u0022Single\u0022,\u0022Small\u0022,\u0022Large\u0022))\r\n\r\n#Checking the structure of the data\r\nstr(titanic)\r\n```\r\n\r\n\r\n## Exploratory Data Analysis on Pclass, Sex and Age\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Pclass, fill=Survived)) + \r\n  geom_bar(aes(y = (..count..)/sum(..count..)), alpha=0.9, position=\u0022dodge\u0022) +\r\n  scale_fill_brewer(palette = \u0022Dark2\u0022, direction = -1) +\r\n  scale_y_continuous(labels=percent, breaks=seq(0,0.6,0.05)) +\r\n  ylab(\u0022Percentage\u0022) + \r\n  ggtitle(\u0022Survival Rate based on Pclass\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nOf course as expected, the wealthier passengers in the first class had a higher survival rate, roughly 15%, than the second class and third class passengers.\r\n\r\nLet\u0027s continue on by visualising data of some of the features.\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Sex, fill=Survived)) + \r\n  geom_bar(aes(y = (..count..)/sum(..count..)), alpha=0.9) +\r\n  facet_wrap(~Pclass) + \r\n  scale_fill_brewer(palette = \u0022Dark2\u0022, direction = -1) +\r\n  scale_y_continuous(labels=percent, breaks=seq(0,0.4,0.05)) +\r\n  ylab(\u0022Percentage\u0022) + \r\n  ggtitle(\u0022Survival Rate based on Pclass and Sex\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n```\r\n\r\nIt can be seen that females had a higher survival rate than males in each class. This makes sense due to women and children first policy. \r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Pclass, Age)) + \r\n  geom_violin(aes(fill=Survived), alpha=0.9) +\r\n  facet_wrap(~Survived) + \r\n  scale_fill_brewer(palette = \u0022Dark2\u0022, direction = -1) +\r\n  ggtitle(\u0022Survival Rate based on Pclass and Age\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nOverall, the passengers in the higher classes tend to be older disregard to whether they survived or not.\r\n\r\n\r\n## Exploratory Data Analysis on Title and FamilySize\r\n\r\n```{r}\r\nmosaicplot(~ Title + Survived, data=titanic, main=\u0027Survival Rate based on Title\u0027, shade=TRUE)\r\n```\r\n\r\nGenerally, male \u0022Mr\u0022 passengers had the poorest survival rate.\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Title)) + \r\n  geom_bar(aes(fill=Survived), alpha=0.9, position=\u0022fill\u0022) +\r\n  facet_wrap(~Pclass) + \r\n  scale_fill_brewer(palette=\u0022Set1\u0022) +\r\n  scale_y_continuous(labels=percent, breaks=seq(0,1,0.1)) +\r\n  ylab(\u0022Percentage\u0022) + \r\n  ggtitle(\u0022Survival Rate based on Pclass and Title\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n```\r\n\r\nThe same information can be depicted from the above graph - the male \u0022Mr\u0022 passengers had the lowest survival rate amongst all the classes.\r\n\r\n```{r}\r\nmosaicplot(~ FamilySize + Survived, data=titanic, main=\u0027Survival Rate based on FamilySize\u0027, shade=TRUE)\r\n```\r\n\r\nLarge families had the worst survival rate than singletons and small families.\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Title)) + \r\n  geom_bar(aes(fill=Survived), alpha=0.9, position=\u0022fill\u0022) +\r\n  facet_wrap(~FamilySize) + \r\n  scale_fill_brewer(palette=\u0022Set1\u0022) +\r\n  scale_y_continuous(labels=percent, breaks=seq(0,1,0.1)) +\r\n  ylab(\u0022Percentage\u0022) + \r\n  ggtitle(\u0022Survival Rate based on FamilySize and Title\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n```\r\n\r\nThe filled bar chart illustrates that we can preserve our rule: large families had the worst survival rate than singletons and small families. Infact, each member of the large families - Master, Miss, Mr and Mrs - suffered the lowest survival rate than their counterparts in other types of families.\r\n\r\n\r\n## Exploratory Data Analysis on Fare and Embarked\r\n\r\n```{r}\r\nggplot(filter(titanic, is.na(Survived)==FALSE), aes(Embarked, Fare)) + \r\n  geom_boxplot(aes(fill=Survived), alpha=0.9) +\r\n  facet_wrap(~Survived) + \r\n  scale_fill_manual(values=c(\u0022#56B4E9\u0022, \u0022#CC79A7\u0022)) +\r\n  ggtitle(\u0022Survival Rate based on Embarked and Fare\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n```\r\n\r\nInterestingly, there is a substantial variation of fares in the survived category, especially from Cherbourg and Southampton ports. \r\n\r\nVisual analysis of data concludes:\r\n\r\n* the wealthier passengers in the first class had a higher survival rate;\r\n\r\n* females had a higher survival rate than males in each class;\r\n\r\n* male \u0022Mr\u0022 passengers had the lowest survival rate amongst all the classes; and\r\n\r\n* large families had the worst survival rate than singletons and small families.\r\n\r\nWoo-ah! After rectifying the missing values and exploring data visually, finally, we are ready to predict whether or not each passenger in the test set survived the sinking of the Titanic. I identify the following features may contribute to the prediction of the survival and include them in the classification algorithms: **Pclass**, **Sex**, **Age**, **SibSp**, **Parch**, **Fare**, **Embarked**, **Title** and **FamilySize**.\r\n\r\n* I ignore the feature **Name** as I have created a new feature Title from it and I believe the title has more predictive power than just name.\r\n\r\n* I ignore the feature **Ticket** as I believe it does not preserve any predictive power on survival.\r\n\r\n* I ignore the feature **Cabin** since it has many missing values.\r\n\r\n\r\n\r\n# Prediction\r\n\r\n## Splitting the dataset into the Training set and Test set\r\n\r\nAfter rectifying the missing values and encoding the categorical features as factors, now we are good to split the dataset into the training and test sets.\r\n\r\n```{r}\r\n# Splitting the dataset into the Training set and Test set\r\ntrain_original \u003c- titanic[1:891, c(\u0022Survived\u0022,\u0022Pclass\u0022,\u0022Sex\u0022,\u0022Age\u0022,\u0022SibSp\u0022,\u0022Parch\u0022,\u0022Fare\u0022,\u0022Embarked\u0022,\u0022Title\u0022,\u0022FamilySize\u0022)]\r\ntest_original \u003c- titanic[892:1309, c(\u0022Pclass\u0022,\u0022Sex\u0022,\u0022Age\u0022,\u0022SibSp\u0022,\u0022Parch\u0022,\u0022Fare\u0022,\u0022Embarked\u0022,\u0022Title\u0022,\u0022FamilySize\u0022)]\r\n```\r\n\r\n## Splitting the training set into the Training set and Validation set\r\n\r\nNow I split the training set into the training set (80% of training data) and validation set (20% of training data) for the evaluation purposes of the fitted models.\r\n\r\n```{r}\r\n# Splitting the Training set into the Training set and Validation set\r\nset.seed(789)\r\nsplit = sample.split(train_original$Survived, SplitRatio = 0.8)\r\ntrain = subset(train_original, split == TRUE)\r\ntest = subset(train_original, split == FALSE)\r\n```\r\n\r\n\r\n## Logistic Regression\r\n\r\nThe first maching learning classification algorithm I use in the analysis is the most popular and the simplest Logistic Regression. But wait! we cannot simply blindly fit the Logistic Regression. This algorithm comes with a price. We need to CHECK THE ASSUMPTIONS!!! Let\u0027s check the Logistic Regression assumptions: features should be independent from each other and residuals are not autocorrelated.\r\n\r\n```{r}\r\n# Show the correlation of numeric features\r\ncor(train[,unlist(lapply(train,is.numeric))])\r\n```\r\nIn statistics, two variables are strongly correlated if the correlation coefficient is either greater than 0.75 (some say 0.70 and some even say 0.8) or less than -0.75. Having a glance at the correlation matrix, none of the numeric features are strongly correlated. Hence, the **Multicollinearity** (a given feature in the model can be approximated by a linear combination of the other features in the model) does not exist among numeric features.\r\n\r\n```{r, warning=FALSE}\r\n# Show the p-value of Chi Square tests\r\nps = chisq.test(train$Pclass, train$Sex)$p.value\r\npe = chisq.test(train$Pclass, train$Embarked)$p.value\r\npt = chisq.test(train$Pclass, train$Title)$p.value\r\npf = chisq.test(train$Pclass, train$FamilySize)$p.value\r\nse = chisq.test(train$Sex, train$Embarked)$p.value\r\nst = chisq.test(train$Sex, train$Title)$p.value\r\nsf = chisq.test(train$Sex, train$FamilySize)$p.value\r\net = chisq.test(train$Embarked, train$Title)$p.value\r\nef = chisq.test(train$Embarked, train$FamilySize)$p.value\r\ntf = chisq.test(train$Title, train$FamilySize)$p.value\r\ncormatrix = matrix(c(0, ps, pe, pt, pf,\r\n                     ps, 0, se, st, sf,\r\n                     pe, se, 0, et, ef,\r\n                     pt, st, et, 0, tf,\r\n                     pf, sf, ef, tf, 0), \r\n                   5, 5, byrow = TRUE)\r\nrow.names(cormatrix) = colnames(cormatrix) = c(\u0022Pclass\u0022, \u0022Sex\u0022, \u0022Embarked\u0022, \u0022Title\u0022, \u0022FamilySize\u0022)\r\ncormatrix\r\n```\r\nI use Chi Square test to test the independence of factors/categorical features. Since all the p-values \u003c 0.05, we reject each Ho:Two factors are independent at 5% significance level and indeed at any reasonable level of significance. This violates the independence assumption of features and can be confirmed that multicollinearity does exist among factors. I will deal with this issue down the road and now go ahead and fit the logistic regression model.\r\n\r\n```{r}\r\n# Fitting Logistic Regression to the Training set\r\nclassifier = glm(Survived ~ ., family = binomial(link=\u0027logit\u0027), data = train)\r\n\r\n# Choosing the best model by AIC in a Stepwise Algorithm\r\n# The step() function iteratively removes insignificant predictor variables from the model.\r\nclassifier \u003c- step(classifier)\r\nsummary(classifier)\r\n```\r\nMmm... The factor **Sex** is not statistically significant at any reasonable level of significance (p-value = 0.976960 \u003e 0.05 or 0.01 or even 0.1); however, it is still in the best model. Furthermore, notice the standard error of **Sexmale**, **TitleMiss** and **TitleMrs** are very large. This has something (No!!! everything) to do with multicollinearity. As an effect of multicollinearity, the standard error (hence, the variance) of model coefficients for Sexmale, TitleMiss and TitleMrs became very large. \r\n\r\n```{r}\r\nvif(classifier)\r\n```\r\nAh hah! `vif` function delivers very high Generalized Variable Inflation Factor (GVIF) for factors Sex and Title. This confirmes the multicollinearity between factors **Sex** and **Title**.\r\n\r\n**_How to handle multicollinearity?_**\r\n\r\n\r\nWe want to make our model robust. I omit the factor **Sex** from the logistic regression model because it exhibits a high degree of multicollinearity.\r\n\r\n```{r}\r\n# Fitting Logistic Regression to the Training set again without the factor Sex \r\nclassifier = glm(Survived ~ . -Sex, family = binomial(link=\u0027logit\u0027), data = train)\r\n\r\n# Choosing the best model by AIC in a Stepwise Algorithm\r\n# The step() function iteratively removes insignificant features from the model.\r\nclassifier \u003c- step(classifier)\r\nsummary(classifier)\r\nvif(classifier)\r\ndurbinWatsonTest(classifier)\r\n```\r\nThe model looks good. The standard errors are in a reasonable range. GVIF values are all less than 5. Furthermore, since Durbin-Watson test results with D-W Statistic 1.96 and p-value \u003e 0.05, we do not reject Ho:Residuals are not autocorrelated. Hence, we can conclude there is sufficient evidence to say residuals are not autocorrelated. Hooray! The assumptions are checked and they are passed. \r\n\r\nAccording to the best model, the features **Pclass**, **Age**, **Title** and **FamilySize** significantly contribute to the model in predicting survival. We will see how well the model predicts on new data in the validation set. \r\n\r\n```{r}\r\n# Predicting the Validation set results\r\nprob_pred = predict(classifier, type = \u0027response\u0027, newdata = test)\r\ny_pred = ifelse(prob_pred \u003e 0.5, 1, 0)\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred \u003e 0.5) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n\r\n# Use the predictions to build a ROC curve to assess the performance of our model\r\nfitpred = prediction(prob_pred, test$Survived)\r\nfitperf = performance(fitpred,\u0022tpr\u0022,\u0022fpr\u0022)\r\nplot(fitperf,col=\u0022green\u0022,lwd=2,main=\u0022ROC Curve\u0022)\r\nabline(a=0,b=1,lwd=2,lty=2,col=\u0022gray\u0022)\r\n```\r\n\r\nThe ROC (Receiver Operating Characteristics) curve is a graphical representation of the performnace of the classifier and it shows the performance of our model rises well above the diagonal line. This indicates that our logistic regression model performs better than just a random guess. The logistic regression model delivers a whooping 0.8539 accuracy interms of predicting the survival.\r\n\r\n\r\n\r\n## Support Vector Machines\r\n\r\nSecondly, I use Support Vector Machines (SVM) for classification. In order to use SVM, we need to remember to do one thing - Feature Scaling! Because the SVM classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters.\r\n\r\n```{r}\r\n# Checking the variance of numeric features\r\npaste(\u0027Age variance: \u0027,var(train$Age),\u0027, SibSp variance: \u0027,var(train$SibSp),\u0027, Parch variance: \u0027,var(train$Parch),\u0027, Fare variance: \u0027,var(train$Fare))\r\n```\r\nThe variances of **Age** and **Fare** seem very high. Let\u0027s do feature scaling to standardize these features so that all features are on the same scale and no feature is dominated by the other.\r\n\r\n```{r}\r\n# Feature Scaling - use scale() to standardize the feature columns\r\nstandardized.train = cbind(select(train, Survived, Pclass, Sex, SibSp, Parch, Embarked, Title, FamilySize), Age = scale(train$Age), Fare = scale(train$Fare))\r\npaste(\u0027Age variance: \u0027,var(standardized.train$Age),\u0027, Fare variance: \u0027,var(standardized.train$Fare))\r\n\r\nstandardized.test = cbind(select(test, Survived, Pclass, Sex, SibSp, Parch, Embarked, Title, FamilySize), Age = scale(test$Age), Fare = scale(test$Fare))\r\npaste(\u0027Age variance: \u0027,var(standardized.test$Age),\u0027, Fare variance: \u0027,var(standardized.test$Fare))\r\n```\r\nNow that we have done the feature scaling, we can fit SVM to predict survival. First I fit a linear SVM.\r\n\r\n```{r}\r\n# Fitting Linear SVM to the Training set\r\nclassifier = svm(Survived ~ .,\r\n                 data = standardized.train,\r\n                 type = \u0027C-classification\u0027,\r\n                 kernel = \u0027linear\u0027)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nWow! The linear model accuracy is 0.8764. That is great and I am happy with the result. Let\u0027s fit a non-linear radial kernel and see whether the accuracy will be improved.\r\n\r\n```{r}\r\n# Fitting Non-linear SVM to the Training set\r\nclassifier = svm(Survived ~ .,\r\n                 data = standardized.train,\r\n                 type = \u0027C-classification\u0027,\r\n                 kernel = \u0027radial\u0027)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nYes! The accuracy has been improved by 1% to 0.8820. Also, note that both linear SVM and non-linear SVM accuracies are higher than the accuracy for logistic regression model. \r\n\r\n**_How about Parameter Tuning?_**\r\n\r\n\r\nThe best non-linear SVM performance occurs with cost=1 and gamma=0.0625. Now I tune these parameters to attempt to improve our model (remember, our model is already a good model).\r\n\r\n```{r}\r\n# Tuning the model\r\n# Applying Grid Search to find the best parameters\r\ntune.results \u003c- tune(svm,\r\n                     Survived ~ .,\r\n                     data = standardized.train,\r\n                     kernel=\u0027radial\u0027,\r\n                     ranges=list(cost=2^(-2:2), gamma=2^(-6:-2)))\r\nsummary(tune.results)\r\n# The best non-linear SVM performance occurs with cost=4 and gamma=0.125\r\n\r\n# Fitting Non-linear SVM to the Training set\r\nclassifier = svm(Survived ~ .,\r\n                 data = standardized.train,\r\n                 type = \u0027C-classification\u0027,\r\n                 kernel = \u0027radial\u0027,\r\n                 cost = 4,\r\n                 gamma = 0.125)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nThe accuracy went down to 0.8315. We were not able to improve our model. Infact, the best non-linear SVM was already a good model with accuracy 0.8820. I retain that model as my best non-linear SVM model with cost=1 and gamma=0.0625.\r\n\r\n\r\n\r\n## Decision Tree\r\n\r\nAs we all know, Random Forest is a more powerful algorithm over just a single tree. However, the Decision Tree classification preserve the interpretability which the random forest algorithm lacks. \r\n\r\nThe Decision Tree does not require feature scaling. Let\u0027s fit a decision tree model to our training data.\r\n\r\n```{r}\r\n# Fitting Decision Tree Classification Model to the Training set\r\nclassifier = rpart(Survived ~ ., data = train, method = \u0027class\u0027)\r\n\r\n# Tree Visualization\r\nrpart.plot(classifier, extra=4)\r\n```\r\n\r\nThe single tree uses five features **Title**, **Pclass**, **Fare**, **Age** and **FamilySize** for classification. Let\u0027s see how well our model performs with the data in the validation set.\r\n\r\n```{r}\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = test[,-which(names(test)==\u0022Survived\u0022)], type=\u0027class\u0027)\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nAccuracy of a single tree is 0.8371. Overfitting can easily occur in Decision Tree classification. We can idenfity that evaluating the model using **k-Fold Cross Validation**. Or we might be able to improve the model. Let\u0027s do 10-fold cross validation to find out whether we could improve the model. \r\n\r\n```{r}\r\n# Applying k-Fold Cross Validation\r\nset.seed(789)\r\nfolds = createMultiFolds(train$Survived, k = 10, times = 5)\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, index = folds)\r\nclassifier_cv \u003c- train(Survived ~ ., data = train, method = \u0022rpart\u0022, trControl = control)\r\n\r\n# Tree Visualization\r\nrpart.plot(classifier_cv$finalModel, extra=4)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier_cv, newdata = test[,-which(names(test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nWe were able to improve the model after 10-fold cross validation. The accuracy has been improved to 0.8427 but note the improved model uses only three features **Title**, **Pclass** and **Fare** for classification.\r\n\r\n\r\n\r\n## Random Forests\r\n\r\nRandom forests improve predictive accuracy by generating a large number of bootstrapped trees (based on random samples of variables). Random Forest is a prowerful machine learning algorithm which holds a relatively high classification accuracy.\r\n\r\n```{r}\r\n# Fitting Random Forest Classification to the Training set\r\nset.seed(432)\r\nclassifier = randomForest(Survived ~ ., data = train)\r\n\r\n# Choosing the number of trees\r\nplot(classifier)\r\n```\r\n\r\nThe green, black and red lines represent error rate for death, overall and survival, respectively. The overall error rate converges to around 17%. Interestingly, our model predicts death better than survival. Since the overall error rate converges to a constant and does not seem to further decrease, our choice of default 500 trees in the `randomForest` function is a good choice.\r\n\r\n```{r}\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = test[,-which(names(test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nThe accuracy is 0.8427 and which is greater than the accuracy of just a single tree 0.8371 (without 10-fold cross validation). Let\u0027s see if 10-fold cross validation can improve our model as it did for the Decision Tree classification.\r\n\r\n```{r}\r\n# Applying k-Fold Cross Validation\r\nset.seed(651)\r\nfolds = createMultiFolds(train$Survived, k = 10)\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, index = folds)\r\nclassifier_cv \u003c- train(Survived ~ ., data = train, method = \u0022rf\u0022, trControl = control)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier_cv, newdata = test[,-which(names(test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nAccuracy went down to 0.8258. We were not able to improve the random forest model using 10-fold cross validation.\r\n\r\nAs mentioned previously in the Decision Tree section, the random Forest classification suffers in terms of interpretability. We are unable to visualize the 500 trees and identify important features of the model. However, we can assess the **Feature Importance** using the Gini index measure. Let\u0027s plot mean Gini index across all trees and identify important features.\r\n\r\n```{r}\r\n# Feature Importance\r\ngini = as.data.frame(importance(classifier))\r\ngini = data.frame(Feature = row.names(gini), \r\n                  MeanGini = round(gini[ ,\u0027MeanDecreaseGini\u0027], 2))\r\ngini = gini[order(-gini[,\u0022MeanGini\u0022]),]\r\n\r\nggplot(gini,aes(reorder(Feature,MeanGini), MeanGini, group=1)) + \r\n  geom_point(color=\u0027red\u0027,shape=17,size=2) + \r\n  geom_line(color=\u0027blue\u0027,size=1) +\r\n  scale_y_continuous(breaks=seq(0,60,10)) +\r\n  xlab(\u0022Feature\u0022) + \r\n  ggtitle(\u0022Mean Gini Index of Features\u0022) +\r\n  theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\r\n```\r\n\r\nThe feature **Title** has the highest mean gini index, hence the highest importance. **Fare** is also realtively high important and it is followed by **Age** of the passengers. \r\n\r\n\r\n\r\n## Naive Bayes\r\n\r\nLastly, I use Naive Bayes algorithm to predict survival. Naive Bayes classification is a simple but effective algorithm; it is faster compared to many other iterative algorithms; it does not need feature scaling; and its foundation is the Bayes Theorem. \r\n\r\nHowever, Naive Bayes is based on the assumption that conditional probability of each feature given the class is independent of all the other features. The assumption of independent conditional probabilities means the features are completely independent of each other. This assumption was already checked in the Logistic Regression section and we have found that numeric features are independent to each other, however, the categorical features are not. By assuming the idependence assumption of all the features, let\u0027s fit a naive bayes model to our training data.\r\n\r\n```{r}\r\n# Fitting Naive Bayes to the Training set\r\nclassifier = naiveBayes(Survived ~ ., data = train)\r\n\r\n# Predicting the Validation set results\r\ny_pred = predict(classifier, newdata = test[,-which(names(test)==\u0022Survived\u0022)])\r\n\r\n# Checking the prediction accuracy\r\ntable(test$Survived, y_pred) # Confusion matrix\r\nerror \u003c- mean(test$Survived != y_pred) # Misclassification error\r\npaste(\u0027Accuracy\u0027,round(1-error,4))\r\n```\r\nWow! Naive Bayes classification performs well for our validation data with an accuracy of 0.8427. Note that the accuracy is identical to the Random Forest classification accuracy (without 10-fold cross validation).\r\n\r\n\r\n\r\n# Discussion\r\n\r\n## Comparison of models\r\n\r\nLogistic Regression\r\n\r\n* The assumptions were met and the accuracy of the best model is 0.8539. \r\n\r\n\r\n* Four features, **Title**, **Pclass**, **Age** and **FamilySize**, significantly contribute to the model in predicting survival.\r\n\r\n\r\n* Confusion matix consists of 9 false positives and 17 false negatives.\r\n\r\nLinear SVM\r\n\r\n* Scaled some features and gained an accuracy of 0.8764.\r\n\r\n* Confusion matix consists of 4 false positives and 18 false negatives.\r\n\r\nNon-linear Radial SVM\r\n\r\n* Scaled some features and secured the highest accuracy of 0.8820.\r\n\r\n* Parameter tuning did not improve the model accuracy.\r\n\r\n* Confusion matix consists of 5 false positives and 16 false negatives.\r\n\r\nDecision Tree\r\n\r\n* Were able to improve the model using 10-fold cross validation. \r\n\r\n* The accuracy was improved from 0.8371 to 0.8427 and the improved model uses three features, **Title**, **Fare** and **Pclass**, for classification. \r\n\r\n* Confusion matix consists of 11 false positives and 17 false negatives.\r\n\r\nRandom Forest\r\n\r\n* Gained an accuracy of 0.8427 and 10-fold cross validation did not improve the model accuracy. \r\n\r\n*  The first five important features based on their importance (highest to lowest) are as follows: **Title**, **Fare**, **Age**, **Sex** and **Pclass**. \r\n\r\n*  Confusion matix consists of 10 false positives and 18 false negatives.\r\n\r\nNaive Bayes\r\n\r\n* Gained an accuracy of 0.8427 which is identical to the Random Forest accuracy. \r\n\r\n* The independence among features was assumed.\r\n\r\n* Confusion matix consists of 11 false positives and 17 false negatives.\r\n\r\n\r\n## Results\r\n\r\n```{r}\r\n# Predicting the Test set results\r\ny_pred = predict(classifier, newdata = test_original)\r\n\r\n# Save the results\r\nresults \u003c- data.frame(PassengerID = titanic[892:1309,\u0022PassengerId\u0022], Survived = y_pred)\r\n\r\n# Write the results to a csv file\r\nwrite.csv(results, file = \u0027PredictingTitanicSurvival.csv\u0027, row.names = FALSE, quote=FALSE)\r\n```\r\n\r\nOnce I submitted my results from above six models, suprisingly, the worst performance was given by our best performer of the Validation Set, Non-linear SVM. Logistic Regression, Linear SVM, Decision Tree and Random Forest executed roughly the same score and closely followed by the Naive Bayes score.\r\n\r\n\r\n## Conclusion\r\n\r\nI am super happy I have finished my first kaggle kernel. With this exploration, I have acquired a great deal of knowledge about Machine Learning algorithms and their pros and cons. Also, I have developed skills on handling missing data, feature engineering and exploratory data analysis.\r\n\r\nI thank the kaggle community for publicly sharing the scripts which greatly helps new machine learning folks like me to learn and explore the field further.\r\n\r\n**Thank you everyone for reading my first kaggle kernel. I would be very much appreciated if you could upvote if you found my kernel useful or you just liked it. That will keep me motivated :) Also, I welcome your comments and suggestions!**","dateCreated":"2017-12-14T04:46:25.873Z"},"resources":null,"isolatorResults":"\u003cresults\u003e\u003cdisk_kb_free\u003e940524\u003c/disk_kb_free\u003e\u003cdocker_image_id\u003esha256:f1b1e4ed8fbef48a9e5d120d057cb02d0080d777a5b0764135737edd10d36c3f\u003c/docker_image_id\u003e\u003cdocker_image_name\u003egcr.io/kaggle-images/rstats\u003c/docker_image_name\u003e\u003cexit_code\u003e0\u003c/exit_code\u003e\u003cfailure_message /\u003e\u003cout_of_memory\u003eFalse\u003c/out_of_memory\u003e\u003crun_time_seconds\u003e127.652272453008\u003c/run_time_seconds\u003e\u003csucceeded\u003eTrue\u003c/succeeded\u003e\u003ctimeout_exceeded\u003eFalse\u003c/timeout_exceeded\u003e\u003cused_all_space\u003eFalse\u003c/used_all_space\u003e\u003cwas_killed\u003eFalse\u003c/was_killed\u003e\u003c/results\u003e","runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageDigest":null,"dockerImageId":"sha256:f1b1e4ed8fbef48a9e5d120d057cb02d0080d777a5b0764135737edd10d36c3f","dockerImageName":"kaggle/rstats","diskKbFree":940524,"failureMessage":"","exitCode":0,"queuedSeconds":0,"outputSizeBytes":0,"runTimeSeconds":127.652272453008,"usedAllSpace":false,"timeoutExceeded":false,"isValidStatus":false,"wasGpuEnabled":false,"wasInternetEnabled":false,"outOfMemory":false,"invalidPathErrors":false,"succeeded":true,"wasKilled":false},"outputFilesTotalSizeBytes":4749147,"dockerImageVersionId":34,"usedCustomDockerImage":false},"author":{"id":1407771,"displayName":"Thilaksha Silva","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"thilakshasilva","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1407771-kg.JPG","profileUrl":"/thilakshasilva","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":1,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"baseUrl":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms","collaborators":{"owner":{"userId":1407771,"groupId":null,"groupMemberCount":null,"profileUrl":"/thilakshasilva","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1407771-kg.JPG","name":"Thilaksha Silva","slug":"thilakshasilva","userTier":1,"joinDate":null,"type":"owner","isUser":true,"isGroup":false},"collaborators":[]},"initialTab":null,"log":"[{\n  \u0022data\u0022: \u0022\\n\\nprocessing file: script.Rmd\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.0749007560079917\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |                                                                 |   0%\\r  |                                                                       \\r  |.                                                                |   1%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2.2210838649771176\n},{\n  \u0022data\u0022: \u0022  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.                                                                |   2%\\nlabel: unnamed-chunk-1 (with options) \\nList of 2\\n $ message: logi FALSE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 2.2521814549691044\n},{\n  \u0022data\u0022: \u0022\\nAttaching package: \u0027dplyr\u0027\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.367559442005586\n},{\n  \u0022data\u0022: \u0022The following objects are masked from \u0027package:stats\u0027:\\n\\n    filter, lag\\n\\nThe following objects are masked from \u0027package:base\u0027:\\n\\n    intersect, setdiff, setequal, union\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.3989209319697693\n},{\n  \u0022data\u0022: \u0022Loading required package: Rcpp\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.4354191870079376\n},{\n  \u0022data\u0022: \u0022## \\n## Amelia II: Multiple Imputation\\n## (Version 1.7.4, built: 2015-12-05)\\n## Copyright (C) 2005-2017 James Honaker, Gary King and Matthew Blackwell\\n## Refer to http://gking.harvard.edu/amelia/ for more information\\n## \\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 2.4694975469610654\n},{\n  \u0022data\u0022: \u0022\\nAttaching package: \u0027car\u0027\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 3.8215826519881375\n},{\n  \u0022data\u0022: \u0022The following object is masked from \u0027package:dplyr\u0027:\\n\\n    recode\\n\\nLoading required package: gplots\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 3.85399192199111\n},{\n  \u0022data\u0022: \u0022\\nAttaching package: \u0027gplots\u0027\\n\\nThe following object is masked from \u0027package:stats\u0027:\\n\\n    lowess\\n\\nLoading required package: methods\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 3.8844942859723233\n},{\n  \u0022data\u0022: \u0022randomForest 4.6-12\\nType rfNews() to see new features/changes/bug fixes.\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.132936890004203\n},{\n  \u0022data\u0022: \u0022\\nAttaching package: \u0027randomForest\u0027\\n\\nThe following object is masked from \u0027package:ggplot2\u0027:\\n\\n    margin\\n\\nThe following object is masked from \u0027package:dplyr\u0027:\\n\\n    combine\\n\\nLoading required package: lattice\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.165417285985313\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..                                                               |   3%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 5.453016340965405\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...                                                              |   4%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 5.48607008298859\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-2 (with options) \\nList of 1\\n $ warning:\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 5.834845025965478\n},{\n  \u0022data\u0022: \u0022 logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 5.86769689200446\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...                                                              |   5%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....                                                             |   6%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.115229700983036\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-3\\n\\r  |                                                                       \\r  |.....                                                            |   7%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....                                                            |   8%\\nlabel: unnamed-chunk-4\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.148324824986048\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |......                                                           |   9%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.......                                                          |  10%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.333383586956188\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-5\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.365675610955805\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.......                                                          |  11%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.402165931009222\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |........                                                         |  12%\\nlabel: unnamed-chunk-6 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 6.433078120986465\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........                                                        |  13%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.........                                                        |  14%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 7.914806474989746\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-7\\n\\r  |                                                                       \\r  |..........                                                       |  15%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...........                                                      |  16%\\nlabel: unnamed-chunk-8\\n\\r  |                                                                       \\r  |...........                                                      |  18%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............                                                     |  19%\\nlabel: unnamed-chunk-9\\n\\r  |                                                                       \\r  |.............                                                    |  20%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.............                                                    |  21%\\nlabel: unnamed-chunk-10\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 7.947068999987096\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............                                                   |  22%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............                                                  |  23%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 9.2028817589744\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-11\\n\\r  |                                                                       \\r  |...............                                                  |  24%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................                                                 |  25%\\nlabel: unnamed-chunk-12 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 9.233869301970117\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.................                                                |  26%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................                                                |  27%\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 10.204589160974137\n},{\n  \u0022data\u0022: \u0022\\nlabel: unnamed-chunk-13\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 10.23660609696526\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................                                               |  28%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................                                              |  29%\\nlabel: unnamed-chunk-14\\n\\r  |                                                                       \\r  |...................                                              |  30%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....................                                             |  31%\\nlabel: unnamed-chunk-15\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.523047044000123\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.....................                                            |  32%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....................                                            |  33%\\nlabel: unnamed-chunk-16\\n\\r  |                                                                       \\r  |......................                                           |  34%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.......................                                          |  35%\\nlabel: unnamed-chunk-17\\n\\r  |                                                                       \\r  |.......................                                          |  36%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........................                                         |  37%\\nlabel: unnamed-chunk-18\\n\\r  |                                                                       \\r  |.........................                                        |  38%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.........................                                        |  39%\\nlabel: unnamed-chunk-19\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.631553886982147\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..........................                                       |  40%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...........................                                      |  41%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.840663642971776\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-20\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.874128162977286\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...........................                                      |  42%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................                                     |  43%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 13.819810524000786\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-21\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 13.850985714991111\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................                                    |  44%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 14.976882660004776\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................                                    |  45%\\nlabel: unnamed-chunk-22\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 15.007679600967094\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............................                                   |  46%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 15.14328595297411\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...............................                                  |  47%\\nlabel: unnamed-chunk-23\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 15.175076096958946\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...............................                                  |  48%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................................                                 |  49%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 15.997974118974525\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-24\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 16.03059951798059\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.................................                                |  51%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 16.154409573005978\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................                               |  52%\\nlabel: unnamed-chunk-25\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 16.185270616959315\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................                               |  53%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................................                              |  54%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 16.984633011976257\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-26\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.016438249964267\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |....................................                             |  55%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.782022986968514\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |....................................                             |  56%\\nlabel: unnamed-chunk-27\\n\\r  |                                                                       \\r  |.....................................                            |  57%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......................................                           |  58%\\nlabel: unnamed-chunk-28\\n\\r  |                                                                       \\r  |......................................                           |  59%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.......................................                          |  60%\\nlabel: unnamed-chunk-29\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.814825929002836\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |........................................                         |  61%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........................................                         |  62%\\nlabel: unnamed-chunk-30 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.845723458973225\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................................                        |  63%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........................................                       |  64%\\nlabel: unnamed-chunk-31\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.877149896987248\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..........................................                       |  65%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...........................................                      |  66%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 19.275642491993494\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-32\\n\\r  |                                                                       \\r  |............................................                     |  67%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................................                     |  68%\\nlabel: unnamed-chunk-33\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 19.308295134978835\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................................                    |  69%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..............................................                   |  70%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.18214212899329\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-34\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.214876297977753\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............................................                   |  71%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............................................                  |  72%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.469889989995863\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-35\\n\\r  |                                                                       \\r  |................................................                 |  73%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................................................                 |  74%\\nlabel: unnamed-chunk-36\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.5024655929883\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.................................................                |  75%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..................................................               |  76%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.59265977598261\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-37\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.623725507990457\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................................               |  77%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................................................              |  78%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.768444777000695\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-38\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 21.801457781984936\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |....................................................             |  79%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....................................................             |  80%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 22.08411294798134\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-39\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 22.116104116954375\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.....................................................            |  81%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......................................................           |  82%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 38.447833534970414\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-40\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 38.48266447999049\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |......................................................           |  84%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 39.40888604801148\n},{\n  \u0022data\u0022: \u0022\\r  |.......................................................          |  85%\\nlabel: unnamed-chunk-41\\n\\r  |                                                                       \\r  |........................................................         |  86%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........................................................         |  87%\\nlabel: unnamed-chunk-42\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 39.443388182960916\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................................................        |  88%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........................................................       |  89%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 50.93589102599071\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-43\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 50.97024212300312\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..........................................................       |  90%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...........................................................      |  91%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 51.34568358998513\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-44\\n\\r  |                                                                       \\r  |............................................................     |  92%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................................................     |  93%\\nlabel: unnamed-chunk-45\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 51.38011039496632\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................................................    |  94%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 126.19502654799726\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............................................................   |  95%\\nlabel: unnamed-chunk-46\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 126.22856658499222\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............................................................   |  96%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............................................................  |  97%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 126.60131996200653\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-47\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 126.6360134530114\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |................................................................ |  98%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 126.70005341095384\n},{\n  \u0022data\u0022: \u0022  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................................................................ |  99%\\nlabel: unnamed-chunk-48\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 126.73310444096569\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.................................................................| 100%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 126.88143311499152\n},{\n  \u0022data\u0022: \u0022\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 126.91445621399907\n},{\n  \u0022data\u0022: \u0022output file: /kaggle/working/script.knit.md\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 126.91878452897072\n},{\n  \u0022data\u0022: \u0022/usr/local/bin/pandoc +RTS -K512m -RTS /kaggle/working/script.utf8.md --to html --from markdown+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash --output /kaggle/working/__results__.html --smart --email-obfuscation none --standalone --section-divs --table-of-contents --toc-depth 3 --template /usr/local/lib/R/site-library/rmarkdown/rmd/h/default.html --highlight-style haddock --number-sections --variable \u0027theme:readable\u0027 --include-in-header /tmp/RtmpSXBGKs/rmarkdown-str12d645feb.html --mathjax --variable \u0027mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0027 \\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 127.06336674897466\n},{\n  \u0022data\u0022: \u0022\\nOutput created: __results__.html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 127.38924796995707\n},{\n  \u0022data\u0022: \u0022There were 19 warnings (use warnings() to see them)\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 127.42338649899466\n}]","outputFiles":[{"ownerInfo":null,"kernelVersionOutputFileId":9282013,"kernelVersionId":1940189,"kernelId":455390,"size":0,"fullPath":"PredictingTitanicSurvival.csv","previewUrl":"/kernels/preview.json/1940189/77b03aca-7a75-fabc-af51-cdc0e11db0ef/PredictingTitanicSurvival.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/1940189/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..RJmESyvPtmbCu6ARwYeffA.hEuHCYWqjxh3Dv6C4wpyx3P65A-il7Nj8YwHbeG4iox96yI162PgbZ5Nbm4Xl1YG5qlcIp5hjjXW753Gps1Mit0l5CzPBDg8RA0iWh-tBkS7QJu6BAVIGHyulYXhTgOKDoMQVgyG-GxtdyDfun8EYPDh_aaGgvSLiVard7Cm9p7URHp28zpeDqEs5hgI__UE.hwSD9AhgbZDhcEVbu_XQpA/PredictingTitanicSurvival.csv","fileType":".csv","contentLength":2839,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"PredictingTitanicSurvival.csv","description":null}],"outputFilesCropped":false,"ouputFilesOwnerInfo":{"databundleVersionId":0,"dataset":null,"competition":null,"kernel":{"kernelId":455390,"kernelVersionId":1940189,"dataviewToken":"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JnIK41y2P0iVkgZmijoiNA.VgfMFjZWBPFBJ9jCCNc1CeuwnZk_1P1FU9B-_7ufB5f7aY5EGuMfvO2IcFuz-NCQ1a_DxdKQ5vMqAS1uYA3kpOAm5-ulGRoc0loR2-zwQF9HEP_nsqEjPOtKFXjhjEO9xsLLY8HACAkMkZWRBh5oThAyCNjaCii1zTqOyuLs0XQ03DimJikM8-7pOcWU8OiuQvM8kwaNl9_lv8rVpPvyhr5Aza5FIZt4exDZRks-vShpbZqm_qekSJih3YkvjYcFcijdepX74vPjpp2J3-RjGpWpikDRmQOdFY7ZmJodzmTKAncfzw8T6tpsBv6Ovc-B5qDCags4kxz1oE7myH8dUWarkIq3wSAykiHLDLtHae0jIegF0rPnX0ZexCZh6O9xJkw-FMgpdH7c4aZwNOQyRSQPG7DtHQ38SMSGwLmDhurZTDwmtU113S8Wyx7NZOCbvONy2NJ3xE8JhdNIv_BF8PLXqgEMaNhNyOVJ0-twvESmpH1J62EV0Klmwjsb_nHRhd7ILMCwKBJpXG1hFcpJh441B0YIUU9DCQkUERJFHeGyIAxyKbCfEj8H4uO5wgG7RzFBPmk2s-TR_Rrk6Nx61ilM5XjDwWI3YVKhMNV-OIM.o7pvxLiqid2d2GTQi8uRRg","scope":"thilakshasilva/predicting-titanic-survival-using-five-algorithms"},"previewsDisabled":false},"pageMessages":[],"dataSources":[{"imageUrl":"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png","sourceUrl":"/c/titanic","slug":"titanic","lastUpdated":"2012-09-28T21:13:33.55Z","overview":"Start here! Predict survival on the Titanic and get familiar with ML basics","sourceType":"competition","sourceVersionType":null,"sourceId":3136,"sourceVersionNumber":null,"maxVersionNumber":null,"descriptionMimeType":"text/html","deleted":false,"private":false,"privateButVisible":false,"ownerInfo":{"databundleVersionId":26502,"dataset":null,"competition":{"competitionId":3136,"dataviewToken":null,"scope":"c/titanic"},"kernel":null,"previewsDisabled":true},"type":"dataSource","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"id":63842,"blobFileId":37991,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/gender_submission.csv","creationDate":"2017-02-01T01:49:18Z","isDummy":false,"size":3258,"fullPath":"../input/gender_submission.csv","previewUrl":"kernels/competition-preview/3136?relativePath=gender_submission.csv","downloadUrl":"/c/titanic/download/gender_submission.csv","fileType":".csv","contentLength":3258,"contentType":"text/csv","contentMD5":"MNEHO5ZKXYFUMexgOg3jUw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":418},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n"},{"order":1,"originalType":"","type":"boolean","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Survived","description":"0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"gender_submission.csv","description":"892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,0\n899,0\n900,0\n901,0\n902,0\n903,0\n904,1\n905,0a\n906,1\n907,0\n908,0\n909,0\n910,0\n911,0\n912,1\n913,0\n914,0\n915,0\n916,1\n917,0\n918,0\n919,0\n920,0\n921,0\n922,0\n923,0\n924,0\n925,0\n926,1\n927,0\n928,0\n929,0\n930,0\n931,1\n932,0\n933,0\n934,0\n935,0\n936,1\n937,0\n938,0\n939,0\n940,1\n941,0\n942,1\n943,0\n944,0\n945,1\n946,0\n947,0\n948,0\n949,0\n950,0\n951,1\n952,0\n953,0\n954,0\n955,0\n956,1\n957,0\n958,0\n959,0\n960,0\n961,1\n962,\n963,0\n964,0\n965,0\n966,1\n967,1\n968,0\n969,0\n970,0\n971,0\n972,0\n973,1\n974,0\n975,0\n976,0\n977,0\n978,0\n979,0\n980,0\n981,0\n982,0\n983,0\n984,0\n985,0\n986,0\n987,0\n988,1\n989,0\n990,0\n991,0\n992,1\n993,0\n994,0\n995,0\n996,0\n997,0\n998,0\n999,0\n1000,0\n1001,0\n1002,0\n1003,0\n1004,0\n1005,0\n1006,1\n1007,0\n1008,0\n1009,0\n1010,1\n1011,0\n1012,0\n1013,0\n1014,1\n1015,0\n1016,0\n1017,0\n1018,0\n1019,0\n1020,0\n1021,0\n1022,0\n1023,0\n1024,0\n1025,0\n1026,0\n1027,0\n1028,0\n1029,0\n1030,0\n1031,0\n1032,0\n1033,1\n1034,1\n1035,0\n1036,0\n1037,0\n1038,1\n1039,0\n1040,0\n1041,0\n1042,1\n1043,0\n1044,0\n1045,0\n1046,0\n1047,0\n1048,1\n1049,0\n1050,0\n1051,0\n1052,0\n1053,0\n1054,0\n1055,0\n1056,0\n1057,0\n1058,1\n1059,0\n1060,0\n1061,0\n1062,0\n1063,0\n1064,0\n1065,0\n1066,0\n1067,0\n1068,0\n1069,1\n1070,0\n1071,1\n1072,0\n1073,1\n1074,1\n1075,0\n1076,1\n1077,0\n1078,0\n1079,0\n1080,1\n1081,0\n1082,0\n1083,0\n1084,0\n1085,0\n1086,0\n1087,0\n1088,1\n1089,0\n1090,0\n1091,0\n1092,0\n1093,0\n1094,1\n1095,0\n1096,0\n1097,0\n1098,0\n1099,0\n1100,0\n1101,0\n1102,0\n1103,0\n1104,1\n1105,0\n1106,0\n1107,0\n1108,0\n1109,1\n1110,1\n1111,0\n1112,0\n1113,0\n1114,0\n1115,0\n1116,0\n1117,0\n1118,0\n1119,0\n1120,0\n1121,0\n1122,1\n1123,0\n1124,0\n1125,0\n1126,1\n1127,0\n1128,1\n1129,0\n1130,0\n1131,1\n1132,0\n1133,0\n1134,1\n1135,0\n1136,0\n1137,1\n1138,0\n1139,0\n1140,0\n1141,0\n1142,0\n1143,0\n1144,1\n1145,0\n1146,0\n1147,0\n1148,0\n1149,0\n1150,0\n1151,0\n1152,0\n1153,0\n1154,0\n1155,0\n1156,0\n1157,0\n1158,0\n1159,0\n1160,0\n1161,0\n1162,1\n1163,0\n1164,1\n1165,0\n1166,0\n1167,0\n1168,0\n1169,0\n1170,0\n1171,0\n1172,0\n1173,0\n1174,0\n1175,0\n1176,0\n1177,0\n1178,0\n1179,1\n1180,0\n1181,0\n1182,0\n1183,0\n1184,0\n1185,1\n1186,0\n1187,0\n1188,0\n1189,0\n1190,1\n1191,0\n1192,0\n1193,0\n1194,0\n1195,0\n1196,0\n1197,0\n1198,1\n1199,0\n1200,1\n1201,0\n1202,0\n1203,0\n1204,0\n1205,0\n1206,1\n1207,0\n1208,1\n1209,0\n1210,0\n1211,0\n1212,0\n1213,0\n1214,0\n1215,0\n1216,1\n1217,0\n1218,0\n1219,1\n1220,0\n1221,0\n1222,0\n1223,0\n1224,0\n1225,0\n1226,0\n1227,0\n1228,0\n1229,0\n1230,0\n1231,0\n1232,0\n1233,0\n1234,1\n1235,1\n1236,0\n1237,0\n1238,0\n1239,0\n1240,0\n1241,0\n1242,1\n1243,0\n1244,1\n1245,1\n1246,0\n1247,0\n1248,1\n1249,0\n1250,0\n1251,0\n1252,1\n1253,0\n1254,0\n1255,0\n1256,1\n1257,1\n1258,0\n1259,0\n1260,0\n1261,0\n1262,0\n1263,1\n1264,0\n1265,0\n1266,1\n1267,1\n1268,0\n1269,0\n1270,1\n1271,0\n1272,0\n1273,0\n1274,0\n1275,0\n1276,0\n1277,1\n1278,0\n1279,0\n1280,0\n1281,0\n1282,1\n1283,0\n1284,0\n1285,0\n1286,0\n1287,1\n1288,0\n1289,1\n1290,0\n1291,0\n1292,1\n1293,0\n1294,0\n1295,1\n1296,0\n1297,0\n1298,0\n1299,1\n1300,0\n1301,0\n1302,0\n1303,1\n1304,0\n1305,0\n1306,1\n1307,0\n1308,0\n1309,0"},{"id":63841,"blobFileId":2613,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/test.csv","creationDate":"2013-06-28T13:40:24.227Z","isDummy":false,"size":28629,"fullPath":"../input/test.csv","previewUrl":"kernels/competition-preview/3136?relativePath=test.csv","downloadUrl":"/c/titanic/download/test.csv","fileType":".csv","contentLength":28629,"contentType":"text/csv","contentMD5":"dTO4Lq5LWCYQy9aKpjawFw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":418},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"1"},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Pclass","description":"1"},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Name","description":"the name of the passenger"},{"order":3,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Sex","description":null},{"order":4,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Age","description":null},{"order":5,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"SibSp","description":"of siblings / spouses aboard the Titanic"},{"order":6,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Parch","description":"of parents / children aboard the Titanic"},{"order":7,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Ticket","description":"Ticket number"},{"order":8,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Fare","description":"Passenger fare"},{"order":9,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Cabin","description":"Cabin number"},{"order":10,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Embarked","description":"Port of Embarkation"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"test.csv","description":"test data to check the accuracy of the model created\n"},{"id":63840,"blobFileId":2307,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/train.csv","creationDate":"2013-06-28T13:40:25.23Z","isDummy":false,"size":61194,"fullPath":"../input/train.csv","previewUrl":"kernels/competition-preview/3136?relativePath=train.csv","downloadUrl":"/c/titanic/download/train.csv","fileType":".csv","contentLength":61194,"contentType":"text/csv","contentMD5":"IwnMXwR4Ltm7YBbZ9OOBzw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":891},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"type should be integers"},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Survived","description":"Survived or Not "},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Pclass","description":"Class of Travel"},{"order":3,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Name","description":"Name of Passenger"},{"order":4,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Sex","description":"Gender"},{"order":5,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Age","description":"Age of Passengers"},{"order":6,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"SibSp","description":"Number of Sibling/Spouse aboard"},{"order":7,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Parch","description":"Number of Parent/Child aboard"},{"order":8,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Ticket","description":null},{"order":9,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Fare","description":null},{"order":10,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Cabin","description":null},{"order":11,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Embarked","description":"The port in which a passenger has embarked. C - Cherbourg, S - Southampton, Q = Queenstown"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"train.csv","description":"contains data \n"}],"name":"Titanic: Machine Learning from Disaster","description":"\u003ch3\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe data has been split into two groups:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etraining set (train.csv)\u003c/li\u003e\n\u003cli\u003etest set (test.csv)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003e The training set \u003c/b\u003eshould be used to build your machine learning models. For the training set, we provide the outcome (also known as the ground truth) for each passenger. Your model will be based on features like passengers gender and class. You can also use \u003ca href=\u0022https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\u0022 target=\u0022_blank\u0022\u003e feature engineering \u003c/a\u003eto create new features.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eThe test set \u003c/b\u003eshould be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\u003c/p\u003e\n\u003cp\u003eWe also include \u003cb\u003egender_submission.csv\u003c/b\u003e, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\u003c/p\u003e\n\u003ch3\u003eData Dictionary\u003c/h3\u003e\n\u003ctable style=\u0022width: 100%;\u0022\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\u003cth\u003e\u003cb\u003eVariable\u003c/b\u003e\u003c/th\u003e\u003cth\u003e\u003cb\u003eDefinition\u003c/b\u003e\u003c/th\u003e\u003cth\u003e\u003cb\u003eKey\u003c/b\u003e\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esurvival\u003c/td\u003e\n\u003ctd\u003eSurvival\u003c/td\u003e\n\u003ctd\u003e0 = No, 1 = Yes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epclass\u003c/td\u003e\n\u003ctd\u003eTicket class\u003c/td\u003e\n\u003ctd\u003e1 = 1st, 2 = 2nd, 3 = 3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esex\u003c/td\u003e\n\u003ctd\u003eSex\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAge\u003c/td\u003e\n\u003ctd\u003eAge in years\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esibsp\u003c/td\u003e\n\u003ctd\u003e# of siblings / spouses aboard the Titanic\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eparch\u003c/td\u003e\n\u003ctd\u003e# of parents / children aboard the Titanic\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eticket\u003c/td\u003e\n\u003ctd\u003eTicket number\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003efare\u003c/td\u003e\n\u003ctd\u003ePassenger fare\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecabin\u003c/td\u003e\n\u003ctd\u003eCabin number\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eembarked\u003c/td\u003e\n\u003ctd\u003ePort of Embarkation\u003c/td\u003e\n\u003ctd\u003eC = Cherbourg, Q = Queenstown, S = Southampton\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eVariable Notes\u003c/h3\u003e\n\u003cp\u003e\u003cb\u003epclass\u003c/b\u003e: A proxy for socio-economic status (SES)\u003cbr /\u003e 1st = Upper\u003cbr /\u003e 2nd = Middle\u003cbr /\u003e 3rd = Lower\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003eage\u003c/b\u003e: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003esibsp\u003c/b\u003e: The dataset defines family relations in this way...\u003cbr /\u003e Sibling = brother, sister, stepbrother, stepsister\u003cbr /\u003e Spouse = husband, wife (mistresses and fiancs were ignored)\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003eparch\u003c/b\u003e: The dataset defines family relations in this way...\u003cbr /\u003e Parent = mother, father\u003cbr /\u003e Child = daughter, son, stepdaughter, stepson\u003cbr /\u003e Some children travelled only with a nanny, therefore parch=0 for them.\u003c/p\u003e"}],"versions":[{"id":1940189,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2017-12-14T04:46:25.873Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":1,"linesInsertedFromPrevious":1,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:f1b1e4ed8fbef48a9e5d120d057cb02d0080d777a5b0764135737edd10d36c3f","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":127.652272453008,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1940189","versionNumber":11,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1940043,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2017-12-14T04:05:31.657Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":0,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:f1b1e4ed8fbef48a9e5d120d057cb02d0080d777a5b0764135737edd10d36c3f","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":124.67705317802,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1940043","versionNumber":10,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1861034,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2017-12-04T01:15:03.613Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":3,"linesInsertedFromPrevious":1,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":117.348601156002,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1861034","versionNumber":9,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1850276,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2017-12-02T08:18:15.013Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":1,"linesInsertedFromPrevious":1,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":120.012929477998,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1850276","versionNumber":8,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1850149,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2017-12-02T07:59:15.987Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":1,"linesInsertedFromPrevious":5,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":127.787613517001,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1850149","versionNumber":7,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1849155,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2017-12-02T03:59:19.643Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":5,"linesInsertedFromPrevious":1,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":132.368468247001,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1849155","versionNumber":6,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1849117,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2017-12-02T03:46:24.83Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":2,"linesInsertedFromPrevious":2,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":128.115054157,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1849117","versionNumber":5,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1849107,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2017-12-02T03:43:36.767Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":0,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":1,"failureMessage":"The kernel returned an unsuccessful exit code (1).","isValidStatus":true,"runTimeSeconds":8.5201504760189,"succeeded":false,"timeoutExceeded":false,"usedAllSpace":false},"status":"error","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1849107","versionNumber":4,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1849104,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"R","lastRunTime":"2017-12-02T03:43:15.533Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":29,"linesInsertedFromPrevious":811,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":1,"failureMessage":"The kernel returned an unsuccessful exit code (1).","isValidStatus":true,"runTimeSeconds":0.902972286001386,"succeeded":false,"timeoutExceeded":false,"usedAllSpace":false},"status":"error","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1849104","versionNumber":3,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1811287,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"R","lastRunTime":"2017-11-26T11:39:19.207Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":10,"linesInsertedFromPrevious":1,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":1,"failureMessage":"The kernel returned an unsuccessful exit code (1).","isValidStatus":true,"runTimeSeconds":0.730123576999176,"succeeded":false,"timeoutExceeded":false,"usedAllSpace":false},"status":"error","title":"Predicting Titanic Survival using Five Algorithms","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1811287","versionNumber":2,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null},{"id":1811282,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"R","lastRunTime":"2017-11-26T11:38:44.173Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":39,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:9ea6ab1a817a1c4f71202acf3ca1f13ed730b1ba200a9514ac3a2f628fd6cbca","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":1,"failureMessage":"The kernel returned an unsuccessful exit code (1).","isValidStatus":true,"runTimeSeconds":0.742920960998163,"succeeded":false,"timeoutExceeded":false,"usedAllSpace":false},"status":"error","title":"Predicting Titanic Survival","url":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms?scriptVersionId=1811282","versionNumber":1,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null}],"categories":{"categories":[{"id":13102,"name":"beginner","displayName":"beginner","fullPath":"audience \u003e beginner","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27beginner%27","tagUrl":"/tags/beginner","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":87,"competitionCount":0,"scriptCount":8307,"totalCount":8394},{"id":13402,"name":"random forest","displayName":"random forest","fullPath":"algorithms \u003e random forest","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27random forest%27","tagUrl":"/tags/random-forest","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":19,"competitionCount":0,"scriptCount":640,"totalCount":659},{"id":13404,"name":"logistic regression","displayName":"logistic regression","fullPath":"algorithms \u003e logistic regression","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27logistic regression%27","tagUrl":"/tags/logistic-regression","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":23,"competitionCount":0,"scriptCount":529,"totalCount":552},{"id":13411,"name":"svm","displayName":"SVM","fullPath":"algorithms \u003e svm","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27svm%27","tagUrl":"/tags/svm","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":6,"competitionCount":0,"scriptCount":217,"totalCount":223},{"id":13406,"name":"naive bayes","displayName":"naive bayes","fullPath":"algorithms \u003e naive bayes","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27naive bayes%27","tagUrl":"/tags/naive-bayes","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":5,"competitionCount":0,"scriptCount":138,"totalCount":143}],"type":"script"},"submitToCompetitionInfo":null,"downloadAllFilesUrl":"/kernels/svzip/1940189","submission":null,"menuLinks":[{"href":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms/notebook","text":"Report","title":"Notebook","tab":"report","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms/code","text":"Code","title":"Code","tab":"code","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms/data","text":"Data","title":"Data","tab":"data","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms/output","text":"Output","title":"Output","tab":"output","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms/comments","text":"Comments","title":"Comments","tab":"comments","count":15,"showZeroCountExplicitly":true,"reportEventCategory":null,"reportEventType":null},{"href":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms/log","text":"Log","title":"Log","tab":"log","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms/versions","text":"Versions","title":"Versions","tab":"versions","count":11,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/thilakshasilva/predicting-titanic-survival-using-five-algorithms/forks","text":"Forks","title":"Forks","tab":"forks","count":51,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null}],"rightMenuLinks":[],"callToAction":{"href":"/kernels/fork-version/1940189","text":"Fork Script","title":"Fork Script","tab":null,"count":null,"showZeroCountExplicitly":false,"reportEventCategory":"kernels","reportEventType":"anonymousKernelForkCreation"},"voteButton":{"totalVotes":57,"hasAlreadyVotedUp":false,"hasAlreadyVotedDown":false,"canUpvote":true,"canDownvote":false,"voteUpUrl":"/kernels/vote?id=455390","voteDownUrl":null,"voters":[{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/394642-fb.jpg","displayName":"Gloria Li","profileUrl":"/gloriali","tier":"Novice","tierInt":0,"userId":394642,"userName":"gloriali"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Jenn Gordienko","profileUrl":"/brklngirl","tier":"Novice","tierInt":0,"userId":395409,"userName":"brklngirl"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Ugur Yigit","profileUrl":"/uguryigit","tier":"Novice","tierInt":0,"userId":681642,"userName":"uguryigit"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/703333-kg.jpg","displayName":"Angelbeast","profileUrl":"/angelry","tier":"Novice","tierInt":0,"userId":703333,"userName":"angelry"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/726625-kg.jpg","displayName":"kashyapbarua","profileUrl":"/kashyapbarua","tier":"Contributor","tierInt":1,"userId":726625,"userName":"kashyapbarua"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/727797-kg.jpg","displayName":"Anisotropic","profileUrl":"/arthurtok","tier":"Grandmaster","tierInt":4,"userId":727797,"userName":"arthurtok"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/739786-kg.JPG","displayName":"Frdric Kosmowski","profileUrl":"/fkosmowski","tier":"Contributor","tierInt":1,"userId":739786,"userName":"fkosmowski"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Prince Modi","profileUrl":"/princemodi2329","tier":"Novice","tierInt":0,"userId":757561,"userName":"princemodi2329"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Kasper P. Lauritzen","profileUrl":"/kplauritzen","tier":"Contributor","tierInt":1,"userId":759773,"userName":"kplauritzen"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"adx349","profileUrl":"/abhishekdandona","tier":"Novice","tierInt":0,"userId":915465,"userName":"abhishekdandona"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/923208-kg.JPG","displayName":"Ashwin Balachandar S","profileUrl":"/ashivash","tier":"Contributor","tierInt":1,"userId":923208,"userName":"ashivash"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/932832-kg.png","displayName":"KumarHalake","profileUrl":"/kumarhalake","tier":"Contributor","tierInt":1,"userId":932832,"userName":"kumarhalake"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"LeoLee","profileUrl":"/guangzhengli","tier":"Novice","tierInt":0,"userId":993697,"userName":"guangzhengli"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1113123-fb.jpg","displayName":"Mousumi","profileUrl":"/mousumisinha","tier":"Novice","tierInt":0,"userId":1113123,"userName":"mousumisinha"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1148711-gr.jpg","displayName":"Arun S","profileUrl":"/arun010203","tier":"Novice","tierInt":0,"userId":1148711,"userName":"arun010203"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1219008-kg.png","displayName":"Selim ","profileUrl":"/yssefunc","tier":"Contributor","tierInt":1,"userId":1219008,"userName":"yssefunc"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1240075-kg.jpg","displayName":"Dan Kaziyev","profileUrl":"/kzsorry","tier":"Novice","tierInt":0,"userId":1240075,"userName":"kzsorry"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1381225-gr.jpg","displayName":"PRANEETH KALLURI","profileUrl":"/ksrpraneeth","tier":"Novice","tierInt":0,"userId":1381225,"userName":"ksrpraneeth"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1385372-fb.jpg","displayName":"Natasja","profileUrl":"/natasja","tier":"Novice","tierInt":0,"userId":1385372,"userName":"natasja"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Guomj","profileUrl":"/gmjftcdc","tier":"Novice","tierInt":0,"userId":1394002,"userName":"gmjftcdc"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1407771-kg.JPG","displayName":"Thilaksha Silva","profileUrl":"/thilakshasilva","tier":"Contributor","tierInt":1,"userId":1407771,"userName":"thilakshasilva"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Din S","profileUrl":"/chinnisomuri","tier":"Novice","tierInt":0,"userId":1468767,"userName":"chinnisomuri"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"neon","profileUrl":"/neon2chen","tier":"Novice","tierInt":0,"userId":1468992,"userName":"neon2chen"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1469270-gp.jpg","displayName":"Xiao Shi Liang (Shelwin)","profileUrl":"/shelwin","tier":"Novice","tierInt":0,"userId":1469270,"userName":"shelwin"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"zhangyulei","profileUrl":"/zhangyulei","tier":"Novice","tierInt":0,"userId":1472153,"userName":"zhangyulei"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1546403-fb.jpg","displayName":"Elias Calva","profileUrl":"/eliasc76","tier":"Novice","tierInt":0,"userId":1546403,"userName":"eliasc76"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Kishan","profileUrl":"/branji","tier":"Novice","tierInt":0,"userId":1572375,"userName":"branji"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Tereza Nanasi","profileUrl":"/terezan","tier":"Contributor","tierInt":1,"userId":1585390,"userName":"terezan"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1636313-fb.jpg","displayName":"Darien Schettler","profileUrl":"/dschettler8845","tier":"Contributor","tierInt":1,"userId":1636313,"userName":"dschettler8845"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1661505-fb.jpg","displayName":"Jungwon Seo","profileUrl":"/wsbtgt","tier":"Novice","tierInt":0,"userId":1661505,"userName":"wsbtgt"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1691344-fb.jpg","displayName":"Saigiriraj","profileUrl":"/padegal","tier":"Novice","tierInt":0,"userId":1691344,"userName":"padegal"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1701411-fb.jpg","displayName":"Vinnie","profileUrl":"/vinicius35","tier":"Contributor","tierInt":1,"userId":1701411,"userName":"vinicius35"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1723677-kg.jpg","displayName":"Chris Deotte","profileUrl":"/cdeotte","tier":"Grandmaster","tierInt":4,"userId":1723677,"userName":"cdeotte"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1743547-gp.jpg","displayName":"VINAY HARITSA","profileUrl":"/vinaymharitsa","tier":"Novice","tierInt":0,"userId":1743547,"userName":"vinaymharitsa"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Radhe Shyam Thakur","profileUrl":"/radhethakur","tier":"Novice","tierInt":0,"userId":1816911,"userName":"radhethakur"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1829923-gp.jpg","displayName":"Samarth Kejriwal","profileUrl":"/samarth220194","tier":"Novice","tierInt":0,"userId":1829923,"userName":"samarth220194"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"MrUpSideDown","profileUrl":"/upsidedownabc","tier":"Novice","tierInt":0,"userId":1866340,"userName":"upsidedownabc"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1942989-kg.png","displayName":"Marcos Jr.","profileUrl":"/marcosjunior","tier":"Novice","tierInt":0,"userId":1942989,"userName":"marcosjunior"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1976530-gr.jpg","displayName":"suengj","profileUrl":"/suengj","tier":"Novice","tierInt":0,"userId":1976530,"userName":"suengj"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2021860-gp.jpg","displayName":"Soubhik","profileUrl":"/soubhikkhankary28","tier":"Novice","tierInt":0,"userId":2021860,"userName":"soubhikkhankary28"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Paul Morton","profileUrl":"/pmorto","tier":"Novice","tierInt":0,"userId":2096267,"userName":"pmorto"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2099845-gp.jpg","displayName":"muralikrishna","profileUrl":"/amuralikrishna","tier":"Novice","tierInt":0,"userId":2099845,"userName":"amuralikrishna"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2132705-gp.jpg","displayName":"Nirmalya Chakraborty","profileUrl":"/nirmalyachakraborty","tier":"Novice","tierInt":0,"userId":2132705,"userName":"nirmalyachakraborty"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Viraj Mohile","profileUrl":"/virajmohile","tier":"Contributor","tierInt":1,"userId":2234974,"userName":"virajmohile"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Sonam Rana","profileUrl":"/sonamrana2908","tier":"Contributor","tierInt":1,"userId":2238142,"userName":"sonamrana2908"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2368741-kg.jpg","displayName":"Vlad Tasca","profileUrl":"/vladtasca","tier":"Contributor","tierInt":1,"userId":2368741,"userName":"vladtasca"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2429326-gp.jpg","displayName":"Lucas Parisi","profileUrl":"/lucasparisi","tier":"Novice","tierInt":0,"userId":2429326,"userName":"lucasparisi"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2434660-kg.jpg","displayName":"Benazir","profileUrl":"/fatehbenazir","tier":"Novice","tierInt":0,"userId":2434660,"userName":"fatehbenazir"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Aida","profileUrl":"/aidpenfer","tier":"Novice","tierInt":0,"userId":2460894,"userName":"aidpenfer"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"funytan","profileUrl":"/funytan","tier":"Novice","tierInt":0,"userId":2485867,"userName":"funytan"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"ShiHaiFeng","profileUrl":"/andrewshf","tier":"Novice","tierInt":0,"userId":2486332,"userName":"andrewshf"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2528443-kg.png","displayName":"Mark_T","profileUrl":"/mt97262","tier":"Contributor","tierInt":1,"userId":2528443,"userName":"mt97262"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2537315-kg.jpg","displayName":"NiloOo","profileUrl":"/niloofarzangeneh","tier":"Novice","tierInt":0,"userId":2537315,"userName":"niloofarzangeneh"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"alexandreevskiy","profileUrl":"/alexandreevskiy","tier":"Novice","tierInt":0,"userId":2584526,"userName":"alexandreevskiy"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Namrata Mangalgi","profileUrl":"/namratanm","tier":"Novice","tierInt":0,"userId":2690530,"userName":"namratanm"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"Rodrigo Serrano","profileUrl":"/rodserr","tier":"Novice","tierInt":0,"userId":3041260,"userName":"rodserr"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"patty","profileUrl":"/pattybh","tier":"Novice","tierInt":0,"userId":3176496,"userName":"pattybh"}],"currentUserInfo":null,"showVoters":true,"alwaysShowVoters":true},"parentDataSource":null,"parentName":"Titanic: Machine Learning from Disaster","parentUrl":"/c/titanic","thumbnailImageUrl":"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png","canWrite":false,"canAdminister":false,"datasetHidden":false,"forkParentIsRedacted":false,"forkDiffLinesChanged":0,"forkDiffLinesDeleted":0,"forkDiffLinesInserted":0,"forkDiffUrl":null,"forkParentAuthorDisplayName":null,"forkParentAuthorUrl":null,"forkParentTitle":null,"forkParentUrl":null,"canSeeDataExplorerV2":true,"canSeeRevampedViewer":true,"canSeeInnerTableOfContents":true,"canSeeCopyAndEditText":true,"simplifiedViewer":false,"kernelOutputDataset":null});performance && performance.mark && performance.mark("KernelViewer.componentCouldBootstrap");</script>

<form action="/thilakshasilva/predicting-titanic-survival-using-five-algorithms" id="__AjaxAntiForgeryForm" method="post"><input name="X-XSRF-TOKEN" type="hidden" value="CfDJ8LdUzqlsSWBPr4Ce3rb9VL8jXM5sZPXXv5pD0uksDxEMixYYaAL09Hs6LfcevquS0c3ETHFpeyB2S9mmymMlxtq2ltVWxGXT038_UQmdTfno6G-h8qNOOepmn0bLyJzlwKTHM3szp3ipM3Mm-vU2jys" /></form>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["STIX", "TeX"],
            linebreaks: {
                automatic: true
            },
            EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
        },
        tex2jax: {
            inlineMath: [["\\(", "\\)"], ["\\\\(", "\\\\)"]],
            displayMath: [["$$", "$$"], ["\\[", "\\]"]],
            processEscapes: true,
            ignoreClass: "tex2jax_ignore|dno"
        },
        TeX: {
            noUndefined: {
                attributes: {
                    mathcolor: "red",
                    mathbackground: "#FFEEEE",
                    mathsize: "90%"
                }
            }
        },
        Macros: {
            href: "{}"
        },
        skipStartupTypeset: true,
        messageStyle: "none"
    });
</script>
<script type="text/javascript" async crossorigin="anonymous" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



    </div>

        <div class="site-layout__footer">
            <footer class="site-footer">
    <div class="site-footer__content">
        <div class="site-footer__copyright">
            <span>&copy; 2019 Kaggle Inc</span>
        </div>
        <nav class="site-footer__nav">
            <a href="/team">Our Team</a>
            <a href="/terms">Terms</a>
            <a href="/privacy">Privacy</a>
            <a href="/contact">Contact/Support</a>
        </nav>
        <nav class="site-footer__social">
            <div data-component-name="SocialIcons" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push();performance && performance.mark && performance.mark("SocialIcons.componentCouldBootstrap");</script>
        </nav>
    </div>
</footer>

        </div>
</div>




    </main>
</body>
</html>
