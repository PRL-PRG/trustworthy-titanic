---
title: "Titanic Exploration, Engineering, and Ensembles"
author: "Bryan P. Holman"
date: "April 3, 2017"
output: 
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

I'm not the first person to post an in-depth Kernel to explore, recognize meaningful features, engineer new ones, and employ machine learning algorithms on this dataset; but I am a little new to this! And I believe that there is no better way to learn than by teaching (Thanks, Feynman!).

As a brief introduction, I am trained in meteorology and statistics, and am fascinated by ensemble and bootstrap methods; so you can be sure both of those topics will come up before this Kernel concludes.

Let's get started! Let's load in some libraries we will need throughout the kernel.

```{r package loading}
# Load packages

library(ggplot2) # data visualization
library(dplyr) # data manipulation
```

Now let's load both the training and test dataset, and combine those into one data frame for exploration and feature engineering.

```{r load data}
# Load the datasets
df.train <- read.csv('../input/train.csv', stringsAsFactors = FALSE)
df.test <- read.csv('../input/test.csv', stringsAsFactors = FALSE)

# Combine both datasets into one data frame
df.full <- bind_rows(df.train, df.test)

# See what the dataset contains!
str(df.full)
```

We have 1309 observations (rows) to work with, 891 from `df.train` and 418 from `df.test`. It looks like many of these features (e.g., `Survived`, `Pclass`) are discrete (can contain only certain values), and will be better represented as factors. We also have missing data (`NA` for 6th observation in `Age`, empty strings for `Cabin`) we will have to deal with. It also isn't immediately obvious which features will be good predictors of survival. Best approach is to tackle these issues one at a time.

# Data Exploration

## Survival

Let's begin by looking at what we are trying to predict, survival:

``` {r survival}
# How many in the train dataset survived?
table(df.full$Survived)

# Factorize Survived
df.full$Survived <- factor(df.full$Survived)
```

So sad! 61% in the training dataset didn't make it! I actually never saw the movie and don't remember studying the Titanic in school, just remember that it was a tragedy and this definitely suggests that! Note that I turned Survived into a factor, since it is clearly a discrete variable.

## Passenger Class

Now let's check out passenger class (`Pclass`):

``` {r pclass}
# What values does Pclass contain?
table(df.full$Pclass)
```

Looks like there are only first, second, and third class passengers, with more third class passengers than first and second combined. We'll factorize this variable since it is discrete. Let's quickly glance and see if this feature exposes a trend in survival.

``` {r pclass exploration}
# Factorize Pclass
df.full$Pclass <- factor(df.full$Pclass)

# Use ggplot to visualize relationship between Pclass & Survived
ggplot(df.full[1:891,], aes(x = Pclass, fill = Survived)) +
    geom_bar(stat='count', position='dodge', color = 'black') +
    scale_x_discrete(breaks=c(1:3)) +
    labs(x = 'Passenger Class') + theme_minimal()
```

There definitely appears to be a relationship! Despite there being so many more passengers with 3rd class tickets, less survived than 1st class ticketholders. I wonder if distinguishing upper class (1st and 2nd) vs. lower class (3rd) will be a helpful predictor in the end, so let's create it.

```{r upper vs lower}
df.full$TicketClass[df.full$Pclass %in% c(1, 2)] <- 'upper'
df.full$TicketClass[df.full$Pclass == 3] <- 'lower'
df.full$TicketClass <- factor(df.full$TicketClass)

table(df.full$TicketClass)
```

As noted earlier, more lower class passengers than upper!

## Passenger Name

As many others have done (e.g., [Megan Risdal](https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic/notebook)), one useful piece of information here is the title of someone's name. We can also make ethnicity assumptions based on Passenger names, but we'll do this in the Feature Engineering phase. On to the next feature.

## Sex

Fortunately no missing information for this feature. It appears that there are about twice as many men as women on board, but odds of surviving are much better for a woman!

```{r}
# Factorize Sex
df.full$Sex <- factor(df.full$Sex)

ggplot(df.full[1:891,], aes(x = Sex, fill = Survived)) +
    geom_bar(stat='count', position='dodge', color = 'black') +
    scale_x_discrete() + labs(x = 'Sex') + theme_minimal()
```

No doubt, then, that this feature will be extremely useful when building our models.

## Age

This feature is plagued with missing values, but before we discuss this let's first see what the distribution is.

```{r age hist}
ggplot(df.full[1:891,], aes(x = Age, fill = Survived)) + 
    geom_histogram(binwidth = 5, color = 'black') +
    theme_minimal()
```

Looks like the training dataset is filled mostly with people in their 20s and 30s, and your odds of survival are much higher if you are young! I wonder how gender also affects this.

```{r age hist sex}
ggplot(df.full[1:891,], aes(x = Age, fill = Survived)) + 
    geom_histogram(binwidth = 5, color = 'black') +
    theme_minimal() + facet_wrap( ~ Sex, ncol = 2)
```

Wow! Some interesting things in here! If you were a 10 - 15 yr. old girl the odds of survival are slim. And for men the best chances of survival are if you are below the age of 15. Above that, survival chances dwindle drastically. This is important, because it suggests that distinguishing between an adult and a child for women is not nearly as important as doing so for men. We will have to determine the magic age between 15 and 20 where odds of survival decrease. Now on to missing values, there are ...

```{r missing age}
# Number of observations missing age
sum(is.na(df.full$Age))
```

263 missing values! What on earth can we do here. One approach is to randomly sample the known ages from the dataset in order to fill in the missing values (e.g., see [Becky Wang's Kernel](https://www.kaggle.com/beiqiwang/titanic/predictive-analysis-of-survival-rate-on-titanic/notebook)). Another option is to use predictive imputation (e.g., see [Megan Risdal's Kernel](https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic/notebook)). Predictive imputation is certainly the more elegant of the two solutions, and as we just learned, properly distinguishing between men and boys can be very helpful during our classification. But we will have to process some other features first before doing so. That's not to say that we can't do some of this on our own. For example, 18 passengers do not have ages recorded, have a value greater than one in the `SibSp` feature, meaning they have lots of siblings, and are likely children (under the age of 18 or 21).

```{r missing age children}
# How many passengers have no age recorded but are likely children?
length(df.full$Age[is.na(df.full$Age) & df.full$SibSp > 1])
```

This is useful information, since it will help us verify the accuracy of the predictive imputation we'll do later. Also, it might be useful to create a column for distinguishing children or adults. We'll get more into this later.

```{r children vs. adults}
# Create a column distinguishing child or adult
df.full$ChildAdult[is.na(df.full$Age) & df.full$SibSp > 1] <- 'child'
```

## Siblings, Spouses, Parents, and Children

This is an interesting feature because it really needs to be split into two since siblings and spouses are completely different things! So we will have to distinguish between the two a little bit later. We will have to tackle the missing age values first.

```{r siblings and spouses}
ggplot(df.full[1:891,], aes(x = SibSp, fill = Survived)) + 
    geom_bar(stat='count', position='dodge', color = 'black') +
    scale_x_continuous() + labs(x = '# of Siblings/Spouses') + theme_minimal()
```

Looks like there was one family of 9 kids and all of those in the training set didn't survive. Sad! Initially appears that odds of survival are poor when traveling with no siblings or spouses. I really like [Megan Risdal's idea](https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic/notebook) of adding `SibSp` to `Parch` to create the feature family size:

```{r family size}
# Create the family size feature
df.full$FamilySize <- df.full$SibSp + df.full$Parch + 1

# Plot family size/survival histogram
ggplot(df.full[1:891,], aes(x = FamilySize, fill = Survived)) + 
    geom_bar(stat='count', position='dodge', color = 'black') +
    scale_x_continuous(breaks = 1:11) + labs(x = 'Family Size') + theme_minimal()
```

This plot is truly enlightening. If you were aboard the Titanic alone (most passengers were), your odds of survival are 2 to 1. For families of size 2, 3, or 4, there is a better chance of surviving than not. Families greater than 4 people are also more likely to not survive. Because of this, Megan creates a discrete feature that distinguishes between single, small, and large families.

```{r FamilySizeDiscrete}
# Create the feature FamilySizeDiscrete, based on the previous plot
df.full$FamilySizeDiscrete[df.full$FamilySize == 1] <- 'single'
df.full$FamilySizeDiscrete[df.full$FamilySize > 1 & df.full$FamilySize < 5] <- 'small'
df.full$FamilySizeDiscrete[df.full$FamilySize > 4] <- 'large'

# Now make this feature a factor
df.full$FamilySizeDiscrete <- factor(df.full$FamilySizeDiscrete)

# Show this how this new variable influences surviving
ggplot(df.full[1:891,], aes(x = FamilySizeDiscrete, fill = Survived)) + 
    geom_bar(stat='count', position='dodge', color = 'black') +
    scale_x_discrete() + labs(x = 'Family Size') + theme_minimal()
```

Beautiful. Really drives that point home. I'll note though that the trend for large families isn't as robust as the others due to the small amount of samples.

## Ticket

These ticket numbers seem to not be particularly useful. Appear to be a bunch of randomish characters:

```{r ticket}
# Preview of the Ticket feature
head(df.full$Ticket)
```

Therefore we will skip this one for now and possibly come back at it later. At least there are no missing ticket numbers!

## Fare