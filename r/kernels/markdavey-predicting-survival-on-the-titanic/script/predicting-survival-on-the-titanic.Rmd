---
title: "Titanic Survival"
author: "Amber Thomas"
output: 
  html_document:
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is my first project on Kaggle and my first attempt at machine learning.  I'll do my best to illustrate what I've down and the logic behind my actions, but feedback is very much welcome and appreciated!

### Loading Necessary Packages


```{r message = FALSE}
# For data manipulation and tidying
library(dplyr)
library(data.table)

# For data visualizations
library(ggplot2)

# For modeling and predictions
library(caret)
library(glmnet)
library(ranger)
library(e1071)
```

### Importing Data

The data were downloaded directly from the [Kaggle Website](https://www.kaggle.com/c/titanic/data).  Before binding the training and test sets into a single data file, I added a column called "Dataset" and labelled rows from the training file "train" and rows from the testing file "test".

```{r}
train <- read.csv('../input/train.csv', header = TRUE, stringsAsFactors = FALSE)
train$Dataset <- "train"

test <- read.csv('../input/test.csv', header = TRUE, stringsAsFactors = FALSE)
test$Dataset <- "test"

full <- bind_rows(train, test)
```


The full dataset can then be inspected:
```{r}
str(full)
```

It appears that several of these variables should be represented as factors and thus should be reclassified. 

```{r}
factor_variables <- c('PassengerId', 'Survived', 'Pclass', 'Sex', 'Embarked', 'Dataset')
full[factor_variables] <- lapply(full[factor_variables], function(x) as.factor(x))
```


```{r}
names <- full$Name

titles <-  gsub("^.*, (.*?)\\..*$", "\\1", names)

full$Titles <- titles

```

```{r}
full$Titles <- gsub("Dona|Lady|Madame|the Countess", "Lady", full$Titles)
full$Titles <- gsub("Don|Jonkheer|Sir", "Sir", full$Titles)

full$Titles <- as.factor(full$Titles)

full <- mutate(full, FamilySize = SibSp + Parch + 1)



full$TravelGroup <- NA

full2 <- arrange(full, Ticket)

full2 <- (transform(full2, TravelGroup = match(Ticket, unique(Ticket))))

# Can't forget to make those Travel Groups into factors!
full2$TravelGroup <- as.factor(full2$TravelGroup)

full3 <- full2 %>% 
            group_by(TravelGroup) %>% 
            mutate(GroupSize = n()) %>%
            ungroup()

filtered <- filter(full3, GroupSize == 1)

# How many were listed as being onboard with siblings or spouses?
fSibSp <- filtered[filtered$SibSp > 0, ]
nrow(fSibSp)

# How many were listed as being onboard with parents or children?
fParch <- filtered[filtered$Parch > 0, ]
nrow(fParch)

# How many of those people overlapped both groups?
sum(fSibSp$PassengerId %in% fParch$PassengerId)


# Resort the dataset by Passenger Number
full4 <- arrange(full3, PassengerId)

# Where did this passenger leave from? What was their class?
full4[1044, c(3, 12)]
```

```{r}
full4 %>%
  filter(Pclass == '3' & Embarked == 'S') %>%
  summarise(missing_fare = median(Fare, na.rm = TRUE))

```

Looks like the median cost for a 3rd class passenger leaving out of Southampton was 8.05. That seems like a logical value for this passenger to have paid.

Time to replace that NA with 8.05

```{r}
full4$Fare[1044] <- 8.05

summary(full4$Fare)
```

Hooray! No more NA values for Fare. 

### Missing Embarkment

Which passengers have no listed embarkment port?
```{r}
full4$Embarked[full4$Embarked == ""] <- NA

full4[(which(is.na(full4$Embarked))), 1]
```

Ok, so Passenger numbers 62 and 830 are each missing their embarkment ports. Let's look at their class of ticket and their fare. 

```{r}
full4[c(62, 830), c(1,3,10)]
```

Both passengers had first class tickets that they spent 80 (pounds?) on. Let's see the embarkment ports of others who bought similar kinds of tickets. 

```{r}
full4 %>%
  group_by(Embarked, Pclass) %>%
  filter(Pclass == "1") %>%
  summarise(mfare = median(Fare),
            n = n())
```

Looks like the median price for a first class ticket departing from 'C' (Charbourg) was 77 (in comparison to our 80).  While first class tickets departing from 'Q' were only slightly more expensive (median price 90), only 3 first class passengers departed from that port.  It seems far more likely that passengers 62 and 830 departed with the other 141 first-class passengers from Charbourg. 

Now to replace their NA values with 'C'. And drop any unused levels.
```{r}
# Assign empty embark ports to 'C'
full4$Embarked[c(62,830)] <- 'C'

# Drop unused levels (since there should be no more blanks)
full4$Embarked <- droplevels(full4$Embarked)

# Check to make sure there are no NA's or blanks
levels(full4$Embarked)
```

Yay! No more NA values for Embarked. 

### Missing Age

This one is a bit trickier. 263 passengers have no age listed.  Taking a median age of all passengers doesn't seem like the best way to solve this problem, so it may be easiest to try to predict the passengers' age based on other known information. 

I've decided to use the `caret` package for predicting age. 

Generate a random forest model on the full dataset (minus the age values that are NA)
 
```{r results = 'hide'}
predicted_age <- train(
  Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Titles + FamilySize + GroupSize,
  tuneGrid = data.frame(mtry = c(2, 3, 7)),
  data = full4[!is.na(full4$Age), ],
  method = "ranger",
  trControl = trainControl(
      method = "cv", number = 10,
      repeats = 10, verboseIter = TRUE),
  importance = 'impurity'
  )

```

Let's look at what factors were the most important in modeling age:

```{r echo = FALSE}
# Creating a Variable Importance variable
 vimp <- varImp(predicted_age)

# Plotting "vimp"
 ggplot(vimp, 
        top = dim(vimp$importance)[1]
        )
```

Wow! Looks like it was a good idea to split out Titles!

Now to use this information to predict the ages of passengers with missing ages and filling in their NA values.

```{r}
full4$Age[is.na(full4$Age)] <- predict(predicted_age, full4[is.na(full4$Age),])

# Check the summary to make sure there are no more NA values
summary(full4$Age)
```

Let's take a quick look at the age distribution of passengers with originally known ages, and the age distribution of the entire group (known and predicted ages) to make sure we didn't terribly skew the distribution. 

```{r echo = FALSE, fig.show = 'hold', fig.width = 4, fig.height = 3.5 }
hist(full3$Age, 
     main = "Known Age Distribution", 
     xlab = "Known Age", 
     col = "#56B4E9",
     breaks = 20)
hist(full4$Age,
     main = "Known + Predicted Age Distribution",
     xlab = "Known Age",
     col = "#D55E00",
     breaks = 20)
```

Hmm, seems to have shifted a bit, but that could be due to a greater lack of age information collected for middle-aged passengers. 

## Modeling for Survival

First things first, I need to split out the test and training data back into separate data sets, now called `train_complete` and `test_complete`. 

```{r}
train_complete <- full4[full4$Dataset == 'train', ]
test_complete <- full4[full4$Dataset == 'test', ]
```

Because I plan on using the `caret` package for all of my modeling, I'm going to generate a standard `trainControl` so that those tuning parameters remain consistent throughout the various models.

### Creating trainControl
I will create a system that will perform 10 repeats of a 10-Fold cross-validation of the data. 
```{r}
myControl <- trainControl(
	  method = "cv", 
	  number = 10,
	  repeats = 10, 
	  verboseIter = TRUE
  )
```

### Fitting a random forest model

The first type of model I'd like to use is a random forest model (using the `ranger` and `caret` packages).

```{r results = 'hide'}
rf_model <- train(
    Survived ~ Age + Pclass + Sex + SibSp + Parch + Fare + Embarked + Titles + FamilySize + 
      TravelGroup + GroupSize,
    tuneGrid = data.frame(mtry = c(2, 5, 8, 10, 15)),
    data = train_complete, 
    method = "ranger", 
    trControl = myControl,
    importance = 'impurity'
)
```

### Fitting a glmnet model

Next, we'll try a glmnet model, also from the `caret` package. 

```{r results = 'hide'}
glm_model <- train(
    Survived ~ Age + Pclass + Sex + SibSp + Parch + Fare + Embarked + Titles + FamilySize + 
      TravelGroup + GroupSize, 
    method = "glmnet",
    tuneGrid = expand.grid(alpha = 0:1,
      lambda = seq(0.0001, 1, length = 20)),
    data = train_complete,
    trControl = myControl
)

setnames(train_complete,"Survived","SurvivedFact")
reviewFit <- predict(glm_model, train_complete)

```



Looks like the glmnet model is slightly more accurate than the random forest model, so we'll use that to predict the survival rate.

Ok, time to make some predictions. 

## Predicting Survival

Although I generated two models above, the glmnet model provided higher accuracy, so I'll use that model to predict survival in the test set. 

```{r}
# Reorder the data by Passenger ID number
test_complete <- test_complete %>%
                  arrange(PassengerId)

# Make predicted survival values
my_prediction <- predict(glm_model, test_complete)
```

### Preparing the prediction for Kaggle

The instructions on Kaggle indicate that they are expecting a csv file with 2 columns: Passenger ID and Survived.  I need to make sure that my data are arranged properly. 

```{r}
# Create a data frame with two columns: PassengerId & Survived where Survived contains my predictions.
#my_solution_5 <- data.frame(PassengerID = test$PassengerId, Survived = my_prediction)
head(reviewFit)
# Write the solution to a csv file 
#write.csv(my_solution_5, file = "my_solution_5.csv", row.names = FALSE)
```

### Testing with Kaggle

Looks like that submission scored 0.80383! Not bad!!

*I'd love to hear any feedback you may have on this process. Thanks in advance!*

