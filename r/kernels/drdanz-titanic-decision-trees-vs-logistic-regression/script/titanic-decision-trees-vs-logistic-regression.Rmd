---
title: "Titanic Survival: Comparing Decision Trees with Logistic Regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a comparison of Titanic survival prediction accuracy using a Decision Tree or Logistic Regression. Decision trees have the disadvantage that they can't use continuous independent variables, so rather than binning numeric variables such as Age and Fare, I wanted to see how the simple tree compares with logistic regression with and without these continuous variables.

```{r}
# load packages and data
library(rpart)
library(rpart.plot)
library(lme4)
library(magrittr)

setwd("../input") # for loading data in Kaggle
data <- read.csv("train.csv")

# make output more clear later
levels(data$Embarked) <- c("", "Cherbourg", "Queenstown", "Southampton")
```

First I'm randomly splitting 20% of the training data rows off for validation.
```{r}
# Index data for training/validation split
n <- nrow(data)
set.seed(10)
idx.train <- sample(1:n, n * 0.8)
idx.val <- sample((1:n)[-idx.train])
```

### 1: Decision Tree (only categorical variables)
Now on to generate a simple decision tree, using the rpart package. We only look at categorical variables, and omit continuous columns (or arbitrary ones e.g. ticket numbers).
```{r 1}
tree_cols <- c(3, 5, 7, 8, 12)
x_train <- data[idx.train, tree_cols]
x_val <- data[idx.val, tree_cols]
y_train <- data[idx.train, 2]
y_val <- data[idx.val, 2]

x <- cbind(x_train, y_train)

# grow tree 
fit <- rpart(y_train ~ ., data = x, method="class")

# summarize and plot tree
print(fit)
prp(fit, extra = 4)
```

Now we use the tree to predict survival in the set-aside validation set:
```{r}
predicted_dt <- predict(fit, x_val)
predicted_dt <- ifelse(predicted_dt[,1] <= 0.5, 1, 0) # reformat to binary

# calculate accuracy of prediction
acc_dt <- sum(predicted_dt == y_val, na.rm = 1) / length(y_val)
cat(paste0("Validation accuracy for the decision tree is ",
           round(acc_dt,3), 
           "."))
```

### 2a: Logistic Regression (only categorical variables)
The same data is used to run a logistic regression. The output tells us which variables significantly impact survival (p-value under 0.05). 
```{r 2a}
# fit logistic regression model
fit_lr <- glm(y_train ~ ., data = x, family = binomial(link = "logit"))
summary(fit_lr)
```
Let's see how accurate this model is:
```{r}
predicted_lr = predict(fit_lr, x_val)
predicted_lr <- ifelse(predicted_lr >= 0, 1, 0) # reformat to binary

# calculate accuracy of prediction
acc_lr <- sum(predicted_lr == y_val, na.rm = 1) / length(y_val)
cat(paste0("Validation accuracy for the category-only logistic regression is ",
           round(acc_lr,3), 
           "."))
```

### 2b: Logistic Regression (including continuous variables)
Now we run the full logistic regression including the continuous variables. This requires dealing with some NAs in the raw data. We could omit these rows (more conservative approach), but let's instead enter the variable mean or the most common factor level. This is for practice, after all.
```{r 2b}
## Logistic regression w. all data incl. continuous ----------------------------
x_train_all <- data[idx.train, c(3, 5:8, 10, 12)] # still omits names and tickets
x_val_all <- data[idx.val, c(3, 5:8, 10, 12)]

# complete gaps in  data (could remove rows with NA, but decided to complete w. dummy data)
# function to replace NAs with column mean
NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
NA2mean_df <- function(df){
  nums <- unlist(lapply(df, is.numeric)) # index numeric columns
  done <- replace(df, nums, lapply(df[, nums], NA2mean))
  return(done)
}

# function to fill empty character cells with most frequent factor
fill_facs <- function(df) {
  for (i in 1:ncol(df)) {
    levels(df[, i])[levels(df[, i]) == ""] <-
      names(which.max(table(df[, i])))
  }
  return(df)
}

# replace NAs with column mean and replace empty cells with most frequent factor
x_train_all %<>% NA2mean_df() %>% fill_facs()
x_val_all %<>% NA2mean_df() %>% fill_facs()

# paste together
x_all <- cbind(x_train_all, y_train)

# fit model
fit_lr_all <- glm(y_train ~ ., data = x_all, family = binomial(link = "logit"))
summary(fit_lr_all)
```

Finally, we can see if how accurate this is:
```{r}

predicted_lr_all = predict(fit_lr_all, x_val_all) 
predicted_lr_all <- ifelse(predicted_lr_all >= 0, 1, 0) # reformat to binary

# calculate accuracy of prediction
acc_lr_all <- sum(predicted_lr_all == y_val, na.rm = 1) / length(y_val)
cat(paste0("Validation accuracy for the full logistic regression is ",
           round(acc_lr_all,3), 
           "."))
```

Turns out here isn't much difference in accuracy between models. This is most likely due to the relative unimportance of age and ticket price for survival. If those variables had a larger impact, the full logistic regression should be the most accurate predictor.

### 2c: Logistic Regression with factorial interactions
Of course, a simple logistic regression model with only first order effects won't capture interactions between the variables, which the Decision Tree does to an extent. So now let's add interactions between the variables with the highest explanatory power. A bit backwards in terms of good practice, usually one would start with the more complex model and pare it down.

```{r}
# fit model with interactions
fit_lr_fact <-
  glm(
  y_train ~ . + (Sex * Pclass) + (Sex * Embarked) + (Pclass * Embarked),
  data = x_all,
  family = binomial(link = "logit")
  )

summary(fit_lr_fact)

predicted_lr_fact = predict(fit_lr_fact, x_val_all) 
predicted_lr_fact <- ifelse(predicted_lr_fact >= 0, 1, 0) # reformat to binary

# calculate accuracy of prediction
acc_lr_fact <- sum(predicted_lr_fact == y_val, na.rm = 1) / length(y_val)
cat(paste0("Validation accuracy for the full logistic regression is ",
           round(acc_lr_fact, 3), 
           "."))
```
Accuracy increases by a small amount.