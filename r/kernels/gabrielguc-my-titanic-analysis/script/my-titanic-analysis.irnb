{"cells":[{"metadata":{"_cell_guid":"91dd0ff5-3c4a-4293-8ed1-654b31aa6328","_uuid":"7ebb7003c3cfb67e1d017c2516e6642de86753e9"},"cell_type":"markdown","source":"# Introduction\nThis notebook will explore different approaches to analyze the Titanic Challenge. The aim is to give some examples of possible solutions on R programming, without reaching a fine tunning of each model.\nThe outline of this kernel will be structured as follows:\n1.  Loading data\n1.  Preparing and exploring data\n1.  Bayesian approach\n1.  Logit regression approach (binomial)\n1.  Neural Network approach \n1.  Conclusion \n\n"},{"metadata":{"_cell_guid":"68009fe2-bdad-48c1-90a6-eee4ff160743","_uuid":"351e5f9dde675e5d140c4c8af399f57081cb42ad","trusted":false},"cell_type":"code","source":"# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages\n# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats\n# For example, here's several helpful packages to load in \nlibrary(ggplot2) # Data visualization\nlibrary(readr) # CSV file I/O, e.g. the read_csv function\nlibrary(e1071) # For Bayesian approach","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d4a995d0-25f9-46be-9675-c26228451b4b","_uuid":"7477402dd1a7f9c7e3660d0be94e3801a3c43151"},"cell_type":"markdown","source":"# 1. Loading data\nFirstly, we will load the training dataset (891 registers) with the function *read.csv*. I have used one of the options available to assign a NA value to empty attributes."},{"metadata":{"_kg_hide-output":false,"_cell_guid":"27c370a7-3113-47f5-aeab-88541a287595","_uuid":"f248b974d5529fdeae8df13087322d0c9672cd53","trusted":false},"cell_type":"code","source":"raw_train <- read.csv(file=\"../input/train.csv\", header=TRUE, sep=\",\", na.strings=c(\"\")) #891 reg","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8783c59b-03bf-46c0-a3a8-0fa86f177d5d","_uuid":"fe9005c9ce4baebd814e9785a62308a08f07da02"},"cell_type":"markdown","source":"# 2. Preparing and exploring data\nIn the next cells, it has been done a exploratory analysis to prepare data for the applying model."},{"metadata":{"_cell_guid":"5b197010-403d-4efb-95ab-6b3713636859","_uuid":"fde10debe8638dd3e598a8092cfa45e9172daea0","trusted":false},"cell_type":"code","source":"# Number of NAs for each attribute:\nsapply(raw_train,function(x) sum(is.na(x)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db707f91-3a38-4ec0-88db-baa71f6ce690","_uuid":"fa20855523572e122c9298d59158311914e947e3"},"cell_type":"markdown","source":"A optional (quick) treatment here is to assign the average to Age attribute where we find a NA value, and to assign the mode value for example to the Embarked missing values. These both treatments, as a I tested, will not lead to a better performance of the model.."},{"metadata":{"_cell_guid":"13e0b61e-eef3-4466-bcf2-d961cbe0352e","_uuid":"c8c697fd91fb4bb28d4aacc1777d378cdaebaeb5","trusted":false},"cell_type":"code","source":"# Function to calculate the mode of a distribution\nmoda <- function(x) {\n  tab <- table(x)\n  return(names(tab[which.max(tab)]))\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb153b4f-0d78-4d1d-8f4b-5cc678416585","_uuid":"e77f6827486e8d79d4b4491a2ab2d131bc42e22b","trusted":false},"cell_type":"code","source":"# Assign the average of the Age attribute:\nraw_train$Age[is.na(raw_train$Age)] <- mean(raw_train$Age,na.rm=T)\n\n# Assign the mode of the Embarked attribute:\nmoda_embarked <- moda(raw_train$Embarked[!is.na(raw_train$Embarked)])\nraw_train$Embarked[is.na(raw_train$Embarked)] <- moda_embarked\ntrain_nb <- raw_train\ntrain_glm <- raw_train","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b9ef516-838c-4889-8ee7-0ae5f7bcab72","_uuid":"b725533378a60820e1b93f50a8908bd002994581","trusted":false},"cell_type":"code","source":"# Another option would be to remove the NA values:\n# train_nb <- raw_train[!is.na(raw_train$Age), ]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e0cd68a4-9e43-498d-bdd1-09df592e9892","_uuid":"575b66d1319d299e5ac3a8c04f501fa996c22abe"},"cell_type":"markdown","source":"# 3. Bayesian Approach\nTo apply a Naive Bayes model, it is necessary to assign a discrete value for each value of every feature. This Naive Bayes model does not work well with continuous variables as the Age."},{"metadata":{"_cell_guid":"3204c5ca-b686-462a-91f9-d53a50cf8d3e","_uuid":"ba7bd662f266878892dcfc16caba3ed1b398cb28","trusted":false},"cell_type":"code","source":"# Age Histogram, and the mean marked with red-dashed line.\nggplot(train_nb, aes(x=Age)) +\n    geom_histogram(binwidth=5, colour=\"black\", fill=\"white\") +\n    geom_vline(aes(xintercept=mean(Age, na.rm=T)),   # Ignore NA values for mean\n               color=\"red\", linetype=\"dashed\", size=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8e08d8c-76f7-472d-8bdd-91f99202c23f","_uuid":"43b1ecc52a8ada49dfd0d4b6f2abca0af134ce02","trusted":false},"cell_type":"code","source":"# Generating a new variable discretizing the Age:\ntrain_nb$AgeTramo <- cut(train_nb$Age, breaks = seq(0,80,5))\n\n#Assigning factors to discrete variables, including de label feature (Survived):\ntrain_nb$Pclass_f <- factor(train_nb$Pclass)\ntrain_nb$SibSp_f <- factor(train_nb$SibSp)\ntrain_nb$Parch_f <- factor(train_nb$Parch)\ntrain_nb$Survived_f <- factor(train_nb$Survived)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3da0c803-b7f7-4820-bf87-f19b029e63d7","_uuid":"8e994225df7a17405cf3926c3689d69b7e263b2e","trusted":false},"cell_type":"code","source":"clasificador <- naiveBayes(Survived_f ~ Pclass_f + Sex + AgeTramo + SibSp_f + Parch_f + Embarked,\n                            data=train_nb)\ntrain_nb_features <- train_nb[ , c(\"Pclass_f\", \"Sex\", \"AgeTramo\", \"SibSp_f\", \"Parch_f\", \"Embarked\")]\npredicted_train_nb<-predict(clasificador, train_nb_features)\nmatrizconf<-table(predicted_train_nb, train_nb$Survived_f)\n#Resultados\nmatrizconf\nsum(diag(matrizconf))/sum(matrizconf)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"642fa9f5-ca77-488c-a818-0c86ab5bb423","_uuid":"2ee8f9cb3a708087aa4db5543d1f281ffebe179c"},"cell_type":"markdown","source":"## Accuracy for Naive Bayes predictor: 79,5%"},{"metadata":{"_cell_guid":"55c0a9cf-543f-46b4-9443-b52061010bbd","_uuid":"9b230c68734dfd6162c5711963aeaa60ee0d39e9"},"cell_type":"markdown","source":"# 4. Logit regression approach (binomial)\nHere, we will repeat the loading and preparing stage (removing NAs values for Age attribute). After the function call to apply de binomial regression is *glm*. In this case, the features need not be discretized. One important issue to take care is the threshold used to separate the positive and negative predictions."},{"metadata":{"_cell_guid":"9cb94cfb-f0cd-4792-9ffd-792b41bb3f2a","_uuid":"f92c6cc85ac1281a7393dc6868b268f6f079176f","trusted":false},"cell_type":"code","source":"raw_train <- read.csv(file=\"../input/train.csv\", header=TRUE, sep=\",\", na.strings=c(\"\")) # 891 reg\ntrain_glm <- raw_train[!is.na(raw_train$Age), ] # 712 reg","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8c73633-6f72-4b5d-9742-9a50c82a07b7","_uuid":"6d4acb5efbb9995edb991f320ae33b57d263124d","trusted":false},"cell_type":"code","source":"# Applying GLM regression model with the binomial family:\nclasificador_glm <- glm(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + Embarked + Fare,\n                        data = train_glm,\n                        family=binomial())\nsummary(clasificador_glm) #output\ntrain_glm_features <- train_glm[ , c(\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\")]\npredicted_train_glm <- predict(clasificador_glm, newdata=train_glm_features)\n\n# Segmentation of the positive and negative predictions.\npredicted_train_glm_bin <- ifelse(predicted_train_glm < 0.5, 0, 1)\n# Confusion matrix to calculate the accuracy of the model:\nmatrizconf<-table(predicted_train_glm_bin, train_glm$Survived)\nmatrizconf\nsum(diag(matrizconf))/sum(matrizconf)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"933eb05b-8730-4ce3-838b-857d9faeabf4","_uuid":"e6c0067497e70b42120aa79ed9c47764aaff95e2"},"cell_type":"markdown","source":"## Accuracy for Logit (binomial) predictor: 81%"},{"metadata":{"_cell_guid":"82f4a37d-62b3-40e3-b8a9-d9b9cf17328e","_uuid":"e4cdc7c825d15089812b166c5a234d08db322eff"},"cell_type":"markdown","source":"# 5. Neural Network approach\nIn this point, it will be used a Multi-Layered Perceptron neural network for binary classification. I will repeat a similar data preprocessing as before examples."},{"metadata":{"_cell_guid":"916ac67c-671e-4d32-9057-6baef0c869e4","_uuid":"15a3a3f1888e5ca2d73bbce98ba978d656457d2c","trusted":false},"cell_type":"code","source":"library(keras) # For Neural Network deployment","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fb5a18a-1b2c-4206-9f56-8888040d691c","_uuid":"57319e159293e022730faf82b5b3ea4976a1fea1","trusted":false},"cell_type":"code","source":"raw_train <- read.csv(file=\"../input/train.csv\", header=TRUE, sep=\",\", na.strings=c(\"\")) #891 reg\nraw_train <- raw_train[!is.na(raw_train$Age), ] # 714 reg\nraw_train <- raw_train[!is.na(raw_train$Embarked), ] # 712 reg","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a0e97113-5326-4bcf-8fac-30a23638b895","_uuid":"f73783bd604784a96396cafca9a1468cc10f44da"},"cell_type":"markdown","source":"It is importante to note that for this neural network approach, it is necessary to transform all categorical data to numeric data, and to rescale the values between 0 and 1."},{"metadata":{"_cell_guid":"7f8fbf0e-20f1-4bb6-b41f-d800a215b2fd","_uuid":"f80d534a8bfa8ad130d2187e50fb9436e4090113","trusted":false},"cell_type":"code","source":"# Transform categorical variables to numeric\nraw_train$NumSex <- as.numeric(factor(raw_train$Sex,labels=c(1,2)))\nraw_train$NumEmbarked <- as.numeric(factor(raw_train$Embarked,labels=c(1,2,3)))\n\n# Generate the feature matrix and the label vector:\ntrain_nn <- raw_train[ , c(\"Pclass\", \"NumSex\", \"Age\", \"SibSp\", \"Parch\", \"NumEmbarked\", \"Fare\")]\nlabel_nn <- raw_train$Survived\n\n# Rescale the values:\ntrain_nn$Pclass <-  (train_nn$Pclass - min(train_nn$Pclass)) /\n                    (max(train_nn$Pclass) - min(train_nn$Pclass))\ntrain_nn$NumSex <-  (train_nn$NumSex - min(train_nn$NumSex)) /\n                    (max(train_nn$NumSex) - min(train_nn$NumSex))\ntrain_nn$Age <-  (train_nn$Age - min(train_nn$Age)) /\n                    (max(train_nn$Age) - min(train_nn$Age))\ntrain_nn$SibSp <-  (train_nn$SibSp - min(train_nn$SibSp)) /\n                    (max(train_nn$SibSp) - min(train_nn$SibSp))\ntrain_nn$Parch <-  (train_nn$Parch - min(train_nn$Parch)) /\n                    (max(train_nn$Parch) - min(train_nn$Parch))\ntrain_nn$NumEmbarked <-  (train_nn$NumEmbarked - min(train_nn$NumEmbarked)) /\n                    (max(train_nn$NumEmbarked) - min(train_nn$NumEmbarked))\ntrain_nn$Fare <-  (train_nn$Fare - min(train_nn$Fare)) /\n                    (max(train_nn$Fare) - min(train_nn$Fare))\n\n# Convert dataframe to numeric matrix:\ncolnames(train_nn)=NULL\nfor (i in 1:7){train_nn[,i]=as.numeric(train_nn[,i])}\ntrain_nn=as.matrix(train_nn)\nlabel_nn=as.matrix(label_nn)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b4cd1a4d-c9ce-4c03-b918-e444ed5d77fc","_uuid":"762c67453f5a9e879a5713a6a919587be312a11e"},"cell_type":"markdown","source":"Now it is the time to generate the MLP for binary classification with the following code:"},{"metadata":{"_cell_guid":"19907110-b09b-4d90-9d5a-b28525ac3a03","_uuid":"bed379679137cb11d11e914ce34ee0f8142991c3","trusted":false},"cell_type":"code","source":"set.seed(300)\nmodel <- keras_model_sequential() \n\nmodel %>% \n  layer_dense(units = 256, activation = 'relu', input_shape = dim(train_nn)[2]) %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 256, activation = 'relu') %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 1, activation = 'sigmoid')\n\nmodel %>% compile(\n  loss = 'binary_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nhistory <- model %>% fit(\n  train_nn,label_nn,\n  epochs = 70, batch_size = 128\n)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d7fe1fa7-e5c1-435e-8601-0c62e73efa37","_uuid":"af0d16b792bf5ab70ac88c1f599b6e6e5722c270","trusted":false},"cell_type":"code","source":"plot(history)\neval <- model %>% evaluate(train_nn, label_nn)\neval","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cd1f0b3d-7cc6-46aa-91e3-c7dec03a554c","_uuid":"40f91780fac42934c3c7dfc5c53b1033a8231251"},"cell_type":"markdown","source":"## Accuracy for MLP binary classificator: 83,6%"},{"metadata":{},"cell_type":"markdown","source":"# 6. Conclusion\nTo sum up, we can say that, without a fine tunning of each model, the accuracy of simple models as Bayes or Logit is not so different from other methods more complex as Neural Network approach. "}],"metadata":{"language_info":{"name":"R","file_extension":".r","mimetype":"text/x-r-source","pygments_lexer":"r","codemirror_mode":"r","version":"3.4.2"},"kernelspec":{"name":"ir","display_name":"R","language":"R"}},"nbformat":4,"nbformat_minor":1}