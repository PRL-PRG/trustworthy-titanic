
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> 
> #This kernal is meant to showcase how a neural network could be built using the titanic dataset as an example
> #Once the neural network is functioning, we then have a base against which to measure our improvement
> #as we begin the process of feature engineering - see 'What's in a NAme' kernal - excellent source
> 
> 
> #load libraries 
> 
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

Warning message:
package ‘dplyr’ was built under R version 3.6.2 
> library(neuralnet)

Attaching package: ‘neuralnet’

The following object is masked from ‘package:dplyr’:

    compute

> library(randomForest)
randomForest 4.6-14
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:dplyr’:

    combine

> 
> #reading training and test set in
> 
> full <- read.csv('../input/train.csv', stringsAsFactors = F)
> 
> # Because this is an old dataset, I will assume you all know where the missing values
> # are, and also have an opinion on how best to deal with them
> 
> sapply(full,function(x) sum(is.na(x)))
PassengerId    Survived      Pclass        Name         Sex         Age 
          0           0           0           0           0         177 
      SibSp       Parch      Ticket        Fare       Cabin    Embarked 
          0           0           0           0           0           0 
> 
> #Let's select the variables we are interested in
> 
> variables <- c('Survived','Pclass','Sex','Age','Embarked',
+                  'Fare','Parch','SibSp')
> full <- full[variables]
> 
> #let's fill in the missing values
> 
> sapply(full, function(x) sum(is.na(x)))
Survived   Pclass      Sex      Age Embarked     Fare    Parch    SibSp 
       0        0        0      177        0        0        0        0 
> 
> 
> full$Age[is.na(full$Age)] <- mean(full$Age,na.rm=T)
> full$Fare[is.na(full$Fare)] <- mean(full$Fare,na.rm=T)
> 
> #let's have a closer look at Embarked - around 70% have S
> count(full, Embarked)
  Embarked   n
1            2
2        C 168
3        Q  77
4        S 644
> 
> full$Embarked[is.na(full$Embarked)] <- 'S'
> 
> sapply(full, function(x) sum(is.na(x)))
Survived   Pclass      Sex      Age Embarked     Fare    Parch    SibSp 
       0        0        0        0        0        0        0        0 
> 
> full$Embarked <- as.factor(full$Embarked)
> full$Sex <- as.factor(full$Sex)
> 
> #Ok sweet, no nulls
> #Let's use random forest to see which variables are most important
> 
> rf_model <- randomForest(factor(Survived) ~ Pclass + Sex + Age + Embarked +
+                          Fare + Parch + SibSp,
+                          data = full)
> 
> importance    <- importance(rf_model)
> varImportance <- data.frame(Variables = row.names(importance), 
+                             Importance = round(importance[ ,'MeanDecreaseGini'],2))
> 
> #now we see the variables we want. SO let's divide into a training and test set
> 
> chosen_variables <- c('Survived','Pclass','Sex','Age',
+                'Fare','SibSp')
> full_prepped <- full[chosen_variables]
> 
> #let's split back into the train and test set
> train <- full_prepped[1:800,]
> test <- full_prepped[801:891,]
> 
> #the neuralnet packages requires input in the form of a matrix, otherwise it doesn't seem to work
> 
> m <- model.matrix( 
+   ~ Survived + Pclass + Sex+ Age + 
+     Fare + SibSp,data = train)
> 
> nn <- neuralnet( 
+   Survived ~ Pclass + Sexmale+ Age + 
+     Fare + SibSp, 
+   data=m, hidden=2, threshold=0.01, linear.output = F)
> 
> #now we do the same thing to the test set to test the nn
> m1 <- model.matrix( 
+   ~ Survived + Pclass + Sex+ Age + 
+     Fare + SibSp,data = test)
> 
> #Compute the nn
> res <- neuralnet::compute(nn, m1[,c("Pclass","Sexmale","Age", 
+     "Fare","SibSp")])
> 
> #take result and put it into the global environment, and then join back to the test set.
> 
> pred_train = round(res$net.result)
> test_final <- cbind(test,pred_train)
> 
> test_final$correct[test_final$Survived == test_final$pred_train ] <- 1
> 
> #let's see what portion our nn got correct ~ 85%. not bad for a dirty quick version. next step - feature engineering!
> paste(sum(test_final$correct, na.rm = TRUE)/91)
[1] "0.846153846153846"
> 
> 
> proc.time()
   user  system elapsed 
  6.317   0.308   6.648 
