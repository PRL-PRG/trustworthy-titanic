{"cells":[
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": true
  },
  "outputs": [],
  "source": "options(jupyter.plot_mimetypes = \"image/png\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#####################################################################################\n################################ TITANIC SURVIVAL ###################################\n#####################################################################################\n\n## STEP 1: First of all, I load the libraries and two functions that I will use later, one of them for calculating error rate\n## and another one for building a dendrogram \nlibrary(sandwich, verbose = FALSE, warn.conflicts = FALSE)\nlibrary(lattice, verbose = FALSE, warn.conflicts = FALSE)\nlibrary(ggplot2, verbose = FALSE, warn.conflicts = FALSE)\nlibrary(caret, verbose = FALSE, warn.conflicts = FALSE)\nlibrary(foreach, verbose = FALSE, warn.conflicts = FALSE)\nlibrary(randomForest, verbose = FALSE, warn.conflicts = FALSE)\n\nto.dendrogram <- function(dfrep, rownum = 1, height.increment = 0.1){\n  \n  if(dfrep[rownum, 'status'] == -1){\n    rval <- list()\n    \n    attr(rval, \"members\") <- 1\n    attr(rval, \"height\") <- 0.0\n    attr(rval, \"label\") <- dfrep[rownum, 'prediction']\n    attr(rval, \"leaf\") <- TRUE\n    \n  }else{\n    left <- to.dendrogram(dfrep, dfrep[rownum, 'left daughter'], height.increment)\n    right <- to.dendrogram(dfrep, dfrep[rownum, 'right daughter'], height.increment)\n    rval <- list(left, right)\n    \n    attr(rval, \"members\") <- attr(left, \"members\") + attr(right, \"members\")\n    attr(rval, \"height\") <- max(attr(left, \"height\"), attr(right, \"height\")) + \n      height.increment\n    attr(rval, \"leaf\") <- FALSE\n    attr(rval, \"edgetext\") <- dfrep[rownum, 'split var']\n  }\n  \n  class(rval) <- \"dendrogram\"\n  \n  return(rval)\n}\n\nerr.rate <- function (x) {\n  num <- x$confusion[1] + x$confusion[4]\n  den <- x$confusion[1] + x$confusion[2]+x$confusion[3]+x$confusion[4]\n  ac <- 1 - (num / den)\n  ac <- ac * 100\n  return(ac)\n}"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "## STEP 2: I load the datasets\ntrain <- read.csv(\"../input/train.csv\", head = TRUE, sep = \",\")\ntest <- read.csv(\"../input/test.csv\", head = TRUE, sep = \",\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "## STEP 3: Before making predictions, I have to analyze the datasets and clean all the mistakes\nstr(train)\nstr(test)\n\nsummary(train)\nsummary(test)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "## STEP 4: Cleaning the datasets\n# I select the variables that I will analyze\ndftrain <- train[,c(2, 3, 5, 6, 7, 8, 10, 12)]\ndftest <- test[,c(2, 4, 5, 6, 7, 9, 11)]\n\n# I create a variable called \"Family\" which is the sum of Parch and SibSp. I would like to test if this var will increase \n# my model accuracy\ndftrain$Family <- dftrain$SibSp + dftrain$Parch\ndftest$Family <- dftest$SibSp + dftest$Parch\n\n# I transform into factors the vars \"Pclass\" and \"Survived\". Moreover, I transform the class of \"Embarked\" to character \n# for replacing NAs later\ndftrain$Survived <- factor(dftrain$Survived, levels = 0:1, labels = c(\"Not survived\", \"Survived\"))\n\ndftrain$Pclass <- as.factor(dftrain$Pclass)\ndftest$Pclass <- as.factor(dftest$Pclass)\n\ndftrain$Embarked <- as.character(dftrain$Embarked)\n\n# A randomForest doesn't deal with NAs so I am going to replace them\ndftrain$Age <- ifelse(is.na(dftrain$Age), mean(dftrain$Age, na.rm = TRUE), dftrain$Age)\ndftest$Age <- ifelse(is.na(dftest$Age), mean(dftest$Age, na.rm = TRUE), dftest$Age)\n\nwhich(dftrain$Embarked == \"\")\ndftrain$Embarked[c(62,830)] = \"S\"\ndftrain$Embarked <- factor(dftrain$Embarked)\n\ndftest$Fare <- ifelse(is.na(dftest$Fare), mean(dftest$Fare, na.rm = TRUE), dftest$Fare)\n\nsummary(dftrain)\nsummary(dftest)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "## STEP 5: MODELS\n# I will use randomForest. I will try some models and I will choose one which has the highest accuracy\nf1 <- Survived ~ Sex\nf2 <- Survived ~ Sex + Fare\nf3 <- Survived ~ Sex + Fare + Age\nf4 <- Survived ~ Sex + Fare + Age + Pclass\nf5 <- Survived ~ Sex + Fare + Age + Pclass + SibSp \nf6 <- Survived ~ Sex + Fare + Age + Pclass + Parch\nf7 <- Survived ~ Sex + Fare + Age + Pclass + SibSp + Parch\nf8 <- Survived ~ Sex + Fare + Age + Pclass + Family\nf9 <- Survived ~ Sex + Fare + Age + Pclass + SibSp + Embarked\nf10 <- Survived ~ Sex + Fare + Age + Pclass + Parch + Embarked\nf11 <- Survived ~ Sex + Fare + Age + Pclass + Family + Embarked\nf12 <- Survived ~ Sex + Fare + Age + Pclass + SibSp + Parch + Embarked\nf13 <- Survived ~ Sex + Fare + Age + Pclass + Embarked\n\nset.seed(1234) #set seed\n\nmd1 <- randomForest(f1, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd2 <- randomForest(f2, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd3 <- randomForest(f3, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd4 <- randomForest(f4, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd5 <- randomForest(f5, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd6 <- randomForest(f6, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd7 <- randomForest(f7, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd8 <- randomForest(f8, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd9 <- randomForest(f9, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd10 <- randomForest(f10, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd11 <- randomForest(f11, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd12 <- randomForest(f12, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\nmd13 <- randomForest(f13, data = dftrain, ntree = 80, proximity = TRUE, importance = TRUE)\n\nModelsErrRate <- round(c(err.rate(md1),err.rate(md2),err.rate(md3),err.rate(md4),err.rate(md5),err.rate(md6),err.rate(md7),err.rate(md8),err.rate(md9),err.rate(md10),err.rate(md11),err.rate(md12),err.rate(md13)),3)\nprint(ModelsErrRate)\n\n# I can see that the best accuracy are for models 4, 8 and 11. I choose the model 4 because it obtains the same value as other but it is the simplest one.\n# I will plot the error rates and confusion matrix for this model\nplot(md4, main = \"Error rate over trees\")\nprint(md4)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "## STEP 6: Now, I am going to obtain the dendrogram and variable's importance\n# Dendrogram instructions and plot\ntree <- getTree(md4, 1, labelVar = TRUE)\nd <- to.dendrogram(tree)\nstr(d)\nplot(d,center=TRUE, leaflab = \"none\", edgePar = list(t.cex = 1, p.col = NA, p.lty = 0))\nMDSplot(md4, dftrain$Survived, k=2, palette = 1:3)\n\n#Importance of vars\nimportance(md4)\nvarImpPlot(md4, main = \"Average Importance plots\")\n"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "## STEP 7: Predictions\npredictions <- predict(md4, dftest)\nresults <- data.frame(test$PassengerId, predictions)"
 }
],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"}}, "nbformat": 4, "nbformat_minor": 0}