
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> 
> # Introduction
> 
> #This is my first stab at a Kaggle script. I have chosen to work with the Titanic dataset after spending some time poking around on the site and looking at other scripts made by other Kagglers for inspiration. I will also focus on doing some illustrative data visualizations along the way. I'll then use `randomForest` to create a model predicting survival on the Titanic. I am new to machine learning and hoping to learn a lot, so feedback is very welcome!
> 
> #There are three parts to my script as follows:
> 
> #* Feature engineering
> #* Missing value imputation
> #* Prediction!
> 
> ## Load and check data
> 
> # Load packages
> library('ggplot2') # visualization
Warning message:
package ‘ggplot2’ was built under R version 3.6.2 
> library('ggthemes') # visualization
> library('scales') # visualization
Warning message:
package ‘scales’ was built under R version 3.6.2 
> library('dplyr') # data manipulation

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

Warning message:
package ‘dplyr’ was built under R version 3.6.2 
> library('mice') # imputation

Attaching package: ‘mice’

The following objects are masked from ‘package:base’:

    cbind, rbind

Warning message:
package ‘mice’ was built under R version 3.6.2 
> library('randomForest') # classification algorithm
randomForest 4.6-14
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:dplyr’:

    combine

The following object is masked from ‘package:ggplot2’:

    margin

> 
> train <- read.csv('../input/train.csv', stringsAsFactors = F)
> test  <- read.csv('../input/test.csv', stringsAsFactors = F)
> #train <- read.csv('C:/users/abrooks/Downloads/train.csv', stringsAsFactors = F)
> #test  <- read.csv('C:/users/abrooks/Downloads/test.csv', stringsAsFactors = F)
> 
> full  <- bind_rows(train, test) # bind training & test data
> 
> # check data
> str(full)
'data.frame':	1309 obs. of  12 variables:
 $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
 $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
 $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
 $ Name       : chr  "Braund, Mr. Owen Harris" "Cumings, Mrs. John Bradley (Florence Briggs Thayer)" "Heikkinen, Miss. Laina" "Futrelle, Mrs. Jacques Heath (Lily May Peel)" ...
 $ Sex        : chr  "male" "female" "female" "female" ...
 $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
 $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
 $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
 $ Ticket     : chr  "A/5 21171" "PC 17599" "STON/O2. 3101282" "113803" ...
 $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
 $ Cabin      : chr  "" "C85" "" "C123" ...
 $ Embarked   : chr  "S" "C" "S" "S" ...
> 
> 
> # Feature Engineering
> ## What's in a name?
> 
> #The first variable which catches my attention is **passenger name** because we can break it down into additional meaningful variables which can feed predictions or be used in the creation of additional new variables. For instance, **passenger title** is contained within the passenger name variable and we can use **surname** to represent families. Let's do some **feature engineering**!
> 
> # Grab title from passenger names
> full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
> 
> # Show title counts by sex
> table(full$Sex, full$Title)
        
         Capt Col Don Dona  Dr Jonkheer Lady Major Master Miss Mlle Mme  Mr Mrs
  female    0   0   0    1   1        0    1     0      0  260    2   1   0 197
  male      1   4   1    0   7        1    0     2     61    0    0   0 757   0
        
          Ms Rev Sir the Countess
  female   2   0   0            1
  male     0   8   1            0
> 
> # Titles with very low cell counts to be combined to "rare" level
> rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
+                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
> 
> # Also reassign mlle, ms, and mme accordingly
> full$Title[full$Title == 'Mlle']        <- 'Miss'
> full$Title[full$Title == 'Ms']          <- 'Miss'
> full$Title[full$Title == 'Mme']         <- 'Mrs'
> full$Title[full$Title %in% rare_title]  <- 'Rare Title'
> 
> # Show title counts by sex again
> table(full$Sex, full$Title)
        
         Master Miss  Mr Mrs Rare Title
  female      0  264   0 198          4
  male       61    0 757   0         25
> 
> # Finally, grab surname from passenger name
> full$Surname <- sapply(full$Name,
+                        function(x) strsplit(x, split = '[,.]')[[1]][1])
> 
> ## Do families sink or swim together?
> 
> 
> # Create a family size variable including the passenger themselves
> full$Fsize <- full$SibSp + full$Parch + 1
> 
> # Create a family variable
> full$Family <- paste(full$Surname, full$Fsize, sep='_')
> 
> #What does our family size variable look like? To help us understand how it may relate to survival, let's plot it among the training data.
> 
> # Use ggplot2 to visualize the relationship between family size & survival
> ggplot(full[1:891,], aes(x = Fsize, fill = factor(Survived))) +
+   geom_bar(stat='count', position='dodge') +
+   scale_x_continuous(breaks=c(1:11)) +
+   labs(x = 'Family Size') +
+   theme_few()
> 
> # Discretize family size
> full$FsizeD[full$Fsize == 1] <- 'singleton'
> full$FsizeD[full$Fsize < 5 & full$Fsize > 1] <- 'small'
> full$FsizeD[full$Fsize > 4] <- 'large'
> 
> # Show family size by survival using a mosaic plot
> mosaicplot(table(full$FsizeD, full$Survived), main='Family Size by Survival', shade=TRUE)
> 
> 
> #The mosaic plot shows that we preserve our rule that there's a survival penalty among singletons and large families, but a benefit for passengers in small families. I want to do something further with our age variable, but `r sum(is.na(full$Age))` rows have missing age values, so we will have to wait until after we address missingness.
> 
> ## Treat a few more variables ...
> 
> #What's left? There's probably some potentially useful information in the **passenger cabin** variable including about their **deck**. Let's take a look.
> 
> # This variable appears to have a lot of missing values
> full$Cabin[1:28]
 [1] ""            "C85"         ""            "C123"        ""           
 [6] ""            "E46"         ""            ""            ""           
[11] "G6"          "C103"        ""            ""            ""           
[16] ""            ""            ""            ""            ""           
[21] ""            "D56"         ""            "A6"          ""           
[26] ""            ""            "C23 C25 C27"
> 
> # The first character is the deck. For example:
> strsplit(full$Cabin[2], NULL)[[1]]
[1] "C" "8" "5"
> 
> # Create a Deck variable. Get passenger deck A - F:
> full$Deck<-factor(sapply(full$Cabin, function(x) strsplit(x, NULL)[[1]][1]))
> 
> 
> 
> # Missingness
> 
> #Now we're ready to start exploring missing data and rectifying it through imputation. There are a number of different ways we could go about doing this. Given the small size of the dataset, we probably should not opt for deleting either entire observations (rows) or variables (columns) containing missing values. We're left with the option of either replacing missing values with a sensible values given the distribution of the data, e.g., the mean, median or mode. Finally, we could go with prediction. We'll use both of the two latter methods and I'll rely on some data visualization to guide our decisions.
> 
> ## Sensible value imputation
> 
> # Passengers 62 and 830 are missing Embarkment
> full[c(62, 830), 'Embarked']
[1] "" ""
> 
> 
> # Get rid of our missing passenger IDs
> embark_fare <- full %>%
+   filter(PassengerId != 62 & PassengerId != 830)
> 
> # Use ggplot2 to visualize embarkment, passenger class, & median fare
> ggplot(embark_fare, aes(x = Embarked, y = Fare, fill = factor(Pclass))) +
+   geom_boxplot() +
+   geom_hline(aes(yintercept=80),
+              colour='red', linetype='dashed', lwd=2) +
+   scale_y_continuous(labels=dollar_format()) +
+   theme_few()
Warning message:
Removed 1 rows containing non-finite values (stat_boxplot). 
> 
> 
> # Since their fare was $80 for 1st class, they most likely embarked from 'C'
> full$Embarked[c(62, 830)] <- 'C'
> 
> #We're close to fixing the handful of NA values here and there. Passenger on row 1044 has an NA Fare value.
> 
> # Show row 1044
> full[1044, ]
     PassengerId Survived Pclass               Name  Sex  Age SibSp Parch
1044        1044       NA      3 Storey, Mr. Thomas male 60.5     0     0
     Ticket Fare Cabin Embarked Title Surname Fsize   Family    FsizeD Deck
1044   3701   NA              S    Mr  Storey     1 Storey_1 singleton <NA>
> 
> #This is a third class passenger who departed from Southampton ('S'). Let's visualize Fares among all others sharing their class and embarkment (n = `r nrow(full[full$Pclass == '3' & full$Embarked == 'S', ]) - 1`).
> 
> ggplot(full[full$Pclass == '3' & full$Embarked == 'S', ],
+        aes(x = Fare)) +
+   geom_density(fill = '#99d6ff', alpha=0.4) +
+   geom_vline(aes(xintercept=median(Fare, na.rm=T)),
+              colour='red', linetype='dashed', lwd=1) +
+   scale_x_continuous(labels=dollar_format()) +
+   theme_few()
Warning message:
Removed 1 rows containing non-finite values (stat_density). 
> 
> #From this visualization, it seems quite reasonable to replace the NA Fare value with median for their class and embarkment which is $`r  median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE)`.
> 
> # Replace missing fare value with median fare for class/embarkment
> full$Fare[1044] <- median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE)
> 
> ## Predictive imputation
> 
> #Finally, as we noted earlier, there are quite a few missing **Age** values in our data. We are going to get a bit more fancy in imputing missing age values. Why? Because we can. We will create a model predicting ages based on other variables.
> 
> # Show number of missing Age values
> sum(is.na(full$Age))
[1] 263
> 
> table(full$Title[is.na(full$Age)],full$Sex[is.na(full$Age)])
            
             female male
  Master          0    8
  Miss           51    0
  Mr              0  176
  Mrs            27    0
  Rare Title      0    1
> tapply(full$Age[!is.na(full$Age)],full$Title[!is.na(full$Age)], summary)
$Master
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.330   2.000   4.000   5.483   9.000  14.500 

$Miss
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.17   15.00   22.00   21.82   30.00   63.00 

$Mr
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  11.00   23.00   29.00   32.25   39.00   80.00 

$Mrs
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  14.00   26.50   35.00   36.92   46.00   76.00 

$`Rare Title`
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  23.00   38.75   47.50   45.18   53.00   70.00 

> 
> tapply(full$Age[!is.na(full$Age)],full$Title[!is.na(full$Age)], summary)
$Master
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.330   2.000   4.000   5.483   9.000  14.500 

$Miss
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.17   15.00   22.00   21.82   30.00   63.00 

$Mr
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  11.00   23.00   29.00   32.25   39.00   80.00 

$Mrs
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  14.00   26.50   35.00   36.92   46.00   76.00 

$`Rare Title`
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  23.00   38.75   47.50   45.18   53.00   70.00 

> #boxplot(full$Age[!is.na(full$Age)]~full$Title[!is.na(full$Age)],title="Age Distribution by Title", xlab="Title",ylab="Age (Years)")
> 
> library(data.table)

Attaching package: ‘data.table’

The following objects are masked from ‘package:dplyr’:

    between, first, last

> 
> 
> #We could definitely use `rpart` (recursive partitioning for regression) to predict missing ages, but I'm going to use the `mice` package for this task just for something different. You can read more about multiple imputation using chained equations in r [here](http://www.jstatsoft.org/article/view/v045i03/v45i03.pdf) (PDF). Since we haven't done it yet, I'll first factorize the factor variables and then perform mice imputation.
> 
> # Make variables factors into factors
> factor_vars <- c('PassengerId','Pclass','Sex','Embarked',
+                  'Title','Surname','Family','FsizeD')
> 
> full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))
> 
> # Set a random seed
> set.seed(129)
> 
> # Perform mice imputation, excluding certain less-than-useful variables:
> mice_mod <- mice(full[, !names(full) %in% c('PassengerId','Name','Ticket','Cabin','Family','Surname','Survived')], method='rf')

 iter imp variable
  1   1  Age  Deck
  1   2  Age  Deck
  1   3  Age  Deck
  1   4  Age  Deck
  1   5  Age  Deck
  2   1  Age  Deck
  2   2  Age  Deck
  2   3  Age  Deck
  2   4  Age  Deck
  2   5  Age  Deck
  3   1  Age  Deck
  3   2  Age  Deck
  3   3  Age  Deck
  3   4  Age  Deck
  3   5  Age  Deck
  4   1  Age  Deck
  4   2  Age  Deck
  4   3  Age  Deck
  4   4  Age  Deck
  4   5  Age  Deck
  5   1  Age  Deck
  5   2  Age  Deck
  5   3  Age  Deck
  5   4  Age  Deck
  5   5  Age  Deck
Warning message:
Number of logged events: 50 
> 
> # Save the complete output
> mice_output <- complete(mice_mod)
> #Let's compare the results we get with the original distribution of passenger ages to ensure that nothing has gone completely awry.
> 
> # Plot age distributions
> par(mfrow=c(1,2))
> hist(full$Age, freq=F, main='Age: Original Data',
+      col='darkgreen', ylim=c(0,0.04))
> hist(mice_output$Age, freq=F, main='Age: MICE Output',
+      col='lightgreen', ylim=c(0,0.04))
> 
> #Things look good, so let's replace our age vector in the original data with the output from the `mice` model.
> 
> # Replace Age variable from the mice model.
> full$Age <- mice_output$Age
> 
> # Show new number of missing Age values
> sum(is.na(full$Age))
[1] 0
> 
> agebreaks <- c(0,1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,500)
> agelabels= c("0-1","1-4","5-9","10-14","15-19","20-24","25-29","30-34","35-39","40-44","45-49","50-54","55-59","60-64","65-69","70-74","75-79","80-84","85+")
> 
> setDT(full)[ , agegroups:= cut(Age, breaks= agebreaks, right= FALSE, labels= agelabels)]
> 
> 
> boxplot(full$Age[!is.na(full$Age)]~full$Title[!is.na(full$Age)],title="Age Distribution by Title", xlab="Title",ylab="Age (Years)")
> 
> 
> #ggplot(full, aes(x = full$Title, y = full$Fare, fill = factor(full$agegroups))) +
> #  geom_boxplot() +
> #  geom_hline(aes(yintercept=80),
> #             colour='red', linetype='dashed', lwd=2) +
> #  scale_y_continuous(labels=dollar_format()) +
> #  theme_few()
> 
> 
> #ggplot(full[full$Fare<270], aes(x = full$Title[full$Fare<270], y = full$Fare[full$Fare<270], fill = factor(full$agegroups[full$Fare<270]))) +
> #  geom_boxplot() +
> #  geom_hline(aes(yintercept=80),
> #             colour='red', linetype='dashed', lwd=2) +
> #  scale_y_continuous(labels=dollar_format()) +
> #  theme_few()
> 
> 
> 
> #We've finished imputing values for all variables that we care about for now! Now that we have a complete Age variable, there are just a few finishing touches I'd like to make. We can use Age to do just a bit more feature engineering ...
> 
> ## Feature Engineering: Round 2
> 
> #Now that we know everyone's age, we can create a couple of new age-dependent variables: **Child** and **Mother**. A child will simply be someone under 18 years of age and a mother is a passenger who is 1) female, 2) is over 18, 3) has more than 0 children (no kidding!), and 4) does not have the title 'Miss'.
> 
> # First we'll look at the relationship between age & survival
> #ggplot(full[1:891,], aes(Age, fill = factor(Survived))) +
> #  geom_histogram() +
>   # I include Sex since we know (a priori) it's a significant predictor
> #  facet_grid(.~Sex) +
> #  theme_few()
> 
> #table(full$agegroups,full$Survived)
> #ggplot(full[1:891,], aes(x=full$agegroups[1:891], fill = factor(full$Survived[1:891]))) +
> #  geom_bar() +
>   # I include Sex since we know (a priori) it's a significant predictor
> #  facet_grid(.~Sex) +
> #  theme_few()
> 
> ##I think there could be a slight difference between young men and young ladies.
> # Create the column child, and indicate whether child or adult
> full$Child[full$Age < 18] <- 'Child'
> full$Child[full$Age >= 18 & full$Sex == 'female'] <- 'Adult'
> full$Child[full$Age >= 14 & full$Sex == 'male'] <- 'Adult'
> 
> # Show counts
> table(full$Child, full$Survived)
       
          0   1
  Adult 507 277
  Child  42  65
> 
> 
> #Looks like being a child doesn't hurt, but it's not going to necessarily save you either! We will finish off our feature engineering by creating the **Mother** variable. Maybe we can hope that mothers are more likely to have survived on the Titanic.
> 
> 
> # Adding Mother variable
> full$Mother <- 'Not Mother'
> full$Mother[full$Sex == 'female' & full$Parch > 0 & full$Age > 18 ] <- 'Mother'
> 
> # Show counts
> table(full$Mother, full$Survived)
            
               0   1
  Mother      20  50
  Not Mother 529 292
> 
> # Finish by factorizing our two new factor variables
> full$Child  <- factor(full$Child)
> full$Mother <- factor(full$Mother)
> 
> #All of the variables we care about should be taken care of and there should be no missing data. I'm going to double check just to be sure:
> 
> md.pattern(full)
    PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked
204           1      1    1   1   1     1     1      1    1     1        1
687           1      1    1   1   1     1     1      1    1     1        1
91            1      1    1   1   1     1     1      1    1     1        1
327           1      1    1   1   1     1     1      1    1     1        1
              0      0    0   0   0     0     0      0    0     0        0
    Title Surname Fsize Family FsizeD agegroups Child Mother Survived Deck     
204     1       1     1      1      1         1     1      1        1    1    0
687     1       1     1      1      1         1     1      1        1    0    1
91      1       1     1      1      1         1     1      1        0    1    1
327     1       1     1      1      1         1     1      1        0    0    2
        0       0     0      0      0         0     0      0      418 1014 1432
> 
> #Wow! We have finally finished treating all of the relevant missing values in the Titanic dataset which has included some fancy imputation with `mice`. We have also successfully created several new variables which we hope will help us build a model which reliably predicts survival.
> 
> 
> # Prediction
> 
> #At last we're ready to predict who survives among passengers of the Titanic based on variables that we carefully curated and treated for missing values. For this, we will rely on the `randomForest` classification algorithm; we spent all that time on imputation, after all.
> 
> ## Split into training & test sets
> 
> #Our first step is to split the data back into the original test and training sets.
> 
> 
> # Split the data back into a train set and a test set
> train <- full[1:891,]
> test <- full[892:1309,]
> 
> ## Building the model
> 
> #We then build our model using `randomForest` on the training set.
> 
> # Set a random seed
> set.seed(754)
> 
> # Build the model (note: not all possible variables are used)
> rf_model <- randomForest(factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + agegroups+
+                            Fare + Embarked + Title +
+                            FsizeD + Child + Mother,
+                          data = train)
> 
> # Show model error
> plot(rf_model, ylim=c(0,0.36))
> legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
> 
> #The black line shows the overall error rate which falls below 20%. The red and green lines show the error rate for 'died' and 'survived' respectively. We can see that right now we're much more successful predicting death than we are survival. What does that say about me, I wonder?
> 
> # Variable importance
> 
> #Let's look at relative variable importance by plotting the mean decrease in Gini calculated across all trees.
> 
> # Get importance
> importance    <- importance(rf_model)
> varImportance <- data.frame(Variables = row.names(importance),
+                             Importance = round(importance[ ,'MeanDecreaseGini'],2))
> 
> # Create a rank variable based on importance
> rankImportance <- varImportance %>%
+   mutate(Rank = paste0('#',dense_rank(desc(Importance))))
> 
> # Use ggplot2 to visualize the relative importance of variables
> ggplot(rankImportance, aes(x = reorder(Variables, Importance),
+                            y = Importance, fill = Importance)) +
+   geom_bar(stat='identity') +
+   geom_text(aes(x = Variables, y = 0.5, label = Rank),
+             hjust=0, vjust=0.55, size = 4, colour = 'red') +
+   labs(x = 'Variables') +
+   coord_flip() +
+   theme_few()
> 
> 
> #Whoa, glad we made our title variable! It has the highest relative importance out of all of our predictor variables. I think I'm most surprised to see that passenger class fell to `r rankImportance[rankImportance$Variable == 'Pclass', ]$Rank`, but maybe that's just bias coming from watching the movie Titanic too many times as a kid.
> 
> ## Prediction!
> 
> #We're ready for the final step --- making our prediction! When we finish here, we could iterate through the preceding steps making tweaks as we go or fit the data using different models or use different combinations of variables to achieve better predictions. But this is a good starting (and stopping) point for me now.
> 
> # Predict using the test set
> prediction <- predict(rf_model, test)
> 
> # Save the solution to a dataframe with two columns: PassengerId and Survived (prediction)
> solution <- data.frame(PassengerID = test$PassengerId, Survived = prediction)
> 
> # Write the solution to file
> write.csv(solution, file = 'rf_mod_Solution.csv', row.names = F)
> 
> # Conclusion
> 
> #Thank you for taking the time to read through my first exploration of a Kaggle dataset. I look forward to doing more. Again, this newbie welcomes comments and suggestions!
> 
> proc.time()
   user  system elapsed 
 27.556   0.538  28.022 
