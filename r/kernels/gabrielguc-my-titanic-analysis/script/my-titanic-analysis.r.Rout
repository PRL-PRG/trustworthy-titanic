
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> # This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
> # The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
> # For example, here's several helpful packages to load in 
> library(ggplot2) # Data visualization
Warning message:
package ‘ggplot2’ was built under R version 3.6.2 
> library(readr) # CSV file I/O, e.g. the read_csv function
> library(e1071) # For Bayesian approach
> 
> raw_train <- read.csv(file="../input/train.csv", header=TRUE, sep=",", na.strings=c("")) #891 reg
> 
> # Number of NAs for each attribute:
> sapply(raw_train,function(x) sum(is.na(x)))
PassengerId    Survived      Pclass        Name         Sex         Age 
          0           0           0           0           0         177 
      SibSp       Parch      Ticket        Fare       Cabin    Embarked 
          0           0           0           0         687           2 
> 
> # Function to calculate the mode of a distribution
> moda <- function(x) {
+   tab <- table(x)
+   return(names(tab[which.max(tab)]))
+ }
> 
> # Assign the average of the Age attribute:
> raw_train$Age[is.na(raw_train$Age)] <- mean(raw_train$Age,na.rm=T)
> 
> # Assign the mode of the Embarked attribute:
> moda_embarked <- moda(raw_train$Embarked[!is.na(raw_train$Embarked)])
> raw_train$Embarked[is.na(raw_train$Embarked)] <- moda_embarked
> train_nb <- raw_train
> train_glm <- raw_train
> 
> # Another option would be to remove the NA values:
> # train_nb <- raw_train[!is.na(raw_train$Age), ]
> 
> # Age Histogram, and the mean marked with red-dashed line.
> ggplot(train_nb, aes(x=Age)) +
+     geom_histogram(binwidth=5, colour="black", fill="white") +
+     geom_vline(aes(xintercept=mean(Age, na.rm=T)),   # Ignore NA values for mean
+                color="red", linetype="dashed", size=1)
> 
> # Generating a new variable discretizing the Age:
> train_nb$AgeTramo <- cut(train_nb$Age, breaks = seq(0,80,5))
> 
> #Assigning factors to discrete variables, including de label feature (Survived):
> train_nb$Pclass_f <- factor(train_nb$Pclass)
> train_nb$SibSp_f <- factor(train_nb$SibSp)
> train_nb$Parch_f <- factor(train_nb$Parch)
> train_nb$Survived_f <- factor(train_nb$Survived)
> 
> clasificador <- naiveBayes(Survived_f ~ Pclass_f + Sex + AgeTramo + SibSp_f + Parch_f + Embarked,
+                             data=train_nb)
> train_nb_features <- train_nb[ , c("Pclass_f", "Sex", "AgeTramo", "SibSp_f", "Parch_f", "Embarked")]
> predicted_train_nb<-predict(clasificador, train_nb_features)
> matrizconf<-table(predicted_train_nb, train_nb$Survived_f)
> #Resultados
> matrizconf
                  
predicted_train_nb   0   1
                 0 475 109
                 1  74 233
> sum(diag(matrizconf))/sum(matrizconf)
[1] 0.7946128
> 
> raw_train <- read.csv(file="../input/train.csv", header=TRUE, sep=",", na.strings=c("")) # 891 reg
> train_glm <- raw_train[!is.na(raw_train$Age), ] # 712 reg
> 
> # Applying GLM regression model with the binomial family:
> clasificador_glm <- glm(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + Embarked + Fare,
+                         data = train_glm,
+                         family=binomial())
> summary(clasificador_glm) #output

Call:
glm(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
    Embarked + Fare, family = binomial(), data = train_glm)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7233  -0.6447  -0.3799   0.6326   2.4457  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  5.637407   0.634550   8.884  < 2e-16 ***
Pclass      -1.199251   0.164619  -7.285 3.22e-13 ***
Sexmale     -2.638476   0.222256 -11.871  < 2e-16 ***
Age         -0.043350   0.008232  -5.266 1.39e-07 ***
SibSp       -0.363208   0.129017  -2.815  0.00487 ** 
Parch       -0.060270   0.123900  -0.486  0.62666    
EmbarkedQ   -0.823545   0.600229  -1.372  0.17005    
EmbarkedS   -0.401213   0.270283  -1.484  0.13770    
Fare         0.001432   0.002531   0.566  0.57165    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 960.90  on 711  degrees of freedom
Residual deviance: 632.34  on 703  degrees of freedom
  (2 observations deleted due to missingness)
AIC: 650.34

Number of Fisher Scoring iterations: 5

> train_glm_features <- train_glm[ , c("Pclass", "Sex", "Age", "SibSp", "Parch", "Embarked", "Fare")]
> predicted_train_glm <- predict(clasificador_glm, newdata=train_glm_features)
> 
> # Segmentation of the positive and negative predictions.
> predicted_train_glm_bin <- ifelse(predicted_train_glm < 0.5, 0, 1)
> # Confusion matrix to calculate the accuracy of the model:
> matrizconf<-table(predicted_train_glm_bin, train_glm$Survived)
> matrizconf
                       
predicted_train_glm_bin   0   1
                      0 394 105
                      1  30 183
> sum(diag(matrizconf))/sum(matrizconf)
[1] 0.8103933
> 
> library(keras) # For Neural Network deployment
Warning message:
package ‘keras’ was built under R version 3.6.2 
> 
> raw_train <- read.csv(file="../input/train.csv", header=TRUE, sep=",", na.strings=c("")) #891 reg
> raw_train <- raw_train[!is.na(raw_train$Age), ] # 714 reg
> raw_train <- raw_train[!is.na(raw_train$Embarked), ] # 712 reg
> 
> # Transform categorical variables to numeric
> raw_train$NumSex <- as.numeric(factor(raw_train$Sex,labels=c(1,2)))
> raw_train$NumEmbarked <- as.numeric(factor(raw_train$Embarked,labels=c(1,2,3)))
> 
> # Generate the feature matrix and the label vector:
> train_nn <- raw_train[ , c("Pclass", "NumSex", "Age", "SibSp", "Parch", "NumEmbarked", "Fare")]
> label_nn <- raw_train$Survived
> 
> # Rescale the values:
> train_nn$Pclass <-  (train_nn$Pclass - min(train_nn$Pclass)) /
+                     (max(train_nn$Pclass) - min(train_nn$Pclass))
> train_nn$NumSex <-  (train_nn$NumSex - min(train_nn$NumSex)) /
+                     (max(train_nn$NumSex) - min(train_nn$NumSex))
> train_nn$Age <-  (train_nn$Age - min(train_nn$Age)) /
+                     (max(train_nn$Age) - min(train_nn$Age))
> train_nn$SibSp <-  (train_nn$SibSp - min(train_nn$SibSp)) /
+                     (max(train_nn$SibSp) - min(train_nn$SibSp))
> train_nn$Parch <-  (train_nn$Parch - min(train_nn$Parch)) /
+                     (max(train_nn$Parch) - min(train_nn$Parch))
> train_nn$NumEmbarked <-  (train_nn$NumEmbarked - min(train_nn$NumEmbarked)) /
+                     (max(train_nn$NumEmbarked) - min(train_nn$NumEmbarked))
> train_nn$Fare <-  (train_nn$Fare - min(train_nn$Fare)) /
+                     (max(train_nn$Fare) - min(train_nn$Fare))
> 
> # Convert dataframe to numeric matrix:
> colnames(train_nn)=NULL
> for (i in 1:7){train_nn[,i]=as.numeric(train_nn[,i])}
> train_nn=as.matrix(train_nn)
> label_nn=as.matrix(label_nn)
> 
> set.seed(300)
> model <- keras_model_sequential() 
OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/
