---
title: "Titanic LightGBM modeling with R"
author: "stat17_hb"
output: 
  html_document: 
    numbersection: yes
    theme: yeti
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=F)
```


# Load Libraries

```{r Load Libraries}
library(tidyverse)
library(lightgbm)
```

# Load Data

```{r Load Data}
train <- read_csv("../input/train.csv")
test <- read_csv("../input/test.csv")
```

# Preprocessing

## Data structure

```{r Data structure}
glimpse(train)
glimpse(test)
```

## Check Missing Values

```{r Check Missing Values}
colSums(is.na(train))
colSums(is.na(test))
```

+ train에는 Age에 177개, Cabin에 687개, Embarked에 2개의 결측값이 있고, test에는 Age에 86개, Fare에 1개, Cabin에 327개의 결측값이 있다.

## Make full dataset

```{r full}
train$key <- "train"
test$key <- "test"
full <- rbind(train %>% select(-Survived), test)
Y_train <- train$Survived
```

## Missing value imputation

### Age

```{r Age summary}
full$Age %>% summary
```

+ Age의 최솟값이 0.17이다. 17을 잘못 입력한 것일 수 있다.

```{r}
full %>% filter(is.na(Age)==0) %>% arrange(Age) %>% head(20)
full$Age %>% table 
```

+ Name 변수를 함께 고려해 보았을 때(Miss 같은 호칭) 몇몇 Age 값들이 잘못 입력된 것으로 보인다.
+ 0.17 -> 17, 0.33 -> 33, 0.42 -> 42, 0.67 -> 67, 0.75 -> 75, 0.83 -> 83, 0.92 -> 92로 변환하자.

```{r}
full[which(full$Age==0.17),"Age"] <- 17
full[which(full$Age==0.33),"Age"] <- 33
full[which(full$Age==0.42),"Age"] <- 42
full[which(full$Age==0.67),"Age"] <- 67
full[which(full$Age==0.75),"Age"] <- 75
full[which(full$Age==0.83),"Age"] <- 83
full[which(full$Age==0.92),"Age"] <- 92
```

```{r}
full$Age %>% quantile(na.rm=T)
```

Age의 결측값들을 NA 그룹으로 묶어서 AgeGroup 변수 생성

```{r Age group}
full <- full %>% mutate(AgeGroup = case_when(is.na(Age) ~ "NA",
                                            Age >=0 & Age < 21 ~ "below_Q1",
                                            Age >=21 & Age < 28 ~ "Q1_Q2",
                                            Age >=28 & Age < 39 ~ "Q2_Q3",
                                            Age >=39 ~ "above_Q3"))
full$AgeGroup %>% table
```



### Fare

```{r}
full %>% filter(is.na(Fare))
```

+ Fare(요금)은 Pclass(객실 등급)에 따라 정해질 것이므로 Pclass=3일때 Fare의 평균값으로 대체하는 것이 좋아 보인다.

```{r}
full %>% group_by(Pclass) %>% summarise(mean_Fare=mean(Fare, na.rm=T))
full[is.na(full$Fare), "Fare"] <- 13.30289
```

### Cabin

```{r}
full %>% filter(is.na(Cabin)) %>% head(10)
full$Cabin %>% unique
```

+ 범주가 너무 많다. 우선 모델링에서 제외하는 것이 좋아 보인다.

### Embarked

```{r}
full %>% filter(is.na(Embarked))
full$Embarked %>% table
full$Embarked <- replace(full$Embarked, which(is.na(full$Embarked)), 'S')
```

+ major class로 결측치를 채웠다.

```{r}
colSums(is.na(full))
```

+ 결측치 처리 완료

# Feature engineering

## FamilySize

```{r}
full$FamilySize <- full$SibSp + full$Parch + 1
full$FamilyGroup <- rep("a", nrow(full))
full$FamilyGroup[full$FamilySize == 1] <- 'Single' 
full$FamilyGroup[full$FamilySize < 5 & full$FamilySize >= 2] <- 'Small' 
full$FamilyGroup[full$FamilySize >= 5] <- 'Big' 
full$FamilyGroup <- factor(full$FamilyGroup)
```

## Drop unused variables

```{r}
full4model <- full %>% select(-PassengerId, -Name, -Cabin, -Ticket, -SibSp, -Parch, -Age)
```

## Transform cagorical variables to factor type

```{r}
cat_var <- c(names(full4model)[sapply(full4model, class)=="character"], "Pclass")
full4model[cat_var] <- lapply(full4model[cat_var], factor)
str(full4model)
```

## Split train, test

```{r}
X_train <- full4model %>% filter(key=="train") %>% select(-key)
X_test <- full4model %>% filter(key=="test") %>% select(-key)

str(X_train)
str(X_test)
```

## Dummy coding for categorical variables

```{r Dummy coding}
X_train.1 <- model.matrix(~.-1, data=X_train)[,-1] 
X_test.1 <- model.matrix(~.-1, data=X_test)[,-1]
```

# Modeling

## Light GBM

[https://www.kaggle.com/andrewmvd/lightgbm-in-r](https://www.kaggle.com/andrewmvd/lightgbm-in-r)

```{r lgb.Dataset}
train.lgb <- lgb.Dataset(data=X_train.1, label=Y_train)
```

```{r lgb.grid}
lgb.grid <- list(objective = "binary",
                metric = "auc",
                min_sum_hessian_in_leaf = 1,
                feature_fraction = 0.7,
                bagging_fraction = 0.7,
                bagging_freq = 5,
                min_data = 100,
                max_bin = 50,
                lambda_l1 = 8,
                lambda_l2 = 1.3,
                min_data_in_bin=100,
                min_gain_to_split = 10,
                min_data_in_leaf = 30,
                is_unbalance = F)
```

```{r lgb.cv}
lgb.cv <- lgb.cv(param=lgb.grid, data=train.lgb, learning_rate = 0.01, num_leaves = 25, num_threads = 4, 
                nrounds = 7000, early_stopping_rounds = 50, eval_freq = 20, nfold = 5, stratified = F)
best.iter <- lgb.cv$best_iter
names(lgb.cv$record_evals[[2]])
names(lgb.cv$record_evals[[2]]$auc)
lgb.cv$record_evals[[2]]$auc$eval[best.iter]
lgb.cv$record_evals[[2]]$auc$eval_err[best.iter]
lgb.cv$best_score
```

### Find optimal learning_rate and cutoff value

```{r train validation}
set.seed(1)
train.idx <- sample(nrow(X_train.1), 0.8*nrow(X_train.1))
X_train <- X_train.1[train.idx, ]
label_train <- Y_train[train.idx]
X_valid <- X_train.1[-train.idx, ]
label_valid <- Y_train[-train.idx]
train.lgb <- lgb.Dataset(data=X_train, label=label_train)
```

```{r}
tuneGrid <- expand.grid(learning_rate=c(0.01,0.05,0.1,0.2,0.3), cutoff=c(0.3,0.4,0.5,0.6), num_leaves=c(5,10,15,20,25))
tuneGrid$acc <- rep(0,nrow(tuneGrid))
set.seed(1)
for (i in 1:nrow(tuneGrid)){
    fit.lgb <- lgb.train(param=lgb.grid, data=train.lgb, learning_rate = tuneGrid$learning_rate[i],
                      num_leaves = tuneGrid$num_leaves[i], num_threads = 4 , nrounds = best.iter,
                      eval_freq = 20)
    prob.lgb <- predict(fit.lgb, X_valid)
    pred.lgb <- ifelse(prob.lgb>tuneGrid$cutoff[i], 1,0)
    confusion.mat <- table(pred.lgb, label_valid)
    acc <- sum(diag(confusion.mat))/sum(confusion.mat)
    tuneGrid[i, "acc"] <- acc
}

tuneGrid[which.max(tuneGrid$acc), ]
bestParam <- tuneGrid[which.max(tuneGrid$acc), ]
```

### Final model

```{r}
train.lgb <- lgb.Dataset(data=X_train.1, label=Y_train)
fit.lgb <- lgb.train(param=lgb.grid, data=train.lgb, learning_rate = bestParam$learning_rate,
                      num_leaves = bestParam$num_leaves, num_threads = 4 , nrounds = best.iter,
                      eval_freq = 20)
prob.lgb <- predict(fit.lgb, X_test.1)
pred.lgb <- ifelse(prob.lgb > bestParam$cutoff, 1, 0)

submission <- read.csv("../input/sample_submission.csv")
submission.lgb <- submission
submission.lgb$Survived <- pred.lgb
write.csv(submission.lgb, "submission_lgb.csv", row.names=F)
```