---
title: "Titanic"
author: "Liwen Huang"
date: "11/5/2018"
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: readable
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(ggplot2)
library(dplyr)
```

# Introduction
As my first baby kaggle project, I would mine the very famous Titanic dataset. Compare predictions through logistic regression and machine learning modelling techniques.

# Explorative Data Analysis

## Import the dataset
```{r}
train <- fread("../input/train.csv")
```

```{r}
test <- fread ("../input/test.csv")
submission <- fread("../input/gender_submission.csv")
```

```{r}
full <- bind_rows(train, test)
```

## Overview of the variables
```{r}
str(train)
```
```{r}
head(train)
```

## Exam missing values
```{r}
sapply(full, function(x)sum(is.na(x) | x == ""))
```
* Notice we need the `|` as fread's different treatment of missing values with numerics and strings.
* There are considerable missing `Age` and `Cabin`, we will decide later whether they should be abandoned or imputed to enhance the prediction.

Add a female column for convenenice.
```{r}
train$Female <- train$Sex == "female"
```

Naturally, `Fare` and `Pcalss` should be closely related. We would take a close look between these two.
```{r}
ggplot(full, aes(group = as.factor(Pclass), y = Fare, color = as.factor(Pclass))) + 
  geom_boxplot() + 
  coord_cartesian(xlim = c(-0.5, 0.5), ylim = c(-50, 300))
```

The average fare for each class
```{r}
aggregate(Fare ~ Pclass, data = train, mean)
```

Take a look at the two passengers with missing Embark values
```{r}
full[full$Embarked == ""]
```
* Both passengers are 1st class female in cabin B28

Take a look at embarkemnt, class and fare
```{r}
ggplot(data = full, aes(x = Embarked, y = Fare, fill = factor(Pclass))) +
  geom_boxplot() +
  geom_hline(aes(yintercept=80), 
    colour='red', linetype='dashed', lwd=1) 
```
* A reasonable guess that that both passengers embarked at "C", as the mean fare for the 1st class embarked here is 80 dollars.

```{r}
full$Embarked[full$Embarked == ""] <- "C"
```

The one passenger with missing fare
```{r}
full[is.na(full$Fare), ]
```
* Passenger is a 60-year-old male embarked on "S" with no family members in his company
* The easier thing to fill in this datapoint is take the average of the third class fare, we have already caculated this value from a previous step.

```{r}
full$Fare[which(is.na(full$Fare))] <- 13.67555
```



## Correlation among variables
```{r}
cor(train[,-c("PassengerId", "Sex", "Name", "Ticket", "Cabin", "Embarked")])
```
* We can tell from the correlation matrix that `Sex` and `PClass` are probably the strongest predictors, we will viusalize their impact on `Survived`

```{r}
ggplot(data = train, aes(x = Survived, fill = Sex)) +
                     geom_bar(width=.5, position = "dodge")
```

```{r}
ggplot(data = train, aes(x = Survived, fill = as.factor(Pclass) )) +
                     geom_bar(width=.5, position = "dodge") 
```

## The missing `Age` variable
### Correlation with other variables
Since `Age` has so many missing obs, we exclude the incomplete cases and try again in the train dataset.
```{r}
cor(train[,-c("PassengerId", "Sex", "Name", "Ticket", "Cabin", "Embarked")], use = "complete.obs")
```

```{r}
cor(full[,-c("PassengerId", "Sex", "Name", "Ticket", "Cabin", "Embarked")], use = "complete.obs")
```

* Using `Age` in complete obs, the result suggest that it is might have weak to moderate relation with `Pclass`, `Sibsp` and `Parch`. 
* Intuitiion tells us that we should impute this variable to enhance prediction. 

### A rough model without the imputation
We first make a rough model before the imputation in order to compare it with the later model. I have a few expectation for the imputation
* The distribution of age should be similar unless there is a clear pattern of how `Age` is missing.
* the imputated should not drastically change the model 
```{r}
train_logit1 <- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train, family = "binomial")
```

```{r}
summary(train_logit1)
```
* This model seems reasonable 
* `Pclass`, `Sex`, `Age` are significant predictors.

### Distribution of the `Age` variable
What does the distribution of `Age` look like in the train dataset?
```{r}
hist(train$Age, breaks = 50, col = "lightblue", freq = F)
lines(density(train$Age, na.rm = T), col = "red")
lines(density(train$Age, adjust = 4, na.rm = T), lty="dotted", col="darkgreen", lwd=2)
```

Compare this with the test and full dataset
```{r}
hist(test$Age, breaks = 50, col = "lightblue", freq = F)
lines(density(test$Age, na.rm = T), col = "red")
lines(density(test$Age, adjust = 4, na.rm = T), lty="dotted", col="darkgreen", lwd=2)
```

Finally, in the full dataset.
```{r}
hist(full$Age, breaks = 50, col = "lightblue", freq = F)
lines(density(full$Age, na.rm = T), col = "red")
lines(density(full$Age, adjust = 4, na.rm = T), lty="dotted", col="darkgreen", lwd=2)
```

A summary of the three datasets.
```{r}
summary(train$Age, rm.na = T)
summary(test$Age, rm.na = T)
summary(full$Age, rm.na = T)
```

* The `Age` variable have more or less pattern in the train, test, and full datasets.
* Among the available data, the distribution of age is slightly skewed with medium and mean both around 30
* There are more concentration of passagers in their 20s and 30s. 


### Whose ages are missing?
```{r}
Age_missing <- full[is.na(full$Age)]
```

Take a look at the raw numbers and ratio of different classes among full dataset and within `Age` missing obs.
```{r}
table(full$Pclass)
table(full$Pclass)/dim(full)[1]
```

```{r}
table(Age_missing$Pclass)
table(Age_missing$Pclass)/dim(Age_missing)[1]
```

Take a look at the raw numbers and ratio of `Sex` among full dataset and within `Age` missing obs.
```{r}
table(full$Sex)
table(full$Sex)/dim(full)[1]
```

```{r}
table(Age_missing$Sex)
table(Age_missing$Sex)/dim(Age_missing)[1]
```

* By eyeballing the numbers, higher proproting of the `Age` missing obs are 3rd class passsengers
* Slightly higher proprotion of them are males.

Let's also visualize the `Age` distribution among classes and gender groups
```{r}
ggplot (data = full, aes(Age, fill = as.factor(Pclass))) +
  geom_histogram (aes(y = ..density..), alpha = 0.7, position = 'identity', binwidth = 2, na.rm = T) +
  geom_density(alpha = 0.2, linetype="dotted", na.rm = T)
```

```{r}
summary(full$Age[full$Pclass == 1])
summary(full$Age[full$Pclass == 2])
summary(full$Age[full$Pclass == 3])
```
* 1st class passengers have more of a symmatric distribution of `Age`
* Passengers tend to be younger as class gets lower

Relationship between `Age` and `Sex`
```{r}
ggplot (data = full, aes(Age, fill = Sex))+
  geom_histogram (aes(y = ..density..), alpha = 0.7, position = 'identity', binwidth = 2, na.rm = T) +
  geom_density(alpha = 0.2, linetype="dotted", na.rm = T)
```
* `Age` does not seem to have drastically different distribution in different gender groups.


# Impute `Age` with mice (Multivariate Imputation by Chained Equations )
```{r}
library(mice)
mice_mod <- mice(data = full[, c("Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Cabin", "Embarked" )], method='rf') 
```

```{r}
mice_output <- complete(mice_mod)
```

Compare the original data with the imputed data
```{r}
par(mfrow = c(1, 2))

hist(full$Age, breaks = 50, col = "darkblue", freq = F, main = "Age: Original Data")
lines(density(full$Age, na.rm = T), col = "red")
lines(density(full$Age, adjust = 4, na.rm = T), lty="dotted", col="darkgreen", lwd=2)

hist(mice_output$Age, breaks = 50, col = "lightblue", freq = F, main = "Age: MICE output")
lines(density(mice_output$Age, na.rm = T), col = "red")
lines(density(mice_output$Age, adjust = 4, na.rm = T), lty="dotted", col="darkgreen", lwd=2)
```

```{r}
par(mfrow = c(1, 2))

ggplot (data = full, aes(Age, fill = as.factor(Pclass))) +
  geom_histogram (aes(y = ..density..), alpha = 0.7, position = 'identity', binwidth = 2, na.rm = T) +
  geom_density(alpha = 0.2, linetype="dotted", na.rm = T) +
  ggtitle("Age: Original Data")

ggplot (data = mice_output, aes(Age, fill = as.factor(Pclass))) +
  geom_histogram (aes(y = ..density..), alpha = 0.7, position = 'identity', binwidth = 2, na.rm = T) +
  geom_density(alpha = 0.2, linetype="dotted", na.rm = T) + 
  ggtitle("Age: MICE output")
```

```{r}
full$Age <- mice_output$Age
sum(is.na(full$Age))
```
* The imputed data looks good, we'll try to the model again and observe the change.

# The Missing `Cabin` variable
```{r}
full$Cabin[1:30]
```
* The first letter represnt the Deck, and the number probably represent the room
* We know from our previous analysis that there are 1014 missing obs among the 1309 obs, there are probably very little use we can extract from this information

# Logistic Model
## Model with the imputed dataset
```{r}
train_logit2 <- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = full[!is.na(full$Survived), ], family = "binomial")
```

```{r}
summary(train_logit1)
summary(train_logit2)
```

* The coefficients look similar, but let's look at more substantial metrics for the model evaluation.

## Model Evaluation
### ANOVA
```{r}
anova(train_logit1, test = 'Chisq')
anova(train_logit2, test = 'Chisq')
```

Logistic regression is infamous for not so intuitive prediction. For example, we would interpret the coefficients as follows:
* For every one unit change in `Pclass`, the log odds of survival increases by -1.199251  -- 0.3014 less chance
* log adds for male passagers to survived is -2.638476 less than female passengers -- 0.0714 less chance
...
We would want our prediction to be a vector of probability or binary outcome.

### Determine the best cutoff
We first generate the prodiction as a vector of probability.
```{r}
train_predict1 <- predict (train_logit1, data = full[1:891, ], type = "response")
train_predict2 <- predict (train_logit2, data = full[1:891, ], type = "response")
```

Plot the histogram of the actual survivals and death to the predicted probabilities of survival. Looks like 0.6 instead of the default choice of 0.5 may be a good threshhold.
```{r}
train_1 <- cbind(train, train_predict1)
train_2 <- cbind(train, train_predict2)
```


```{r}
ggplot (data = train, aes(train_predict2, fill = as.factor(Survived))) +
  geom_histogram (alpha = 0.5, position = 'identity', binwidth = 0.01) +
  ggtitle("MICE outcome")
```


Visualize this result to get a clearer picture of what the cutoff should be
```{r}
library(ROCR)
```


```{r}
result2 <- prediction(train_predict2, train$Survived)
evaluation2 <- performance(result2, "acc")
plot(evaluation2)
abline(v = 0.6, col = "red")
abline(h = 0.81, col = "red")
```

* Although the accuracy is slightly lower in the imputated data, but the difference is subtle

### Confusion Table and ROC
Next, identify the best cut-off scores using accuracy as the criterion.

For the imputated data
```{r}
max_index2 <- which.max(slot(evaluation2, "y.values")[[1]])
max_index2
```

```{r}
acc2 <- slot(evaluation2, "y.values")[[1]][max_index2]
cut_off2<- slot(evaluation2, "x.values")[[1]][max_index2]
print(c(Accuracy = acc2, Cutoff = cut_off2))
```

Using 0.6061 as a cutt off score will gives the best accuracy, which is 0.8137
```{r}
train_outcome2<- ifelse(train_predict2> 0.6121, 1, 0)
```

```{r}
conf_table2 <- table(train$Survived, train_outcome2)
conf_table2
```

At this point, our True Positive rate is :
```{r}
conf_table2[2,2]/sum(conf_table2[,2])
```

And our False Positive rate is :
```{r}
conf_table2[2,1]/sum(conf_table2[,1])
```

Draw the ROC to get a visual evaluation of this classifier at different cutoff points.
```{r}
roc2 <- performance(result2, "tpr", "fpr")
plot(roc2, colorize = T)
abline(a = 0, b = 1, col = "grey")
```

Next, we will compute the AUC:

```{r}
auc2 <- performance(result2, "auc") %>%
        slot(., "y.values") %>%
        unlist() %>%
        round(., 4)
auc2
```

Compare the two rocs with legend on side by side
```{r}

plot(roc2, colorize = T)
abline(a = 0, b = 1, col = "grey")
legend(0.65, 0.2, auc2, title = "AUC2")
```
* In general, the logistic regression gives reasonable prediction
* With the imputation, our model has very similar predicting accuracy

### Pseudo R^2
McFadden’s R2, which is defined as 1−[ln(LM)/ln(L0)] where ln(LM) is the log likelihood value for the fitted model and ln(L0) is the log likelihood for the null model with only an intercept as a predictor. The measure ranges from 0 to just under 1, with values closer to zero indicating that the model has no predictive power.
```{r}
library(pscl)
pR2(train_logit2)
```
* the Pseudo R^2 suggest the logistic model has a weak ~ medium predicting power.
* with the imputated data, the predicting power is even lower.

# Improving the model by mining more information from the variables
## The passenger names
```{r}
full$Name[1:20]

```
* Titles would be the string between `,` and `.`

```{r}
Title <- gsub("(.*,\\s)|(\\..*)", "", full$Name)
```

```{r}
table(Title, full$Sex)
```
* "Mlle" and "Mme" are French for "Miss" and "Madame"
* "Don" and "Dona" are honorific titles in Spanish, rougly means "Lord"
* "Col" and "Major" are military titles
* "Rev" is short for "Reverend", preachers 
* "Master" are loyal title for boys or young men

Take a look at the survival situations among all titles
```{r}
table(Title[1:891], train$Survived)
```

To get a clear picture of the trend, let's categorize some of the rare titles. This might not be a perfect regrouping.
```{r}
#"Mlle", "Mme", "Ms" => "Miss"
Title[Title == "Mlle" | Title == "Mme" | Title == "Ms"] <- "Miss"

# Assign the upper class to one category
Title[Title == "Don" | Title == "Dona" | Title == "Master" | Title == "Lady" | 
      Title == "Sir" | Title == "Jonkheer" | Title == "the Countess"] <- "Upper"

# Assign military titles and Reverend to a service category
Title[Title == "Capt" | Title == "Col" | Title == "Major" | Title == "Rev" | Title == "Dr" ] <- "Service"

```


Take a look at the survival situations among all titles
```{r}
tb_ts <- table(Title[1:891], train$Survived)
tb_ts
```

```{r}
chisq.test(tb_ts)
```
* Looks like title do have some impact on survival.

## Family sizes
```{r}
table(full$SibSp, full$Survived)
```

```{r}
table(full$Parch, full$Survived)
```
* Majority of the passengers are by themselves
* Why would anybody has more than 4 parents onboard?
* Passengers mihgt have less chance of survival if they are by themselves or with many family members

Combine the parent numbers and sibling numbers
```{r}
Fam_size <- full$Parch + full$SibSp
```


```{r}
tb_fams <- table(Fam_size, full$Survived)
tb_fams
```

Regroup this variables into singleton (0), small (1-4), and big (>4) 
```{r}
Fam_sizeD <- vector(mode = "numeric", length = length(Fam_size))
Fam_sizeD[Fam_size == 0] <- "Singleton"
Fam_sizeD[Fam_size > 0 & Fam_size < 5] <- "Small"
Fam_sizeD[Fam_size >= 5] <- "Big"
```

```{r}
tb_fams <- table(Fam_sizeD, full$Survived)
tb_fams
```

```{r}
chisq.test(tb_fams)
```
* There might be a some dependency between family size and survival

##Add the title and family size into the model
```{r}
library(tibble)
```

```{r}
full$Title <- Title
full$Fam_sizeD <- Fam_sizeD
```

##The enhanced model
```{r}
train_logit3 <- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + 
    Fare + Embarked + Fam_sizeD + Title , family = "binomial", data = full[1:891, ])
```

```{r}
summary(train_logit3)
```
* Hooray, The AIC is bumped up!

Let's look at other model evaluation indexes, we will repeat the step before and make a vector of probabilities first
```{r}
train_predict3 <- predict(train_logit3, full[1:891], type = "response")
```


```{r}
result3 <- prediction(train_predict3, train$Survived)
evaluation3 <- performance(result3, "acc")
plot(evaluation3)
abline(v = 0.6, col = "red")
abline(h = 0.81, col = "red")
```
* We have succefully bumped up the accuracy rate.

Next, identify the best cut-off scores using accuracy as the criterion.
```{r}
max_index3 <- which.max(slot(evaluation3, "y.values")[[1]])
max_index3
```

```{r}
acc3 <- slot(evaluation3, "y.values")[[1]][max_index3]
cut_off3 <- slot(evaluation3, "x.values")[[1]][max_index3]
print(c(Accuracy = acc3, Cutoff = cut_off3))
```

Using 0.6291 as a cutt off score will gives the best accuracy, which is 0.8137
```{r}
train_outcome3<- ifelse(train_predict3 > 0.6291, 1, 0)
```

```{r}
conf_table3 <- table(train$Survived, train_outcome3)
conf_table3
```

At this point, our True Positive rate is :
```{r}
conf_table3[2,2]/sum(conf_table3[,2])
```

And our False Positive rate is :
```{r}
conf_table3[2,1]/sum(conf_table3[,1])
```

Next, we will compute the AUC:
```{r}
auc3 <- performance(result3, "auc") %>%
        slot(., "y.values") %>%
        unlist() %>%
        round(., 4)
auc3
```

```{r}
roc3 <- performance(result3, "tpr", "fpr")
plot(roc3, colorize = T)
abline(a = 0, b = 1, col = "grey")
legend(0.85, 0.2, auc3, title = "AUC3")
```
```{r}
auc2
```

Pseudo R^2
```{r}
pR2(train_logit3)
pR2(train_logit2)
```
* All the indexes seem improvements.



# Machine Learning

##Decision Tree
```{r}
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
```

```{r}
train_tree <- rpart(formula = Survived ~ as.factor(Pclass) + Sex + Age + SibSp + Parch + 
    Fare + Embarked + Fam_sizeD + Title, data = full[1:891, ], method = "class")
```

```{r}
fancyRpartPlot(train_tree)
```

##Evaluate the outcome
```{r}
tree_predict <- predict(train_tree, data = full[1:891, ], type = "class")
```

```{r}
conf_table4 <- table(train$Survived, tree_predict)
conf_table4
```

The true positive rate is:
```{r}
conf_table4[2,2]/sum(conf_table4[,2])
```

and the false positive rate is:
```{r}
conf_table4[2,1]/sum(conf_table4[,1])
```

* Just based off eyeballing the confusion table, the tree model seems to give a better prediction result.

## Random Forest
```{r}
library(randomForest)
```

```{r}
library(dplyr)
full = full %>% mutate_if(is.character, as.factor)
```


```{r}
set.seed(111)
train_rf <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + 
    Fare + Embarked + Fam_sizeD + Title, data = full[1:891, ], importance = T)

```

Take a look at the confusion table 
```{r}
train_rf$confusion
```

Take a look at the important variables
```{r}
varImpPlot(train_rf)
```

Take a look at the model error rate
```{r}
plot(train_rf, ylim = c(0, 0.36))

legend("topright", colnames(train_rf$err.rate), col = 1:3, fill = 1:3)
```

* This looks decent

# The prediction
```{r}
test_predict <- predict(train_rf, full[892:1309, ])
```

```{r}
# Save the solution to a dataframe with two columns: PassengerId and Survived (prediction)
solution <- data.frame(PassengerID = test$PassengerId, Survived = test_predict)

# Write the solution to file
write.csv(solution, file = 'rf_Solution.csv', row.names = F)
```

