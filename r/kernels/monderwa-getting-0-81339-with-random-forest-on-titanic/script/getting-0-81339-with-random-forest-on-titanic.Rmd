---
title: 'Titanic with Random Forest and mlr (score 0.81339)'
author: 'Martijn Onderwater'
date: 'October 30th, 2017'
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: readable
    highlight: tango
---

#Introduction
For the past few months I have been experimenting with the Titanic competition. I found the work of my
fellow machine learning entousiasts very usefull, so I decided to share my progress in this kernel.

My objectives for experimenting with the Titanic competition are the following (in order):

1. Experiment with decision trees, regression, random forest, gbm, and xgboost to get a good feel
   for the algorithms and the parameters.
2. Learn a model that is well-fitted (little overfitting), with good learning curves and a cross-validated
   score that matches the score on the testset closely.
3. Achieve a score that gets me into the top 10% of the leaderboard

I have spent a good deal of time on this, mostly playing with random forest, gbm, xgboost, and feature
engineering. For this kernel I'll
focus on random forest as the model. I did a lot of data analysis during my experiments, but because there
are several kernels available with a good data analysis ([1](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic), [2](https://www.kaggle.com/omarelgabry/a-journey-through-titanic), [3](http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/)), I'll skip that part. In terms of features I have
limited this kernel to only those that matter for the final result.

If, after reading this kernel, you have ideas for improvements please let me know in the comments
or via [LinkedIn](https://www.linkedin.com/in/martijnonderwater/)!

#Loading data and dataset

```{r, message=FALSE, warning=FALSE}

#libraries
library(randomForestSRC)
library(ggRandomForests)
library(mlr)
library(data.table)
library(ggplot2)
library(arules)

#init
seed = 314159
set.seed(seed)

#read data
colClassesTrain=c("integer","factor", "factor", "character", "factor", "numeric",  "integer", "integer", "character", "numeric", "character", "factor")
colClassesTest=c("integer", "factor", "character", "factor", "numeric",  "integer", "integer", "character", "numeric", "character", "factor")
trainData = read.csv("../input/train.csv", colClasses=colClassesTrain)
testData = read.csv("../input/test.csv", colClasses = colClassesTest)

```



#Feature engineering

## preprocessing

Before extracting some features from the given data set, we combine the test and train set. I use the PassengerId
as rowname, so that I don't have to worry if I change the ordering at some point.

```{r, message=FALSE, warning=FALSE}

#remove passengerId
rownames(trainData) = trainData$PassengerId
trainData$PassengerId = NULL
rownames(testData) = testData$PassengerId
testData$PassengerId = NULL

#combine train and test prior to preprocessing
survived = trainData$Survived
trainData$Survived = NULL
theData = rbind(trainData,testData)

# seperate some features that we will delete later #
delFeatures = c("Name", "Cabin", "Ticket","Embarked", "Parch", "SibSp")
tmpFeatures = theData[delFeatures]
theData[,delFeatures]=NULL
```

##Family Size
From Sibsp and Parch we can get the size of a family. 

```{r, message=FALSE, warning=FALSE}
#family size
theData$FamilySize = as.integer(tmpFeatures$SibSp + tmpFeatures$Parch +1)
```

##Title
The name of a passenger also contains his/her title. Later we will see that this plays a large role in predicting
survival. Intuitively, the title combines a person's gender with socioeconomic status. I can imagine that women
and rich people are more likely to survive, and feature 'title' can capture both these aspects.

```{r, message=FALSE, warning=FALSE}
#get title from Name
theData$Title = sapply(tmpFeatures$Name, function(name){
  tmp=strsplit(name, ',');
  title = NA
  if(length(tmp[[1]]>1)){
    tmp = strsplit(tmp[[1]][2], split= "[.]");
    if(length(tmp[[1]])>1){
      title = trimws(tmp[[1]][1])
    }
  }
  title
})
theData$Title[theData$Title=='Mme'] = "Mlle"
theData$Title[theData$Title %in% c('Capt', 'Col','Major', 'Dr', 'Rev')] = 'Officer'
theData$Title[theData$Title %in% c('Jonkheer','Don', 'Sir', 'the Countess', 'Dona', 'Lady')] = 'Royalty'
theData$Title[theData$Title %in% c('Mme', 'Ms','Mrs')] = 'Mrs'
theData$Title[theData$Title %in% c('Mlle', 'Miss')] = 'Miss'
theData$Title[theData$Title %in% c('Mr')] = 'Mr'
theData$Title[theData$Title %in% c('Master')] = 'Master'
theData$Title = as.factor(theData$Title)

```

##Cabin
The Cabin field contains a letter representing the deck, and a number for the cabin itself. This is an
interesting field, because it implicitely contains information about the location on the ship of the cabin.
This may be important: perhaps if a passenger is closer to the stairs or lifeboats, he/she is more likely to survive.
Looking at the [layout](https://www.encyclopedia-titanica.org/titanic-deckplans/tank-top.html) of the
decks, each deck is numbered from the stern to the bow, in 4 to 6 rows. The plans also show where the stairs are.

I used this to create the following features:

1. Deck - a letter
2. CabinNumber - a number
3. CabinMod4 - captures on which row a cabin may be (modulo 4 seemed to work better than modulo 6)
4. CabinInterval - captures if the cabin is closer to stern or bow

Feature CabinNumber is only extracted from Cabin to create CabinMod4 and CabinInterval, and deleted afterwards.
Also, the Cabin field is a bit messy:

1. Some people have more than 1 Cabin. They are always on the same Deck and close together. For simplicity
   I only keep the first, but other choices are possible (and perhaps better).
2. 7 people have the letter 'F in front of a valid Cabin value, for instance Miss Anna Peters in 'F E69'. I don't
   know what it means, so I remove it.
3. The Cabin field has a lot of NAs.


```{r, message=FALSE, warning=FALSE}
#remove 'F ' from Cabin field
tmpFeatures$Cabin = gsub(pattern="F ", replacement="", x=tmpFeatures$Cabin)

#Parse Cabin
theData$Deck = NA
theData$CabinNumber = NA
for (i in 1:length(tmpFeatures$Cabin)){
  cabin = tmpFeatures$Cabin[i]
  if(cabin!=""){
    tmp=strsplit(cabin, ' ');
    theData$Deck[i] = substr(cabin, 0,1)#note: when a passenger has multiple rooms then they are all on the same deck.
    for(j in 1:length(tmp[[1]])){
      cbn = tmp[[1]][j]
      cbn = gsub("[a-zA-Z]", "", cbn)
      if(j==1){
        theData$CabinNumber[i]=as.integer(cbn)
      }
      #use j==2, j==3, j==4 to get the other cabins of a passenger
    }
    
  }
}
theData$Deck = as.factor(theData$Deck)
theData$CabinNumber = as.integer(theData$CabinNumber)

#Starboard-port information
theData$CabinMod4 = theData$CabinNumber%%4
theData$CabinMod4 = as.factor(theData$CabinMod4)

#Stern-bow information
boundaries = c(0,10,20,30,40,50,70,90,120,1000)
for(b in seq_len(length(boundaries)-1)){
  lower = boundaries[b]
  upper = boundaries[b+1]
  theData$CabinInterval[theData$CabinNumber>=lower & theData$CabinNumber<upper] = paste("Range", lower, upper)
}
theData$CabinInterval = as.factor(theData$CabinInterval)

#clean up
theData$CabinNumber = NULL

```
##Parsing Ticket
The Ticket field is very messy, with some records containing only a number, whereas others have
both a number and some text. For simplicity I remove all the
text, and use only the number to create a
discretized version of the number in a tickets.


```{r, message=FALSE, warning=FALSE}
#parse ticket information
theData$TicketNumber = NA
for (i in 1:length(tmpFeatures$Ticket)){
    ticket = tmpFeatures$Ticket[i]
    if(ticket!=""){
      tmp=strsplit(ticket, ' ');#should yield 1,2 or 3 elements. 
      nElts = length(tmp[[1]])
      
      if(nElts==1){#we have only a ticketnumber or a ticketletter
    	num = strtoi(tmp[[1]])
    	if(!is.na(num)){
    	  theData$TicketNumber[i] = num#field has only a number
    	}
      }
      else{#we have both ticketletter and ticketnumber.
    	theData$TicketNumber[i] = tail(tmp[[1]], n=1)
      }
    }
}
theData$TicketNumber = as.integer(theData$TicketNumber)
theData$TicketNumberInterval = discretize(theData$TicketNumber, method="frequency",categories=15)
theData$TicketNumber = NULL
```

##Fare
The dataset contains the price paid by passengers for a ticket (Fare). Note that Fare is not a per-person price,
but the joint price for all passengers on the ticket. We use this info to create a FarePerPerson feature.
There are some NAs in Fare and we impute these from the median Fare of 3rd class male passengers between age
55 and 65 (values obtained via data analysis). A few other passengers have Fare 0, which sounds unlikely.
I kept them as is, but perhaps these are better treated as NAs.

Also, I scale the values of Fare and FarePerPerson logarithmically. This is a left-over of my initial attempts
with regression, but it is not a necessary step for Random Forests. Unfortunately, removing the transformation
reduces the score. I kept the transformation, but I expect that with some extra effort I should be able to get a
good score without the transformation.


```{r, message=FALSE, warning=FALSE}
#impute simply from Sex, Embarked, Age, Pclass
idxs = theData$Sex=="male" & theData$Age>55 & theData$Age<65 & theData$Pclass=="3"
idxs[is.na(idxs)] = FALSE#there are some NAs in Age
theData$Fare[is.na(theData$Fare)] = median(theData[idxs,]$Fare, na.rm=TRUE)

#Fare per person
theData$FarePerPerson = NA
for(i in 1:length(tmpFeatures$Ticket)){
  idxs = tmpFeatures$Ticket == tmpFeatures$Ticket[i]
  theData$FarePerPerson[i] = theData$Fare[i]/sum(idxs)
}

#scale
theData$FarePerPerson = log10(1+theData$FarePerPerson)
theData$Fare = log10(1+theData$Fare)

```

##General processing
I add NA as a level for all factors. As a consequence, RF can no longer impute missing values for factors. I
didn't experiment too much here, but it may be that omitting this step for certain factors (especially
those with relatively few with NAs) can improve the score.

```{r, message=FALSE, warning=FALSE}
#factors get NA as level
facts <- sapply(theData, is.factor)
for(i  in 1:ncol(theData)){
  if(facts[i]){
    idxs = is.na(theData[,i])
    if(any(idxs)){
      #theData[,i] = addNA(theData[,i], ifany=TRUE)  #I cant seem to select records later that have the NA level
      levels(theData[,i]) = c(levels(theData[,i]),"Unknown")
      theData[idxs,i] = "Unknown"
    }
    
    theData[,i] = droplevels(theData[,i])
  }
}
```

##Age
The Age field in the dataset is quite clean and can be readily used as a feature. I again scaled Age with log10
(see my doubts above), and this time I used a RF for imputation (more on imputation later).

```{r, message=FALSE, warning=FALSE}
#age to log
theData$Age = log10(theData$Age)

#impute Age with rf
imputeLrn = makeLearner("regr.randomForestSRC", fix.factors.prediction = TRUE)
pars = list(mtry=7, ntree=234, sampsize=1000, nsplit=11, nodesize=15)
imputeLrn = setHyperPars(imputeLrn, par.vals=pars)
imp = mlr::impute(obj=theData, cols = list(Age = imputeLearner(imputeLrn))) #
theData = imp$data
```

##Undo separation of test and train
Well, this is straightforward.

```{r, message=FALSE, warning=FALSE}
ntrain = nrow(trainData)
trainData = theData[1:ntrain,]
testData = theData[(ntrain+1):nrow(theData),]
trainData$Survived = survived
rm(ntrain,theData, survived)

#and some cleanup
rm(tmpFeatures)

```

#Training the model

##Setup mlr
For training the model I use mlr, a relatively new machine learning library in R. First, I create a task
that describes what I want to learn. Next I set the hyperparameters (obtained with tuning - more on this later) and ... ready to go!

```{r, message=FALSE, warning=FALSE}

#setup mlr
levels(trainData$Survived) = c("Died", "Lived")
titanic.task = makeClassifTask(id = "titanic", data = trainData, target = 'Survived', positive = "Lived", fixup.data = "no")

#setup learner
set.seed(seed)
lrn = makeLearner("classif.randomForestSRC", fix.factors.prediction = TRUE)
pars = list(mtry=6, ntree=213, sampsize=750, nsplit=16, nodesize=15)
lrn = setHyperPars(lrn, par.vals = pars)
```

##Assessing performance
I use two techniqes to assess the performance of the model with the hyperparameters from above:

1. Cross validation
2. Learning curves

I have a lot to say about the outcome of these, so I'll put all that in a separate section in this kernel.

```{r, message=FALSE, warning=FALSE}
#cv performance
set.seed(seed)
resamp = makeResampleDesc("RepCV", folds = 10, reps=2, stratify = TRUE, predict="both")
r = resample(learner = lrn, task = titanic.task, resampling = resamp, show.info = TRUE, models = TRUE, measures=list(acc,setAggregation(acc, train.mean)))
r$aggr#test:

#learning curves
lrn = setHyperPars(lrn,par.vals=list(importance=FALSE))
resamp = makeResampleDesc("RepCV", folds=10, reps=3,stratify = TRUE, predict="both")
lc = generateLearningCurveData(learners = lrn,
                               task = titanic.task,
                               percs = seq(0.5, 1, by = 0.1), measures = list(acc, setAggregation(acc, train.mean)),
                               resampling = resamp
                               , show.info = TRUE)
plotLearningCurve(lc, facet="learner")+ scale_y_continuous(breaks=seq(0.8,1,by=0.01))

```

##Feature importance
The figure below shows the features and their importance in the random forest model. As with other kernels, Title, Pclass,
Sex are important features. It is nice to see that the CabinMod4 also plays a decent role in prediction.

```{r, message=FALSE, warning=FALSE}

#train
lrn = setHyperPars(lrn,par.vals=list(importance=TRUE))
mod = train(lrn, titanic.task)
pred = predict(mod, titanic.task)

#importance of variables
tmp = gg_vimp(getLearnerModel(mod, more.unwrap=TRUE))
plot(tmp)

```

##Write output
The trained model can now be used to create a prediction. Submitting this on Kaggle yields score 0.81339. 

```{r, message=FALSE, warning=FALSE}

newSeed = 10
set.seed(newSeed)

#train
lrn = setHyperPars(lrn,par.vals=list(importance=TRUE, seed=-newSeed))
mod = train(lrn, titanic.task)
pred = predict(mod, titanic.task)


#write submission
pred = predict(mod, newdata = testData)
nTest = nrow(testData)
tmp = data.frame(PassengerId=integer(nTest), Survived=factor(nTest, levels=c("0", "1")))
tmp$PassengerId = as.integer(rownames(testData))
tmp$Survived = as.integer(pred$data$response)-1#note: rely on Died<Lived alphabetically
write.table(tmp, file = paste("submission",newSeed,".csv", sep=""), quote = FALSE, sep = ",",row.names = FALSE,col.names = TRUE)

```

#Discussion and improvements

##Performance assessment
When you run the repeated CV in the code above, you get a mean accuracy of 0.851 and 0.894 on the training set
and validation set, respectively (note: mlr uses the term 'test' instead of 'validation' here). With my initial experiments these two values were much further apart, because
I was overfitting badly. By carefully choosing the hyperparameters (in particular sampsize, mtry, and ntrees),
the situation improved a lot. The learning curves show a reasonable scenario, with training accuracy decreasing
and test accuracy increasing as we feed the learner more data.

Of course, I would be more happy if the submission score on Kaggle (0.81339) is a bit closer to the mean
accuracy on the validation set. Inspecting the values on the validation in CV shows quite a large variation.
This is possibly because there are not enough points in the fold, but it is also possible that my features
are not capturing the right information. There is room for improvement here, I think. On the positive side,
during experiments I did notice that an improvement in score on the CV also yielded a better score on Kaggle.

The submission score for this kernel is 0.81339, and I would like to asses how representative this score
is for the model. To this end, I trained the model with 30 different values for the 'seed' variable
and submitted the resulting csv-files. The boxplot below show the scores Kaggle returned, ranging from 0.78947 to
0.81339 (I used one of the seeds that yielded 0.81339 for this kernel). For me it is not unexpected that there is
some variation in the score, since training a random forest model also has various sources of randomness (sampsize, mtry, ...).
It is useful to be aware of this inherent randomness.


```{r, message=FALSE, warning=FALSE}
vals =c(0.80382,0.81339,0.79904,0.80861,0.80382,0.80861,0.7942,0.78947,0.80382,0.80861,0.80861,0.80861,0.80382,0.80382,0.81339,0.79904,0.81339,0.80861,0.80861,0.80382,0.79904,0.80861,0.79425,0.80861,0.79904,0.80861,0.79904,0.81339,0.80382,0.80861)
vals = as.data.frame(vals)
vals$group=" "
ggplot(data=vals, aes(x=group, y=vals)) +
    geom_boxplot(fill='#A4A4A4', color="black")+
    geom_dotplot(binaxis='y', stackdir='center', dotsize=1)+
    labs(x="",y="Accuracy on testset",title="Boxplot of accuracy on testset")


```

##Tuning
Choosing the right values for hyperparameters is essential to prevent overfitting. Discovering such values
requires a combination of experience and tuning. I used mlr to do some tuning in two places, namely for
the final model and for the imputation of missing values in Age. The process of tuning is messy and I stopped
when I found some reasonable values. But I think improvements are possible. For instance, it seems that using
a random forest with 213 trees (as I do) is perhaps a bit much for a dataset of 891 records. Also, I paid very
little attention to the hyperparameters of the imputation of Age, so better parameters may be found.

##Imputation
The Random Forest algorithm has a build-in method for imputation, so the imputations I do are not needed from an algorithmic
perspective. I tried leaving my imputations out, but that didn't improve the score so they are still present.
Let me know what you think of this! For factors I added NA as a level, which makes most sense when there are a
lot of NAs (imputing a lot of values from a few values does not seem useful). However, the TicketNumberInterval
feature has only a few NAs, so perhaps that can be treated differently. Also, the passengers with Fare 0 could
perhaps be better treated as NAs.

##Discretization
During my experiments I had a few discretized versions of numerical features in my dataset (e.g. Fare and Age).
That was a bad idea: RF is much better at discretization than I am. Also, keeping both the numerical and
discretized feature only makes the learning task more difficult for RF. There are of course situations where
discretization does make sence, for instance with TicketNumberInterval. Here, I grouped the numbers together
that were typically close to the same stairs. It should vary per Deck (which I didn't do), so TicketNumberInterval
can probably be improved.

##Log scaling of Age, Fare, FarePerPerson
I mentioned earlier that I preprocess Age, Fare, FarePerPerson by taking the logarithm. I don't think this
should be necessary, since it is a monotonic scaling and the RF algorithm is perfectly capable of handling
the unscaled data. Still, removing the scaling decreased my score, so kept it for this kernel. Maybe this is
just luck?


##Additional features
There are a lot of other features that can be constructed from the data:

* Embarked, SibSp, Parch (I left them out because I could without decreasing the score)
* Different grouping of titles
* Mother/Father
* Grouping based on last name
* Use the letters in the Ticket (SOTON, CA, etc)
* Young children and elderly people may not be able to swim
* Families tend to live or die as a whole (I didn't check this myself)
* ...



#Advise for beginners
When starting with machine learning the amount of options and choices to make can be overwhelming. Just to name
a few:

* Which features do I use?
* If not all, what feature selection method do I use?
* Should I do oversampling?
* Which algorithm should I use?
* Which parameters do I tune and in what range?
* Should I impute missing values?

My advise would be to focus on the proces and much less on the score on Kaggle. At first, stick to a few basic features,
and one algorithm - preferable one that you feel you understand. If your algorithm requires imputation of missing values,
just fill them with, e.g, mode or mean (improvements can be made later). Try to quickly get a situation with:

1. Learning curves that show little overfitting
2. Kaggle score is close to CV score (don't worry if it is a low score)

This requires finding good hyperparameters. Use tuning for this, but also (mainly?) your own understanding of the algorithm.

Once you are here, start engineering new features or improving existing features using the CV score as a target.
Every now and then, check out the learning curves to see that you
are still not overfitting. Also, try modifying the hyperparameters occasionally to improve your CV score.
Submit to Kaggle as often as you like/can to get confident that improvements in CV
score also yield improvements on Kaggle.

Slowly, this should increase your score, but - most importantly - your understanding of the data and the techniques.
If at some point you get stuck with an overfitted model, return to a previous version and try again (good version
control is useful here). Only when your first algorithm seems incapable of learning more is it time to try another.
Note that now you have to go through the whole process again.

For a small dataset like Titanic, feature selection can quite easily be a manual and experimental step in the process.
For other beginner-competitions with larger datasets doing feature selection manually is not convenient, which makes
the small Titanic competition such a great start for beginners.

At this point, you should feel confident enough to try things like oversampling and imputation. Your experience
so far is at a good enough level now to properly judge the effect of changes to you approach.


#Final remarks
Thanks for your attention, and feel free to add comments to the discussion below and to connect on
[LinkedIn](https://www.linkedin.com/in/martijnonderwater/)!





