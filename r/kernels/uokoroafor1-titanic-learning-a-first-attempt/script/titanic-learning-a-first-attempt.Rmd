---
title: "Titanic - Learning"
output:
  html_document: default
  pdf_document: default
---
```{r global_options, include=F}
knitr::opts_chunk$set(fig.width=5, fig.height=4)
```

```{r setup, include=FALSE}
rm(list=ls()) # Clear the variables

library(tree); library(ggplot2);library(reshape2); library(randomForest); library(e1071); library(caret) # Load the relevant libraries
library(tidyverse)
```

### Contents
1. [Introduction](#intro)
2. [Sorting out the variables](#fix_vars)
3. [Fitting the models](#fits)
4. [Submitting results and checking](#submit)

### Introduction {#intro}
This is my first attempt at a Kaggle kernel. I am using the Titanic dataset from kaggle to learn the rudiments of the major machine learning techniques. You can find details of the Titanic dataset [here](https://www.kaggle.com/c/titanic). The first action as always is to load in the data. I have saved the train and test data as csv files. I will load them into the environment and saved them as `train` and `test`. I am very much a newbie so I'm grateful for any pointers. At this stage, I won't do any feature selection (except for when obvious). I will save that for the next attempt in the hope that it will improve predictability. I've also not looked at the algorithm's efficacy at predicting survival vs non-survival. Again, I'll leave that for a further attempt.

In this write up, I used the `tree`, `ggplot2`, `reshape2`, `randomForest`, `e1071` and `caret` libraries.

```{r data_load}
train<-read.csv("../input/train.csv", header=T)
test<-read.csv("../input/test.csv", header=T)
```

### Sorting out the variables (light feature engineering) {#fix_vars}
The data is inspected to see if any variables need fixing and to ensure they are treated the right way (e.g. factors treated as factors not integers).

```{r fix_vars, echo=F}
# Let's dig in further into the data.
summary(train)
```
There are 12 variables in the data detailed below:

Feature       | Description
--------------|-----------------------------------------------------
Survived      | Whether a Passenger Survived = 1 or Died = 0
Pclass        | Passenger's Travel Class
Name          | Passenger's Name
Sex           | Passenger's Sex
Age           | Passenger's Age
SibSp         | Number of siblings/spouses a passenger has on board
Parch         | Number of parents/children a passenger has on board
Ticket        | Passenger's Ticket number
Fare          | Passenger's Fare
Cabin         | Passenger's Cabin
Embarked      | Passenger's Port of embarkation

There are **`r nrow(train)`** observations in the training set and **`r nrow(test)`** in the test set. Each variable in inspected in turn and the appropriate treatment is decided.

##### PassengerId: 
This looks to be a number from 1 to `r nrow(train)`. It is unlikely to have predictive power. It won't be part of the analysis.

##### Pclass: 
```{r, echo=F, warning=F}
# Compute a histogram of `PClass`
p<-ggplot(data=train, aes(train$Pclass, fill=as.factor(train$Survived))) + 
  geom_histogram(binwidth=0.5)+
  labs(title="Histogram of Pclass")+
labs(x="Passenger Class")

p<-p+scale_fill_discrete(name = "Survived")
p
```

This is said to be a proxy for social class. There are `r sum(is.na(train$Pclass))` missing variables so there is no need for proxying or replacing. However, it looks like  the variables are being treated as integers. While they are ordinal, we would rather have them treated as factors so as not to impose an arbitrary scale on the variable. Hence it will be treated as a factor. As seen above, the lower the class, the higher the chances of survival were.

```{r, echo=F}
train$Pclass<-as.factor(train$Pclass)
test$Pclass<-as.factor(test$Pclass)
```

##### Age: 
```{r, echo=F, warning=F}
p<-ggplot(data=train, aes(train$Age, fill=as.factor(train$Survived))) + 
  geom_histogram(binwidth=15)+
  labs(title="Histogram of Age")+
labs(x="Age")

p<-p+scale_fill_discrete(name = "Survived")
p
```

Looking at the histogram above, it appears there appears to be a non-linear relationship between the rate of Survival and Age. This might lend credence to treating age as categorical variable. `Age` ranges from `r min(min(na.omit(train$Age)),min(na.omit(train$Age)))` to `r max(max(na.omit(train$Age)),max(na.omit(train$Age)))` and there are `r sum(is.na(train$Age))` missing variables in the training set and `r sum(is.na(test$Age))` in the test set so there is a need to act. Potential steps to take include:

1. **Delete NA's**: This could be problematic given there missing values in the training and test sets.

2. **Delete the `Age` variable**: Again the histogram above shows there appear to be different survival rates for different ages so there is clearly some value to including the variable.

3. **Replace the missing values**: There are several possiblities here.
  + Replace the NAs with the mean Age. The mean is `r round(mean(na.omit(train$Age)),1)` in the training set and `r round(mean(na.omit(test$Age)),1)` in the test set. Given there is a need to be strict with the approach, we assume we do not know the average age of the test set or any unseen data.
  
  + Replace the NAs with Ages permuted randomly around the mean. This is interesting but ultimately for the purposes of replication, this will be avoided.
  
  + Estimate the `Age` by using a prediction model on the remaining variables to predict it for the missing variables. I like this approach and tried it using a simple linear regression model elsewhere. I have seen some use the MICE package. I would like to dig deeper into the MICE package before deploying it but [here's](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) a great example of its use by Megan Risdal. 
  
So I'll apply the first action here and replace NAs with the mean Age of the training set (`r round(mean(na.omit(train$Age)),1)`). I will also look at creating ~5/6 categories for `Age` as well and see which fares better. I initially split the age evenly over 15 year windows but as there are not too many passengers in their 70s, this was not suitable. The (1/6)th percentiles (sextiles??)  for the Age variable are `r (quantile(train$Age,seq(1/6,5/6,1/6),na.rm=T))`. I'll create a feature called `AgeCat` which will be a categorical variable for Age using the quantiles above as bounds. I'll hold on to both `Age` and `Agecat` to see if there are material differences to the results of using them.

```{r, echo=F}
m<-mean(train$Age,na.rm=T)
train$Age[is.na(train$Age)]<-m
test$Age[is.na(test$Age)]<-m

cat_age<-function(age){
  z<-character(length(age))
  for(i in 1:length(age)){
  if(age[i]<18){z[i]<-"under18"}
  else if(age[i]<23){z[i]<-"18_23"}
  else if(age[i]<28){z[i]<-"23_28"}
    else if(age[i]<34){z[i]<-"28_34"}
    else if(age[i]<44){z[i]<-"34_44"}
  else if(age[i]<81){z[i]<-"41_81"}}
  return(z)}

AgeCat<-cat_age(train$Age)
train$AgeCat<-as.factor(AgeCat)
# train$AgeCat

AgeCat<-cat_age(test$Age)
test$AgeCat<-as.factor(AgeCat)
# test$AgeCat

#### Plotting
p<-ggplot(data=train, aes(train$AgeCat, fill=as.factor(train$Survived))) + 
  geom_bar()+
  labs(title="Histogram of Categorical Age")+
labs(x="Age")

p<-p+scale_fill_discrete(name = "Survived")
p

```

##### Sex: 
```{r, echo=F, warning=F}
p<-ggplot(data=train, aes(train$Sex, fill=as.factor(train$Survived))) + 
  geom_bar()+
  labs(title="Bar Chart of Sex")+
labs(x="Sex")

p<-p+scale_fill_discrete(name = "Survived")
p
```

Here there are `r sum(is.na(train$Sex))` missing variables in the training set and `r sum(is.na(test$Sex))` in the test set so there is no need to replace any missing values. There are `r sum(train$Sex=="female")` women and `r sum(train$Sex=="male")` men in the training set with `r round(sum(train$Survived[train$Sex=="female"])/sum(train$Sex=="female"),3)` and `r round(sum(train$Survived[train$Sex=="male"])/sum(train$Sex=="male"),3)` rates of survival respectively. Women are thus more likely to have survived than men.

```{r}
train$Sex<-as.factor(train$Sex)
test$Sex<-as.factor(test$Sex)
```

##### Fare: 
```{r, echo=F, warning=F}
p<-ggplot(data=train, aes(train$Fare, fill=as.factor(train$Survived))) + 
  geom_histogram(binwidth=50)+
  labs(title="Histogram of Fare")+
labs(x="Fare")

p<-p+scale_fill_discrete(name = "Survived")
p
```

This is the fare paid. The assumption is that there would be some correlation between `Fare` and `Pclass`. The fares range from `r min(min(na.omit(train$Fare)),min(na.omit(train$Fare)))` to `r max(max(na.omit(train$Fare)),max(na.omit(train$Fare)))`. Here there are `r sum(is.na(train$Fare))` missing variables in the training set and `r sum(is.na(test$Fare))` in the test set.I will replace the missing Fare in the test set with the average Fare. As expected, the average fare for each class is `r round(mean(train$Fare[train$Pclass==1]),1)` for Class 1, `r round(mean(train$Fare[train$Pclass==2]),1)` for Class 2 and `r round(mean(train$Fare[train$Pclass==3]),1)` for Class 3. 

```{r, echo=F}
test$Fare[is.na(test$Fare)]<-mean(train$Fare)
```
As seen in the histogram above, the higher the fare, the higher the chances of survival.

##### SibSp: 
```{r, echo=F, warning=F}
p<-ggplot(data=train, aes(train$SibSp, fill=as.factor(train$Survived))) + 
  geom_histogram(binwidth=0.5)+
  labs(title="Histogram of Sibling and Spouse")+
labs(x="SibSp")

p<-p+scale_fill_discrete(name = "Survived")
p
```

This is the number of siblings or spouses a passenger had on the boat. It ranges from `r min(min(na.omit(train$SibSp)),min(na.omit(train$SibSp)))` to `r max(max(na.omit(train$SibSp)),max(na.omit(train$SibSp)))` with `r sum(is.na(train$SibSp))` missing values. Passengers with one or two siblings or spouses have the highest chances of survival.

##### Parch: 
```{r, echo=F, warning=F}
p<-ggplot(data=train, aes(train$Parch, fill=as.factor(train$Survived))) + 
  geom_histogram(binwidth=0.5)+
  labs(title="Histogram of Parent and Child")+
labs(x="Parch")

p<-p+scale_fill_discrete(name = "Survived")
p
```

This is the number of parents or children a passenger had on the boat. It ranges from `r min(min(na.omit(train$Parch)),min(na.omit(train$Parch)))` to `r max(max(na.omit(train$Parch)),max(na.omit(train$Parch)))` with `r sum(is.na(train$Parch))` missing values. 

Passengers with one, two or three parents or children have the highest chances of survival.

##### Family Size:
[Completely unorginally](https://www.kaggle.com/jasonm/large-families-not-good-for-survival), we consider whether having a large, small or no family increases ones chances of survival. A family size variable is created that adds `Parch`, `SibSp` and 1 (for the passenger of course).

```{r, echo=F, warning=F}
FamSize<-train$Parch+train$SibSp+1
train$FamSize<-FamSize

FamSize1<-test$Parch+test$SibSp+1
test$FamSize<-FamSize1

p<-ggplot(data=train, aes(train$FamSize, fill=as.factor(train$Survived))) + 
  geom_histogram(binwidth=0.5)+
  labs(title="Histogram of Family Size")+
labs(x="FamSize")

p<-p+scale_fill_discrete(name = "Survived")
p
```

From the histogram above, we see that `FamSize` ranges from `r min(min(na.omit(train$FamSize)),min(na.omit(train$FamSize)))` to `r max(max(na.omit(train$FamSize)),max(na.omit(train$FamSize)))` with `r sum(is.na(train$FamSize))` missing values. Being in a Family of 2,3 or 4 increases ones chances of survival. This however does not hold when a passenger is alone. Furthermore, the relationship does not appear to have any kind of direction. Thus treating family size as an integer might not prove adequate. There are a three possible ways around this:

1. Treat family as a *categorical variable*. This is probably the best way but it will lead to an explosion in the number of variables.

2. Perform a *transformation* on family size so that its value drops (or spikes) the further away you are from 2,3 or 4. 

3. Create a *three level categorical variable* to capture if a passenger has no family, has a small family or a large family. This is my preferred option given that it prevents the proliferation of variables that would occur with option 1 and avoid the use of a transformation given the relationship is not linear. We create a variable called `FamSizeCat` to handle this.

```{r echo=F}
catfam<-function(x){
  z<-character(length(x))
  for(i in 1:length(x)){
  if(x[i]==1){z[i]<-"alone"}
  else if(x[i]<5){z[i]<-"small"}
  else if (x[i]>=5){z[i]<-"large"}}
  return(z)
}

FamSizeCat<-catfam(train$FamSize)
train$FamSizeCat<-as.factor(FamSizeCat)

FamSizeCat<-catfam(test$FamSize)
test$FamSizeCat<-as.factor(FamSizeCat)

p<-ggplot(data=train, aes(train$FamSizeCat, fill=as.factor(train$Survived))) + 
  geom_bar()+
  labs(title="Bar Chart of Categorical Family Size")+
labs(x="Categorical Family Size")

p<-p+scale_fill_discrete(name = "Survived")
p
```

Looks like we've made an improvement there then! We will move ahead with the categorical variable set up for family size. As you can see there are dermacations around rates of survival in using these three categorical variables. Furthermore, the survival rate for people by themselves is nuanced enough that there is in fact a need to give them their separate category.

##### Ticket:
This is just the ticket number and it is unlikelyto have any predictive power. Nothing to see here!

##### Embarked:

This is the port from which the passenger embarked. There are `r sum((train$Embarked)=="")` missing variables in the training set and `r sum((test$Embarked)=="")` in the test set. Not exactly sure if this will have any role to play in predicting survival, but it's prudent to keep it in for now. If I do a further kernel, I'll evaluate what variables aid predictability a bit deeper.

Now we can handle the missing `Embarked` values. The survival rates for the ports are `r round(sum(train$Survived[train$Embarked=="C"])/sum(train$Embarked=="C"),3)` for **C**, `r round(sum(train$Survived[train$Embarked=="Q"])/sum(train$Embarked=="Q"),3)` for **Q** and `r round(sum(train$Survived[train$Embarked=="S"])/sum(train$Embarked=="S"),3)` for **S**. 

Both of the passengers missing their `Embarked` variable have survived. I am going to be needlessly pedantic about this and I make no apologies about that. We have $\Pr(Survived|Embarked)$ above and we know that: $$\Pr(Embarked|Survived)=\frac{\Pr(Survived|Embarked)*\Pr(Embarked)}{\Pr(Survived)}$$. 

Thus we can estimate the probability of the passenger having embarked from each port as **`r round((sum(train$Survived[train$Embarked=="C"])/sum(train$Embarked=="C"))*(sum(train$Embarked=="C")/sum(train$Embarked!=""))/(sum(train$Survived)/length(train$Survived)),2)`** 

for **C**, **`r round((sum(train$Survived[train$Embarked=="Q"])/sum(train$Embarked=="Q"))*(sum(train$Embarked=="Q")/sum(train$Embarked!=""))/(sum(train$Survived)/length(train$Survived)),2)`** for **Q** and **`r round((sum(train$Survived[train$Embarked=="S"])/sum(train$Embarked=="S"))*(sum(train$Embarked=="S")/sum(train$Embarked!=""))/(sum(train$Survived)/length(train$Survived)),2)`** for **S**. Thus we will allocate both to the "S" class.

```{r, echo=F, warning=F}
train$Embarked[train$Embarked==""]<-"S"
test$Embarked[test$Embarked==""]<-"S"

train$Embarked<-as.factor(train$Embarked)
test$Embarked<-as.factor(test$Embarked)

p<-ggplot(data=train, aes(train$Embarked, fill=as.factor(train$Survived))) + 
  geom_bar()+
  labs(title="Bar Chart of Port of Embarkation")+
labs(x="Embarked")

p<-p+scale_fill_discrete(name = "Survived")
p
```

##### Cabin:

This is the cabin. Here there are `r sum(train$Cabin=="")` missing variables in the training set and `r sum(test$Cabin=="")` in the test set. While it would have been interesting to dig further into the cabins to investigate likelihood of survival, there are far too many missing variables to make any sense of it. I've also seen some create make this into a `Has_Cabin` Boolean variable - also interesting. Not going to do either at this stage. It will be removed!

### Alors on fixe! {#fits}

Now we can begin to model. We will look at the methods below:

1. [Logistic Regression](#logreg)
2. [Decision Tree Algorithm](#tree)
3. [Random Forests](#forest)
4. [Support Vector Machine (SVM)](#SVM)
5. [Gradient Boosting](#grad)

To evaluate the methods, we will use 10 fold cross validation where possible and estimate the misclassification errors for all the methods above. We randomly split the data into 10 even groups.

To do this, we shuffle the indices of the dataset, calculate the modulo to base 10 and assign this as the grouping for that Passenger ID.

```{r}
set.seed(1011)
n_folds<-10 # We want 10 folds
indx<-sample(1:nrow(train),replace=F) # Randomly shuffle the rows in the dataframe
fold<-(indx%%n_folds)+1 # Calculate the modulo to base 10 and add 1. This will return a number between 1 and 10. This number will be its fold.

train$fold<-fold
```

Now that we have created our folds, we set out with carrying out the algorithm.

##### 1. Logistic Regression: {#logreg}

  + We run the analysis including the categorical variable for family size `FamSizeCat` but without using Categorical Age `AgeCat`
```{r}
err_cv<-numeric(n_folds)
err_train<-numeric(n_folds)
for(i in 1:n_folds){
fit<-glm(Survived~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train[train$fold!=i,],family="binomial")

fitted<-predict.glm(fit, type="response")
fitted1<-ifelse(fitted>0.5,1,0)
t1<-sum(diag(table(train$Survived[train$fold!=i],fitted1)))/sum(table(train$Survived[train$fold!=i],fitted1))

fitted.values2<-predict.glm(fit,newdata = train[train$fold==i,],type="response")
fitted.values3<-ifelse(fitted.values2>0.5,1,0)
t2<-sum(diag(table(train$Survived[train$fold==i],fitted.values3)))/sum(table(train$Survived[train$fold==i],fitted.values3))
err_train[i]<-t1
err_cv[i]<-t2
}
```
+ This gives a **training error** of **`r 1-round(mean(err_train),3)`** and a **cross-validation error** of **`r 1-round(mean(err_cv),3)`**.

+ Run the same with but with a Categorical Age `AgeCat`
```{r echo=F}
err_cv<-numeric(n_folds)
err_train<-numeric(n_folds)
for(i in 1:n_folds){
fit<-glm(Survived~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train[train$fold!=i,],family="binomial")

fitted<-predict.glm(fit, type="response")
fitted1<-ifelse(fitted>0.5,1,0)
t1<-sum(diag(table(train$Survived[train$fold!=i],fitted1)))/sum(table(train$Survived[train$fold!=i],fitted1))


fitted.values2<-predict.glm(fit,newdata = train[train$fold==i,],type="response")
fitted.values3<-ifelse(fitted.values2>0.5,1,0)
t2<-sum(diag(table(train$Survived[train$fold==i],fitted.values3)))/sum(table(train$Survived[train$fold==i],fitted.values3))
err_train[i]<-t1
err_cv[i]<-t2}
```

+ This gives a **training error** of **`r 1-round(mean(err_train),3)`** and a **cross-validation error** of **`r 1-round(mean(err_cv),3)`**.

##### 2. Categorisation Tree: {#tree}

Given some of the nuance within the variables, I think a decision tree will perform better than a logistic regression.

+ We run the analysis including the categorical variable for family size `FamSizeCat` and Numerical Age `AgeCat`
```{r}
err_cv<-numeric(n_folds)
err_train<-numeric(n_folds)
for(i in 1:n_folds){
fit<-tree(Survived~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train[train$fold!=i,])

fitted<-predict(fit, train[train$fold!=i,])
fitted1<-ifelse(fitted>0.5,1,0)
t1<-sum(diag(table(train$Survived[train$fold!=i],fitted1)))/sum(table(train$Survived[train$fold!=i],fitted1))


fitted.values2<-predict(fit, train[train$fold==i,])
fitted.values3<-ifelse(fitted.values2>0.5,1,0)
t2<-sum(diag(table(train$Survived[train$fold==i],fitted.values3)))/sum(table(train$Survived[train$fold==i],fitted.values3))
err_train[i]<-t1
err_cv[i]<-t2
}
```
+ This gives a **training error** of **`r 1-round(mean(err_train),3)`** and a **cross-validation error** of **`r 1-round(mean(err_cv),3)`**.

+ Run the same with but with a Categorical Age `AgeCat`
```{r echo=F}
err_cv<-numeric(n_folds)
err_train<-numeric(n_folds)
for(i in 1:n_folds){
fit<-tree(Survived~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train[train$fold!=i,])

fitted<-predict(fit, train[train$fold!=i,])
fitted1<-ifelse(fitted>0.5,1,0)
t1<-sum(diag(table(train$Survived[train$fold!=i],fitted1)))/sum(table(train$Survived[train$fold!=i],fitted1))


fitted.values2<-predict(fit, train[train$fold==i,])
fitted.values3<-ifelse(fitted.values2>0.5,1,0)
t2<-sum(diag(table(train$Survived[train$fold==i],fitted.values3)))/sum(table(train$Survived[train$fold==i],fitted.values3))
err_train[i]<-t1
err_cv[i]<-t2
}
# print(paste("Training Accuracy is",as.character(round(mean(err_train),3))))
# print(paste("Classification Accuracy is",as.character(round(mean(err_cv),3))))
```
+ This gives a **training error** of **`r 1-round(mean(err_train),3)`** and a **cross-validation error** of **`r 1-round(mean(err_cv),3)`**.

##### 3. Random Forest: {#forest}

For me details on random forests in R, read this article on  [R-bloggers](https://www.r-bloggers.com/random-forests-in-r/). We will run a random forest algorithm and evaluate what the estimates for the error rates are. Given that Out-Of-Bag estimation is built into random forests, there is no need to carry out cross-validation.

```{r}
fit<-randomForest(as.factor(Survived)~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train)

plot(fit,main="Error rates with numerical Age")
legend('top', colnames(fit$err.rate), col=1:3,fill=1:3,horiz=T)
```

The Out-Of-Bag estimate plateaus a classification error of `r round(fit$err.rate[500,1],3)`. Thus performing better than logistic regression and trees. We run the same analysis with categorical age. 

```{r echo=F}
fit<-randomForest(as.factor(Survived)~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train)

plot(fit,main="Error rates with categorical Age")
legend('top', colnames(fit$err.rate), col=1:3,fill=1:3,horiz=T)
```

The Out-Of-Bag estimate plateaus at a classification error of **`r round(fit$err.rate[500,1],3)`**. The best performance thus far.

##### 4. Support Vector Machines: {#SVM}

Again for more details on SVM, you can look at the ever resourceful [R_bloggers](https://www.r-bloggers.com/machine-learning-using-support-vector-machines/). We'll first run a simple SVM on the numerical Age set and then perform some tuning on the parameters

```{r echo=F}
err_cv<-numeric(n_folds)
err_train<-numeric(n_folds)
for(i in 1:n_folds){
fit<-svm(as.factor(Survived)~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train[train$fold!=i,])

fitted1<-predict(fit, train[train$fold!=i,])
# fitted1<-ifelse(fitted>0.5,1,0)
t1<-sum(diag(table(train$Survived[train$fold!=i],fitted1)))/sum(table(train$Survived[train$fold!=i],fitted1))


fitted.values3<-predict(fit, train[train$fold==i,])
# fitted.values3<-ifelse(fitted.values2>0.5,1,0)
t2<-sum(diag(table(train$Survived[train$fold==i],fitted.values3)))/sum(table(train$Survived[train$fold==i],fitted.values3))
err_train[i]<-t1
err_cv[i]<-t2
}

```
+ This gives a **training error** of **`r 1-round(mean(err_train),3)`** and a **cross-validation error** of **`r 1-round(mean(err_cv),3)`**.

```{r}
tunedModel <- tune.svm(Survived~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,  data = train,gamma = seq(0.04,2,0.04), cost = seq(2,6,2),kernel = "radial") 
 
gamma1<-tunedModel$best.parameters[1]
  cost1<-tunedModel$best.parameters[2]
```

Tuning suggests the use of $\gamma$ = `r tunedModel$best.parameters[1]` and cost = `r tunedModel$best.parameters[2]` which will give a misclassification error of **`r round(tunedModel$best.performance,3)`**. We run the same for categorical Age below.

```{r, include=F}
y1<-round(tunedModel$best.performance,3)
tunedModel <- tune.svm(Survived~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,  data = train,gamma = seq(0.04,0.2,0.04), cost = seq(2,6,2),kernel = "radial") 
 
gamma2<-tunedModel$best.parameters[1]
  cost2<-tunedModel$best.parameters[2]
y2<-round(tunedModel$best.performance,3)
```

Tuning suggests the use of $\gamma$ = `r tunedModel$best.parameters[1]` and cost = `r tunedModel$best.parameters[2]` which will give a misclassification error of **`r round(tunedModel$best.performance,3)`**. Thus it appears SVM works better when we use a numerical value for Age.

##### 5. Gradient Boosting: {#grad}

Finally, we have a crack at Gradient Boosting

```{r,echo=F}
fitControl <- trainControl(method = "repeatedcv", number = 4, repeats = 4)

err_cv<-numeric(n_folds)
err_train<-numeric(n_folds)

for(i in 1:n_folds){
gbmFit1 <- train(as.factor(Survived)~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,  data = train[train$fold!=i,], method = "gbm", trControl = fitControl,verbose = FALSE)
gbm_dev <- predict(gbmFit1, train[train$fold!=i,],type= "prob")[,2]
gbm_dev1<-ifelse(gbm_dev<0.5,0,1)
t1<-sum(diag(table(train$Survived[train$fold!=i],gbm_dev1)))/sum(table(train$Survived[train$fold!=i],gbm_dev1))

gbm_dev <- predict(gbmFit1, train[train$fold==i,],type= "prob")[,2]
gbm_dev1<-ifelse(gbm_dev<0.5,0,1)
s1<-sum(diag(table(train$Survived[train$fold==i],gbm_dev1)))/sum(table(train$Survived[train$fold==i],gbm_dev1))

err_cv[i]<-s1;err_train[i]<-t1}
```

+ This gives a **training error** of **`r 1-round(mean(err_train),3)`** and a **cross-validation error** of **`r 1-round(mean(err_cv),3)`**.
+ The same analysis is run using the categorical Age variable.

```{r, echo=F}
fitControl <- trainControl(method = "repeatedcv", number = 4, repeats = 4)

err_cv<-numeric(n_folds)
err_train<-numeric(n_folds)

for(i in 1:n_folds){
gbmFit1 <- train(as.factor(Survived)~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,  data = train[train$fold!=i,], method = "gbm", trControl = fitControl,verbose = FALSE)
gbm_dev <- predict(gbmFit1, train[train$fold!=i,],type= "prob")[,2]
gbm_dev1<-ifelse(gbm_dev<0.5,0,1)
t1<-sum(diag(table(train$Survived[train$fold!=i],gbm_dev1)))/sum(table(train$Survived[train$fold!=i],gbm_dev1))

gbm_dev <- predict(gbmFit1, train[train$fold==i,],type= "prob")[,2]
gbm_dev1<-ifelse(gbm_dev<0.5,0,1)
s1<-sum(diag(table(train$Survived[train$fold==i],gbm_dev1)))/sum(table(train$Survived[train$fold==i],gbm_dev1))

err_cv[i]<-s1;err_train[i]<-t1}
```
+ This gives a **training error** of **`r 1-round(mean(err_train),3)`** and a **cross-validation error** of **`r 1-round(mean(err_cv),3)`**.

We can now evaluate the overall performances which are shown below. The table below shows the cross validation error estimates

Method        | Numerical |Categorical
--------------|-----------|-----------
Logistic Reg  | 0.190     | 0.187
Tree          | 0.177     | 0.173
Random Forest^1^| `r y1`  | `r y2`
SVM^1^           | 0.172     | 0.180
GBM           | 0.177     | 0.182

*1 Note that for random forests and SVM, we've used an Out-of-bag estimate not a 10-fold cv estimate so the results are not like for like in those cases.* 

Setting random forests aside then it looks like the classification tree performs the best. It will be interesting to see how they measure up in actual submission


### 4. Submitting and checking results {#submit}
Now we run through each of the algorithms reviewed and predict and submit to the Kaggle competition to evaluate which performs the best. I expect random forests to SVM to perform the best based on what we have seen thus far.

```{r, echo=F, eval=F}
### 1. Logistic Regression
# Logistic Regression (Numerical Age)
fit1a<-glm(Survived~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train[train$fold!=i,],family="binomial")
fitted1a<-predict.glm(fit1a,newdata=test, type="response")
fitted1a<-ifelse(fitted1a>0.5,1,0)
subs<-data.frame(cbind(test$PassengerId,fitted1a))
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"LogReg_1a.csv",row.names=F)

# Logistic Regression (Categorical Age)
fit1b<-glm(Survived~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train[train$fold!=i,],family="binomial")
fitted1b<-predict.glm(fit1b,newdata=test, type="response")

fitted1b<-ifelse(fitted1b>0.5,1,0)
subs<-data.frame(cbind(test$PassengerId,fitted1b))
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"LogReg_1b.csv",row.names=F)

#### 2. Decision Tree
### Tree (Numerical Age)
fit2a<-tree(Survived~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train)
fitted2a<-predict(fit2a, test)
fitted2a<-ifelse(fitted2a>0.5,1,0)
subs<-data.frame(cbind(test$PassengerId,fitted2a))
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"Tree_2a.csv",row.names=F)

### Tree (Categorical Age)
fit2b<-tree(Survived~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train)
fitted2b<-predict(fit2b, test)
fitted2b<-ifelse(fitted2b>0.5,1,0)
subs<-data.frame(cbind(test$PassengerId,fitted2b))
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"Tree_2b.csv",row.names=F)

#### 3. Random Forests
### Random Forest (Numerical Age)
levels(test$Embarked) <- levels(train$Embarked)

fit3a<-randomForest(as.factor(Survived)~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train)
fitted3a<-predict(fit3a, test)
# fitted3a<-ifelse(fitted3a>0.5,1,0)
subs<-data.frame(test$PassengerId,fitted3a)
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"Forest_3a.csv",row.names=F)

### Random Forest (Categorical Age)
fit3b<-randomForest(as.factor(Survived)~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train)
fitted3b<-predict(fit3b, test)
# fitted3b<-ifelse(fitted3b>0.5,1,0)
subs<-data.frame(test$PassengerId,fitted3b)
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"Forest_3b.csv",row.names=F)

#### 4. Support Vector Machine
### SVM (Numerical Age)

fit4a <- svm(as.factor(Survived)~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train,gamma=gamma1,cost=cost1,kernel = "radial",type="C-classification") 
fitted4a<-as.character(predict(fit4a, test))

subs<-data.frame(cbind(test$PassengerId,fitted4a))
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"SVM_4a.csv",row.names=F)

### SVM (Categorical Age)
fit4b <- svm(as.factor(Survived)~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,data=train,gamma=gamma2,cost=cost2,kernel = "radial",type="C-classification")
fitted4b<-as.character(predict(fit4b,test))

subs<-data.frame(cbind(test$PassengerId,fitted4b))
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"SVM_4b.csv",row.names=F)

#### 5. Gradient Boosting Method
### Gradient Boosting (Numerical Age)
fit5a <- train(as.factor(Survived)~Pclass+Sex+Age+FamSizeCat+Embarked+Fare+SibSp+Parch,  data = train, method = "gbm", trControl = fitControl,verbose = FALSE)
fitted5a <- predict(fit5a, test,type= "prob")[,2]
fitted5a<-ifelse(fitted5a>0.5,1,0)
subs<-data.frame(cbind(test$PassengerId,fitted5a))
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"GBM_5a.csv",row.names=F)

### Gradient Boosting (Categorical Age)
fit5b <- train(as.factor(Survived)~Pclass+Sex+AgeCat+FamSizeCat+Embarked+Fare+SibSp+Parch,  data = train, method = "gbm", trControl = fitControl,verbose = FALSE)
fitted5b <- predict(fit5b, test,type= "prob")[,2]
fitted5b<-ifelse(fitted5b>0.5,1,0)
subs<-data.frame(cbind(test$PassengerId,fitted5b))
colnames(subs)<-c("PassengerId","Survived")
write.csv(subs,"GBM_5b.csv",row.names=F)
```
And this is how things measure up when we submit to Kaggle. Note that earlier we used misclassification rate while Kaggle gives us the correct classification rate. So just take them away from one to make them equivalent.

Method        | Numerical |Categorical
--------------|-----------|-----------
Logistic Reg  | 0.76076   | 0.76076
Tree          | 0.78468   | 0.76555
Random Forest | 0.76076   | 0.77033
SVM           | 0.78468   | 0.78468
GBM           | 0.75199   | 0.75199

Thanks for reading! Comments welcome.
