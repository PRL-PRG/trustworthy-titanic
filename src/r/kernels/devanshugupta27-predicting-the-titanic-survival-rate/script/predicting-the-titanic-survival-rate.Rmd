---
title: "Titanic Survival Prediction"
author: "Devanshu Gupta"
date: "August 25, 2017"
output:
  html_document: 
    keep_md: true

---

# 1 Introduction
###This is my first stab at a Kaggle script. I will focus on doing some illustrative data visualizations along the way. I'll then use randomForest in a different way to create a model predicting survival on the Titanic. I am new to machine learning in R and hoping to learn a lot, so feedback is very welcome!
There are 3 parts as:

  * Data Munging and Cleaning
  * EDA
  * Prediction


## 1.1 Loading and examining the dataset and its variables.
```{r message=FALSE}
#Loading the necessary libraries
library(ggplot2)
library(lattice)
library(caret)
library(ranger)
library(dplyr)
library(e1071)
```

```{r}
#Reading the train and the test datasets.
train <- read.csv("../input/train.csv",stringsAsFactors = FALSE)
test <- read.csv("../input/test.csv", stringsAsFactors = FALSE)
```
```{r}
#Checking the structure of the dataset
str(train)
```
```{r}
#Examining the summary of the dataset
summary(train)
```

The variables of the following dataset are as:

1. PassengerId - Id of each passenger.
2. Survived    - Describing whether passenger survived or not.
                      0 - Not Survived, 1 - Survived
3. Pclass      - Class of each passenger 
4. Name        - Name of the passenger
5. Sex         - Sex of the passenger
6. Age         - Age of the passenger
7. SibSp       - Number of siblings/spouses aboard
8. Parch	     - Number of parents/children aboard
9. Ticket      - Ticket number
10. Fare	     - Fare
11. Cabin	     - Cabin
12. Embarked	 - Port of embarkation

#2 Data Munging and Cleaning
## 2.1 Changing type to factor

Various variable in the dataset should be represented as factors but are represented as numeric, which does not make any sense.

These variables are:
1 Survived
2 Pclass
3 Sex
4 SibSp
5 Parch
6 Embarked

```{r}
#Converting Survived to a factor 
train$Survived <- factor(train$Survived)

#Converting Pclass to a factor
train$Pclass <- factor(train$Pclass)

#Converting Sex to a factor
train$Sex  <- factor(train$Sex)

#Converting SibSp to a factor
train$SibSp <- factor(train$SibSp)

#Converting Parch to a factor
train$Parch <- factor(train$Parch)

#Converting Embarked to a factor
train$Embarked <- factor(train$Embarked, ordered = FALSE)

```
## 2.2 Name Issues

 The Name of the passengers has a issue. The naming convention also seems to be somewhat archaic and scores low on human readability. 

For females, their original name is the one inside the "()". 
For Example: The original name of **Cumings, Mrs. John Bradley (Florence Briggs Thayer)** is **"Florence Briggs Thayer"**.
For males, their original name is as:
Original name of the person named in dataset as **"Braund, Mr. Owen Harris"** is **"Owen Harris Braud"**

 The name can be converted by means of a function involving a if-else statement check.

```{r}
head(train$Name)
```
We can see that names are not as we use names now.
```{r}
convert_name <- function(name) {
  
  if (grepl("\\(.*\\)", name)) {           # women: take name from inside parentheses
    gsub("^.*\\((.*)\\)$", "\\1", name)
  } else {                                # men: take name before comma and after title
    gsub("^(.*),\\s[a-zA-Z\\.]*\\s(.*)$", "\\2 \\1", name)
  }
}
#grepl() searches for pattern and is gives a logical result. 
#gsub(pattern, replacement, string) is used to replace every occurence of pattern in the string with the replacement.  

###The pattern is as :
#  * ^        denotes starting of pattern
#  * .*       denotes occurence of any character zero or more times.
#  * \\(      denotes that we are actually looking for '(' in the string. Names of females of the dataset are inside paranthesis.
#  * (.*)     denotes a back-reference.
#  * \\)      denotes we actually want to look for ) in the string.
#  * \\1      denotes first back-reference. For every occurence of (.*), there is a back-reference 
#  * \\s      matches a space
# * [a-zA-z] This sprecifies character ranges. All characters in a-z and A-Z are matched.
#  * $        denotes end of string.
```
Calling the function convert_name to the passenger name.
```{r}
pass_names <- train$Name

clean_pass_names <- vapply(pass_names, FUN = convert_name,
                           FUN.VALUE = character(1), USE.NAMES = FALSE)

train$Name <-  clean_pass_names

#The function is applied to pass_names (The vector that contains all the names of the train dataset) via vapply so as to use convert_names for all names in the dataset.

head(train$Name)
```
We can see that the names of the passengers have been cleaned.

# 3 Exploratory Data Analysis
## 3.1 Pclass v/s Survived

```{r}
train %>% 
    ggplot(aes(x = Pclass, fill = Survived)) + 
          geom_bar()
```

### Inference from the Graph 
From the graph, it is clear that the number of passenger who survived is independent on the Class of passenger, while the number of passenger who couldn't survived seems to be dependent on the class of passenger. 
Same can be established by the the table function.  
```{r}
tab <- table(train$Pclass, train$Survived)
prop.table(tab,1)
```
So while 62.96% of passenger from Class 1 survived, only 24.23% passenger belonging to the Class 3 could survive.

##3.2 Sex v/s Survived

```{r}
train %>%
      ggplot(aes(x = Sex, fill = Survived)) + 
            geom_bar(stat = "count", position = "fill")
```

### **Inference from the Graph** 
From the graph, it is clear that a large propotion of female passengers survived while only a thin population of male passengers could survive, eventhough the number of male passenger aboard was almost two times of the female passengers.

Same can be established by the the table function.  
```{r}
tab <- table(train$Sex, train$Survived)
prop.table(tab,1)
```
So while 74.20% of female passenger survived, only 18.89% of the male passenger could survive.

##3.3 Age v/s Survived
```{r}
train %>%
      ggplot(aes(x = Age, fill = Survived)) + 
            geom_histogram()

```

### Inference from the Graph 

While there seems an uniform trend among age on the number of survivals, some inferences can be deduced.
The rate of survival among infants was high.
Also, most of the passenger belong to the 20-40 year age group.

##3.4 Embarked v/s Survived
```{r}
train %>%
  filter(Embarked %in% c("S","C","Q")) %>%
  ggplot() +
  geom_bar(aes(Embarked, fill = Pclass), position = "dodge") +
  facet_grid(~ Survived)
```

## Inference from Graph
Proportion of passengers survived seems to be equal for passenger from port Q and S. 
The same can be established from the prop table.
```{r}
tab <- table(train$Embarked, train$Survived)
prop.table(tab,1)
```
So, while 38.96% of passengers embarked from port Queenstown survived and 33.69% of those embarked from Southampton survived, 55.35% of passengers embarked from Cherbourg port survived.

# 4 Prediction

## 4.1 Imputation

```{r}
sum(is.na(train$Age))
```
177 values are missing in the dataset. Simply ignoring the missing values can cause the model to overfit and can also result in bias analysis. 
 Here, we can use Median Imputation, which is the best type of imputing data where data is Missing At Random(MAR).

```{r}
#An example using median imputation
#train(x,y, preProcess = "medianImpute")
```
### Determining which variables to choose as Independent Variables

As we can see from the above graphs, Pclass, Age, Sex and Embarked have shown a distinct behaviour towards Survival rate.
So, I will choose these variable as the independent variables for my model.

## 4.2 Modelling 
 I am performing Random Forest, but in a different manner.
 The accuracy of the model can be improved by using a cross-validation. Cross- Validation makes folds of the wole dataset and then apply to the model several times and choosing the one with the best accuracy.

```{r}
#Choosing independent columns 
x <- train[,c("Age","Pclass","Sex","Embarked")]
#Choosing the dependent column
y <- train$Survived
```
Performing modelling
```{r}
#Set a random seed
set.seed(123)

#the method "ranger" here is a fast alternative of randomForest.
#trainControl is used to define cross-validation.
model<- train(x = x,y = y,preProcess = "medianImpute", method = "ranger", trControl = trainControl(method = "cv", number = 10))
```
Checking the model
```{r}

model
```
## 4.3 Prediction !
This is the final point, performing prediction on the test dataset.
```{r}
# Predict using the test set
prediction <- predict(model, test)

# Save the solution to a dataframe with two columns: PassengerId and Survived (prediction)
solution <- data.frame(PassengerID = test$PassengerId, Survived = prediction)

# Write the solution to file
write.csv(solution, file = 'rfSolution.csv', row.names = F)
```