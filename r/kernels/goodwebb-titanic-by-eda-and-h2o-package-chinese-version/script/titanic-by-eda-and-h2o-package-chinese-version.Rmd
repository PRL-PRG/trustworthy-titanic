---
title: Titanic By EDA and H2o 

date: '2018-01-15'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

<center><img src="http://img.technews.tw/wp-content/uploads/2016/04/titanic-sink.jpg"></center>
圖片來源:http://technews.tw.com
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
```
# Introduction 
1912年4月15日，鐵達尼號撞上一座冰山後沉沒，2224名乘客和機組人員中有1502人遇難。 這一聳人聽聞的悲劇震撼了國際社會，
並導致了更好的船舶安全條例，也翻拍成電影。
沉船導致生命損失的原因之一是乘客和船員沒有足夠的救生艇。 雖然倖存下來的運氣有一些因素，但一些人比其他人更有可能生存，
比如婦女，兒童和上層階級。
以下內容會透過R做探索性統計分析(EDA)和H2o套件建模的使用，來探討甚麼樣的人比較容易在此災難生存和預測生存狀況

# Preparations {.tabset .tabset-fade .tabset-pills}

## Loading libraries
所使用的套件
```{r,message = FALSE}
library(dplyr)
library(plotly)
library(ggplot2)
library(h2o)  #h2o建模使用
library(rpart)
library(rpart.plot)
```

## Data

資料欄位如下:<br>

- Survived:是否存活 1:是 0:否
- Pclass:乘客的社會經濟狀態，1代表Upper，2代表Middle，3代表Lower
- Name:乘客姓名
- Sex :性別
- Age:年齡
- SibSp:同旅行的兄弟姊妹和配偶數
- Parch:同旅行父母或子女數
- Ticket:船票號
- Fare:船價
- Cabin:乘客所在的艙位
- Embarked:乘客登船的港口

新增欄位如下:

- **Title** :紀錄乘客姓名稱號
- **Fsize** :家庭數目

```{r}
trainData<-read.csv("../input//train.csv", stringsAsFactors = F,na.strings = c("NA", ""))
testData<-read.csv("../input//test.csv", stringsAsFactors = F,na.strings = c("NA", ""))
#train :891位,test:418位 
dim(trainData)
dim(testData)
trainData$Survived<-as.factor(trainData$Survived)
FullData<-bind_rows(trainData,testData)
```

觀察資料狀態如下:
- 訓練資料有891筆、測試資料有418筆
- 有遺漏值的欄位:Age:263筆、Fare:1筆、Embarked:2筆

```{r}
summary(FullData)
sapply(FullData, function(x) {sum(is.na(x))})
```
# feature visualisations and engineering {.tabset .tabset-fade .tabset-pills}

以下所使用的畫圖套件是`ggplot2`和`plotly`，可以讓使用者去做互動呈現

## Sex

<font color=#A52A2A size=6 >女生生存率比較大</font><br>
由下圖可知，女生的確生存率比較高
```{r}
gg <- ggplot(trainData, aes(Sex,fill=Survived)) +geom_bar(position = "dodge")+geom_label(stat='count', aes(label=..count..))+labs(x = 'Training data only')
#ggplotly(gg,tooltip = c("x", "fill","count"))
gg
#gga <- ggplot(FullData, aes(Sex,fill=Survived)) +geom_bar(position = "dodge")+labs(x = 'All data')
#ggplotly(gga,tooltip = c("x", "fill","count"))
```

## Pclass
<font color=#A52A2A size=6 >高經濟地位生存大</font><br>

Pclass=3幾乎沒人生存，Pclass=1幾乎生存，合理懷疑比較高等地位的人比較可以生存

```{r}
gg2 <- ggplot(trainData, aes(Pclass,fill=Survived)) +geom_bar(position = "dodge")+geom_label(stat='count', aes(label=..count..))+labs(x = 'Training data only')
gg2

```

## Age

<font color=#A52A2A size=6>年紀小先逃難</font><br>

按照故事的劇情小孩和婦女會先逃難，由圖可知，Age<18的生存人數的確高於遇難人數

```{r}
gg3 <- ggplot(trainData, aes(Age,col=Survived)) +geom_freqpoly(binwidth=2)
ggplotly(gg3)

```

## Fare

<font color=#A52A2A size=6 >沒錢的人該死</font><br>
由圖可知，大致上票價低的比較不容易獲救
```{r}

gg4 <- ggplot(trainData, aes(Fare,col=Survived)) +geom_freqpoly(binwidth=10)
ggplotly(gg4)

```
因圖裡有很多極端值，無法直接圖看出訊息，在此計算比例，我們以fare:80為基點，發現
在存活人數中，大於fare:80的人數約占**17%**，而在非存活人數中，大於fare:80的人數只占**3%**
表示的高票價比較容易存活。
```{r}
TotalSur<-sum(trainData$Survived==1)
TotalunSur<-sum(trainData$Survived==0)
sum(trainData[trainData$Survived==1,]$Fare>=80)/TotalSur
sum(trainData[trainData$Survived==0,]$Fare>=80)/TotalunSur
```
## Parch & SibSp &Fisize
<font color=#A52A2A size=6 >家庭成員越多和單身的人，越無法生存</font><br>
由圖可知，大致上家庭成員在1到2位比較容易生存，而大家庭的人員都無法生存

```{r, warning = FALSE}
gg5 <- ggplot(trainData, aes(SibSp,fill=Survived)) +geom_bar(position = "dodge")
ggplotly(gg5)
gg6 <- ggplot(trainData, aes(Parch,fill=Survived)) +geom_bar(position = "dodge")
ggplotly(gg6)
#Fsize
#Family size
FullData$Fsize<-FullData$SibSp+FullData$Parch+1
gg8 <- ggplot(FullData[1:891,],aes(Fsize,fill=Survived)) +geom_bar(position = "dodge")+geom_label(stat='count', aes(label=..count..))
gg8

```

特別拉出在訓練資料中SibSp=8成員名單，這些人員的確是同家庭，且都是未生存
再拉出**測試資料**中對於剛剛有相同**Ticket=CA. 2343**發現有4位，這些人資訊的確來自同家庭
未來可預測為未生存(Survived=0),之後建模後可再回來討論此case,因為當中有**Age小**的影響

```{r}
trainData[trainData$SibSp=="8",]
testData[testData$Ticket=="CA. 2343",]
```

## Embarked

<font color=#A52A2A size=6 >乘客登船的港口也有相關</font><br>
由圖可知，有一筆NA資料，其中Embarked=C，生存率比較高
```{r, warning = FALSE}
gg7 <- ggplot(trainData, aes(Embarked,fill=Survived)) +geom_bar(position = "dodge")+geom_label(stat='count', aes(label=..count..))+labs(x = 'Training data only')
gg7
```

## Title
<font color=#A52A2A size=6 >姓氏稱謂跟生存有關</font><br>
針對Name欄位處理，抓取每個乘客的姓氏稱謂
```{r}
#資料處理
#新增一欄位Title紀錄乘客稱位
FullData$Title <- gsub('(.*, )|(\\..*)', '', FullData$Name)
#以下為Sex和title次數表
table(FullData$Sex, FullData$Title)
another_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 
                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
FullData$Title[FullData$Title == 'Mlle']        <- 'Miss' 
FullData$Title[FullData$Title == 'Ms']          <- 'Miss'
FullData$Title[FullData$Title == 'Mme']         <- 'Mrs' 
FullData$Title[FullData$Title=='Mr' & FullData$Pclass=='1'] <- 'MrP1'
FullData$Title[FullData$Title=='Mr' & FullData$Pclass!='1'] <- 'MrP23'
FullData$Title[FullData$Title %in% another_title]  <- 'Another'
table(FullData$Sex, FullData$Title)

```

由圖可知，Miss和Mrs稱謂，生存率高
```{r, warning = FALSE}
gg7 <- ggplot(FullData[1:891,], aes(Title,fill=Survived)) +geom_bar(position = "dodge")
ggplotly(gg7)
```

# feature relation {.tabset .tabset-fade .tabset-pills}


```{r, warning = FALSE}

```
# Machine Learning 

這邊我們使用了`H2o`套件來完成來完成以下動作。H2o是深度學習有名的套件，它提供了一個到H2o的接口。
H2O用Java編寫，又快又可擴展，它不僅提供深度學習的功能，還提供許多其他流行的機器學習的演算法和模型。
一開始先初始化H2o可以看到裡面細節內容如下:
```{r, warning = FALSE}
h2o.init()

```

## Predicting missing value 

Age欄位我們使用h2o的randomfroest來做遺失值的預測
```{r, warning = FALSE}

FullData$Survived<-as.factor(FullData$Survived)
FullData$Sex<-as.factor(FullData$Sex)
FullData$Title<-as.factor(FullData$Title)
FullData$Embarked<-as.factor(FullData$Embarked)
data1<-as.h2o(FullData,destination_frame = "FullData")
tempmodel<-h2o.randomForest(x=colnames(FullData)[-(1:2)],y="Age",training_frame = data1)                        
predictdata<-as.data.frame(h2o.predict(tempmodel,data1))
head(predictdata,5)
missAgePassengerId<-FullData[is.na(FullData$Age),]$PassengerId
FullData[missAgePassengerId,]$Age=predictdata[missAgePassengerId,]

```
接著預測Fare遺失的欄位:
```{r, warning = FALSE}

tempmodel2<-h2o.randomForest(x=colnames(FullData)[-(1:2)],y="Fare",training_frame = data1)                        
predictdata2<-as.data.frame(h2o.predict(tempmodel2,data1))
missFarePassengerId<-FullData[is.na(FullData$Fare),]$PassengerId
FullData[missFarePassengerId,]$Fare=predictdata2[missFarePassengerId,]

```
接著預測Embarked遺失的欄位:

直接觀察此兩筆資料，發現他們是在Pclass=1的同一張票且fare(票價)也相同
```{r,waring=FALSE}
FullData[which(is.na(FullData$Embarked)),c( 'Title', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked') ]


```
我們對Pclass和Embarked做分群計算中位數median如下:
```{r,waring=FALSE}
FullData[(!is.na(FullData$Embarked) & !is.na(FullData$Fare)),] %>%
  group_by(Embarked, Pclass) %>%
  summarise(Fare=median(Fare))

```
發現Embarked=c和Pclass=1時，最接近兩位的票價，因此預測兩筆Embarked=C
```{r,waring=FALSE}

FullData$Embarked[c(62, 830)] <- 'C'

```
## Decision tree
```{r, warning =FALSE,message = FALSE}
baseData<-trainData[,c("Survived","Sex","Age","Pclass")]
# 先把資料區分成 train=0.8, test=0.2 
train.index <- sample(x=1:nrow(baseData), size=ceiling(0.8*nrow(baseData) ))
train <- baseData[train.index, ]
test <- baseData[-train.index, ]
cart.model<- rpart(Survived ~. , 
                    data=train)
prp(cart.model,         # 模型
    faclen=0,           # 呈現的變數不要縮寫
    fallen.leaves=TRUE, # 讓樹枝以垂直方式呈現
    shadow.col="gray",  # 最下面的節點塗上陰影
    # number of correct classifications / number of observations in that node
    extra=2)

```
<br>測試資訊預測:

```{r, warning =FALSE,message = FALSE}
pred <- predict(cart.model, newdata=test, type="class")
# 用table看預測的情況
table(real=test$Survived, predict=pred)
# 計算預測準確率 = 對角線的數量/總數量
confus.matrix <- table(real=test$Survived, predict=pred)

```
<font color=#A52A2A size=6 >decision tree model精準度:</font><br>
```{r, warning =FALSE,message = FALSE}
sum(diag(confus.matrix))/sum(confus.matrix) # 對角線的數量/總數量

```
## randomForest

我們使用h20的randomForest方法來建模:

- 10-fold交叉驗證
- h2orfmodel1:所有變數丟進去
- h2orfmdoel2:只丟幾個重要的
```{r, warning = FALSE,message = FALSE}
baseData<-FullData[1:891,]
traindatah2o<-as.h2o(baseData,destination_frame = "aa")
#testdatah2o<-as.h2o(data2,destination_frame = "bb")
h20rfmodel1<-h2o.randomForest(x=colnames(traindatah2o)[-1],y="Survived",training_frame =traindatah2o,nfolds=10,keep_cross_validation_predictions=TRUE) 
h2o.varimp_plot(h20rfmodel1)
h20rfmodel1
cat(paste('<b>精準度為',h20rfmodel1@model$cross_validation_metrics_summary[1,1],'</b>'))
```
<font color=#A52A2A size=6 >h2orfmdoel1精準度:</font><br>
```{r}
h20rfmodel1@model$cross_validation_metrics_summary[1,1]
```
<font color=#A52A2A size=6 >h2orfmodel2精準度:</font><br>
```{r, warning = FALSE,message = FALSE}
h20rfmodel2<-h2o.randomForest(x=colnames(traindatah2o)[-c(1,4,7,8,9,11)],y="Survived",training_frame =traindatah2o,nfolds=10,keep_cross_validation_predictions=TRUE) 
h20rfmodel2@model$cross_validation_metrics_summary[1,1]
```

## xgboost

```{r, warning = FALSE,message = FALSE}
xgb<-h2o.xgboost(x =colnames(traindatah2o)[-c(1,4,7,8,9,11)],
                       y ="Survived",
                       training_frame = traindatah2o,
                       model_id = "light_xgb_model_synAI",
                       stopping_rounds = 20,
                       stopping_metric = "logloss",
                       distribution = "AUTO",
                       score_tree_interval = 2,
                       learn_rate = 0.1,
                       ntrees = 50,
                       max_depth = 5,
                       subsample = 0.75,
                       # colsample_bytree = 0.75,
                       tree_method = "auto",
                       grow_policy = "depthwise",
                       booster = "gbtree")
                       
h2o.varimp_plot(xgb)    
xgb@model
```
## deeplearning

使用`h2o.deeplearning`方法，當中有很多參數可以去調動如下:

- hidden :神經元層次和個數
- epoch :當一個完整的資料集通過了神經網路一次並且返回了一次，這個過程稱為一個 epoch。 
- L1和L2函數:用於減少模型擬合的兩種密切相關的技術,L1:Lasso,L2:ridge regression
- activation :激活函數。EX:"Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout"
<br>
..........等等
-------------------------------------------------------------------
<br>
設定幾個模型如下:
- model1=hidden:c(100),epho:10
- model2=hidden:c(200,200,200),epho:10
- model3=hidden:c(200),epho:50
- model4=hidden:c(100),epho:10,l1:0.05
- model5=hidden:c(200),epho:50,只丟重要變數
```{r, warning = FALSE,message = FALSE}
model1<-h2o.deeplearning(x=colnames(traindatah2o)[-1],y="Survived",training_frame = traindatah2o,
                         activation = "RectifierWithDropout",hidden=c(100),
                         epochs=10,variable_importances = TRUE,nfolds=7,keep_cross_validation_predictions=TRUE)
model1
```
<font color=#A52A2A size=6 >mdoel1精準度:</font><br>
```{r, warning = FALSE,message = FALSE}
model1@model$cross_validation_metrics_summary[1,1]
```

```{r, warning = FALSE,message = FALSE}
model2<-h2o.deeplearning(x=colnames(traindatah2o)[-1],y="Survived",training_frame = traindatah2o,
                         activation = "RectifierWithDropout",hidden=c(200,200,200),
                         epochs=10,variable_importances = TRUE,nfolds=7,keep_cross_validation_predictions=TRUE)
model3<-h2o.deeplearning(x=colnames(traindatah2o)[-1],y="Survived",training_frame = traindatah2o,
                         activation = "RectifierWithDropout",hidden=c(200),
                         epochs=50,variable_importances = TRUE,nfolds=7,keep_cross_validation_predictions=TRUE) 
model4<-h2o.deeplearning(x=colnames(traindatah2o)[-1],y="Survived",training_frame = traindatah2o,
                         activation = "RectifierWithDropout",hidden=c(100),
                         epochs=10,l1=0.05,variable_importances = TRUE,nfolds=7,keep_cross_validation_predictions=TRUE)    
model5<-h2o.deeplearning(x=colnames(traindatah2o)[-c(1,4,7,8,9,11)],y="Survived",training_frame = traindatah2o,
                         activation = "RectifierWithDropout",hidden=c(200),
                         epochs=50,variable_importances = TRUE,nfolds=7,keep_cross_validation_predictions=TRUE)                           
```
<font color=#A52A2A size=6 >mdoel2精準度:</font><br>
```{r, warning = FALSE,message = FALSE}
model2@model$cross_validation_metrics_summary[1,1]
```
<font color=#A52A2A size=6 >mdoel3精準度:</font><br>
```{r, warning = FALSE,message = FALSE}
model3@model$cross_validation_metrics_summary[1,1]
```
<font color=#A52A2A size=6 >mdoel4精準度:</font><br>
```{r, warning = FALSE,message = FALSE}
model4@model$cross_validation_metrics_summary[1,1]
```
<font color=#A52A2A size=6 >mdoel5精準度:</font><br>
```{r, warning = FALSE,message = FALSE}
model5@model$cross_validation_metrics_summary[1,1]
```
## final prediction

最後模型選擇`h2o.deeplearning的*model5*和randomFroest的*h2orfmdoel2*來預測測試資料
```{r, warning = FALSE,message = FALSE}
finaltestData<-FullData[892:1309,]
finaltestData<-as.h2o(finaltestData,destination_frame = "finaltestData")
finally<-as.data.frame(predict(model5,finaltestData))
solution <- data.frame(PassengerID = FullData[892:1309,"PassengerId"], Survived = finally$predict)
write.csv(solution, file = 'h2o_mod_Solution.csv', row.names = F)
finally2<-as.data.frame(predict(h20rfmodel2,finaltestData))
solution2 <- data.frame(PassengerID = FullData[892:1309,"PassengerId"], Survived = finally2$predict)
write.csv(solution2, file = 'rf_mod_Solution.csv', row.names = F)
finally3<-as.data.frame(predict(xgb,finaltestData))
solution3 <- data.frame(PassengerID = FullData[892:1309,"PassengerId"], Survived = finally3$predict)
write.csv(solution3, file = 'h2o_mod_xgboost_Solution.csv', row.names = F)
```
# Conclusion

<font color=#0000C6 size=5 >以上是本次的分析，精準度目前試著最高只到0.78左右，未來可以多嘗試從資料集去尋找新的變數或資訊，</font>
<font color=#0000C6 size=5 >或是嘗試其他演算法來使用，若有疑問可以給建議，感謝</font>
---