---
title: 'Data Analysis: Exploring Survival on the Titanic'
author: "Scottish Chemjong"
date: "8 January 2017"
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

# Package used: 

```{r message=FALSE}

library(tidyverse)
library(VIM)
library(RColorBrewer)
library(viridis)
library(DT)
library(magrittr)
library(scales)
library(ggstance)
library(mice)
library(stringr)
library(mice)
library(party)
library(caret)
library(ROCR)
library(e1071)
library(randomForest)
library(adabag)
library(ggstance)
```

# Load data:

The datasets given to us are split into two tables - train & test. The logic behind this division is to clean, tidy, impute and explore important varibales in train sets and build a model to predict survivals from the test set. We have survival data for train dataset but none for test dataset. 

VARIABLE   -     DESCRIPTIONS:

survival    -    Survival
                (0 = No; 1 = Yes)
                
pclass       -   Passenger Class
                (1 = 1st; 2 = 2nd; 3 = 3rd)
                
name         -   Name

sex           -  Sex

age            - Age

sibsp          - Number of Siblings/Spouses Aboard

parch          - Number of Parents/Children Aboard

ticket         - Ticket Number

fare           - Passenger Fare

cabin          - Cabin

embarked       - Port of Embarkation
                (C = Cherbourg; Q = Queenstown; S = Southampton)


```{r }


raw_train <- read.csv('../input/train.csv', stringsAsFactors = F)
raw_test  <- read.csv('../input/test.csv', stringsAsFactors = F)
```

## Check data

raw_train data has 891 obs. and 12 variables

raw_test data has 418 obs. and 11 variables

```{r}
summary (raw_train)

summary (raw_test)
```

```{r}
colnames_train <- names(raw_train)

colnames_test <- names(raw_test)

```

Lets check variables which are in raw_train set but not in raw_test

```{r}
setdiff (colnames_train, colnames_test)
```

Lets check variables which are in raw_test set but not in raw_train

```{r}
setdiff (colnames_test, colnames_train)
```

## Bind rows

We will combine the two datasets so that when we need to impute or see paterns, more observations is better.

```{r}
data1 <- bind_rows(raw_train, raw_test)

data1$Survived <- as.factor(data1$Survived)
data1$SibSp <- as.factor(data1$SibSp)
data1$Parch <- as.factor(data1$Parch)
```


# Missing values {#anchor}

Missing value count per variables used in data1
```{r}
data1 %>% 
        map_dbl(~sum(is.na(.)))

map_dbl(data1, ~sum((is.na(.) == T)/length(.))*100)
```

We know that test data had 418 obs. and it didn't have survival data. There are lot of missing values in Cabin and Age variable.

```{r message=F, results='hide'}

# aggr_col<- viridis (n = 4, alpha = 0.5,  begin = 0, end =1, option = "D" )

aggr_col <- c('lightseagreen', 'red')

aggr (data1, col = aggr_col, combined = T, sortVars = T, sortCombs = T, cex.axis = .8)
```

Above plot displays all existing combinations of missing (red) and observed (lightseagreen) values. For example, this plot reveals that if variable Cabin are missing, they are also missing Age and Fare variables. However, when Embarked variable is missing, Cabin, Age and Fare variables have data. We are not concerned with Survival's missing observations since they are mostly from test dataset.

# Feature Engineering

## Extracting Title from name

Sex variable doesn't tell us whether a person is married or single but the name title does. Title can also even predict if they are in family or not.

Lets extract title from names.

```{r}
data1$Title <- str_replace_all(data1$Name, '(.*, )|(\\..*)', "")

table(data1$Sex,data1$Title)


print("Unique Titles are: "); unique(data1$Title)


```

We will replace Capt|Col|Don|Dr|Jonkheer|Major|Master|Rev|Sir|the Countess to Others. Also, there are Miss which are spelled incorrecly - "Mme", "Ms", "Lady", "Mlle". We will replace them with Miss.


```{r}
replace_var <- c("Mme", "Ms", "Lady", "Mlle")

data1$Title <-  gsub("Mme|Ms|Lady|Mlle", "Miss", data1$Title)

data1$Title <- gsub("Capt|Col|Don|Dr|Jonkheer|Major|Master|Rev|Sir|the Countess", "Others", data1$Title )

data1$Title <-gsub("Othersa", "Others", data1$Title )



```

## Family numbers

Another variable we can create from given data is the size of the family.

sibsp           Number of Siblings/Spouses Aboard
parch           Number of Parents/Children Aboard


Large Family more then 4 people, Mid Family more than 1 but less then 5 and single is 1.

```{r}

data1 <- data1 %>% 
        mutate(Fam_size = as.numeric(SibSp) + as.numeric(Parch) + 1) %>% 
        mutate(Family = case_when(.$Fam_size == 1 ~ "Single", .$Fam_size >1 & .$Fam_size < 5 ~ "Mid Family", .$Fam_size >4 ~ "Large Family"))
        
table(data1$Family)
```
There are lot of observation missing from Cabin ~ 1K missing observations. I am not sure how to deal with this so I'll only extract the first letter of the Cabin from avaialble observations and keep it for future analysis.

```{r}
data1$Cabin_initial <- str_sub(data1$Cabin, 1,1)

unique(data1$Cabin_initial)
```

# Dealing with missing values

The data can be missing for many reasons. Generally, there are 3 steps required in dealing with missing data:

1. Find the missing data
2. Investigate the cause of missing data &
3. Replace the missing values with sensible data or delete the missing data rows.

We do not know the cause of missing data in variables - Cabin, Age, Embarked and Fare. However, for this analysis we will treat those variables as Missing Completely At Random (MCAR) which means there is no systematic reason as to why the data are missing.


So far we know where are the missing data and concluded that it is MCAR. How do we replace/delete the missing values?

## Imputing missing values

1. Firstly, lets us look at missing values of Embarked variables. We know from our missing data plot [Missing values](#anchor)  that there are no other variables missing when combined with Embarked variables.



```{r}
embarked_missing <- data1 %>% 
        filter(is.na(Embarked))

datatable(embarked_missing)
```

There are two observation with missing data from Embarked variable. We also know that there are three Port of Embarkation -> C = Cherbourg; Q = Queenstown; S = Southampton. The table above shows that both tickets were from Pclass 1; Fare $80; Ticket no  113572 & Cabin B28. 

```{r}
data1 %$% 
        unique(Cabin) %>% 
        summary()
        


```

There are 187 unique Cabin numbers.

```{r}
data1 %$%
        unique(Ticket) %>% 
        summary()
```

There are 929 unique ticket numbers.

We know that there are 1, 2 & 3 passangers class. Let us check their fares.

```{r}

data1 %>% 
        filter(!is.na(Embarked)) %>% 
        ggplot(aes(x = as.factor(Pclass), y = Fare, fill = as.factor(Pclass)))+
        geom_boxplot()+
        facet_wrap(~as.factor(Embarked), scales = "free")+
        labs(x = "Pclass")+
        scale_y_continuous(labels = scales::dollar)
```

We can see from the box plot that Pclass 1 median fare is approx $80 when embarked from c which is Cherbourg. Lets confirm it.

```{r}
data1 %>% 
        filter(Pclass == 1, Embarked == "C") %>% 
        summarise(Pclass1_median = median(Fare, na.rm=T))
        
```

I think it is sensible to assign "C" for those two tickets whose Embarked values were missing.

```{r}
data1$Embarked[data1$PassengerId == 62 | data1$PassengerId == 830] <- "C"

```

2. We also know PassengerId 1044 has Fare value missing. Let have a look:

```{r}
fare_na <- data1 %>% 
        filter(is.na(Fare))
datatable(fare_na)

```

PassengerId 1044 has Pclass 3 and embarked from S = Southampton.



```{r}
data1 %>% 
        filter(Pclass == 3, Embarked == "S") %>% 
        ggplot(aes(x = Fare, y = ..density..))+
        geom_density(show.legend = F, fill = "peachpuff3")+
        geom_vline(aes(xintercept = median(Fare, na.rm = T)),linetype = "dashed", lwd = 1, col = "red")+
        scale_x_continuous(labels = scales::dollar)+
        labs(title = "Density Plot - $Fares for Pclass = 3 & Embarked = 'S' ")


data1 %>% 
        filter(Pclass == 3, Embarked == "S") %>% 
        ggplot(aes(x = Pclass, y = Fare))+
        geom_boxplot()+
        labs(x = "Pclass 3", title = "Box Plot - $Fares for Pclass = 3 & Embarked = 'S' ")

median_fare_p3_S <- data1 %>% 
        filter(Pclass == 3, Embarked == "S") %$%
        median(Fare, na.rm=T)

paste0("The median fare for Pclas 3 & Embarked = 'S' is $", median_fare_p3_S)

```

The density plot which show the counts and the area underneath the polygon is one. This plot tells us approx \$8.05 accounts to around 15 percent of the fare in Pclass 3 and Embarked from S. I think it is sensible to replace NA with \$8.05



```{r}
data1$Fare[data1$PassengerId == 1044] <- median_fare_p3_S

```

3. Age variable

```{r}
# names(data1)

var_factors <- c("PassengerId", "Pclass", "Sex", "Embarked", "Title", "Fam_size", "Family")

data1[var_factors] <- map_df(data1[var_factors], ~as.factor(.))



```

We'll use MICE imputation method to impute missing Age variables. 

Info: https://cran.r-project.org/web/packages/mice/mice.pdf

We'll use Pclass, SibSp, Parch, Fare, Embarked, Title, Fam_size variables as predictors to impute missing values in Age.

```{r results = "hide", message = F}

set.seed(1)

names(data1)

data1$Age_org <- data1$Age

data1$Age_imp <- is.na(data1$Age)

mice_m1 <- data1 %>% 
        select(Pclass, SibSp, Parch, Fare, Embarked, Title, Fam_size, Age) %>% 
        mice(method = "rf")

mice_m1_output <- complete(mice_m1)

data1$Age <- mice_m1_output$Age

data1 %>% 
        ggplot(aes( x= Age_org, fill = Age_imp))+
        geom_histogram(col = "black", show.legend = F)+
        labs(title = "Histogram of Age")

data1 %>% 
        ggplot(aes(x = Age, fill = Age_imp))+
        geom_histogram(col = "black")+
        labs(title = "Histogram of Age with imputed value highlighted")



```

The shape of histogram has not changed significantlly from original values and after imputing missing values. I think its sensible to use the imputed values.


## Data Exploration

Let us split out data1 sets into original train and test objects but name it as train_edited & test_edited. They are exactly same apart from the new imputed values.

```{r}

train_edited <- data1[1:891,]
test_edited <- data1[892:1309, ]
```

Sex ratio vs survival:

```{r}
prop.table(table(train_edited$Survived, train_edited$Sex),2)



```

~ 74 % of all the females in train data survived whereas only ~18% out of all male population survived. That is huge differences!!! 

```{r}

pclass_Sex <- train_edited %>% 
        group_by(Survived, Pclass, Sex, Family, Title) %>% 
        summarise(n=n())

datatable(pclass_Sex, caption = "Survivor Table")

pclass_Sex %>% 
        ggplot(aes(x = Pclass, y = n))+
        geom_bar(aes(fill = Title), stat="identity")+
        facet_wrap(Survived~Sex, scales = "free_y")
        
pclass_Sex %>% 
        ggplot(aes(x = Pclass, y = n))+
        geom_bar(aes(fill = Family), stat="identity")+
        facet_wrap(Survived~Sex)


```

The above plot clearly shows that there are lot more female survivors then men. In all 3 classes female survivors outnumber male survivors. If you were female you have more chances of survival in Pclass 1 & 2. You'll have a good chance of survival if you were single or in mid size family.

Lets confirm visually that small family have survived better than large family

```{r}
train_edited %>% 
        ggplot(aes(x = Fam_size, fill = Survived))+
        geom_bar(position = "dodge")+
        labs(x = "Family Size")

```


Age vs Survival Boxplot

```{r}
train_edited %>% 
        ggplot(aes(x = Age, y = Survived, fill = Sex))+
        geom_boxploth()+
        geom_jitter(alpha = 0.09)+
        labs(x = "Age")
        
```

Those who did not survive, on average females were much younger than males.

```{r}
datatable(train_edited %>% 
        group_by(Survived, Sex) %>% 
        summarise(average_age = median(Age),Count=n()))
        
```



# Prediction

Let predict which passengers survives on Titanic. I'll build two models using  conditional inference tree and randomForest. 


## Using conditional inference tree (ctree)

We'll split the training set - train_edited into trainset & testset using 70/30 ratio. 

```{r}
split.data <- function(data, p = .7, s=007) {
        set.seed(s)
        n <- nrow(data)
        train_index <- sample(1:n, size= round(p*n)) 
        trainset <- data[train_index, ]
        testset <- data[-train_index, ]
        list(trainset = trainset, testset=testset)
}



allset <- split.data(train_edited)


trainset <- allset$trainset
testset <- allset$testset




```

We'll build our ctree model using Survived ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + Fam_size + Age_mice + Family

```{r}
trainset.ctree <- ctree(Survived ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + Fam_size + Age + Family, trainset)
```

Conditional inference tree of the train_edited set:

```{r fig.width = 8, fig.height = 5}
plot(trainset.ctree)
```

The above tree diagram has confirmed what we already knew from our data exploration. The tree diagram claims that the most important variable is Title and if you're female and travel in Pclass 1 & 2 you have much chances of survival. If you are male the worst case scenario would be to travel in Pclass 2 & 3.


Lets Validate power of prediction with confusion matrix:

Since we had split the train_edited data set into trainset and testset we know the survival status. We can use this information to check our model prediction power.

```{r}

ctree_predict <- predict(trainset.ctree, testset)

conf_matrix_ctree<- confusionMatrix(ctree_predict, testset$Survived)

conf_matrix_ctree

accurary_ctree <- 8.239700e-01 *100

paste0("The the confusion matrix overall accurary is ", accurary_ctree, "% when using Conditional inference tree")

```

Overall accurary is telling us how often is the classifier correct. 82% not bad !!!

## Randomforest

Randomforest is an ensemble learning method that grows multiple decision trees and each decision tree will produce its own prediction results. Enselble learning method combines results produced by different learners into one format. 

```{r}
trainset.rf <- randomForest(Survived ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + Fam_size + Age + Family, data=trainset, importance = T)

rf_predict <- predict(trainset.rf, trainset)

conf_rf <- confusionMatrix(rf_predict, trainset$Survived)

conf_rf


rf_accuracy <- 9.310897e-01*100
 
paste0("The the confusion matrix overall accurary is ", rf_accuracy, "% when using randomForest")


```

```{r}
plot(trainset.rf, main = "Mean square error rate")
legend('topright', legend = colnames(trainset.rf$err.rate), col=1:3, fill=1:3)


```

Above plot shows mean square error rate. The green line shows the error rate for survival whereas red lines shows for the dead. Plot is telling us that we are more successful predicting death than survival.

```{r}

datatable(importance(trainset.rf), caption = "Relative Variable Importance")


```
```{r}
varImpPlot(trainset.rf, main = "Plot of relative variable importance as measured by randomForest")
```

<br>The above plot validates what ctree predicted that Title is the most important variable.


# Final prediction

```{r}
# predictions <- predict(trainset.rf, test_edited)
# 
# solution <- data.frame(PassengerID = test_edited$PassengerId, Survived = predictions)
# 
# write.csv(solution, file = "Titanic_Survival_pred_rf.csv", row.names = F)

## 0.77990 kaggle score

```

After submitting my prediction I got 0.77990 kaggle score. I think its not that bad score since there is always room for tuning model which will give us better score.

# Reference:

1. https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic/notebook

2. Machine learning with R Cookbook - Yu Wei