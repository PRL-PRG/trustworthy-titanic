<!DOCTYPE html>
<html lang="en">
<head>
    <title>Titanic Tragedy: Survival Prediction with ML | Kaggle</title>
    <meta charset="utf-8" />
    <meta name="robots" content="index, follow" />
    <meta name="turbolinks-cache-control" content="no-cache" />
                <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">    <meta name="theme-color" content="#008ABC" />
    <script type="text/javascript">
        window["initialPageLoadStartTime"] = new Date().getTime();
    </script>
    <link rel="dns-prefetch" href="https://www.google-analytics.com" /><link rel="dns-prefetch" href="https://stats.g.doubleclick.net" /><link rel="dns-prefetch" href="https://js.intercomcdn.com" /><link rel="dns-prefetch" href="https://storage.googleapis.com/" />
    <link href="/static/images/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <link rel="manifest" href="/static/json/manifest.json">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:400,300,300italic,400italic,600,600italic,700,700italic" rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet" type='text/css'/>
        <link rel="canonical" href="/mohdbarakat/titanic-tragedy-survival-prediction-with-ml" />                    <link rel="stylesheet" type="text/css" href="/static/assets/vendor.css?v=632d145d8598" />
        <link rel="stylesheet" type="text/css" href="/static/assets/app.css?v=1d00932a7505" />
    
    
 
        <script>
        try{(function(a,s,y,n,c,h,i,d,e){d=s.createElement("style");
        d.appendChild(s.createTextNode(""));s.head.appendChild(d);d=d.sheet;
        y=y.map(x => d.insertRule(x + "{ opacity: 0 !important }"));
        h.start=1*new Date;h.end=i=function(){y.forEach(x => d.deleteRule(x))};
        (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;
        })(window,document,['.site-header-react__nav'],'dataLayer',2000,{'GTM-52LNT9S':true});}catch{}
    </script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-12629138-1', {
            'optimize_id': 'GTM-52LNT9S',
            'displayFeaturesTask': null,
            'send_page_view': false
        });
    </script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12629138-1"></script>

    
<script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
            n.callMethod.apply(n,arguments):n.queue.push(arguments)};
        if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
        n.queue=[];t=b.createElement(e);t.async=!0;
        t.src=v;s=b.getElementsByTagName(e)[0];
        s.parentNode.insertBefore(t,s)}(window,document,'script',
        'https://connect.facebook.net/en_US/fbevents.js');
    fbq("set", "autoConfig", "false", "136809193586742");
    fbq('init', '136809193586742'); 
    fbq('track', 'PageView');
</script>
<noscript>
    <img height="1" width="1" src="https://www.facebook.com/tr?id=136809193586742&ev=PageView&noscript=1"/>
</noscript>

<script>window.intercomSettings = {"app_id":"koj6gxx6"};</script>        <script>(function () { var w = window; var ic = w.Intercom; if (typeof ic === "function") { ic('reattach_activator'); ic('update', intercomSettings); } else { var d = document; var i = function () { i.c(arguments) }; i.q = []; i.c = function (args) { i.q.push(args) }; w.Intercom = i; function l() { var s = d.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'https://widget.intercom.io/widget/koj6gxx6'; var x = d.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x); } if (w.attachEvent) { w.attachEvent('onload', l); } else { w.addEventListener('load', l, false); } } })()</script>
    
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@kaggledatasets" />
    <meta name="og:url" content="https://kaggle.com/mohdbarakat/titanic-tragedy-survival-prediction-with-ml" />
    <meta name="og:title" content="Titanic Tragedy: Survival Prediction with ML" />
    <meta name="og:description" content="Using data from Titanic: Machine Learning from Disaster" />
    <meta name="og:image" content="https://storage.googleapis.com/kaggle-avatars/thumbnails/1560793-gr.jpg" />


    
    

    
    
    
<script type="text/javascript">
    var Kaggle = Kaggle || {};

    Kaggle.Current = {
        antiForgeryToken: 'CfDJ8LdUzqlsSWBPr4Ce3rb9VL8dOj1nawFWJHur1PUdEUTS6ZxzgbZDzQ3s5P2JjK6Jb0guC6g1o3EM5duOpfQWQx8hPqyZCB5cQnBGZRAZeckyk0oarngYwswwS9lnBwWFh5DlP0M44LlxaQ88eHQbZa0',
        isAnonymous: true,
        analyticsToken: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NjAzNTM0OTMsIlVzZXJJZCI6MH0.B5PEzmC9R_GJKJRR-xNoV8oW-d2Uvn_yGiyvFDUbfwM',
        analyticsTokenExpiry: 15,
        internetKernelsEnabled: false,
        
        
        
        
        
        
        
        
        
        
        
    }
        Kaggle.Current.log = function(){};
        Kaggle.Current.warn = function(){};

    var decodeUserDisplayName = function () {
        var escapedUserDisplayName = Kaggle.Current.userDisplayNameEscaped || "";
        try {
            var textVersion = new DOMParser().parseFromString(escapedUserDisplayName, "text/html").documentElement.textContent;
            if (textVersion) {
                return textVersion;
            }
        } catch(ex) {}
        return escapedUserDisplayName;
    }
    Kaggle.Current.userDisplayName = decodeUserDisplayName();
</script>

    

<script type="text/javascript">
    var Kaggle = Kaggle || {};
    Kaggle.PageMessages = [];
</script>

    
<script type="text/javascript">
/* <![CDATA[ */
goog_snippet_vars = function() {
    var w = window;
    w.google_conversion_id = 955616553;
    w.google_conversion_label = "QSjvCKDksHMQqZrWxwM";
    w.google_conversion_value = 0.00;
    w.google_conversion_currency = "USD";
    w.google_remarketing_only = false;
    w.google_conversion_language = "en";
    w.google_conversion_format = "3";
    w.google_conversion_color = "ffffff";
}
// DO NOT CHANGE THE CODE BELOW.
goog_report_conversion = function(url) {
    goog_snippet_vars();
    window.google_conversion_format = "3";
    var opt = new Object();
    opt.onload_callback = function() {
        if (typeof(url) != 'undefined') {
            window.location = url;
        }
    }
    var conv_handler = window['google_trackConversion'];
    if (typeof(conv_handler) == 'function') {
        conv_handler(opt);
    }
}
/* ]]> */
</script>
<script type="text/javascript"
src="//www.googleadservices.com/pagead/conversion_async.js">
</script>



        <script>window['useKaggleAnalytics'] = true;</script>

    <script src="/static/assets/vendor.js?v=4721d2c14786" data-turbolinks-track="reload"></script>
    <script src="/static/assets/app.js?v=1a3cd8c35fe7" data-turbolinks-track="reload"></script>
        <script>
            (function() {
                if ('serviceWorker' in navigator) {
                    navigator.serviceWorker.register("/static/assets/service-worker.js").then(function(reg) {
                        reg.onupdatefound = function() {
                            var installingWorker = reg.installing;
                            installingWorker.onstatechange = function() {
                                switch (installingWorker.state) {
                                case 'installed':
                                    if (navigator.serviceWorker.controller) {
                                        console.log('New or updated content is available.');
                                    } else {
                                        console.log('Content is now available offline!');
                                    }
                                    break;
                                case 'redundant':
                                    console.error('The installing service worker became redundant.');
                                    break;
                                }
                            };
                        };
                    }).catch(function(e) {
                      console.error('Error during service worker registration:', e);
                    });
                }
            })();
        </script>
    <script>
        function handleClientLoad() {
            try {
                gapi.load('client:auth2');
            } catch (e) {
                // In Opera, readystatechange is an unreliable detection of script load, causing
                // this function to be called before gapi exists on the window. The onload callback
                // is still called at the correct time, so the feature works as expected - it's
                // just generating noisy errors.
            }
        }
    </script>
    <script async defer src="https://apis.google.com/js/api.js"
            onload="this.googleApiOnLoad=function(){};handleClientLoad()"
            onreadystatechange="if (this.readyState === 'complete') this.googleApiOnLoad()">
    </script>
</head>
<body data-turbolinks="true">
    <main>
        






<div class="site-layout">
        <div class="site-layout__header">
            <div data-component-name="SiteHeaderContainer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({});performance && performance.mark && performance.mark("SiteHeaderContainer.componentCouldBootstrap");</script>
        </div>

    <div class="site-layout__main-content">
        

<div data-component-name="KernelViewer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({"kernel":{"id":618429,"title":"Titanic Tragedy: Survival Prediction with ML","forkParent":null,"currentRunId":2406024,"mostRecentRunId":2406024,"url":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml","tags":[{"name":"text mining","slug":"text-mining","url":"/tags/text-mining"},{"name":"random forest","slug":"random-forest","url":"/tags/random-forest"},{"name":"svm","slug":"svm","url":"/tags/svm"}],"commentCount":0,"upvoteCount":2,"viewCount":861,"forkCount":1,"bestPublicScore":0.78947,"author":{"id":1560793,"displayName":"Mohammed Barakat","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"mohdbarakat","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1560793-gr.jpg","profileUrl":"/mohdbarakat","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":1,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"isPrivate":false,"updatedTime":"2018-02-09T04:57:32.65Z","selfLink":"/kernels/618429","pinnedDockerImageVersionId":null,"isLanguageTemplate":false,"medal":null,"topicId":null,"readGroupId":null,"writeGroupId":null,"slug":"titanic-tragedy-survival-prediction-with-ml"},"kernelBlob":{"id":7878311,"settings":{"dockerImageVersionId":47,"dataSources":[{"sourceType":"Competition","sourceId":3136,"databundleVersionId":null}],"sourceType":"script","language":"rmarkdown","isGpuEnabled":false,"isInternetEnabled":false},"source":"---\r\ntitle: \u0022Titanic Tragedy: Survival Prediction with Machine Learning\u0022\r\nauthor: \u0022Mohammed K. Barakat\u0022\r\ndate: \u0022February 9, 2018\u0022\r\noutput:\r\n  html_document:\r\n    number_sections: yes\r\n    toc: yes\r\n  pdf_document:\r\n    toc: yes\r\n  word_document:\r\n    toc: yes\r\n---\r\n\r\n```{r setup, include=FALSE}\r\nknitr::opts_chunk$set(echo = TRUE)\r\n```\r\n\r\n# Introduction\r\n\r\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\r\n\r\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\r\n\r\nThis research tries to answer the question of *\u0022what sorts of people were likely to survive\u0022*. Using Machine Learning tools and techniques the research predicts which passengers survived the tragedy.\r\n\r\nMore about the *RMS Titanic* tragedy is available in [Wikipedia](https://en.wikipedia.org/wiki/RMS_Titanic)\r\n\r\n# Executive Summary\r\n\r\nThe research follows the *Reproducible Research* methodology. It starts by defining the objective, acquiring and defining input data, data cleaning, feature engineering, data exploration (EDA), and finally by model training, selection, and prediction.\r\n\r\nSeveral R packages are used in the research. Packages cover data visualization, data manipulation, model training and prediction, and text mining.\r\n\r\nSeveral prediction algorithms have been used. One of which was used to predict missing passenger ages (*Generalized Linear Model (glm)*). Other three algorithms were used to make final predictions of passengers’ survival in the Kaggle test data set. The algorithms used in survival prediction are: *Random Forest (RF)*, *Gradient Boosting Machine (GBM)*, and the *Support Vector Machine (SVM)*.\r\n\r\nAlthough both the *RF* and the *SVM* models were very competitive in terms of prediction accuracy, the *SVM* surpassed the *RF* with an accuracy of **83.9%** versus **82.5%** for the *RF*.\r\n\r\nThe research ends by predicting survival in the Kaggle test set where the survival rate was **37.3%** (**156** passengers have been predicted to survive the tragedy compared to **262** who could not make it).\r\n\r\n# Research Data\r\n\r\nData of the research has been obtained from *Kaggle* and is available under this [link](https://www.kaggle.com/c/titanic/data).\r\n\r\nThe data is split into two data sets:\r\n\r\n* **training set (train.csv)** which will be used to train the prediction models. Since the available testing set does not contain the *ground truth* of each passenger, this training set will be split into training (*trainTemp*) and testing (*testTemp*) data. The temporary training set will be used to train prediction models, whereas the temporary testing set will be used to validate model performance and, hence, selection of the best model before being used to predict passenger survivals in the Kaggle-provided testing set.\r\n\r\n* **testing set (test.csv)** in which the ground truth (outcome) is not provided. The research aims at predicting the outcome in this set. For each passenger in the test set, the best trained model will be used to predict whether or not passengers survived the sinking of the Titanic.\r\n\r\n## *R* Libraries\r\n\r\nThe research uses the libraries loaded below:\r\n\r\n```{r, message=FALSE, warning=FALSE}\r\nlibrary(knitr)\r\nlibrary(ggplot2)\r\nlibrary(ggthemes)\r\nlibrary(gridExtra)\r\nlibrary(scales)\r\nlibrary(dplyr)\r\nlibrary(qdap)\r\nlibrary(pROC)\r\nlibrary(caret)\r\nlibrary(qdap)\r\nlibrary(tm)\r\nlibrary(wordcloud)\r\n```\r\n\r\n## Importing Kaggle data sets\r\n\r\nKaggle *training* and *testing* data sets are imported into *R*. Then, both sets are combined into one set (**allSet**) for *Data Cleaning* and *Feature Engineering* before being used in model training and prediction.\r\n\r\n```{r}\r\ntrain \u003c- read.csv(\u0022../input/train.csv\u0022, stringsAsFactors = F)\r\ntest \u003c- read.csv(\u0022../input/test.csv\u0022, stringsAsFactors = F)\r\nallSet \u003c- bind_rows(train, test) # combining train and test data in one set\r\n```\r\n\r\n## Data dimensions and structure\r\n\r\n```{r}\r\ntrainDim \u003c- dim(train)\r\ntestDim \u003c- dim(test)\r\ntrainR \u003c- trainDim[1];testR \u003c- testDim[1];allSetC \u003c- dim(allSet)[2]\r\n```\r\n\r\nBy having a look at the dimensions of the training and testing data, we see that they have **`r trainR`** and **`r testR`** observations (passengers), respectively. And both of them have **`r allSetC`** columns (features).\r\n\r\nWe can also have a glimpse of the data structure. It is clear that we have features that are either numeric (integers and double) or characters. It\u0027s worth noticing that the outcome variable (*Survived*) is integer, which gives us a hint in the *Feature Engineering* section.\r\n\r\n```{r}\r\nglimpse(allSet)\r\n```\r\n\r\n## Data dictionary\r\n\r\nBelow is a description of each of the features existing in the data sets to give insight of what information we have about passengers.\r\n\r\nFeature|Description\r\n-------|-------------------------------------------------------\r\nPassengerId|Passenger Id\r\nSurvived|Survival (survived = 1, deceased = 0)\r\nPclass|Ticket class (upper = 1, middle = 2, lower = 3)\r\nName|Passenger name\r\nSex|Sex (male, female)\r\nAge|Age in years or fraction\r\nSibSp|Number of siblings / spouses aboard\r\nParch|Number of parents / children aboard\r\nTicket|Ticket number\r\nFare|Passenger fare\r\nCabin|Cabin number\r\nEmbarked|Port of Embarkation  (C = Cherbourg, Q = Queenstown, S = Southampton)\r\n-------------------------------------------------------------------------------\r\n\r\n# Data Cleaning\r\n\r\nBy exploring the data set, as shown in the below subset, it is realized that the data needs to be processed and cleaned. Data cleaning involves removing duplicates, handling missing values, or fixing features classes.\r\n\r\n```{r}\r\nkable(head(allSet))\r\n```\r\n\r\n## Removing duplicate observations\r\n\r\nSince the data sets represent passengers\u0027 details, they should not contain any duplicate record of passengers. *PassengerId* feature is best to be used for uniqueness of observations.\r\n\r\n```{r}\r\nifelse(length(unique(allSet[,1])) == nrow(allSet),\u0022No duplicates\u0022,\u0022Duplicates detected!\u0022)\r\n```\r\nBoth data sets (training and testing) represented by *allSet* do not contain any duplicate observations of passengers.\r\n\r\n## Fixing features classes\r\n\r\nFeatures in the data set must have the correct class (data type) to be used in analysis, model training and prediction. By studying the features classes, we find that *Survived*, *Pclass*, *Sex*, *Cabin*, and *Embarked* should be changed to *Factor* variables instead of characters.\r\n\r\n```{r}\r\nallSet$Survived \u003c- as.factor(allSet$Survived)\r\nallSet$Pclass \u003c- as.factor(allSet$Pclass)\r\nallSet$Sex \u003c- as.factor(allSet$Sex)\r\nallSet$Cabin \u003c- as.factor(allSet$Cabin)\r\nallSet$Embarked \u003c- as.factor(allSet$Embarked)\r\n```\r\n\r\n## Imputing missing values\r\n\r\nMissing values in data sets are usually problematic in data analysis, model training and prediction. Hence, we will replace missing values with NA\u0027s then impute them with reasonable values in necessary features.\r\n\r\nFirst, let\u0027s check the missing values in all variables and replace them with NA\u0027s.\r\n\r\n```{r missingValues}\r\n\r\n# replace missing values with NA across all features\r\nfor (i in 1:allSetC){\r\n  allSet[,i][allSet[,i]== \u0022\u0022] \u003c- NA\r\n}\r\n\r\n# define a function to get number of NAs in each feature\r\ngetNA \u003c- function(dt,NumCol){\r\n       varsNames \u003c- names(dt)\r\n        NAs \u003c- 0\r\n\r\n        for (i in 1:NumCol){\r\n          NAs \u003c- c(NAs, sum(is.na(dt[,i])))\r\n        }\r\n\r\n        NAs \u003c- NAs[-1]\r\n        names(NAs)\u003c- varsNames # make a vector of variable name and count of NAs\r\n\r\n        NAs \u003c- NAs[NAs \u003e 0]\r\n        NAs \r\n}\r\n\r\ngetNA(allSet,allSetC)\r\n\r\n```\r\n\r\nExcluding the Survived feature, as it is the outcome feature to be predicted in the testing data set, we have 4 variables with missing values; Age, Fare, Cabin, and Embarked.\r\n\r\nPredicting *Age* feature from the features available in the data set cannot be directly predicted. Yet, we will deal with it differently in *Feature Engineering* section. Besides, missing values in *Cabin* is huge (1014 out of 1309). Hence, we will exclude this feature from our model.\r\n\r\nWe are left with *Fare* and *Embarked* which both have reasonable number of missing values that we can impute from the given data.\r\n\r\n**Imputing Embarkation missing values**\r\n\r\nLooking at the data, we might be able to predict the passengers\u0027 missing embarkation ports based on their Fare and Class.\r\n\r\n```{r}\r\nallSet[,c(\u0022PassengerId\u0022,\u0022Pclass\u0022,\u0022Fare\u0022,\u0022Embarked\u0022)] %\u003e% filter(is.na(Embarked))\r\n```\r\n\r\nBoth passengers with missing embarkation ports have the same Fare and Class; $80 and 1, respectively.\r\n\r\nUsing the other passengers\u0027 data for Fare and Class, we will predict the embarkation ports of these two passengers.\r\n\r\n```{r, warning=FALSE}\r\n\r\n# filter for complete embarkation records\r\nFareClassComp \u003c- allSet %\u003e% filter(!is.na(Embarked))\r\n\r\n# plot embarkation ports versus fare mapped by passenger class\r\nFareClassComp %\u003e% \r\n        ggplot(aes(x = Embarked, y = Fare, fill = Pclass))+\r\n        geom_boxplot()+\r\n        geom_hline(aes(yintercept = 80),\r\n                   colour = \u0022red\u0022, linetype = \u0022dashed\u0022, lwd = 2)+\r\n        scale_y_continuous(labels = dollar_format())+\r\n        theme_few()\r\n\r\n```\r\n\r\nBased on the boxplot above, we can see that passengers who embarked off Cherbourg port (C) had paid $80 fare on average and had first class aboard. Hence, we can assume with high confidence that both passengers with missing embarkation values had embarked off the same port for having the same Fare and Class values.\r\n\r\n```{r}\r\n# impute missing embarkation values for both passengers\r\nallSet$Embarked[is.na(allSet$Embarked)] \u003c- \u0022C\u0022\r\n```\r\n\r\n**Imputing Fare missing values**\r\n\r\nNow, we have one passenger with missing Fare value.\r\n\r\n```{r}\r\nallSet[,c(\u0022PassengerId\u0022,\u0022Pclass\u0022,\u0022Fare\u0022,\u0022Embarked\u0022)] %\u003e% filter(is.na(Fare))\r\n```\r\n\r\nAs shown in the previous boxplot, we can use the median of the 3rd class passengers who had embarked off port Southampton (which is calculated to be $8.05) to predict the Fare value of passenger 1044.\r\n\r\n```{r}\r\nallSet$Fare[allSet$PassengerId == 1044] \u003c-  median(allSet$Fare[allSet$Pclass == 3 \u0026 allSet$Embarked == \u0022S\u0022], na.rm = T)\r\n```\r\n\r\n# Feature Engineering\r\n\r\n*Feature Engineering* simply refers to creating new features from the existing ones to improve model performance. The following sub-sections handle multiple feature engineering tasks that would make training the model possible, easier, and more accurate in prediction.\r\n\r\n## Creating the *Title* feature\r\n\r\nBy looking at the *Name* feature, we can notice that passenger titles are embedded into their names. A passenger name starts with the surname, then a comma and a space before the title that is followed by a period. E.g. (surname, Mr.). So, we can extract the title with some *Regex* manipulation as shown below.\r\n\r\n```{r}\r\nallSet$Title \u003c- gsub(\u0022(.*, )|(\\\\..*)\u0022,\u0022\u0022,allSet$Name)\r\n\r\n# tabulate titles versus sex\r\ntable(allSet$Sex, allSet$Title)\r\n```\r\n\r\nBy tabulating *Title* versus *Sex* we can find that titles (Capt, Col, Don, Jonkheer, Major, Master, Mr, Rev, Sir) all refer to males. Whereas, (Dona, Lady, Miss, Mlle, Mme, Mrs, Ms, the Countess) refer to female. \u0022Dr\u0022 refers to both.\r\n\r\nHence, we can reduce titles to only three: Mr, Mrs, and Miss according to Sex mapping.\r\n\r\n```{r}\r\nMrTitles \u003c- c(\u0022Capt\u0022, \u0022Col\u0022, \u0022Don\u0022, \u0022Jonkheer\u0022, \u0022Major\u0022, \u0022Master\u0022, \u0022Mr\u0022, \u0022Rev\u0022, \u0022Sir\u0022)\r\nMrsTitles \u003c- c(\u0022Dona\u0022, \u0022Lady\u0022, \u0022Mme\u0022, \u0022Mrs\u0022, \u0022the Countess\u0022)\r\nMissTitles \u003c- c(\u0022Miss\u0022, \u0022Mlle\u0022, \u0022Ms\u0022)\r\n\r\nallSet$Title[allSet$Title %in% MrTitles] \u003c- \u0022Mr\u0022\r\nallSet$Title[allSet$Title %in% MrsTitles] \u003c- \u0022Mrs\u0022\r\nallSet$Title[allSet$Title %in% MissTitles] \u003c- \u0022Miss\u0022\r\nallSet$Title[allSet$Title == \u0022Dr\u0022 \u0026 allSet$Sex == \u0022female\u0022] \u003c- \u0022Mrs\u0022\r\nallSet$Title[allSet$Title == \u0022Dr\u0022 \u0026 allSet$Sex == \u0022male\u0022] \u003c- \u0022Mr\u0022\r\n\r\nallSet$Title \u003c- as.factor(allSet$Title)\r\ntable(allSet$Title)\r\n```\r\n\r\n## Creating the family size (*FamSz*) feature\r\n\r\nBy knowing the siblings/spouses and parches count, we might be able to know the passenger family size. So, we will create a new feature (*FamSz*) by adding the count of siblings and parches to the passenger to know the passenger family size aboard.\r\n\r\n```{r}\r\nallSet$FamSz \u003c- allSet$SibSp + allSet$Parch + 1\r\n```\r\n\r\n## Creating the family size categories (*FamSzCat*)\r\n\r\nAlso, it might be useful to categorize family sizes for easier model training and prediction. So, we will categorize family sizes into *Singles*, *Small*, and *Large*.\r\n\r\n```{r}\r\nallSet$FamSzCat[allSet$FamSz == 1] \u003c- \u0022Singles\u0022\r\nallSet$FamSzCat[allSet$FamSz \u003e 1 \u0026 allSet$FamSz \u003c5] \u003c- \u0022Small\u0022\r\nallSet$FamSzCat[allSet$FamSz \u003e 4] \u003c- \u0022Large\u0022\r\n\r\nallSet$FamSzCat \u003c- as.factor(allSet$FamSzCat)\r\n```\r\n\r\n## Creating the *Surname* feature\r\n\r\nAlthough not all features might be used in prediction, it is worthy to keep/create some features to further explore the data. And in this research it could be interesting to know statistics about families who faced this tragedy. So, we will create a new feature (*Surname*) which gives insight into families onboard Titanic.\r\n\r\n```{r}\r\nallSet$Surname \u003c- sapply(allSet$Name, function(x) strsplit(x, split = \u0022[,]\u0022)[[1]][1])\r\npaste(nlevels(factor(allSet$Surname)), \u0022families were onboard Titanic\u0022)\r\n```\r\n\r\nWe will explore their distribution in the Exploratory Data Analysis (EDA).\r\n\r\n## Creating the age stage *AgeStg* feature\r\n\r\nAs mentioned earlier, predicting the exact age of passengers with missing ages is risky and commits high inaccuracy. Yet, age is believed to have high influence on prediction results of survivals. So, *Age* will be used to create the Age Stage (*AgeStg*) feature which identifies a passenger as a child or an adult. Then, missing age stages will be easier to predict using other features. Passengers below 18 years old will be assigned *Child* values. Whereas those above 18 will be *Adult*.\r\n\r\n```{r}\r\nallSet$AgeStg[allSet$Age \u003c 18 \u0026 !is.na(allSet$Age)] \u003c- \u0022Child\u0022\r\nallSet$AgeStg[allSet$Age \u003e= 18 \u0026 !is.na(allSet$Age)] \u003c- \u0022Adult\u0022\r\n```\r\n\r\n**Imputing missing Age Stage values**\r\n\r\nHere, we go back to Data Cleaning phase as we have a new feature with missing values. We have 263 missing Age Stage values. Which is the same number of missing *Age* values that we noticed in the Data Cleaning phase.\r\n\r\n```{r}\r\nlength(allSet$AgeStg[is.na(allSet$AgeStg)])\r\n```\r\n\r\nTo impute missing values in age stage, we will use one of the modeling algorithms. Since age stage has two possible outcomes (Child and Adult) we will try the *Generalized Linear Model (glm)* with *Forward Stepwise* method to predict the missing values with the best set of features as predictors.\r\n\r\nAs with any Machine Learning tasks, we will combine all data having non-missing age stage values in one data set then split it into training and testing data sets to test for prediction accuracy. Then, we can predict the missing values using the model.\r\n\r\nSince some variables are obviously not taking part in deciding the age stage, we will exclude them from the model upfront. Hence, we will use the following prospect features in the model: *Pclass*, *Sex*, *SibSp*, *Parch*, *Fare*, *FamSz*, and *FamSzCat*.\r\n\r\n\r\n```{r}\r\n# create a vector with the prospect features including AgeStg and PassengerId\r\nvarsNames \u003c- c(\u0022PassengerId\u0022,\u0022Pclass\u0022, \u0022Sex\u0022, \u0022SibSp\u0022, \u0022Parch\u0022, \u0022Fare\u0022, \u0022FamSz\u0022, \u0022FamSzCat\u0022, \u0022AgeStg\u0022)\r\n\r\n# subset the data by the selected features\r\nallSetAgeStg \u003c- allSet[,varsNames]\r\n\r\n# subset into two sets: one with age stage complete, and one with age stage missing\r\nallSetAgeStgComp \u003c- allSetAgeStg[!is.na(allSetAgeStg$AgeStg),]\r\nallSetAgeStgMiss \u003c- allSetAgeStg[is.na(allSetAgeStg$AgeStg),]\r\n\r\n# split the data set with complete age stage into train and test data sets (75/25 ratio)\r\n## number of training rows\r\nnTrain \u003c- 0.75 * nrow(allSetAgeStgComp)\r\n\r\n## sample row IDs\r\nset.seed(3030)\r\nsampleTrain \u003c- sample(nrow(allSetAgeStgComp),nTrain)\r\n\r\n## create train and test data sets\r\nAgeStgTrain \u003c- allSetAgeStgComp[sampleTrain,]\r\nAgeStgTest \u003c- allSetAgeStgComp[-sampleTrain,]\r\n\r\n# use the glm Logistic Regression model to predict the age stage. Use Forward Stepwise algorithm to select the best predictors.\r\n\r\n# build the null model with no predictors\r\nset.seed(3030)\r\nnull_model \u003c- glm(factor(AgeStg)~1, data = AgeStgTrain, family = \u0022binomial\u0022)\r\n\r\n# build the full model with all predictors\r\nset.seed(3030)\r\nfull_model \u003c- glm(factor(AgeStg)~Pclass+Sex+SibSp+Parch+Fare+FamSz+FamSzCat, data = AgeStgTrain, family = \u0022binomial\u0022)\r\n\r\n# perform forward stepwise algorithm to get an economic model with best predictors\r\nstep_model \u003c- step(null_model, scope = list(lower= null_model,upper = full_model),direction = \u0022forward\u0022)\r\n\r\n# estimate the stepwise age stage probability in training and testing data\r\nAgeStgTrain$stepProb \u003c- predict(step_model, data = AgeStgTrain, type = \u0022response\u0022)\r\nAgeStgTest$stepProb \u003c- predict(step_model, newdata = AgeStgTest, type = \u0022response\u0022)\r\n\r\n# create the ROC curve of the stepwise for training and testing data\r\nROC_train \u003c- roc(AgeStgTrain$AgeStg,AgeStgTrain$stepProb)\r\nROC_test \u003c- roc(AgeStgTest$AgeStg,AgeStgTest$stepProb)\r\n\r\n# Plot the ROC of the stepwise model: training and testing\r\nplot(ROC_train,col = \u0022red\u0022)\r\nplot(ROC_test,col = \u0022red\u0022)\r\n\r\n# calculate Area Under the Curve (AUC): training and testing\r\nauc(ROC_train);auc(ROC_test)\r\ntrainAcc \u003c- percent(auc(ROC_train));testAcc \u003c- percent(auc(ROC_test))\r\n```\r\n\r\nUsing the Generalized Linear Model (glm) with Forward Stepwise we get an in-sample accuracy of **`r trainAcc`** and an out-of-sample accuracy of **`r testAcc`**.\r\n\r\nNext, using the Forward Step glm model, we will predict values of age stage (*AgeStg*) in the test data set and validate its accuracy before predicting the actual missing values in the entire data set.\r\n\r\n```{r}\r\n# find the average number of children to set it as threshold of probability\r\nAvgChildCount \u003c- mean(AgeStgTrain$AgeStg == \u0022Child\u0022)\r\n\r\n# Predict Age Stage in testing data if its probability is greater than the average \r\nAgeStgTest$AgeStgPred \u003c- ifelse(AgeStgTest$stepProb \u003e AvgChildCount,\u0022Child\u0022, \u0022Adult\u0022)\r\n\r\n# check accuracy of prediction in testing data\r\nacc \u003c- percent(mean(AgeStgTest$AgeStg == AgeStgTest$AgeStgPred))\r\nacc\r\n```\r\n\r\nAfter applying prediction model to the testing data, accuracy of prediction turned out to be **`r acc`**.\r\n\r\nFinally, we will predict the actual missing values of age stage in the entire data set.\r\n\r\n```{r}\r\n# predicting missing Age Stage using the stepwise, logistic regression model\r\nallSetAgeStgMiss$stepProb \u003c- predict(step_model, newdata = allSetAgeStgMiss, type = \u0022response\u0022)\r\n\r\nallSetAgeStgMiss$AgeStg \u003c- ifelse(allSetAgeStgMiss$stepProb \u003e AvgChildCount,\u0022Child\u0022, \u0022Adult\u0022)\r\n\r\n# update missing Age Stage in the full data\r\nallSet \u003c- left_join(allSet,allSetAgeStgMiss[,c(\u0022PassengerId\u0022,\u0022AgeStg\u0022)], by = \u0022PassengerId\u0022, allSet.x = TRUE, allSet.y = FALSE)\r\n\r\nallSet$AgeStg \u003c- ifelse(is.na(allSet$AgeStg.x),allSet$AgeStg.y,allSet$AgeStg.x)\r\nallSet \u003c- allSet[,!colnames(allSet) %in% c(\u0022AgeStg.x\u0022,\u0022AgeStg.y\u0022)]   \r\n\r\nallSet$AgeStg \u003c- as.factor(allSet$AgeStg)\r\n```\r\n\r\nConfirm that we have no missing values in the data set.\r\n\r\n```{r}\r\ngetNA(allSet,length(allSet))\r\n```\r\n\r\nAs mentioned earlier, *Survived* is the outcome feature to predict, *Age* will be replaced by *AgeStg*, and *Cabin* will not be used for huge number of missing values. Hence, we have all missing values imputed in features necessary for training and prediction.\r\n\r\n# Exploratory Data Analysis (EDA)\r\n\r\nIn this section we try to explore characteristics of the Kaggle training data using descriptive statistics and plots. First, we will revert back to the original training data and split it from the testing data set.\r\n\r\n```{r}\r\ntrain \u003c- allSet[!is.na(allSet$Survived),]\r\ntest \u003c- allSet[is.na(allSet$Survived),]\r\n```\r\n\r\n## Exploring the outcome feature (*Survived*)\r\n\r\n```{r, warning=FALSE}\r\ntrain %\u003e% \r\nggplot(aes(x=Survived, fill = Survived))+\r\n        geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022Survival in the Titanic tragedy\u0022)+\r\n        geom_label(stat=\u0027count\u0027,aes(label=..count..))+\r\n        labs(fill = \u0022Survival (0 = died, 1 = survived)\u0022)\r\n```\r\n\r\n```{r, warning = FALSE}\r\nsurvSumy \u003c- summary(train$Survived)\r\ndied \u003c- survSumy[[1]][1];suvd \u003c- survSumy[[2]][1];surPerc \u003c- percent(suvd/sum(suvd,died))\r\n```\r\n\r\nLooking at the histogram and the summary statistics of the train data, we can see that passengers who survived the tragedy were less than those who died. The survived were **`r suvd`**, while the dead were **`r died`**. The survival rate was **`r surPerc`**.\r\n\r\nLet\u0027s have a look at how the outcome feature (*Survival*) is related to some of the important features in the train data set.\r\n\r\n```{r, warning=FALSE}\r\np1 \u003c- ggplot(train,aes(x=Survived, fill=Pclass))+\r\n  geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022P1: Survival vs Class\u0022)\r\n\r\np2 \u003c- ggplot(train,aes(x=Survived, fill=Sex))+\r\n  geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022P2: Survival vs Sex\u0022)\r\n\r\np3 \u003c- ggplot(train,aes(x= Survived, fill = AgeStg))+\r\n  geom_histogram(stat = \u0022count\u0022, position = \u0022dodge\u0022)+\r\n        labs(x = \u0022P3: Survival vs Age Stage\u0022)\r\n\r\np4 \u003c- ggplot(train,aes(x=Survived, fill=Embarked))+\r\n  geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022P4: Survival vs Embarkment Port\u0022)\r\n\r\np5 \u003c- ggplot(train,aes(x= Survived, y = Fare))+\r\n  geom_boxplot()+\r\n        labs(x = \u0022P5: Survival vs Fare\u0022)\r\n\r\np6 \u003c- ggplot(train,aes(x= Survived, fill = FamSzCat))+\r\n  geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022P6: Survival vs Category of Family Size\u0022)\r\n\r\np7 \u003c- ggplot(train, aes(x = FamSz, fill = Survived)) +\r\n        geom_bar(stat=\u0027count\u0027, position=\u0027dodge\u0027) +\r\n        scale_x_continuous(breaks=c(1:11)) +\r\n        labs(x = \u0027P7: Survival vs Family Size\u0027)\r\n\r\ngrid.arrange(p1,p2,p3,p4,p5,p6,p7,ncol=2)\r\n\r\n```\r\n\r\n**Observations from the plots:**\r\n\r\n* Most deceased passengers were from the lower ticket class (class 3). While survived passengers were almost equally distributed over the three tickets classes.[*Ref. plot:p1*]\r\n\r\n* The majority of deceased passengers were males. And those survived mostly were females. This goes in line with the belief that women (and children) were given the priority to jump to the life boats. [*Ref. plot:p2*]\r\n\r\n* Adult passengers who did not make it in the tragedy were around 5 times the deceased children. And those adults survived, who probably were from upper classes, were around 3 times the survived children. Which again proves that children were given priority over adults-maybe except for some of those with higher classes! [*Ref. plot:p3*]\r\n\r\n* It is clear that most passengers (died and survived) had departed from Southampton port. Distribution of ports over both categories of passengers is almost similar which makes it difficult to be a predicting feature of survival. [*Ref. plot:p4*]\r\n\r\n* The median of deceased passengers fare was slightly smaller than that of passengers who survived. Obviously, those with higher fares could make it as they were from the fortunate higher classes. [*Ref. plot:p5*]\r\n\r\n* It seems that RMS Titanic was mostly a voyage of singletons. However, those unfortunates had lower odds to survive over the small and large families. [*Ref. plot:p6*]\r\n\r\n* In addition to singletons, families with more than 4 members had lower odds to survive than those with 4 and less family members. Maybe this was due to the time wasted by family members looking for each other and trying to get together during the time of sinking. [*Ref. plot:p7*]\r\n\r\n**A peek into Surnames**\r\n\r\nLet\u0027s have a look at passengers\u0027 surnames and see their numbers onboard the Titanic.\r\n\r\n```{r, warning= FALSE}\r\nsurN \u003c- train %\u003e% \r\n        group_by(Surname) %\u003e% \r\n        summarize(count = n()) %\u003e% \r\n        arrange(desc(count))\r\n\r\ncountSurN \u003c- nrow(surN)\r\n        \r\nsurN[1:30,]%\u003e% \r\nggplot(aes(x=reorder(Surname, count),y=count))+\r\n        geom_bar(stat = \u0022identity\u0022)+\r\n        scale_y_continuous(breaks = c(1:10))+\r\n        labs(x = \u0022Surnames\u0022, y = \u0022Number of Passengers\u0022)+\r\n        coord_flip()\r\n``` \r\n\r\nWell, the training data set shows that the **`r trainR`** passengers were grouped into **`r countSurN`** surnames. However, there was no clear skewness towards specific surnames. As shown in the bar chart above, the range of passengers having similar surnames was from 1 to 9 passengers.\r\n\r\n*Andersson* was the highest common surname with 9 persons, followed by *Sage* with 7 names, then a few groups having 6 passengers with common surnames such as *Skoog*, *Panula* and *Johnson*. The rest of passengers are groups of 5 or less members having similar surnames.\r\n\r\nThis can also be noticed in the **wordcloud** figure shown below with more frequent surnames highlighted in larger and bolder text.\r\n \r\n```{r, warning= FALSE}\r\nset.seed(4040)\r\nEmbk_source \u003c- VectorSource(train$Surname)\r\n \r\nEmbk_corp \u003c- VCorpus(Embk_source)\r\n\r\nEmbk_tdm \u003c- TermDocumentMatrix(Embk_corp)\r\nEmbk_m \u003c- as.matrix(Embk_tdm)\r\n\r\nterm_freq \u003c- rowSums(Embk_m)\r\nterm_freq \u003c- sort(term_freq, decreasing = TRUE)\r\nword_freqs \u003c- data.frame(term = names(term_freq), num = term_freq)\r\n\r\nwordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = \u0022blue\u0022)\r\n\r\n```\r\n\r\n\r\n# Predicting Survival in the Testing Data Set\r\n\r\nNow, having all features engineered, and having done the EDA, we can start with the following features as preliminary set of predictors in our training model:\r\n\r\n* **Pclass**\r\n* **Sex**\r\n* **SibSp**\r\n* **Parch**\r\n* **Fare**\r\n* **Embarked**\r\n* **Title**\r\n* **FamSzCat**\r\n* **AgeStg**\r\n\r\n## Checking features variability\r\n\r\nIt is a good practice to make sure the training data does not include predictors with no variability. I.e. predictors that have one or very few unique values relative to the number of observations. This can be detected with the *nzv* value of the *NearZeroVar* function results.\r\n\r\n```{r}\r\n# create a vector with the selected features\r\nselVarsNames \u003c- c(\u0022Pclass\u0022, \u0022Sex\u0022, \u0022SibSp\u0022, \u0022Parch\u0022, \u0022Fare\u0022,\u0022Embarked\u0022 ,\u0022Title\u0022, \u0022FamSzCat\u0022, \u0022AgeStg\u0022)\r\n\r\nnearZeroVar(train[,selVarsNames], saveMetrics = TRUE)\r\n```\r\n\r\nAs shown above all selected predictors have **FALSE** nzv value which indicates that all of them have reasonable variability in the dataset.\r\n\r\n## Prediction Algorithms\r\n\r\nSince the outcome (*Survived*) is a binary variable our prediction model should be based on one of those algorithms which are able to model categorical rather than regression data such as Random Forest, Gradient Boosting Machine (GBM), and Support Vector Machine (SVM).\r\n\r\nTo make sure we select the best model, we will use these three algorithms to train our model then make a comparison on prediction accuracy amongst them to select the best one.\r\n\r\nIn order to be able to validate the trained models and get the *out-of-sample* accuracy before predicting the Kaggle test data we will split the train data set into *trainTemp* and *testTemp* data sets. Train the models on the former, and test on the latter. Then, after selecting the best model, we will predict survival in the original test set given by Kaggle.\r\n\r\n**Using the Random Forest Algorithm**\r\n\r\nFirst, split train data set into *trainTemp* and *testTemp* data sets.\r\n\r\n```{r}\r\n# split the train data set into train and test data sets (75/25 ratio).\r\n\r\n## number of training rows\r\nnTrain \u003c- round(0.75 * nrow(train))\r\n\r\n## sample row IDs\r\nsampleTrain \u003c- sample(nrow(train),nTrain)\r\n\r\n## create trainTemp and testTemp data sets\r\ntrainTemp \u003c- train[sampleTrain,]\r\ntestTemp \u003c- train[-sampleTrain,]\r\n```\r\n\r\nNext, build the training models.\r\n\r\n```{r}\r\nset.seed(2020)\r\n\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, number = 10, repeats = 3)\r\nmodelRF \u003c- train(Survived~Pclass+Sex+SibSp+Parch+Fare+Embarked+Title+FamSzCat+AgeStg, data = trainTemp, method = \u0022rf\u0022, trControl = control)\r\n\r\nprint(modelRF)\r\n```\r\n\r\n**Using Gradient Boosting Machine (GBM) Algorithm**\r\n\r\n```{r}\r\nset.seed(2020)\r\n\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, number = 10, repeats = 3)\r\nmodelGBM \u003c- train(Survived~Pclass+Sex+SibSp+Parch+Fare+Embarked+Title+FamSzCat+AgeStg, data = trainTemp, method = \u0022gbm\u0022, trControl = control, verbose = FALSE)\r\n\r\nprint(modelGBM)\r\n```\r\n\r\n**Using Support Vector Machine (SVM) Algorithm**\r\n\r\n```{r}\r\nset.seed(2020)\r\n\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, number = 10, repeats = 3)\r\nmodelSVM \u003c- train(Survived~Pclass+Sex+SibSp+Parch+Fare+Embarked+Title+FamSzCat+AgeStg, data = trainTemp, method = \u0022svmRadial\u0022, trControl = control)\r\n\r\nprint(modelSVM)\r\n```\r\n\r\n## Models comparison and selection\r\n\r\nNow, we will compare the results of the three models using the *caret* package. The comparison process is built around getting an idea of the spread of the models accuracies. Since each model is evaluated using 3 repeats of 10-fold cross validation, the model will have 30 results (3 repeats of 10-fold cross validation). The objective of comparing results is to compare the accuracy distributions (30 values) between the models. Accuracy distributions will be summarized as box plots and dot plots.\r\n\r\n*Ref.* [Compare Models And Select The Best Using The Caret R Package](https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/)\r\n\r\n```{r}\r\n# collect resamples\r\nresults \u003c- resamples(list(RF=modelRF, GBM=modelGBM, SVM = modelSVM))\r\n\r\n# summarize the distributions\r\ncompSummary \u003c- summary(results)\r\ncompSummary\r\n\r\nmodelRFAcc \u003c- percent(median(compSummary$values$`RF~Accuracy`))\r\nmodelGBMAcc \u003c- percent(median(compSummary$values$`GBM~Accuracy`))\r\nmodelSVMAcc \u003c- percent(median(compSummary$values$`SVM~Accuracy`))\r\n\r\n# boxplots of results\r\nbwplot(results)\r\n\r\n# dot plots of results\r\ndotplot(results)\r\n```\r\n\r\nComparison results of accuracies amongst the three models show that they are very close in prediction performance. *Random Forest* and *SVM* are very competitive having median accuracies of **`r modelRFAcc`** and **`r modelSVMAcc`**, respectively. While *GBM* is slightly lower with median accuracy of **`r modelGBMAcc`**.\r\n\r\n**Models validation**\r\n\r\nAs mentioned earlier, we will validate the models on the testing data set (*testTemp*), then select the best model to predict survival in the Kaggle test set. Here, we will use the *Confusion Matrix* for each model to decide on the highest accuracy obtained and, hence, the best model to adopt.\r\n\r\n**Prediction using the Random Forest model**\r\n\r\n```{r}\r\nrfPred\u003c-predict(modelRF,testTemp)\r\nrfCM\u003c-confusionMatrix(rfPred,testTemp$Survived)\r\nrfCM\r\n\r\nmodelRFacc\u003c-percent(as.numeric(rfCM$overall[1]))\r\nmodelRFerr\u003c-percent(1-(as.numeric(rfCM$overall[1])))\r\n\r\nmodelRFacc;modelRFerr\r\n```\r\n\r\n**Prediction using the Gradient Boosting Machine (GBM) model**\r\n\r\n```{r}\r\ngbmPred\u003c-predict(modelGBM,testTemp)\r\ngbmCM\u003c-confusionMatrix(gbmPred,testTemp$Survived)\r\ngbmCM\r\n\r\nmodelGBMacc\u003c-percent(as.numeric(gbmCM$overall[1]))\r\nmodelGBMerr\u003c-percent(1-(as.numeric(gbmCM$overall[1])))\r\n\r\nmodelGBMacc;modelGBMerr\r\n```\r\n\r\n**Prediction using the Support Vector Machine (SVM)**\r\n\r\n```{r}\r\nsvmPred\u003c-predict(modelSVM,testTemp)\r\nsvmCM\u003c-confusionMatrix(svmPred,testTemp$Survived)\r\nsvmCM\r\n\r\nmodelSVMacc\u003c-percent(as.numeric(svmCM$overall[1]))\r\nmodelSVMerr\u003c-percent(1-(as.numeric(svmCM$overall[1])))\r\n\r\nmodelSVMacc;modelSVMerr\r\n```\r\n\r\n**Summary of models prediction accuracies**\r\n\r\nThe below table shows the final results of prediction accuracies of each model on the test data set (*testTemp*).\r\n\r\n```{r}\r\ntblAcc \u003c- data.frame(\u0022Accuracy\u0022=c(modelRFacc, modelGBMacc, modelSVMacc), \u0022Error\u0022= c(modelRFerr,modelGBMerr,modelSVMerr), row.names = c(\u0022RF\u0022,\u0022GBM\u0022,\u0022SVM\u0022))\r\n\r\ntblAcc\r\n```\r\n\r\nThe summary results show that the SVM model was the best model with **`r modelSVMacc`** accuracy. Hence, the SVM model will be used to predict survival in the Kaggle test set.\r\n\r\n## Applying the selected model on the Kaggle test set\r\n\r\nNow we have trained, validated and selected the best model, we will use it to predict survival in the Kaggle test set.\r\n\r\n```{r}\r\nkaggle_test \u003c- predict(modelSVM, test)\r\n\r\nkaggle_Submit \u003c- data.frame(PassengerId = test$PassengerId, Survived = kaggle_test)\r\n\r\nsubSummary \u003c- table(kaggle_Submit$Survived)\r\nsubSummary\r\n\r\nsurvPerc \u003c- percent(subSummary[2]/(subSummary[1]+subSummary[2]))\r\n```\r\n\r\nWe can see that the selected SVM model predicted survival rate in the Kaggle test set to be **`r survPerc`** (**`r subSummary[2]`** passengers have been predicted to survive the tragedy compared to **`r subSummary[1]`** who could not make it).\r\n\r\n## Final submission file\r\n\r\nAt last, we can create the Kaggle submission csv file based on our prediction.\r\n\r\n```{r}\r\nwrite.csv(kaggle_Submit, file = \u0022Titanic.csv\u0022, row.names = FALSE)\r\n```\r\n\r\n# Conclusion\r\n\r\nThree algorithms were used to make final predictions of passengers’ survival in the Kaggle test data set. The algorithms used in survival prediction are: *Random Forest (RF)*, *Gradient Boosting Machine (GBM)*, and the *Support Vector Machine (SVM)*.\r\n\r\nAlthough both the RF and the SVM models were very competitive in terms of prediction accuracy, the SVM surpassed the RF with an accuracy of **`r modelSVMacc`** versus **`r modelRFacc`** for the RF.\r\n\r\nUsing the SVM model, survival rate in the Kaggle test data was predicted to be **`r survPerc`** (**`r subSummary[2]`** passengers have been predicted to survive the tragedy compared to **`r subSummary[1]`** who died).\r\n","dateCreated":"2018-02-09T04:57:32.503Z"},"kernelRun":{"id":2406024,"kernelId":618429,"status":"complete","type":"batch","sourceType":"script","language":"rmarkdown","title":"Titanic Tragedy: Survival Prediction with ML","dateCreated":"2018-02-09T04:57:32.503Z","dateEvaluated":"2018-02-09T04:57:32.65Z","workerContainerPort":null,"workerUptimeSeconds":1181480,"workerIPAddress":"172.16.1.8     ","scriptLanguageId":5,"scriptLanguageName":"RMarkdown","renderedOutputUrl":"https://www.kaggleusercontent.com/kf/2406024/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..aatDexVxojdGenshx0HpMA.HW4-cz9lKCSSZTQsKAVa_uBmLiWLa7MCJ7zsfc3OJPNLbXU9oO8lf_5PRrb96oZJRM8Nn3Hc4JOE2_6OHeZ0gfOyq_Y3DZf5JVDaxBZI7Wsb4kL7P6EakOmKS3D6k98uxv8alR0vXSZMr6xiKxVWD7yN210N1LO38zDWGVek60yZdxSplptUBg_CHcdyWolh.nW8MjMkFF7IEhmrustv7pw/__results__.html","commit":{"id":7878311,"settings":{"dockerImageVersionId":47,"dataSources":[{"sourceType":"Competition","sourceId":3136,"databundleVersionId":null}],"sourceType":"script","language":"rmarkdown","isGpuEnabled":false,"isInternetEnabled":false},"source":"---\r\ntitle: \u0022Titanic Tragedy: Survival Prediction with Machine Learning\u0022\r\nauthor: \u0022Mohammed K. Barakat\u0022\r\ndate: \u0022February 9, 2018\u0022\r\noutput:\r\n  html_document:\r\n    number_sections: yes\r\n    toc: yes\r\n  pdf_document:\r\n    toc: yes\r\n  word_document:\r\n    toc: yes\r\n---\r\n\r\n```{r setup, include=FALSE}\r\nknitr::opts_chunk$set(echo = TRUE)\r\n```\r\n\r\n# Introduction\r\n\r\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\r\n\r\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\r\n\r\nThis research tries to answer the question of *\u0022what sorts of people were likely to survive\u0022*. Using Machine Learning tools and techniques the research predicts which passengers survived the tragedy.\r\n\r\nMore about the *RMS Titanic* tragedy is available in [Wikipedia](https://en.wikipedia.org/wiki/RMS_Titanic)\r\n\r\n# Executive Summary\r\n\r\nThe research follows the *Reproducible Research* methodology. It starts by defining the objective, acquiring and defining input data, data cleaning, feature engineering, data exploration (EDA), and finally by model training, selection, and prediction.\r\n\r\nSeveral R packages are used in the research. Packages cover data visualization, data manipulation, model training and prediction, and text mining.\r\n\r\nSeveral prediction algorithms have been used. One of which was used to predict missing passenger ages (*Generalized Linear Model (glm)*). Other three algorithms were used to make final predictions of passengers’ survival in the Kaggle test data set. The algorithms used in survival prediction are: *Random Forest (RF)*, *Gradient Boosting Machine (GBM)*, and the *Support Vector Machine (SVM)*.\r\n\r\nAlthough both the *RF* and the *SVM* models were very competitive in terms of prediction accuracy, the *SVM* surpassed the *RF* with an accuracy of **83.9%** versus **82.5%** for the *RF*.\r\n\r\nThe research ends by predicting survival in the Kaggle test set where the survival rate was **37.3%** (**156** passengers have been predicted to survive the tragedy compared to **262** who could not make it).\r\n\r\n# Research Data\r\n\r\nData of the research has been obtained from *Kaggle* and is available under this [link](https://www.kaggle.com/c/titanic/data).\r\n\r\nThe data is split into two data sets:\r\n\r\n* **training set (train.csv)** which will be used to train the prediction models. Since the available testing set does not contain the *ground truth* of each passenger, this training set will be split into training (*trainTemp*) and testing (*testTemp*) data. The temporary training set will be used to train prediction models, whereas the temporary testing set will be used to validate model performance and, hence, selection of the best model before being used to predict passenger survivals in the Kaggle-provided testing set.\r\n\r\n* **testing set (test.csv)** in which the ground truth (outcome) is not provided. The research aims at predicting the outcome in this set. For each passenger in the test set, the best trained model will be used to predict whether or not passengers survived the sinking of the Titanic.\r\n\r\n## *R* Libraries\r\n\r\nThe research uses the libraries loaded below:\r\n\r\n```{r, message=FALSE, warning=FALSE}\r\nlibrary(knitr)\r\nlibrary(ggplot2)\r\nlibrary(ggthemes)\r\nlibrary(gridExtra)\r\nlibrary(scales)\r\nlibrary(dplyr)\r\nlibrary(qdap)\r\nlibrary(pROC)\r\nlibrary(caret)\r\nlibrary(qdap)\r\nlibrary(tm)\r\nlibrary(wordcloud)\r\n```\r\n\r\n## Importing Kaggle data sets\r\n\r\nKaggle *training* and *testing* data sets are imported into *R*. Then, both sets are combined into one set (**allSet**) for *Data Cleaning* and *Feature Engineering* before being used in model training and prediction.\r\n\r\n```{r}\r\ntrain \u003c- read.csv(\u0022../input/train.csv\u0022, stringsAsFactors = F)\r\ntest \u003c- read.csv(\u0022../input/test.csv\u0022, stringsAsFactors = F)\r\nallSet \u003c- bind_rows(train, test) # combining train and test data in one set\r\n```\r\n\r\n## Data dimensions and structure\r\n\r\n```{r}\r\ntrainDim \u003c- dim(train)\r\ntestDim \u003c- dim(test)\r\ntrainR \u003c- trainDim[1];testR \u003c- testDim[1];allSetC \u003c- dim(allSet)[2]\r\n```\r\n\r\nBy having a look at the dimensions of the training and testing data, we see that they have **`r trainR`** and **`r testR`** observations (passengers), respectively. And both of them have **`r allSetC`** columns (features).\r\n\r\nWe can also have a glimpse of the data structure. It is clear that we have features that are either numeric (integers and double) or characters. It\u0027s worth noticing that the outcome variable (*Survived*) is integer, which gives us a hint in the *Feature Engineering* section.\r\n\r\n```{r}\r\nglimpse(allSet)\r\n```\r\n\r\n## Data dictionary\r\n\r\nBelow is a description of each of the features existing in the data sets to give insight of what information we have about passengers.\r\n\r\nFeature|Description\r\n-------|-------------------------------------------------------\r\nPassengerId|Passenger Id\r\nSurvived|Survival (survived = 1, deceased = 0)\r\nPclass|Ticket class (upper = 1, middle = 2, lower = 3)\r\nName|Passenger name\r\nSex|Sex (male, female)\r\nAge|Age in years or fraction\r\nSibSp|Number of siblings / spouses aboard\r\nParch|Number of parents / children aboard\r\nTicket|Ticket number\r\nFare|Passenger fare\r\nCabin|Cabin number\r\nEmbarked|Port of Embarkation  (C = Cherbourg, Q = Queenstown, S = Southampton)\r\n-------------------------------------------------------------------------------\r\n\r\n# Data Cleaning\r\n\r\nBy exploring the data set, as shown in the below subset, it is realized that the data needs to be processed and cleaned. Data cleaning involves removing duplicates, handling missing values, or fixing features classes.\r\n\r\n```{r}\r\nkable(head(allSet))\r\n```\r\n\r\n## Removing duplicate observations\r\n\r\nSince the data sets represent passengers\u0027 details, they should not contain any duplicate record of passengers. *PassengerId* feature is best to be used for uniqueness of observations.\r\n\r\n```{r}\r\nifelse(length(unique(allSet[,1])) == nrow(allSet),\u0022No duplicates\u0022,\u0022Duplicates detected!\u0022)\r\n```\r\nBoth data sets (training and testing) represented by *allSet* do not contain any duplicate observations of passengers.\r\n\r\n## Fixing features classes\r\n\r\nFeatures in the data set must have the correct class (data type) to be used in analysis, model training and prediction. By studying the features classes, we find that *Survived*, *Pclass*, *Sex*, *Cabin*, and *Embarked* should be changed to *Factor* variables instead of characters.\r\n\r\n```{r}\r\nallSet$Survived \u003c- as.factor(allSet$Survived)\r\nallSet$Pclass \u003c- as.factor(allSet$Pclass)\r\nallSet$Sex \u003c- as.factor(allSet$Sex)\r\nallSet$Cabin \u003c- as.factor(allSet$Cabin)\r\nallSet$Embarked \u003c- as.factor(allSet$Embarked)\r\n```\r\n\r\n## Imputing missing values\r\n\r\nMissing values in data sets are usually problematic in data analysis, model training and prediction. Hence, we will replace missing values with NA\u0027s then impute them with reasonable values in necessary features.\r\n\r\nFirst, let\u0027s check the missing values in all variables and replace them with NA\u0027s.\r\n\r\n```{r missingValues}\r\n\r\n# replace missing values with NA across all features\r\nfor (i in 1:allSetC){\r\n  allSet[,i][allSet[,i]== \u0022\u0022] \u003c- NA\r\n}\r\n\r\n# define a function to get number of NAs in each feature\r\ngetNA \u003c- function(dt,NumCol){\r\n       varsNames \u003c- names(dt)\r\n        NAs \u003c- 0\r\n\r\n        for (i in 1:NumCol){\r\n          NAs \u003c- c(NAs, sum(is.na(dt[,i])))\r\n        }\r\n\r\n        NAs \u003c- NAs[-1]\r\n        names(NAs)\u003c- varsNames # make a vector of variable name and count of NAs\r\n\r\n        NAs \u003c- NAs[NAs \u003e 0]\r\n        NAs \r\n}\r\n\r\ngetNA(allSet,allSetC)\r\n\r\n```\r\n\r\nExcluding the Survived feature, as it is the outcome feature to be predicted in the testing data set, we have 4 variables with missing values; Age, Fare, Cabin, and Embarked.\r\n\r\nPredicting *Age* feature from the features available in the data set cannot be directly predicted. Yet, we will deal with it differently in *Feature Engineering* section. Besides, missing values in *Cabin* is huge (1014 out of 1309). Hence, we will exclude this feature from our model.\r\n\r\nWe are left with *Fare* and *Embarked* which both have reasonable number of missing values that we can impute from the given data.\r\n\r\n**Imputing Embarkation missing values**\r\n\r\nLooking at the data, we might be able to predict the passengers\u0027 missing embarkation ports based on their Fare and Class.\r\n\r\n```{r}\r\nallSet[,c(\u0022PassengerId\u0022,\u0022Pclass\u0022,\u0022Fare\u0022,\u0022Embarked\u0022)] %\u003e% filter(is.na(Embarked))\r\n```\r\n\r\nBoth passengers with missing embarkation ports have the same Fare and Class; $80 and 1, respectively.\r\n\r\nUsing the other passengers\u0027 data for Fare and Class, we will predict the embarkation ports of these two passengers.\r\n\r\n```{r, warning=FALSE}\r\n\r\n# filter for complete embarkation records\r\nFareClassComp \u003c- allSet %\u003e% filter(!is.na(Embarked))\r\n\r\n# plot embarkation ports versus fare mapped by passenger class\r\nFareClassComp %\u003e% \r\n        ggplot(aes(x = Embarked, y = Fare, fill = Pclass))+\r\n        geom_boxplot()+\r\n        geom_hline(aes(yintercept = 80),\r\n                   colour = \u0022red\u0022, linetype = \u0022dashed\u0022, lwd = 2)+\r\n        scale_y_continuous(labels = dollar_format())+\r\n        theme_few()\r\n\r\n```\r\n\r\nBased on the boxplot above, we can see that passengers who embarked off Cherbourg port (C) had paid $80 fare on average and had first class aboard. Hence, we can assume with high confidence that both passengers with missing embarkation values had embarked off the same port for having the same Fare and Class values.\r\n\r\n```{r}\r\n# impute missing embarkation values for both passengers\r\nallSet$Embarked[is.na(allSet$Embarked)] \u003c- \u0022C\u0022\r\n```\r\n\r\n**Imputing Fare missing values**\r\n\r\nNow, we have one passenger with missing Fare value.\r\n\r\n```{r}\r\nallSet[,c(\u0022PassengerId\u0022,\u0022Pclass\u0022,\u0022Fare\u0022,\u0022Embarked\u0022)] %\u003e% filter(is.na(Fare))\r\n```\r\n\r\nAs shown in the previous boxplot, we can use the median of the 3rd class passengers who had embarked off port Southampton (which is calculated to be $8.05) to predict the Fare value of passenger 1044.\r\n\r\n```{r}\r\nallSet$Fare[allSet$PassengerId == 1044] \u003c-  median(allSet$Fare[allSet$Pclass == 3 \u0026 allSet$Embarked == \u0022S\u0022], na.rm = T)\r\n```\r\n\r\n# Feature Engineering\r\n\r\n*Feature Engineering* simply refers to creating new features from the existing ones to improve model performance. The following sub-sections handle multiple feature engineering tasks that would make training the model possible, easier, and more accurate in prediction.\r\n\r\n## Creating the *Title* feature\r\n\r\nBy looking at the *Name* feature, we can notice that passenger titles are embedded into their names. A passenger name starts with the surname, then a comma and a space before the title that is followed by a period. E.g. (surname, Mr.). So, we can extract the title with some *Regex* manipulation as shown below.\r\n\r\n```{r}\r\nallSet$Title \u003c- gsub(\u0022(.*, )|(\\\\..*)\u0022,\u0022\u0022,allSet$Name)\r\n\r\n# tabulate titles versus sex\r\ntable(allSet$Sex, allSet$Title)\r\n```\r\n\r\nBy tabulating *Title* versus *Sex* we can find that titles (Capt, Col, Don, Jonkheer, Major, Master, Mr, Rev, Sir) all refer to males. Whereas, (Dona, Lady, Miss, Mlle, Mme, Mrs, Ms, the Countess) refer to female. \u0022Dr\u0022 refers to both.\r\n\r\nHence, we can reduce titles to only three: Mr, Mrs, and Miss according to Sex mapping.\r\n\r\n```{r}\r\nMrTitles \u003c- c(\u0022Capt\u0022, \u0022Col\u0022, \u0022Don\u0022, \u0022Jonkheer\u0022, \u0022Major\u0022, \u0022Master\u0022, \u0022Mr\u0022, \u0022Rev\u0022, \u0022Sir\u0022)\r\nMrsTitles \u003c- c(\u0022Dona\u0022, \u0022Lady\u0022, \u0022Mme\u0022, \u0022Mrs\u0022, \u0022the Countess\u0022)\r\nMissTitles \u003c- c(\u0022Miss\u0022, \u0022Mlle\u0022, \u0022Ms\u0022)\r\n\r\nallSet$Title[allSet$Title %in% MrTitles] \u003c- \u0022Mr\u0022\r\nallSet$Title[allSet$Title %in% MrsTitles] \u003c- \u0022Mrs\u0022\r\nallSet$Title[allSet$Title %in% MissTitles] \u003c- \u0022Miss\u0022\r\nallSet$Title[allSet$Title == \u0022Dr\u0022 \u0026 allSet$Sex == \u0022female\u0022] \u003c- \u0022Mrs\u0022\r\nallSet$Title[allSet$Title == \u0022Dr\u0022 \u0026 allSet$Sex == \u0022male\u0022] \u003c- \u0022Mr\u0022\r\n\r\nallSet$Title \u003c- as.factor(allSet$Title)\r\ntable(allSet$Title)\r\n```\r\n\r\n## Creating the family size (*FamSz*) feature\r\n\r\nBy knowing the siblings/spouses and parches count, we might be able to know the passenger family size. So, we will create a new feature (*FamSz*) by adding the count of siblings and parches to the passenger to know the passenger family size aboard.\r\n\r\n```{r}\r\nallSet$FamSz \u003c- allSet$SibSp + allSet$Parch + 1\r\n```\r\n\r\n## Creating the family size categories (*FamSzCat*)\r\n\r\nAlso, it might be useful to categorize family sizes for easier model training and prediction. So, we will categorize family sizes into *Singles*, *Small*, and *Large*.\r\n\r\n```{r}\r\nallSet$FamSzCat[allSet$FamSz == 1] \u003c- \u0022Singles\u0022\r\nallSet$FamSzCat[allSet$FamSz \u003e 1 \u0026 allSet$FamSz \u003c5] \u003c- \u0022Small\u0022\r\nallSet$FamSzCat[allSet$FamSz \u003e 4] \u003c- \u0022Large\u0022\r\n\r\nallSet$FamSzCat \u003c- as.factor(allSet$FamSzCat)\r\n```\r\n\r\n## Creating the *Surname* feature\r\n\r\nAlthough not all features might be used in prediction, it is worthy to keep/create some features to further explore the data. And in this research it could be interesting to know statistics about families who faced this tragedy. So, we will create a new feature (*Surname*) which gives insight into families onboard Titanic.\r\n\r\n```{r}\r\nallSet$Surname \u003c- sapply(allSet$Name, function(x) strsplit(x, split = \u0022[,]\u0022)[[1]][1])\r\npaste(nlevels(factor(allSet$Surname)), \u0022families were onboard Titanic\u0022)\r\n```\r\n\r\nWe will explore their distribution in the Exploratory Data Analysis (EDA).\r\n\r\n## Creating the age stage *AgeStg* feature\r\n\r\nAs mentioned earlier, predicting the exact age of passengers with missing ages is risky and commits high inaccuracy. Yet, age is believed to have high influence on prediction results of survivals. So, *Age* will be used to create the Age Stage (*AgeStg*) feature which identifies a passenger as a child or an adult. Then, missing age stages will be easier to predict using other features. Passengers below 18 years old will be assigned *Child* values. Whereas those above 18 will be *Adult*.\r\n\r\n```{r}\r\nallSet$AgeStg[allSet$Age \u003c 18 \u0026 !is.na(allSet$Age)] \u003c- \u0022Child\u0022\r\nallSet$AgeStg[allSet$Age \u003e= 18 \u0026 !is.na(allSet$Age)] \u003c- \u0022Adult\u0022\r\n```\r\n\r\n**Imputing missing Age Stage values**\r\n\r\nHere, we go back to Data Cleaning phase as we have a new feature with missing values. We have 263 missing Age Stage values. Which is the same number of missing *Age* values that we noticed in the Data Cleaning phase.\r\n\r\n```{r}\r\nlength(allSet$AgeStg[is.na(allSet$AgeStg)])\r\n```\r\n\r\nTo impute missing values in age stage, we will use one of the modeling algorithms. Since age stage has two possible outcomes (Child and Adult) we will try the *Generalized Linear Model (glm)* with *Forward Stepwise* method to predict the missing values with the best set of features as predictors.\r\n\r\nAs with any Machine Learning tasks, we will combine all data having non-missing age stage values in one data set then split it into training and testing data sets to test for prediction accuracy. Then, we can predict the missing values using the model.\r\n\r\nSince some variables are obviously not taking part in deciding the age stage, we will exclude them from the model upfront. Hence, we will use the following prospect features in the model: *Pclass*, *Sex*, *SibSp*, *Parch*, *Fare*, *FamSz*, and *FamSzCat*.\r\n\r\n\r\n```{r}\r\n# create a vector with the prospect features including AgeStg and PassengerId\r\nvarsNames \u003c- c(\u0022PassengerId\u0022,\u0022Pclass\u0022, \u0022Sex\u0022, \u0022SibSp\u0022, \u0022Parch\u0022, \u0022Fare\u0022, \u0022FamSz\u0022, \u0022FamSzCat\u0022, \u0022AgeStg\u0022)\r\n\r\n# subset the data by the selected features\r\nallSetAgeStg \u003c- allSet[,varsNames]\r\n\r\n# subset into two sets: one with age stage complete, and one with age stage missing\r\nallSetAgeStgComp \u003c- allSetAgeStg[!is.na(allSetAgeStg$AgeStg),]\r\nallSetAgeStgMiss \u003c- allSetAgeStg[is.na(allSetAgeStg$AgeStg),]\r\n\r\n# split the data set with complete age stage into train and test data sets (75/25 ratio)\r\n## number of training rows\r\nnTrain \u003c- 0.75 * nrow(allSetAgeStgComp)\r\n\r\n## sample row IDs\r\nset.seed(3030)\r\nsampleTrain \u003c- sample(nrow(allSetAgeStgComp),nTrain)\r\n\r\n## create train and test data sets\r\nAgeStgTrain \u003c- allSetAgeStgComp[sampleTrain,]\r\nAgeStgTest \u003c- allSetAgeStgComp[-sampleTrain,]\r\n\r\n# use the glm Logistic Regression model to predict the age stage. Use Forward Stepwise algorithm to select the best predictors.\r\n\r\n# build the null model with no predictors\r\nset.seed(3030)\r\nnull_model \u003c- glm(factor(AgeStg)~1, data = AgeStgTrain, family = \u0022binomial\u0022)\r\n\r\n# build the full model with all predictors\r\nset.seed(3030)\r\nfull_model \u003c- glm(factor(AgeStg)~Pclass+Sex+SibSp+Parch+Fare+FamSz+FamSzCat, data = AgeStgTrain, family = \u0022binomial\u0022)\r\n\r\n# perform forward stepwise algorithm to get an economic model with best predictors\r\nstep_model \u003c- step(null_model, scope = list(lower= null_model,upper = full_model),direction = \u0022forward\u0022)\r\n\r\n# estimate the stepwise age stage probability in training and testing data\r\nAgeStgTrain$stepProb \u003c- predict(step_model, data = AgeStgTrain, type = \u0022response\u0022)\r\nAgeStgTest$stepProb \u003c- predict(step_model, newdata = AgeStgTest, type = \u0022response\u0022)\r\n\r\n# create the ROC curve of the stepwise for training and testing data\r\nROC_train \u003c- roc(AgeStgTrain$AgeStg,AgeStgTrain$stepProb)\r\nROC_test \u003c- roc(AgeStgTest$AgeStg,AgeStgTest$stepProb)\r\n\r\n# Plot the ROC of the stepwise model: training and testing\r\nplot(ROC_train,col = \u0022red\u0022)\r\nplot(ROC_test,col = \u0022red\u0022)\r\n\r\n# calculate Area Under the Curve (AUC): training and testing\r\nauc(ROC_train);auc(ROC_test)\r\ntrainAcc \u003c- percent(auc(ROC_train));testAcc \u003c- percent(auc(ROC_test))\r\n```\r\n\r\nUsing the Generalized Linear Model (glm) with Forward Stepwise we get an in-sample accuracy of **`r trainAcc`** and an out-of-sample accuracy of **`r testAcc`**.\r\n\r\nNext, using the Forward Step glm model, we will predict values of age stage (*AgeStg*) in the test data set and validate its accuracy before predicting the actual missing values in the entire data set.\r\n\r\n```{r}\r\n# find the average number of children to set it as threshold of probability\r\nAvgChildCount \u003c- mean(AgeStgTrain$AgeStg == \u0022Child\u0022)\r\n\r\n# Predict Age Stage in testing data if its probability is greater than the average \r\nAgeStgTest$AgeStgPred \u003c- ifelse(AgeStgTest$stepProb \u003e AvgChildCount,\u0022Child\u0022, \u0022Adult\u0022)\r\n\r\n# check accuracy of prediction in testing data\r\nacc \u003c- percent(mean(AgeStgTest$AgeStg == AgeStgTest$AgeStgPred))\r\nacc\r\n```\r\n\r\nAfter applying prediction model to the testing data, accuracy of prediction turned out to be **`r acc`**.\r\n\r\nFinally, we will predict the actual missing values of age stage in the entire data set.\r\n\r\n```{r}\r\n# predicting missing Age Stage using the stepwise, logistic regression model\r\nallSetAgeStgMiss$stepProb \u003c- predict(step_model, newdata = allSetAgeStgMiss, type = \u0022response\u0022)\r\n\r\nallSetAgeStgMiss$AgeStg \u003c- ifelse(allSetAgeStgMiss$stepProb \u003e AvgChildCount,\u0022Child\u0022, \u0022Adult\u0022)\r\n\r\n# update missing Age Stage in the full data\r\nallSet \u003c- left_join(allSet,allSetAgeStgMiss[,c(\u0022PassengerId\u0022,\u0022AgeStg\u0022)], by = \u0022PassengerId\u0022, allSet.x = TRUE, allSet.y = FALSE)\r\n\r\nallSet$AgeStg \u003c- ifelse(is.na(allSet$AgeStg.x),allSet$AgeStg.y,allSet$AgeStg.x)\r\nallSet \u003c- allSet[,!colnames(allSet) %in% c(\u0022AgeStg.x\u0022,\u0022AgeStg.y\u0022)]   \r\n\r\nallSet$AgeStg \u003c- as.factor(allSet$AgeStg)\r\n```\r\n\r\nConfirm that we have no missing values in the data set.\r\n\r\n```{r}\r\ngetNA(allSet,length(allSet))\r\n```\r\n\r\nAs mentioned earlier, *Survived* is the outcome feature to predict, *Age* will be replaced by *AgeStg*, and *Cabin* will not be used for huge number of missing values. Hence, we have all missing values imputed in features necessary for training and prediction.\r\n\r\n# Exploratory Data Analysis (EDA)\r\n\r\nIn this section we try to explore characteristics of the Kaggle training data using descriptive statistics and plots. First, we will revert back to the original training data and split it from the testing data set.\r\n\r\n```{r}\r\ntrain \u003c- allSet[!is.na(allSet$Survived),]\r\ntest \u003c- allSet[is.na(allSet$Survived),]\r\n```\r\n\r\n## Exploring the outcome feature (*Survived*)\r\n\r\n```{r, warning=FALSE}\r\ntrain %\u003e% \r\nggplot(aes(x=Survived, fill = Survived))+\r\n        geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022Survival in the Titanic tragedy\u0022)+\r\n        geom_label(stat=\u0027count\u0027,aes(label=..count..))+\r\n        labs(fill = \u0022Survival (0 = died, 1 = survived)\u0022)\r\n```\r\n\r\n```{r, warning = FALSE}\r\nsurvSumy \u003c- summary(train$Survived)\r\ndied \u003c- survSumy[[1]][1];suvd \u003c- survSumy[[2]][1];surPerc \u003c- percent(suvd/sum(suvd,died))\r\n```\r\n\r\nLooking at the histogram and the summary statistics of the train data, we can see that passengers who survived the tragedy were less than those who died. The survived were **`r suvd`**, while the dead were **`r died`**. The survival rate was **`r surPerc`**.\r\n\r\nLet\u0027s have a look at how the outcome feature (*Survival*) is related to some of the important features in the train data set.\r\n\r\n```{r, warning=FALSE}\r\np1 \u003c- ggplot(train,aes(x=Survived, fill=Pclass))+\r\n  geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022P1: Survival vs Class\u0022)\r\n\r\np2 \u003c- ggplot(train,aes(x=Survived, fill=Sex))+\r\n  geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022P2: Survival vs Sex\u0022)\r\n\r\np3 \u003c- ggplot(train,aes(x= Survived, fill = AgeStg))+\r\n  geom_histogram(stat = \u0022count\u0022, position = \u0022dodge\u0022)+\r\n        labs(x = \u0022P3: Survival vs Age Stage\u0022)\r\n\r\np4 \u003c- ggplot(train,aes(x=Survived, fill=Embarked))+\r\n  geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022P4: Survival vs Embarkment Port\u0022)\r\n\r\np5 \u003c- ggplot(train,aes(x= Survived, y = Fare))+\r\n  geom_boxplot()+\r\n        labs(x = \u0022P5: Survival vs Fare\u0022)\r\n\r\np6 \u003c- ggplot(train,aes(x= Survived, fill = FamSzCat))+\r\n  geom_histogram(stat = \u0022count\u0022)+\r\n        labs(x = \u0022P6: Survival vs Category of Family Size\u0022)\r\n\r\np7 \u003c- ggplot(train, aes(x = FamSz, fill = Survived)) +\r\n        geom_bar(stat=\u0027count\u0027, position=\u0027dodge\u0027) +\r\n        scale_x_continuous(breaks=c(1:11)) +\r\n        labs(x = \u0027P7: Survival vs Family Size\u0027)\r\n\r\ngrid.arrange(p1,p2,p3,p4,p5,p6,p7,ncol=2)\r\n\r\n```\r\n\r\n**Observations from the plots:**\r\n\r\n* Most deceased passengers were from the lower ticket class (class 3). While survived passengers were almost equally distributed over the three tickets classes.[*Ref. plot:p1*]\r\n\r\n* The majority of deceased passengers were males. And those survived mostly were females. This goes in line with the belief that women (and children) were given the priority to jump to the life boats. [*Ref. plot:p2*]\r\n\r\n* Adult passengers who did not make it in the tragedy were around 5 times the deceased children. And those adults survived, who probably were from upper classes, were around 3 times the survived children. Which again proves that children were given priority over adults-maybe except for some of those with higher classes! [*Ref. plot:p3*]\r\n\r\n* It is clear that most passengers (died and survived) had departed from Southampton port. Distribution of ports over both categories of passengers is almost similar which makes it difficult to be a predicting feature of survival. [*Ref. plot:p4*]\r\n\r\n* The median of deceased passengers fare was slightly smaller than that of passengers who survived. Obviously, those with higher fares could make it as they were from the fortunate higher classes. [*Ref. plot:p5*]\r\n\r\n* It seems that RMS Titanic was mostly a voyage of singletons. However, those unfortunates had lower odds to survive over the small and large families. [*Ref. plot:p6*]\r\n\r\n* In addition to singletons, families with more than 4 members had lower odds to survive than those with 4 and less family members. Maybe this was due to the time wasted by family members looking for each other and trying to get together during the time of sinking. [*Ref. plot:p7*]\r\n\r\n**A peek into Surnames**\r\n\r\nLet\u0027s have a look at passengers\u0027 surnames and see their numbers onboard the Titanic.\r\n\r\n```{r, warning= FALSE}\r\nsurN \u003c- train %\u003e% \r\n        group_by(Surname) %\u003e% \r\n        summarize(count = n()) %\u003e% \r\n        arrange(desc(count))\r\n\r\ncountSurN \u003c- nrow(surN)\r\n        \r\nsurN[1:30,]%\u003e% \r\nggplot(aes(x=reorder(Surname, count),y=count))+\r\n        geom_bar(stat = \u0022identity\u0022)+\r\n        scale_y_continuous(breaks = c(1:10))+\r\n        labs(x = \u0022Surnames\u0022, y = \u0022Number of Passengers\u0022)+\r\n        coord_flip()\r\n``` \r\n\r\nWell, the training data set shows that the **`r trainR`** passengers were grouped into **`r countSurN`** surnames. However, there was no clear skewness towards specific surnames. As shown in the bar chart above, the range of passengers having similar surnames was from 1 to 9 passengers.\r\n\r\n*Andersson* was the highest common surname with 9 persons, followed by *Sage* with 7 names, then a few groups having 6 passengers with common surnames such as *Skoog*, *Panula* and *Johnson*. The rest of passengers are groups of 5 or less members having similar surnames.\r\n\r\nThis can also be noticed in the **wordcloud** figure shown below with more frequent surnames highlighted in larger and bolder text.\r\n \r\n```{r, warning= FALSE}\r\nset.seed(4040)\r\nEmbk_source \u003c- VectorSource(train$Surname)\r\n \r\nEmbk_corp \u003c- VCorpus(Embk_source)\r\n\r\nEmbk_tdm \u003c- TermDocumentMatrix(Embk_corp)\r\nEmbk_m \u003c- as.matrix(Embk_tdm)\r\n\r\nterm_freq \u003c- rowSums(Embk_m)\r\nterm_freq \u003c- sort(term_freq, decreasing = TRUE)\r\nword_freqs \u003c- data.frame(term = names(term_freq), num = term_freq)\r\n\r\nwordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = \u0022blue\u0022)\r\n\r\n```\r\n\r\n\r\n# Predicting Survival in the Testing Data Set\r\n\r\nNow, having all features engineered, and having done the EDA, we can start with the following features as preliminary set of predictors in our training model:\r\n\r\n* **Pclass**\r\n* **Sex**\r\n* **SibSp**\r\n* **Parch**\r\n* **Fare**\r\n* **Embarked**\r\n* **Title**\r\n* **FamSzCat**\r\n* **AgeStg**\r\n\r\n## Checking features variability\r\n\r\nIt is a good practice to make sure the training data does not include predictors with no variability. I.e. predictors that have one or very few unique values relative to the number of observations. This can be detected with the *nzv* value of the *NearZeroVar* function results.\r\n\r\n```{r}\r\n# create a vector with the selected features\r\nselVarsNames \u003c- c(\u0022Pclass\u0022, \u0022Sex\u0022, \u0022SibSp\u0022, \u0022Parch\u0022, \u0022Fare\u0022,\u0022Embarked\u0022 ,\u0022Title\u0022, \u0022FamSzCat\u0022, \u0022AgeStg\u0022)\r\n\r\nnearZeroVar(train[,selVarsNames], saveMetrics = TRUE)\r\n```\r\n\r\nAs shown above all selected predictors have **FALSE** nzv value which indicates that all of them have reasonable variability in the dataset.\r\n\r\n## Prediction Algorithms\r\n\r\nSince the outcome (*Survived*) is a binary variable our prediction model should be based on one of those algorithms which are able to model categorical rather than regression data such as Random Forest, Gradient Boosting Machine (GBM), and Support Vector Machine (SVM).\r\n\r\nTo make sure we select the best model, we will use these three algorithms to train our model then make a comparison on prediction accuracy amongst them to select the best one.\r\n\r\nIn order to be able to validate the trained models and get the *out-of-sample* accuracy before predicting the Kaggle test data we will split the train data set into *trainTemp* and *testTemp* data sets. Train the models on the former, and test on the latter. Then, after selecting the best model, we will predict survival in the original test set given by Kaggle.\r\n\r\n**Using the Random Forest Algorithm**\r\n\r\nFirst, split train data set into *trainTemp* and *testTemp* data sets.\r\n\r\n```{r}\r\n# split the train data set into train and test data sets (75/25 ratio).\r\n\r\n## number of training rows\r\nnTrain \u003c- round(0.75 * nrow(train))\r\n\r\n## sample row IDs\r\nsampleTrain \u003c- sample(nrow(train),nTrain)\r\n\r\n## create trainTemp and testTemp data sets\r\ntrainTemp \u003c- train[sampleTrain,]\r\ntestTemp \u003c- train[-sampleTrain,]\r\n```\r\n\r\nNext, build the training models.\r\n\r\n```{r}\r\nset.seed(2020)\r\n\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, number = 10, repeats = 3)\r\nmodelRF \u003c- train(Survived~Pclass+Sex+SibSp+Parch+Fare+Embarked+Title+FamSzCat+AgeStg, data = trainTemp, method = \u0022rf\u0022, trControl = control)\r\n\r\nprint(modelRF)\r\n```\r\n\r\n**Using Gradient Boosting Machine (GBM) Algorithm**\r\n\r\n```{r}\r\nset.seed(2020)\r\n\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, number = 10, repeats = 3)\r\nmodelGBM \u003c- train(Survived~Pclass+Sex+SibSp+Parch+Fare+Embarked+Title+FamSzCat+AgeStg, data = trainTemp, method = \u0022gbm\u0022, trControl = control, verbose = FALSE)\r\n\r\nprint(modelGBM)\r\n```\r\n\r\n**Using Support Vector Machine (SVM) Algorithm**\r\n\r\n```{r}\r\nset.seed(2020)\r\n\r\ncontrol \u003c- trainControl(method = \u0022repeatedcv\u0022, number = 10, repeats = 3)\r\nmodelSVM \u003c- train(Survived~Pclass+Sex+SibSp+Parch+Fare+Embarked+Title+FamSzCat+AgeStg, data = trainTemp, method = \u0022svmRadial\u0022, trControl = control)\r\n\r\nprint(modelSVM)\r\n```\r\n\r\n## Models comparison and selection\r\n\r\nNow, we will compare the results of the three models using the *caret* package. The comparison process is built around getting an idea of the spread of the models accuracies. Since each model is evaluated using 3 repeats of 10-fold cross validation, the model will have 30 results (3 repeats of 10-fold cross validation). The objective of comparing results is to compare the accuracy distributions (30 values) between the models. Accuracy distributions will be summarized as box plots and dot plots.\r\n\r\n*Ref.* [Compare Models And Select The Best Using The Caret R Package](https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/)\r\n\r\n```{r}\r\n# collect resamples\r\nresults \u003c- resamples(list(RF=modelRF, GBM=modelGBM, SVM = modelSVM))\r\n\r\n# summarize the distributions\r\ncompSummary \u003c- summary(results)\r\ncompSummary\r\n\r\nmodelRFAcc \u003c- percent(median(compSummary$values$`RF~Accuracy`))\r\nmodelGBMAcc \u003c- percent(median(compSummary$values$`GBM~Accuracy`))\r\nmodelSVMAcc \u003c- percent(median(compSummary$values$`SVM~Accuracy`))\r\n\r\n# boxplots of results\r\nbwplot(results)\r\n\r\n# dot plots of results\r\ndotplot(results)\r\n```\r\n\r\nComparison results of accuracies amongst the three models show that they are very close in prediction performance. *Random Forest* and *SVM* are very competitive having median accuracies of **`r modelRFAcc`** and **`r modelSVMAcc`**, respectively. While *GBM* is slightly lower with median accuracy of **`r modelGBMAcc`**.\r\n\r\n**Models validation**\r\n\r\nAs mentioned earlier, we will validate the models on the testing data set (*testTemp*), then select the best model to predict survival in the Kaggle test set. Here, we will use the *Confusion Matrix* for each model to decide on the highest accuracy obtained and, hence, the best model to adopt.\r\n\r\n**Prediction using the Random Forest model**\r\n\r\n```{r}\r\nrfPred\u003c-predict(modelRF,testTemp)\r\nrfCM\u003c-confusionMatrix(rfPred,testTemp$Survived)\r\nrfCM\r\n\r\nmodelRFacc\u003c-percent(as.numeric(rfCM$overall[1]))\r\nmodelRFerr\u003c-percent(1-(as.numeric(rfCM$overall[1])))\r\n\r\nmodelRFacc;modelRFerr\r\n```\r\n\r\n**Prediction using the Gradient Boosting Machine (GBM) model**\r\n\r\n```{r}\r\ngbmPred\u003c-predict(modelGBM,testTemp)\r\ngbmCM\u003c-confusionMatrix(gbmPred,testTemp$Survived)\r\ngbmCM\r\n\r\nmodelGBMacc\u003c-percent(as.numeric(gbmCM$overall[1]))\r\nmodelGBMerr\u003c-percent(1-(as.numeric(gbmCM$overall[1])))\r\n\r\nmodelGBMacc;modelGBMerr\r\n```\r\n\r\n**Prediction using the Support Vector Machine (SVM)**\r\n\r\n```{r}\r\nsvmPred\u003c-predict(modelSVM,testTemp)\r\nsvmCM\u003c-confusionMatrix(svmPred,testTemp$Survived)\r\nsvmCM\r\n\r\nmodelSVMacc\u003c-percent(as.numeric(svmCM$overall[1]))\r\nmodelSVMerr\u003c-percent(1-(as.numeric(svmCM$overall[1])))\r\n\r\nmodelSVMacc;modelSVMerr\r\n```\r\n\r\n**Summary of models prediction accuracies**\r\n\r\nThe below table shows the final results of prediction accuracies of each model on the test data set (*testTemp*).\r\n\r\n```{r}\r\ntblAcc \u003c- data.frame(\u0022Accuracy\u0022=c(modelRFacc, modelGBMacc, modelSVMacc), \u0022Error\u0022= c(modelRFerr,modelGBMerr,modelSVMerr), row.names = c(\u0022RF\u0022,\u0022GBM\u0022,\u0022SVM\u0022))\r\n\r\ntblAcc\r\n```\r\n\r\nThe summary results show that the SVM model was the best model with **`r modelSVMacc`** accuracy. Hence, the SVM model will be used to predict survival in the Kaggle test set.\r\n\r\n## Applying the selected model on the Kaggle test set\r\n\r\nNow we have trained, validated and selected the best model, we will use it to predict survival in the Kaggle test set.\r\n\r\n```{r}\r\nkaggle_test \u003c- predict(modelSVM, test)\r\n\r\nkaggle_Submit \u003c- data.frame(PassengerId = test$PassengerId, Survived = kaggle_test)\r\n\r\nsubSummary \u003c- table(kaggle_Submit$Survived)\r\nsubSummary\r\n\r\nsurvPerc \u003c- percent(subSummary[2]/(subSummary[1]+subSummary[2]))\r\n```\r\n\r\nWe can see that the selected SVM model predicted survival rate in the Kaggle test set to be **`r survPerc`** (**`r subSummary[2]`** passengers have been predicted to survive the tragedy compared to **`r subSummary[1]`** who could not make it).\r\n\r\n## Final submission file\r\n\r\nAt last, we can create the Kaggle submission csv file based on our prediction.\r\n\r\n```{r}\r\nwrite.csv(kaggle_Submit, file = \u0022Titanic.csv\u0022, row.names = FALSE)\r\n```\r\n\r\n# Conclusion\r\n\r\nThree algorithms were used to make final predictions of passengers’ survival in the Kaggle test data set. The algorithms used in survival prediction are: *Random Forest (RF)*, *Gradient Boosting Machine (GBM)*, and the *Support Vector Machine (SVM)*.\r\n\r\nAlthough both the RF and the SVM models were very competitive in terms of prediction accuracy, the SVM surpassed the RF with an accuracy of **`r modelSVMacc`** versus **`r modelRFacc`** for the RF.\r\n\r\nUsing the SVM model, survival rate in the Kaggle test data was predicted to be **`r survPerc`** (**`r subSummary[2]`** passengers have been predicted to survive the tragedy compared to **`r subSummary[1]`** who died).\r\n","dateCreated":"2018-02-09T04:57:32.503Z"},"resources":null,"isolatorResults":"\u003cresults\u003e\u003cdisk_kb_free\u003e941004\u003c/disk_kb_free\u003e\u003cdocker_image_digest\u003e52937f97db1d60cc30110f82402aec2d3dec26569f3ae4239adbe334f10492ad\u003c/docker_image_digest\u003e\u003cdocker_image_id\u003esha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a\u003c/docker_image_id\u003e\u003cdocker_image_name\u003egcr.io/kaggle-images/rstats\u003c/docker_image_name\u003e\u003cexit_code\u003e0\u003c/exit_code\u003e\u003cfailure_message /\u003e\u003cinvalid_path_errors\u003eFalse\u003c/invalid_path_errors\u003e\u003cout_of_memory\u003eFalse\u003c/out_of_memory\u003e\u003crun_time_seconds\u003e117.187072226079\u003c/run_time_seconds\u003e\u003csucceeded\u003eTrue\u003c/succeeded\u003e\u003ctimeout_exceeded\u003eFalse\u003c/timeout_exceeded\u003e\u003cused_all_space\u003eFalse\u003c/used_all_space\u003e\u003cwas_killed\u003eFalse\u003c/was_killed\u003e\u003c/results\u003e","runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageDigest":"52937f97db1d60cc30110f82402aec2d3dec26569f3ae4239adbe334f10492ad","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"kaggle/rstats","diskKbFree":941004,"failureMessage":"","exitCode":0,"queuedSeconds":0,"outputSizeBytes":0,"runTimeSeconds":117.187072226079,"usedAllSpace":false,"timeoutExceeded":false,"isValidStatus":false,"wasGpuEnabled":false,"wasInternetEnabled":false,"outOfMemory":false,"invalidPathErrors":false,"succeeded":true,"wasKilled":false},"outputFilesTotalSizeBytes":4247063,"dockerImageVersionId":47,"usedCustomDockerImage":false},"author":{"id":1560793,"displayName":"Mohammed Barakat","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"mohdbarakat","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1560793-gr.jpg","profileUrl":"/mohdbarakat","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":1,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isTvc":false,"isKaggleBot":false,"isAdminOrTvc":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"activationCode":"00000000-0000-0000-0000-000000000000","isPhoneVerified":false},"baseUrl":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml","collaborators":{"owner":{"userId":1560793,"groupId":null,"groupMemberCount":null,"profileUrl":"/mohdbarakat","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1560793-gr.jpg","name":"Mohammed Barakat","slug":"mohdbarakat","userTier":1,"joinDate":null,"type":"owner","isUser":true,"isGroup":false},"collaborators":[]},"initialTab":null,"log":"[{\n  \u0022data\u0022: \u0022\\n\\nprocessing file: script.Rmd\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 3.2116560260765254\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |                                                                 |   0%\\r  |                                                                       \\r  |.                                                                |   1%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 3.406149944057688\n},{\n  \u0022data\u0022: \u0022  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.                                                                |   2%\\nlabel: setup (with options) \\nList of 1\\n $ include: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 3.441033319104463\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..                                                               |   3%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 3.5084144540596753\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...                                                              |   5%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 3.542484564939514\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-1 (with options) \\nList of 2\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 3.661018718034029\n},{\n  \u0022data\u0022: \u0022 $ message: logi FALSE\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 3.694434948032722\n},{\n  \u0022data\u0022: \u0022\\nAttaching package: \u0027dplyr\u0027\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.264091358985752\n},{\n  \u0022data\u0022: \u0022The following object is masked from \u0027package:gridExtra\u0027:\\n\\n    combine\\n\\nThe following objects are masked from \u0027package:stats\u0027:\\n\\n    filter, lag\\n\\nThe following objects are masked from \u0027package:base\u0027:\\n\\n    intersect, setdiff, setequal, union\\n\\nLoading required package: qdapDictionaries\\nLoading required package: qdapRegex\\n\\nAttaching package: \u0027qdapRegex\u0027\\n\\nThe following object is masked from \u0027package:dplyr\u0027:\\n\\n    explain\\n\\nThe following object is masked from \u0027package:ggplot2\u0027:\\n\\n    %+%\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.298537277150899\n},{\n  \u0022data\u0022: \u0022Loading required package: qdapTools\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.335159840993583\n},{\n  \u0022data\u0022: \u0022\\nAttaching package: \u0027qdapTools\u0027\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.500951718073338\n},{\n  \u0022data\u0022: \u0022The following object is masked from \u0027package:dplyr\u0027:\\n\\n    id\\n\\nLoading required package: RColorBrewer\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 4.535168725065887\n},{\n  \u0022data\u0022: \u0022\\nAttaching package: \u0027qdap\u0027\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 5.484502806095406\n},{\n  \u0022data\u0022: \u0022The following object is masked from \u0027package:dplyr\u0027:\\n\\n    %\u003e%\\n\\nThe following object is masked from \u0027package:base\u0027:\\n\\n    Filter\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 5.519316463032737\n},{\n  \u0022data\u0022: \u0022Type \u0027citation(\\\u0022pROC\\\u0022)\u0027 for a citation.\\n\\nAttaching package: \u0027pROC\u0027\\n\\nThe following objects are masked from \u0027package:stats\u0027:\\n\\n    cov, smooth, var\\n\\nLoading required package: lattice\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 5.552782748127356\n},{\n  \u0022data\u0022: \u0022Loading required package: NLP\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 8.26037413510494\n},{\n  \u0022data\u0022: \u0022\\nAttaching package: \u0027NLP\u0027\\n\\nThe following object is masked from \u0027package:qdap\u0027:\\n\\n    ngrams\\n\\nThe following object is masked from \u0027package:ggplot2\u0027:\\n\\n    annotate\\n\\n\\nAttaching package: \u0027tm\u0027\\n\\nThe following objects are masked from \u0027package:qdap\u0027:\\n\\n    as.DocumentTermMatrix, as.TermDocumentMatrix\\n\\nLoading required package: methods\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 8.294686164939776\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |....                                                             |   6%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....                                                             |   7%\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.322208925150335\n},{\n  \u0022data\u0022: \u0022\\nlabel: unnamed-chunk-2\\n\\r  |                                                                       \\r  |.....                                                            |   8%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......                                                           |   9%\\nlabel: unnamed-chunk-3\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.355495776049793\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.......                                                          |  10%\\n   inline R code fragments\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.847659141058102\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.......                                                          |  11%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.891579254064709\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-4\\n\\r  |                                                                       \\r  |........                                                         |  13%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.........                                                        |  14%\\nlabel: unnamed-chunk-5\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.924797812942415\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..........                                                       |  15%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........                                                       |  16%\\nlabel: unnamed-chunk-6\\n\\r  |                                                                       \\r  |...........                                                      |  17%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............                                                     |  18%\\nlabel: unnamed-chunk-7\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.957830253057182\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............                                                    |  20%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.............                                                    |  21%\\nlabel: missingValues\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 8.991017455933616\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............                                                   |  22%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............                                                  |  23%\\nlabel: unnamed-chunk-8\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 9.024487727088854\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |................                                                 |  24%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................                                                 |  25%\\nlabel: unnamed-chunk-9 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 9.057833350030705\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.................                                                |  26%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.008189562009647\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................                                               |  28%\\nlabel: unnamed-chunk-10\\n\\r  |                                                                       \\r  |...................                                              |  29%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................                                              |  30%\\nlabel: unnamed-chunk-11\\n\\r  |                                                                       \\r  |....................                                             |  31%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....................                                            |  32%\\nlabel: unnamed-chunk-12\\n\\r  |                                                                       \\r  |......................                                           |  33%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......................                                           |  34%\\nlabel: unnamed-chunk-13\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.043057931121439\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.......................                                          |  36%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........................                                         |  37%\\nlabel: unnamed-chunk-14\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.076485828030854\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................                                        |  38%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.........................                                        |  39%\\nlabel: unnamed-chunk-15\\n\\r  |                                                                       \\r  |..........................                                       |  40%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...........................                                      |  41%\\nlabel: unnamed-chunk-16\\n\\r  |                                                                       \\r  |............................                                     |  43%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................                                     |  44%\\nlabel: unnamed-chunk-17\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.110429306980222\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................                                    |  45%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..............................                                   |  46%\\nlabel: unnamed-chunk-18\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.146664330968633\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...............................                                  |  47%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...............................                                  |  48%\\nlabel: unnamed-chunk-19\\n\\r  |                                                                       \\r  |................................                                 |  49%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................................                                |  51%\\nlabel: unnamed-chunk-20\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 11.180243819020689\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................                               |  52%\\n   inline R code fragments\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.445361983031034\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................                               |  53%\\nlabel: unnamed-chunk-21\\n\\r  |                                                                       \\r  |...................................                              |  54%\\n   inline R code fragments\\n\\n\\r  |                                                                       \\r  |....................................                             |  55%\\nlabel: unnamed-chunk-22\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.481278642080724\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.....................................                            |  56%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.....................................                            |  57%\\nlabel: unnamed-chunk-23\\n\\r  |                                                                       \\r  |......................................                           |  59%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.......................................                          |  60%\\nlabel: unnamed-chunk-24\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.514141740044579\n},{\n  \u0022data\u0022: \u0022Warning: Ignoring unknown parameters: binwidth, bins, pad\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 12.527932649943978\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |........................................                         |  61%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |........................................                         |  62%\\nlabel: unnamed-chunk-25 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 12.546884532086551\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................................                        |  63%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 14.252585506998003\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..........................................                       |  64%\\nlabel: unnamed-chunk-26 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\\r  |                                                                       \\r  |...........................................                      |  66%\\n   inline R code fragments\\n\\n\\r  |                                                                       \\r  |...........................................                      |  67%\\nlabel: unnamed-chunk-27 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 14.287729976000264\n},{\n  \u0022data\u0022: \u0022Warning: Ignoring unknown parameters: binwidth, bins, pad\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 14.288795604137704\n},{\n  \u0022data\u0022: \u0022Warning: Ignoring unknown parameters: binwidth, bins, pad\\nWarning: Ignoring unknown parameters: binwidth, bins, pad\\nWarning: Ignoring unknown parameters: binwidth, bins, pad\\nWarning: Ignoring unknown parameters: binwidth, bins, pad\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 14.321541082113981\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |............................................                     |  68%\\n  ordinary text without R code\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.581686015008017\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................................                    |  69%\\nlabel: unnamed-chunk-28 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 17.61582550709136\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............................................                   |  70%\\n   inline R code fragments\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.041417225031182\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..............................................                   |  71%\\nlabel: unnamed-chunk-29 (with options) \\nList of 1\\n $ warning: logi FALSE\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 18.07629541796632\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...............................................                  |  72%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |................................................                 |  74%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 19.42280933400616\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-30\\n\\r  |                                                                       \\r  |.................................................                |  75%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.................................................                |  76%\\nlabel: unnamed-chunk-31\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 19.456778234103695\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..................................................               |  77%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |...................................................              |  78%\\nlabel: unnamed-chunk-32\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 19.48959067510441\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |....................................................             |  79%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |....................................................             |  80%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 61.683017410105094\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-33\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 61.71936846594326\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.....................................................            |  82%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |......................................................           |  83%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 70.4139678440988\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-34\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 70.4483595280908\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 111.7828985080123\n},{\n  \u0022data\u0022: \u0022\\r  |.......................................................          |  84%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.......................................................          |  85%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 111.816807490075\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-35\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 111.87115216208622\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |........................................................         |  86%\\n   inline R code fragments\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 115.18632056610659\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.........................................................        |  87%\\nlabel: unnamed-chunk-36\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 115.22081959899515\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |..........................................................       |  89%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |..........................................................       |  90%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 115.34479758702219\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-37\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 115.37922733603045\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |...........................................................      |  91%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |............................................................     |  92%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 115.69197893305682\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-38\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 115.72658419399522\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.............................................................    |  93%\\n  ordinary text without R code\\n\\n\\r  |                                                                       \\r  |.............................................................    |  94%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 115.9783206670545\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-39\\n\\r  |                                                                       \\r  |..............................................................   |  95%\\n   inline R code fragments\\n\\n\\r  |                                                                       \\r  |...............................................................  |  97%\\nlabel: unnamed-chunk-40\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 116.01287299115211\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |................................................................ |  98%\\n   inline R code fragments\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 116.38616434205323\n},{\n  \u0022data\u0022: \u0022\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 116.48253230797127\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |................................................................ |  99%\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 116.5160006149672\n},{\n  \u0022data\u0022: \u0022label: unnamed-chunk-41\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 116.57085800706409\n},{\n  \u0022data\u0022: \u0022\\r  |                                                                       \\r  |.................................................................| 100%\\n   inline R code fragments\\n\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 116.60383608401753\n},{\n  \u0022data\u0022: \u0022output file: /kaggle/working/script.knit.md\\n\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 116.62253787694499\n},{\n  \u0022data\u0022: \u0022/usr/local/bin/pandoc +RTS -K512m -RTS /kaggle/working/script.utf8.md --to html --from markdown+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash --output /kaggle/working/__results__.html --smart --email-obfuscation none --standalone --section-divs --table-of-contents --toc-depth 3 --template /usr/local/lib/R/site-library/rmarkdown/rmd/h/default.html --no-highlight --variable highlightjs=1 --number-sections --variable \u0027theme:bootstrap\u0027 --include-in-header /tmp/RtmpSwyXxo/rmarkdown-str19f4ad04.html --mathjax --variable \u0027mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0027 \\n\u0022,\n  \u0022stream_name\u0022: \u0022stdout\u0022,\n  \u0022time\u0022: 116.77328925509937\n},{\n  \u0022data\u0022: \u0022\\nOutput created: __results__.html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 117.0354434819892\n},{\n  \u0022data\u0022: \u0022Warning messages:\\n1: Removed 1 rows containing non-finite values (stat_boxplot). \\n2: In wordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = \\\u0022blue\\\u0022) :\\n  sage could not be fit on page. It will not be plotted.\\n3: In wordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = \\\u0022blue\\\u0022) :\\n  goodwin could not be fit on page. It will not be plotted.\\n4: In file.rename(from, to) :\\n  cannot rename file \u0027/kaggle/working/__results___files/figure-html\u0027 to \u0027/kaggle/working//kaggle/working/__results___files/figure-html\u0027, reason \u0027No such file or directory\u0027\\n5: In file.rename(from, to) :\\n  cannot rename file \u0027/kaggle/working//kaggle/working/__results___files/figure-html\u0027 to \u0027/kaggle/working/__results___files/figure-html\u0027, reason \u0027No such file or directory\u0027\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 117.06996303913184\n}]","outputFiles":[{"ownerInfo":null,"kernelVersionOutputFileId":10783373,"kernelVersionId":2406024,"kernelId":618429,"size":0,"fullPath":"Titanic.csv","previewUrl":"/kernels/preview.json/2406024/5e8f4832-e844-b96e-df9e-50aa5bfbfcec/Titanic.csv","downloadUrl":"https://www.kaggleusercontent.com/kf/2406024/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C2TUomWdGHjkvnHUhGkVMQ.3nAKE2-w7UL0RgVO03NegoCXGObd7r-zT9M3usO5l_fw-UeR-p6EQzNWrcuo-Z3faqDW_OANAeMPJJQTbKvfOPVDvkWcqJ-Tp80YziMxdsa71ajKM2cnVLU83BssHGJejDrIFMWb6qcGVi86pQnUxxZBcqANgyfB9Q5Cg6MgyYwGWUbVLx6Ty4U-iFw5yBDP._axA7jVcQIbMBwh65PoN4g/Titanic.csv","fileType":".csv","contentLength":3679,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"Titanic.csv","description":null}],"outputFilesCropped":false,"ouputFilesOwnerInfo":{"databundleVersionId":0,"dataset":null,"competition":null,"kernel":{"kernelId":618429,"kernelVersionId":2406024,"dataviewToken":"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..iY_FnZnBOjqKTc1rwKaKtQ.BJQkIVTUEWc8IL3QZkM_G0n8_NyEhQ6Vpt6OhBlAidn7DWR3RthO-o33kbKh75dOeB1sJdoe8nuD6KW83TD3dnDXUWyKvvYJ0T0eZWse2y9A8CJpi7zuH2g3JpshCPsvXCcD4LL4xKKjy8tZgOu4CPEKzMTyKg5XUF1Q17OCccXufbdbFhoSucEq2wKT2CsRkI4XY-MaAzgKTirtrtHAL311rWftFBOZWpjh7woaamJ63QH4BUyOM4VOAQPUjDe5DFamuEeG1MgLucVO1fWsf9cf9GPEw0XA-y-nF8r1OadKfkbALiyU67Df6b1ed0XffuGtw8f844ZWNZLZcZg63sHG6_Pph-9dzR6ye05ezQ13KjWuNfz8ME_Hh3IngJyNNC1Bj8XpyI4folyrEfXmVKibSd2IljXCHiW8bLZkH2TEmTaApAJ25-r7BThx6kkZHr4itjHoC2WM8feVQtLXfQ0oE4B1QizYyAucrsXrmcux9szD715LEnYLIRIm5c6HPu-svuvajsoM8hXy_8Kk0kfX-1T21tSzVXryybRQXuQ._DvYfRmHIn57x2ZTg6FizQ","scope":"mohdbarakat/titanic-tragedy-survival-prediction-with-ml"},"previewsDisabled":false},"pageMessages":[],"dataSources":[{"imageUrl":"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png","sourceUrl":"/c/titanic","slug":"titanic","lastUpdated":"2012-09-28T21:13:33.55Z","overview":"Start here! Predict survival on the Titanic and get familiar with ML basics","sourceType":"competition","sourceVersionType":null,"sourceId":3136,"sourceVersionNumber":null,"maxVersionNumber":null,"descriptionMimeType":"text/html","deleted":false,"private":false,"privateButVisible":false,"ownerInfo":{"databundleVersionId":26502,"dataset":null,"competition":{"competitionId":3136,"dataviewToken":null,"scope":"c/titanic"},"kernel":null,"previewsDisabled":true},"type":"dataSource","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"id":63842,"blobFileId":37991,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/gender_submission.csv","creationDate":"2017-02-01T01:49:18Z","isDummy":false,"size":3258,"fullPath":"../input/gender_submission.csv","previewUrl":"kernels/competition-preview/3136?relativePath=gender_submission.csv","downloadUrl":"/c/titanic/download/gender_submission.csv","fileType":".csv","contentLength":3258,"contentType":"text/csv","contentMD5":"MNEHO5ZKXYFUMexgOg3jUw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":418},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n"},{"order":1,"originalType":"","type":"boolean","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Survived","description":"0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"gender_submission.csv","description":"892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,0\n899,0\n900,0\n901,0\n902,0\n903,0\n904,1\n905,0a\n906,1\n907,0\n908,0\n909,0\n910,0\n911,0\n912,1\n913,0\n914,0\n915,0\n916,1\n917,0\n918,0\n919,0\n920,0\n921,0\n922,0\n923,0\n924,0\n925,0\n926,1\n927,0\n928,0\n929,0\n930,0\n931,1\n932,0\n933,0\n934,0\n935,0\n936,1\n937,0\n938,0\n939,0\n940,1\n941,0\n942,1\n943,0\n944,0\n945,1\n946,0\n947,0\n948,0\n949,0\n950,0\n951,1\n952,0\n953,0\n954,0\n955,0\n956,1\n957,0\n958,0\n959,0\n960,0\n961,1\n962,\n963,0\n964,0\n965,0\n966,1\n967,1\n968,0\n969,0\n970,0\n971,0\n972,0\n973,1\n974,0\n975,0\n976,0\n977,0\n978,0\n979,0\n980,0\n981,0\n982,0\n983,0\n984,0\n985,0\n986,0\n987,0\n988,1\n989,0\n990,0\n991,0\n992,1\n993,0\n994,0\n995,0\n996,0\n997,0\n998,0\n999,0\n1000,0\n1001,0\n1002,0\n1003,0\n1004,0\n1005,0\n1006,1\n1007,0\n1008,0\n1009,0\n1010,1\n1011,0\n1012,0\n1013,0\n1014,1\n1015,0\n1016,0\n1017,0\n1018,0\n1019,0\n1020,0\n1021,0\n1022,0\n1023,0\n1024,0\n1025,0\n1026,0\n1027,0\n1028,0\n1029,0\n1030,0\n1031,0\n1032,0\n1033,1\n1034,1\n1035,0\n1036,0\n1037,0\n1038,1\n1039,0\n1040,0\n1041,0\n1042,1\n1043,0\n1044,0\n1045,0\n1046,0\n1047,0\n1048,1\n1049,0\n1050,0\n1051,0\n1052,0\n1053,0\n1054,0\n1055,0\n1056,0\n1057,0\n1058,1\n1059,0\n1060,0\n1061,0\n1062,0\n1063,0\n1064,0\n1065,0\n1066,0\n1067,0\n1068,0\n1069,1\n1070,0\n1071,1\n1072,0\n1073,1\n1074,1\n1075,0\n1076,1\n1077,0\n1078,0\n1079,0\n1080,1\n1081,0\n1082,0\n1083,0\n1084,0\n1085,0\n1086,0\n1087,0\n1088,1\n1089,0\n1090,0\n1091,0\n1092,0\n1093,0\n1094,1\n1095,0\n1096,0\n1097,0\n1098,0\n1099,0\n1100,0\n1101,0\n1102,0\n1103,0\n1104,1\n1105,0\n1106,0\n1107,0\n1108,0\n1109,1\n1110,1\n1111,0\n1112,0\n1113,0\n1114,0\n1115,0\n1116,0\n1117,0\n1118,0\n1119,0\n1120,0\n1121,0\n1122,1\n1123,0\n1124,0\n1125,0\n1126,1\n1127,0\n1128,1\n1129,0\n1130,0\n1131,1\n1132,0\n1133,0\n1134,1\n1135,0\n1136,0\n1137,1\n1138,0\n1139,0\n1140,0\n1141,0\n1142,0\n1143,0\n1144,1\n1145,0\n1146,0\n1147,0\n1148,0\n1149,0\n1150,0\n1151,0\n1152,0\n1153,0\n1154,0\n1155,0\n1156,0\n1157,0\n1158,0\n1159,0\n1160,0\n1161,0\n1162,1\n1163,0\n1164,1\n1165,0\n1166,0\n1167,0\n1168,0\n1169,0\n1170,0\n1171,0\n1172,0\n1173,0\n1174,0\n1175,0\n1176,0\n1177,0\n1178,0\n1179,1\n1180,0\n1181,0\n1182,0\n1183,0\n1184,0\n1185,1\n1186,0\n1187,0\n1188,0\n1189,0\n1190,1\n1191,0\n1192,0\n1193,0\n1194,0\n1195,0\n1196,0\n1197,0\n1198,1\n1199,0\n1200,1\n1201,0\n1202,0\n1203,0\n1204,0\n1205,0\n1206,1\n1207,0\n1208,1\n1209,0\n1210,0\n1211,0\n1212,0\n1213,0\n1214,0\n1215,0\n1216,1\n1217,0\n1218,0\n1219,1\n1220,0\n1221,0\n1222,0\n1223,0\n1224,0\n1225,0\n1226,0\n1227,0\n1228,0\n1229,0\n1230,0\n1231,0\n1232,0\n1233,0\n1234,1\n1235,1\n1236,0\n1237,0\n1238,0\n1239,0\n1240,0\n1241,0\n1242,1\n1243,0\n1244,1\n1245,1\n1246,0\n1247,0\n1248,1\n1249,0\n1250,0\n1251,0\n1252,1\n1253,0\n1254,0\n1255,0\n1256,1\n1257,1\n1258,0\n1259,0\n1260,0\n1261,0\n1262,0\n1263,1\n1264,0\n1265,0\n1266,1\n1267,1\n1268,0\n1269,0\n1270,1\n1271,0\n1272,0\n1273,0\n1274,0\n1275,0\n1276,0\n1277,1\n1278,0\n1279,0\n1280,0\n1281,0\n1282,1\n1283,0\n1284,0\n1285,0\n1286,0\n1287,1\n1288,0\n1289,1\n1290,0\n1291,0\n1292,1\n1293,0\n1294,0\n1295,1\n1296,0\n1297,0\n1298,0\n1299,1\n1300,0\n1301,0\n1302,0\n1303,1\n1304,0\n1305,0\n1306,1\n1307,0\n1308,0\n1309,0"},{"id":63841,"blobFileId":2613,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/test.csv","creationDate":"2013-06-28T13:40:24.227Z","isDummy":false,"size":28629,"fullPath":"../input/test.csv","previewUrl":"kernels/competition-preview/3136?relativePath=test.csv","downloadUrl":"/c/titanic/download/test.csv","fileType":".csv","contentLength":28629,"contentType":"text/csv","contentMD5":"dTO4Lq5LWCYQy9aKpjawFw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":418},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"1"},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Pclass","description":"1"},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Name","description":"the name of the passenger"},{"order":3,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Sex","description":null},{"order":4,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Age","description":null},{"order":5,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"SibSp","description":"of siblings / spouses aboard the Titanic"},{"order":6,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Parch","description":"of parents / children aboard the Titanic"},{"order":7,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Ticket","description":"Ticket number"},{"order":8,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Fare","description":"Passenger fare"},{"order":9,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Cabin","description":"Cabin number"},{"order":10,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Embarked","description":"Port of Embarkation"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"test.csv","description":"test data to check the accuracy of the model created\n"},{"id":63840,"blobFileId":2307,"databundleVersionId":26502,"databundleVersionObjectType":"file","url":null,"relativePath":"../input/train.csv","creationDate":"2013-06-28T13:40:25.23Z","isDummy":false,"size":61194,"fullPath":"../input/train.csv","previewUrl":"kernels/competition-preview/3136?relativePath=train.csv","downloadUrl":"/c/titanic/download/train.csv","fileType":".csv","contentLength":61194,"contentType":"text/csv","contentMD5":"IwnMXwR4Ltm7YBbZ9OOBzw==","validationErrors":null,"type":"databundleVersionObject","collapsed":false,"info":{"metrics":{"tableMetrics":{"exception":null,"rowCount":891},"columnMetrics":[]},"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":{"delimiter":",","includesHeader":true},"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[{"columns":[{"order":0,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"PassengerId","description":"type should be integers"},{"order":1,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Survived","description":"Survived or Not "},{"order":2,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Pclass","description":"Class of Travel"},{"order":3,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Name","description":"Name of Passenger"},{"order":4,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Sex","description":"Gender"},{"order":5,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Age","description":"Age of Passengers"},{"order":6,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"SibSp","description":"Number of Sibling/Spouse aboard"},{"order":7,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Parch","description":"Number of Parent/Child aboard"},{"order":8,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Ticket","description":null},{"order":9,"originalType":"","type":"numeric","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Fare","description":null},{"order":10,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Cabin","description":null},{"order":11,"originalType":"","type":"string","extendedType":null,"isNullable":false,"isPrimaryKey":false,"isLabel":false,"info":null,"name":"Embarked","description":"The port in which a passenger has embarked. C - Cherbourg, S - Southampton, Q = Queenstown"}],"totalRows":null,"type":"genericTable","collapsed":true,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"children":[],"name":"","description":null}],"name":"train.csv","description":"contains data \n"}],"name":"Titanic: Machine Learning from Disaster","description":"\u003ch3\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe data has been split into two groups:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etraining set (train.csv)\u003c/li\u003e\n\u003cli\u003etest set (test.csv)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003e The training set \u003c/b\u003eshould be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use \u003ca href=\u0022https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\u0022 target=\u0022_blank\u0022\u003e feature engineering \u003c/a\u003eto create new features.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eThe test set \u003c/b\u003eshould be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\u003c/p\u003e\n\u003cp\u003eWe also include \u003cb\u003egender_submission.csv\u003c/b\u003e, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\u003c/p\u003e\n\u003ch3\u003eData Dictionary\u003c/h3\u003e\n\u003ctable style=\u0022width: 100%;\u0022\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\u003cth\u003e\u003cb\u003eVariable\u003c/b\u003e\u003c/th\u003e\u003cth\u003e\u003cb\u003eDefinition\u003c/b\u003e\u003c/th\u003e\u003cth\u003e\u003cb\u003eKey\u003c/b\u003e\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esurvival\u003c/td\u003e\n\u003ctd\u003eSurvival\u003c/td\u003e\n\u003ctd\u003e0 = No, 1 = Yes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epclass\u003c/td\u003e\n\u003ctd\u003eTicket class\u003c/td\u003e\n\u003ctd\u003e1 = 1st, 2 = 2nd, 3 = 3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esex\u003c/td\u003e\n\u003ctd\u003eSex\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAge\u003c/td\u003e\n\u003ctd\u003eAge in years\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003esibsp\u003c/td\u003e\n\u003ctd\u003e# of siblings / spouses aboard the Titanic\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eparch\u003c/td\u003e\n\u003ctd\u003e# of parents / children aboard the Titanic\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eticket\u003c/td\u003e\n\u003ctd\u003eTicket number\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003efare\u003c/td\u003e\n\u003ctd\u003ePassenger fare\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecabin\u003c/td\u003e\n\u003ctd\u003eCabin number\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eembarked\u003c/td\u003e\n\u003ctd\u003ePort of Embarkation\u003c/td\u003e\n\u003ctd\u003eC = Cherbourg, Q = Queenstown, S = Southampton\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003eVariable Notes\u003c/h3\u003e\n\u003cp\u003e\u003cb\u003epclass\u003c/b\u003e: A proxy for socio-economic status (SES)\u003cbr /\u003e 1st = Upper\u003cbr /\u003e 2nd = Middle\u003cbr /\u003e 3rd = Lower\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003eage\u003c/b\u003e: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003esibsp\u003c/b\u003e: The dataset defines family relations in this way...\u003cbr /\u003e Sibling = brother, sister, stepbrother, stepsister\u003cbr /\u003e Spouse = husband, wife (mistresses and fiancés were ignored)\u003cbr /\u003e\u003cbr /\u003e \u003cb\u003eparch\u003c/b\u003e: The dataset defines family relations in this way...\u003cbr /\u003e Parent = mother, father\u003cbr /\u003e Child = daughter, son, stepdaughter, stepson\u003cbr /\u003e Some children travelled only with a nanny, therefore parch=0 for them.\u003c/p\u003e"}],"versions":[{"id":2406024,"kernelVersionId":null,"isForkParent":false,"isNotebook":false,"languageName":"RMarkdown","lastRunTime":"2018-02-09T04:57:32.503Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":741,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-rstats/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/rstats/","dockerImageId":"sha256:0b7b0f4ac1cf07839ba6945c2874afc761b5fb9cbbab6acdc631a1b6c7c7180a","dockerImageName":"gcr.io/kaggle-images/rstats","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":117.187072226079,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Titanic Tragedy: Survival Prediction with ML","url":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml?scriptVersionId=2406024","versionNumber":1,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null}],"categories":{"categories":[{"id":13402,"name":"random forest","displayName":"random forest","fullPath":"algorithms \u003e random forest","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27random forest%27","tagUrl":"/tags/random-forest","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":19,"competitionCount":0,"scriptCount":641,"totalCount":660},{"id":13205,"name":"text mining","displayName":"text mining","fullPath":"analysis \u003e text mining","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27text mining%27","tagUrl":"/tags/text-mining","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":72,"competitionCount":0,"scriptCount":272,"totalCount":344},{"id":13411,"name":"svm","displayName":"SVM","fullPath":"algorithms \u003e svm","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27svm%27","tagUrl":"/tags/svm","fontAwesomeIcon":null,"description":null,"isInherited":false,"datasetCount":6,"competitionCount":0,"scriptCount":218,"totalCount":224}],"type":"script"},"submitToCompetitionInfo":null,"downloadAllFilesUrl":"/kernels/svzip/2406024","submission":{"date":"2018-02-09T05:06:01.81Z","privateScore":null,"publicScore":"0.78947","submissionId":6607731,"teamName":"Mohammed Barakat"},"menuLinks":[{"href":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml/notebook","text":"Report","title":"Notebook","tab":"report","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml/code","text":"Code","title":"Code","tab":"code","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml/data","text":"Data","title":"Data","tab":"data","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml/output","text":"Output","title":"Output","tab":"output","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml/comments","text":"Comments","title":"Comments","tab":"comments","count":0,"showZeroCountExplicitly":true,"reportEventCategory":null,"reportEventType":null},{"href":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml/log","text":"Log","title":"Log","tab":"log","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml/versions","text":"Versions","title":"Versions","tab":"versions","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/mohdbarakat/titanic-tragedy-survival-prediction-with-ml/forks","text":"Forks","title":"Forks","tab":"forks","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null}],"rightMenuLinks":[],"callToAction":{"href":"/kernels/fork-version/2406024","text":"Fork Script","title":"Fork Script","tab":null,"count":null,"showZeroCountExplicitly":false,"reportEventCategory":"kernels","reportEventType":"anonymousKernelForkCreation"},"voteButton":{"totalVotes":2,"hasAlreadyVotedUp":false,"hasAlreadyVotedDown":false,"canUpvote":true,"canDownvote":false,"voteUpUrl":"/kernels/vote?id=618429","voteDownUrl":null,"voters":[{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/682667-kg.jpg","displayName":"Reinhard","profileUrl":"/reisel","tier":"Contributor","tierInt":1,"userId":682667,"userName":"reisel"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1560793-gr.jpg","displayName":"Mohammed Barakat","profileUrl":"/mohdbarakat","tier":"Contributor","tierInt":1,"userId":1560793,"userName":"mohdbarakat"}],"currentUserInfo":null,"showVoters":true,"alwaysShowVoters":true},"parentDataSource":null,"parentName":"Titanic: Machine Learning from Disaster","parentUrl":"/c/titanic","thumbnailImageUrl":"https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png","canWrite":false,"canAdminister":false,"datasetHidden":false,"forkParentIsRedacted":false,"forkDiffLinesChanged":0,"forkDiffLinesDeleted":0,"forkDiffLinesInserted":0,"forkDiffUrl":null,"forkParentAuthorDisplayName":null,"forkParentAuthorUrl":null,"forkParentTitle":null,"forkParentUrl":null,"canSeeDataExplorerV2":true,"canSeeRevampedViewer":true,"canSeeInnerTableOfContents":true,"canSeeCopyAndEditText":true,"simplifiedViewer":false,"kernelOutputDataset":null});performance && performance.mark && performance.mark("KernelViewer.componentCouldBootstrap");</script>

<form action="/mohdbarakat/titanic-tragedy-survival-prediction-with-ml" id="__AjaxAntiForgeryForm" method="post"><input name="X-XSRF-TOKEN" type="hidden" value="CfDJ8LdUzqlsSWBPr4Ce3rb9VL8dOj1nawFWJHur1PUdEUTS6ZxzgbZDzQ3s5P2JjK6Jb0guC6g1o3EM5duOpfQWQx8hPqyZCB5cQnBGZRAZeckyk0oarngYwswwS9lnBwWFh5DlP0M44LlxaQ88eHQbZa0" /></form>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["STIX", "TeX"],
            linebreaks: {
                automatic: true
            },
            EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
        },
        tex2jax: {
            inlineMath: [["\\(", "\\)"], ["\\\\(", "\\\\)"]],
            displayMath: [["$$", "$$"], ["\\[", "\\]"]],
            processEscapes: true,
            ignoreClass: "tex2jax_ignore|dno"
        },
        TeX: {
            noUndefined: {
                attributes: {
                    mathcolor: "red",
                    mathbackground: "#FFEEEE",
                    mathsize: "90%"
                }
            }
        },
        Macros: {
            href: "{}"
        },
        skipStartupTypeset: true,
        messageStyle: "none"
    });
</script>
<script type="text/javascript" async crossorigin="anonymous" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



    </div>

        <div class="site-layout__footer">
            <footer class="site-footer">
    <div class="site-footer__content">
        <div class="site-footer__copyright">
            <span>&copy; 2019 Kaggle Inc</span>
        </div>
        <nav class="site-footer__nav">
            <a href="/team">Our Team</a>
            <a href="/terms">Terms</a>
            <a href="/privacy">Privacy</a>
            <a href="/contact">Contact/Support</a>
        </nav>
        <nav class="site-footer__social">
            <div data-component-name="SocialIcons" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script>var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push();performance && performance.mark && performance.mark("SocialIcons.componentCouldBootstrap");</script>
        </nav>
    </div>
</footer>

        </div>
</div>




    </main>
</body>
</html>
