
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> # This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
> # The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
> # For example, here's several helpful packages to load in 
> 
> library(ggplot2) # Data visualization
Warning message:
package ‘ggplot2’ was built under R version 3.6.2 
> library(readr) # CSV file I/O, e.g. the read_csv function
> library(lattice) # Requirement for 'caret'-package
> library(caret) # Missclassification Error
> library(mice) # Missing Value imputation

Attaching package: ‘mice’

The following objects are masked from ‘package:base’:

    cbind, rbind

Warning message:
package ‘mice’ was built under R version 3.6.2 
> 
> # Input data files are available in the "../input/" directory.
> # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
> 
> system("ls ../input")
gender_submission.csv
test.csv
train.csv
> 
> # Any results you write to the current directory are saved as output.
> 
> #### Import training and test dataset
> train <- read.csv(file = '../input/train.csv')
> test <- read.csv(file = '../input/test.csv')
> 
> 
> str(train)
'data.frame':	891 obs. of  12 variables:
 $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
 $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
 $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
 $ Name       : Factor w/ 891 levels "Abbing, Mr. Anthony",..: 109 191 358 277 16 559 520 629 417 581 ...
 $ Sex        : Factor w/ 2 levels "female","male": 2 1 1 1 2 2 2 2 1 1 ...
 $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
 $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
 $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
 $ Ticket     : Factor w/ 681 levels "110152","110413",..: 524 597 670 50 473 276 86 396 345 133 ...
 $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
 $ Cabin      : Factor w/ 148 levels "","A10","A14",..: 1 83 1 57 1 1 131 1 1 1 ...
 $ Embarked   : Factor w/ 4 levels "","C","Q","S": 4 2 4 4 4 3 4 4 4 2 ...
> 
> # Subset of data
> trainImp <- train[,c(2, 3, 5, 6)]
> testImp <- test[,c(2, 4, 5)]
> 
> # Make sure to add useNA argument.
> table(trainImp$Age, useNA = "ifany")

0.42 0.67 0.75 0.83 0.92    1    2    3    4    5    6    7    8    9   10   11 
   1    1    2    2    1    7   10    6   10    4    3    3    4    8    2    4 
  12   13   14 14.5   15   16   17   18   19   20 20.5   21   22   23 23.5   24 
   1    2    6    1    5   17   13   26   25   15    1   24   27   15    1   30 
24.5   25   26   27   28 28.5   29   30 30.5   31   32 32.5   33   34 34.5   35 
   1   23   18   18   25    2   20   25    2   17   18    2   15   15    1   18 
  36 36.5   37   38   39   40 40.5   41   42   43   44   45 45.5   46   47   48 
  22    1    6   11   14   13    2    6   13    5    9   12    2    3    9    9 
  49   50   51   52   53   54   55 55.5   56   57   58   59   60   61   62   63 
   6   10    7    6    1    8    2    1    4    2    5    2    4    3    4    2 
  64   65   66   70 70.5   71   74   80 <NA> 
   2    3    1    2    1    2    1    1  177 
> # Percent of NA
> print(paste0("Percent of NA ", mean(is.na(train$Age))))
[1] "Percent of NA 0.198653198653199"
> 
> attach(trainImp)
> table(Pclass, Survived, useNA = "ifany")
      Survived
Pclass   0   1
     1  80 136
     2  97  87
     3 372 119
> prop.table(table(Pclass, Survived, useNA = "ifany"),1)
      Survived
Pclass         0         1
     1 0.3703704 0.6296296
     2 0.5271739 0.4728261
     3 0.7576375 0.2423625
> print("========================================")
[1] "========================================"
> table(Sex, Survived, useNA = "ifany")
        Survived
Sex        0   1
  female  81 233
  male   468 109
> prop.table(table(Sex, Survived, useNA = "ifany"),1)
        Survived
Sex              0         1
  female 0.2579618 0.7420382
  male   0.8110919 0.1889081
> detach(trainImp)
> 
> ggplot(data = trainImp, aes(x = Age, fill = as.factor(Survived))) + geom_histogram(alpha = .5) + theme_minimal()
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
Warning message:
Removed 177 rows containing non-finite values (stat_bin). 
> 
> trainLogistic <- glm(formula = Survived ~ Age + Sex + Pclass,
+                      family = binomial(),
+                      data = trainImp)
> summary(trainLogistic)

Call:
glm(formula = Survived ~ Age + Sex + Pclass, family = binomial(), 
    data = trainImp)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7270  -0.6799  -0.3947   0.6483   2.4668  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  5.056006   0.502128  10.069  < 2e-16 ***
Age         -0.036929   0.007628  -4.841 1.29e-06 ***
Sexmale     -2.522131   0.207283 -12.168  < 2e-16 ***
Pclass      -1.288545   0.139259  -9.253  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 964.52  on 713  degrees of freedom
Residual deviance: 647.29  on 710  degrees of freedom
  (177 observations deleted due to missingness)
AIC: 655.29

Number of Fisher Scoring iterations: 5

> trainPred <- (predict(object = trainLogistic, newdata = trainImp[ ,-1]) > 1) * 1 # Remove response-variable
> confusionMatrix(data = table(trainPred, trainImp$Survived), dnn = c("Prediction", "Reference"))
Confusion Matrix and Statistics

         
trainPred   0   1
        0 413 138
        1  11 152
                                          
               Accuracy : 0.7913          
                 95% CI : (0.7596, 0.8206)
    No Information Rate : 0.5938          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.5352          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.9741          
            Specificity : 0.5241          
         Pos Pred Value : 0.7495          
         Neg Pred Value : 0.9325          
             Prevalence : 0.5938          
         Detection Rate : 0.5784          
   Detection Prevalence : 0.7717          
      Balanced Accuracy : 0.7491          
                                          
       'Positive' Class : 0               
                                          
> 
> 
> set.seed(123)
> 
> trainMice <- mice(data = train[ ,c(2, 3, 5, 6, 7, 8, 10)], imputationMethod = 'rf', maxit = 5, m = 1, printFlag = FALSE)
Warning message:
In check.deprecated(...) :
  The 'imputationMethod' argument is no longer supported. Please use 'method' instead.
> testMice <- mice(data = test[ ,c(1, 2, 4, 5, 6, 7)], imputationMethod = 'rf', maxit = 5, m = 1, printFlag = FALSE)
Warning message:
In check.deprecated(...) :
  The 'imputationMethod' argument is no longer supported. Please use 'method' instead.
> trainMiceComplete <- complete(trainMice)
> testMiceComplete <- complete(testMice)
> 
> 
> trainLogisticMice <- glm(formula = Survived ~ Sex + Pclass + Age,
+                          family = binomial(),
+                          data = trainMiceComplete[,c(1, 2, 3, 4)])
> summary(trainLogisticMice)

Call:
glm(formula = Survived ~ Sex + Pclass + Age, family = binomial(), 
    data = trainMiceComplete[, c(1, 2, 3, 4)])

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7079  -0.6470  -0.3903   0.6449   2.4878  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  4.97524    0.45366  10.967  < 2e-16 ***
Sexmale     -2.59332    0.18764 -13.821  < 2e-16 ***
Pclass      -1.26181    0.12546 -10.058  < 2e-16 ***
Age         -0.03655    0.00683  -5.352 8.71e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1186.66  on 890  degrees of freedom
Residual deviance:  796.37  on 887  degrees of freedom
AIC: 804.37

Number of Fisher Scoring iterations: 5

> trainPred <- (predict(object = trainLogisticMice, newdata = trainImp[ ,-1]) > 1) * 1 # Remove response-variable
> confusionMatrix(data = table(trainPred, trainImp$Survived), dnn = c("Prediction", "Reference"))
Confusion Matrix and Statistics

         
trainPred   0   1
        0 413 142
        1  11 148
                                          
               Accuracy : 0.7857          
                 95% CI : (0.7538, 0.8153)
    No Information Rate : 0.5938          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.5216          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.9741          
            Specificity : 0.5103          
         Pos Pred Value : 0.7441          
         Neg Pred Value : 0.9308          
             Prevalence : 0.5938          
         Detection Rate : 0.5784          
   Detection Prevalence : 0.7773          
      Balanced Accuracy : 0.7422          
                                          
       'Positive' Class : 0               
                                          
> 
> trainMiceComplete$AgeInterval <- cut(trainMiceComplete$Age,
+                                     breaks = c(0, 20, 40, Inf),
+                                     labels = c('Children','MiddleAge', 'Older'),
+                                     right = FALSE)
> 
> testMiceComplete$AgeInterval <- cut(testMiceComplete$Age,
+                                     breaks = c(0, 20, 40, Inf),
+                                     labels = c('Children','MiddleAge', 'Older'),
+                                     right = FALSE)
> 
> trainLogisticMice <- glm(formula = Survived ~ Sex + Pclass + AgeInterval,
+                          family = binomial(),
+                          data = trainMiceComplete[,c(1, 2, 3, 5, 8)])
> summary(trainLogisticMice)

Call:
glm(formula = Survived ~ Sex + Pclass + AgeInterval, family = binomial(), 
    data = trainMiceComplete[, c(1, 2, 3, 5, 8)])

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.5015  -0.7387  -0.4292   0.6440   2.5289  

Coefficients:
                     Estimate Std. Error z value Pr(>|z|)    
(Intercept)            4.2630     0.4011  10.628  < 2e-16 ***
Sexmale               -2.6271     0.1870 -14.051  < 2e-16 ***
Pclass                -1.1790     0.1215  -9.702  < 2e-16 ***
AgeIntervalMiddleAge  -0.4370     0.2135  -2.047   0.0406 *  
AgeIntervalOlder      -1.2547     0.2822  -4.447 8.72e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1186.66  on 890  degrees of freedom
Residual deviance:  805.99  on 886  degrees of freedom
AIC: 815.99

Number of Fisher Scoring iterations: 5

> trainPred <- (predict(object = trainLogisticMice, newdata = trainMiceComplete) > 1) * 1 # Remove response-variable
> confusionMatrix(data = table(trainPred, trainImp$Survived), dnn = c("Prediction", "Reference"))
Confusion Matrix and Statistics

         
trainPred   0   1
        0 542 196
        1   7 146
                                          
               Accuracy : 0.7722          
                 95% CI : (0.7432, 0.7993)
    No Information Rate : 0.6162          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.4623          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.9872          
            Specificity : 0.4269          
         Pos Pred Value : 0.7344          
         Neg Pred Value : 0.9542          
             Prevalence : 0.6162          
         Detection Rate : 0.6083          
   Detection Prevalence : 0.8283          
      Balanced Accuracy : 0.7071          
                                          
       'Positive' Class : 0               
                                          
> 
> testPred <- (predict(object = trainLogisticMice, newdata = testMiceComplete) > 1) * 1 # Predict test dataset
> Submission <- data.frame("PassengerId" = testMiceComplete$PassengerId, "Survived" = testPred)
> 
> Since this is my first Kernel and first participation in a competition on Kaggle I feel like i am satisfied.
Error: unexpected symbol in "Since this"
Execution halted
