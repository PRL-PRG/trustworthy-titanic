
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> ## Importing packages
> 
> # This R environment comes with all of CRAN and many other helpful packages preinstalled.
> # You can see which packages are installed by checking out the kaggle/rstats docker image: 
> # https://github.com/kaggle/docker-rstats
> 
> library(tidyverse) # metapackage with lots of helpful functions
â”€â”€ [1mAttaching packages[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.0 â”€â”€
[32mâœ“[39m [34mggplot2[39m 3.3.2     [32mâœ“[39m [34mpurrr  [39m 0.3.4
[32mâœ“[39m [34mtibble [39m 3.0.1     [32mâœ“[39m [34mdplyr  [39m 1.0.2
[32mâœ“[39m [34mtidyr  [39m 1.1.0     [32mâœ“[39m [34mstringr[39m 1.4.0
[32mâœ“[39m [34mreadr  [39m 1.3.1     [32mâœ“[39m [34mforcats[39m 0.5.0
â”€â”€ [1mConflicts[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
[31mx[39m [34mdplyr[39m::[32mfilter()[39m masks [34mstats[39m::filter()
[31mx[39m [34mdplyr[39m::[32mlag()[39m    masks [34mstats[39m::lag()
Warning messages:
1: package â€˜ggplot2â€™ was built under R version 3.6.2 
2: package â€˜tibbleâ€™ was built under R version 3.6.2 
3: package â€˜tidyrâ€™ was built under R version 3.6.2 
4: package â€˜purrrâ€™ was built under R version 3.6.2 
5: package â€˜dplyrâ€™ was built under R version 3.6.2 
> 
> dados = read.csv('../input/train.csv', sep = ',', header = T)
> 
> # Survived x Fare
> library(ggplot2)
> 
> g = ggplot(data = dados, aes(group = Survived, y = Fare, x = Survived))
> g + geom_boxplot(fill = 'lightblue') + xlab("Sobrevivente (1 = sobrevivente)") + ylab("PreÃ§o da passagem (limitado a $200)") + ylim(c(0,200))
Warning message:
Removed 20 rows containing non-finite values (stat_boxplot). 
> 
> 
> # MÃ©dia de Fare 
> dados %>% group_by(Survived) %>% summarise('media' = mean(Fare))
`summarise()` ungrouping output (override with `.groups` argument)
[90m# A tibble: 2 x 2[39m
  Survived media
     [3m[90m<int>[39m[23m [3m[90m<dbl>[39m[23m
[90m1[39m        0  22.1
[90m2[39m        1  48.4
> 
> # Usando o classificador simples: se Fare > 35, survived = 1
> ypred = ifelse(dados$Fare > 35, 1, 0)
> 
> # Obtendo a matriz de confusÃ£o
> library(caret)
Loading required package: lattice

Attaching package: â€˜caretâ€™

The following object is masked from â€˜package:purrrâ€™:

    lift

> confusionMatrix(factor(ypred), factor(dados$Survived), positive = '1')
Confusion Matrix and Statistics

          Reference
Prediction   0   1
         0 474 219
         1  75 123
                                          
               Accuracy : 0.67            
                 95% CI : (0.6381, 0.7009)
    No Information Rate : 0.6162          
    P-Value [Acc > NIR] : 0.0004803       
                                          
                  Kappa : 0.2423          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.3596          
            Specificity : 0.8634          
         Pos Pred Value : 0.6212          
         Neg Pred Value : 0.6840          
             Prevalence : 0.3838          
         Detection Rate : 0.1380          
   Detection Prevalence : 0.2222          
      Balanced Accuracy : 0.6115          
                                          
       'Positive' Class : 1               
                                          
> 
> # Testando a acurÃ¡cia de vÃ¡rios pontos de corte
> tmin = min(dados$Fare)
> tmax = max(dados$Fare)
> 
> # NÃºmero de pontos de corte para testar (entre tmin e tmax)
> npontos = 1000
> pontos = seq(from = tmin, to = tmax, length.out = npontos)
> 
> res = as.data.frame(matrix(ncol=4, nrow=0))
> for(i in 1:npontos) {
+     t = pontos[i]
+     ypred = ifelse(dados$Fare < t, 0, 1)
+     m = confusionMatrix(factor(ypred), factor(dados$Survived))
+     a = m$overall['Accuracy']
+     tb = m$table
+     p = tb[2,2] / sum(tb[,2])
+     r = tb[2, 1] / sum(tb[,1])
+     res[i, 1] = t
+     res[i, 2] = a
+     res[i, 3] = p
+     res[i, 4] = r
+ }
Warning message:
In confusionMatrix.default(factor(ypred), factor(dados$Survived)) :
  Levels are not in the same order for reference and data. Refactoring data to match.
> colnames(res) = c("corte", "acuracia", "tpr", "fpr")
> 
> resFare = res
> # Ponto Ã³timo
> topt = res[which.max(res$acuracia),1]
> print(paste0("Corte Ã³timo = ", topt))
[1] "Corte Ã³timo = 50.7713621621622"
> 
> # ROC curve
> g = ggplot(data = res, aes(y = tpr, x = fpr))
> g + geom_line() + geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), color = "blue", linetype = 'dashed') +
+ xlab('Taxa de falsos positivos (frequÃªncia de alarme falso)') + ylab('Taxa de verdadeiros positivos (recall)') +
+ ggtitle("ROC - Fare")
> 
> # Plotando os resultados
> g = ggplot(data = res, aes(x = corte, y = acuracia))
> g + geom_line() + xlab('Ponto de corte') + ylab('AcurÃ¡cia')
> 
> g = ggplot(data = res[which(res$corte < 100),], aes(x = corte, y = acuracia))
> g + geom_line() + geom_point() + xlab('Ponto de corte') + ylab('AcurÃ¡cia') +
+ geom_vline(xintercept = topt, colour = 'red', linetype = 'dashed')
> 
> # Repetindo para Parch
> g = ggplot(data = dados, aes(group = Survived, y = Parch, x = Survived))
> g + geom_boxplot(fill = 'lightblue') + xlab("Sobrevivente (1 = sobrevivente)") + ylab("Parentes a bordo") 
> 
> # Testando a acurÃ¡cia de vÃ¡rios pontos de corte
> tmin = min(dados$Parch)
> tmax = max(dados$Parch)
> 
> # NÃºmero de pontos de corte para testar (entre tmin e tmax)
> npontos = 1000
> pontos = seq(from = tmin, to = tmax, length.out = npontos)
> 
> res = as.data.frame(matrix(ncol=2, nrow=0))
> for(i in 1:npontos) {
+     t = pontos[i]
+     ypred = ifelse(dados$Parch < t, 0, 1)
+     m = confusionMatrix(factor(ypred), factor(dados$Survived))
+     a = m$overall['Accuracy']
+     tb = m$table
+     p = tb[2,2] / sum(tb[,2])
+     r = tb[2, 1] / sum(tb[,1])
+     res[i, 1] = t
+     res[i, 2] = a
+     res[i, 3] = p
+     res[i, 4] = r
+ }
Warning message:
In confusionMatrix.default(factor(ypred), factor(dados$Survived)) :
  Levels are not in the same order for reference and data. Refactoring data to match.
> colnames(res) = c("corte", "acuracia", "tpr", "fpr")
> 
> resParch = res
> # Ponto Ã³timo
> topt = res[which.max(res$acuracia),1]
> print(paste0("Corte Ã³timo = ", topt))
[1] "Corte Ã³timo = 0.00600600600600601"
> 
> # ROC curve
> g = ggplot(data = res, aes(y = tpr, x = fpr))
> g + geom_line() + geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), color = "blue", linetype = 'dashed') +
+ xlab('Taxa de falsos positivos (frequÃªncia de alarme falso)') + ylab('Taxa de verdadeiros positivos (recall)') +
+ ggtitle("ROC - Parch")
> 
> # Plotando os resultados
> g = ggplot(data = res, aes(x = corte, y = acuracia))
> g + geom_line() + xlab('Ponto de corte') + ylab('AcurÃ¡cia') + geom_point()
> 
> 
> 
> # ROC Parch x Fare
> dfRoc = rbind(cbind.data.frame(variavel = rep('Fare', nrow(resFare)), tpr = resFare$tpr, fpr = resFare$fpr), cbind.data.frame(variavel = rep('Parch', nrow(resParch)), tpr = resParch$tpr, fpr = resParch$fpr))
> 
> g = ggplot(data = dfRoc, aes(y = tpr, x = fpr, color = variavel))
> g + geom_line() + geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), color = "blue", linetype = 'dashed') +
+ xlab('Taxa de falsos positivos (frequÃªncia de alarme falso)') + ylab('Taxa de verdadeiros positivos (recall)') +
+ ggtitle("ROC")
> 
> # GrÃ¡fico Parch x Fare
> g = ggplot(data =dados, aes(x = Parch, y = Fare, color = factor(Survived)))
> g + geom_point() + guides(color = F)
> 
> g = ggplot(data =dados, aes(x = Parch, y = Fare, color = factor(Survived)))
> g + geom_point() + guides(color = F) + geom_hline(yintercept = 50.77, color = 'red', linetype = 'dashed')
> 
> g = ggplot(data =dados, aes(x = Parch, y = Fare, color = factor(Survived)))
> g + geom_point() + guides(color = F) + geom_hline(yintercept = 50.77, color = 'red', linetype = 'dashed') +
+ geom_vline(xintercept = 0.5, color = 'blue', linetype = 'dashed' )
> 
> # Calculando a proporÃ§Ã£o de sobreviventes em cada regiÃ£o
> dados %>% subset(Fare < 50.77 & Parch == 0) %>% summarise(p = sum(Survived) / n())
          p
1 0.2772964
> dados %>% subset(Fare < 50.77 & Parch > 0) %>% summarise(p = sum(Survived) / n())
          p
1 0.4709677
> dados %>% subset(Fare >= 50.77 & Parch == 0) %>% summarise(p = sum(Survived) / n())
          p
1 0.7227723
> dados %>% subset(Fare >= 50.77 & Parch > 0) %>% summarise(p = sum(Survived) / n())
          p
1 0.6206897
> 
> 
> # Ponto de corte Ã³timo para cada regiÃ£o
> tmin = min(dados$Parch)
> tmax = max(dados$Parch)
> 
> # NÃºmero de pontos de corte para testar (entre tmin e tmax)
> npontos = 1000
> pontos = seq(from = tmin, to = tmax, length.out = npontos)
> 
> tmp = dados %>% subset(Fare < 50.77)
> res = as.data.frame(matrix(ncol=2, nrow=0))
> for(i in 1:npontos) {
+     t = pontos[i]
+     ypred = ifelse(tmp$Parch < t, 0, 1)
+     m = confusionMatrix(factor(ypred), factor(tmp$Survived))
+     a = m$overall['Accuracy']
+     res[i, 1] = t
+     res[i, 2] = a
+ }
Warning message:
In confusionMatrix.default(factor(ypred), factor(tmp$Survived)) :
  Levels are not in the same order for reference and data. Refactoring data to match.
> colnames(res) = c("corte", "acuracia")
> 
> # Ponto Ã³timo
> topt1 = res[which.max(res$acuracia),1]
> print(paste0("Corte Ã³timo = ", topt1))
[1] "Corte Ã³timo = 5.003003003003"
> 
> tmp = dados %>% subset(Fare >= 50.77)
> res = as.data.frame(matrix(ncol=2, nrow=0))
> for(i in 1:npontos) {
+     t = pontos[i]
+     ypred = ifelse(tmp$Parch < t, 0, 1)
+     m = confusionMatrix(factor(ypred), factor(tmp$Survived))
+     a = m$overall['Accuracy']
+     res[i, 1] = t
+     res[i, 2] = a
+ }
There were 50 or more warnings (use warnings() to see the first 50)
> colnames(res) = c("corte", "acuracia")
> 
> # Ponto Ã³timo
> topt2 = res[which.max(res$acuracia),1]
> print(paste0("Corte Ã³timo = ", topt2))
[1] "Corte Ã³timo = 0"
> 
> # GrÃ¡fico da regiÃ£o de decisÃ£o
> g = ggplot(data =dados, aes(x = Parch, y = Fare, color = factor(Survived)))
> g + geom_point() + guides(color = F) + geom_hline(yintercept = 50.77, color = 'red', linetype = 'dashed') +
+  geom_segment(aes(x = 5.5, y = -1, xend = 5.5, yend = 50.77), color = "blue", linetype = 'dashed') +
+ geom_segment(aes(x = 0.5, y = 50.77, xend = 0.5, yend = 550), color = "blue", linetype = 'dashed') +
+ scale_y_continuous(expand = c(0, 0))
> 
> # Calculando a proporÃ§Ã£o de sobreviventes em cada regiÃ£o
> dados %>% subset(Fare < 50.77 & Parch <= 5) %>% summarise(p = sum(Survived) / n())
          p
1 0.3187415
> dados %>% subset(Fare < 50.77 & Parch > 5) %>% summarise(p = sum(Survived) / n())
  p
1 0
> dados %>% subset(Fare >= 50.77 & Parch == 0) %>% summarise(p = sum(Survived) / n())
          p
1 0.7227723
> dados %>% subset(Fare >= 50.77 & Parch > 0) %>% summarise(p = sum(Survived) / n())
          p
1 0.6206897
> 
> # Ãrvore
> library(rpart)
> library(rpart.plot)
> 
> dados$Survived = as.factor(dados$Survived)
> 
> # Treinando uma Ã¡rvore com apenas trÃªs variÃ¡veis
> mod1 = rpart(Survived ~ Sex + Age + Pclass, data = dados)
> 
> # Visualizando a Ã¡rvore resultante
> rpart.plot(mod1)
> 
> # Resumo do processo de treinamento
> summary(mod1)
Call:
rpart(formula = Survived ~ Sex + Age + Pclass, data = dados)
  n= 891 

          CP nsplit rel error    xerror       xstd
1 0.44444444      0 1.0000000 1.0000000 0.04244576
2 0.02339181      1 0.5555556 0.5555556 0.03574957
3 0.01461988      2 0.5321637 0.5847953 0.03641573
4 0.01169591      4 0.5029240 0.5672515 0.03602071
5 0.01000000      6 0.4795322 0.5584795 0.03581795

Variable importance
   Sex Pclass    Age 
    70     18     12 

Node number 1: 891 observations,    complexity param=0.4444444
  predicted class=0  expected loss=0.3838384  P(node) =1
    class counts:   549   342
   probabilities: 0.616 0.384 
  left son=2 (577 obs) right son=3 (314 obs)
  Primary splits:
      Sex    splits as  RL,       improve=124.426300, (0 missing)
      Pclass < 2.5  to the right, improve= 43.781830, (0 missing)
      Age    < 6.5  to the right, improve=  8.814172, (177 missing)

Node number 2: 577 observations,    complexity param=0.02339181
  predicted class=0  expected loss=0.1889081  P(node) =0.647587
    class counts:   468   109
   probabilities: 0.811 0.189 
  left son=4 (553 obs) right son=5 (24 obs)
  Primary splits:
      Age    < 6.5  to the right, improve=10.78893, (124 missing)
      Pclass < 1.5  to the right, improve=10.01914, (0 missing)

Node number 3: 314 observations,    complexity param=0.01461988
  predicted class=1  expected loss=0.2579618  P(node) =0.352413
    class counts:    81   233
   probabilities: 0.258 0.742 
  left son=6 (144 obs) right son=7 (170 obs)
  Primary splits:
      Pclass < 2.5  to the right, improve=31.163130, (0 missing)
      Age    < 12   to the left,  improve= 1.891684, (53 missing)
  Surrogate splits:
      Age < 18.5 to the left,  agree=0.564, adj=0.049, (0 split)

Node number 4: 553 observations
  predicted class=0  expected loss=0.1681736  P(node) =0.620651
    class counts:   460    93
   probabilities: 0.832 0.168 

Node number 5: 24 observations
  predicted class=1  expected loss=0.3333333  P(node) =0.02693603
    class counts:     8    16
   probabilities: 0.333 0.667 

Node number 6: 144 observations,    complexity param=0.01461988
  predicted class=0  expected loss=0.5  P(node) =0.1616162
    class counts:    72    72
   probabilities: 0.500 0.500 
  left son=12 (12 obs) right son=13 (132 obs)
  Primary splits:
      Age < 38.5 to the right, improve=3.875163, (42 missing)

Node number 7: 170 observations
  predicted class=1  expected loss=0.05294118  P(node) =0.1907969
    class counts:     9   161
   probabilities: 0.053 0.947 

Node number 12: 12 observations
  predicted class=0  expected loss=0.08333333  P(node) =0.01346801
    class counts:    11     1
   probabilities: 0.917 0.083 

Node number 13: 132 observations,    complexity param=0.01169591
  predicted class=1  expected loss=0.4621212  P(node) =0.1481481
    class counts:    61    71
   probabilities: 0.462 0.538 
  left son=26 (117 obs) right son=27 (15 obs)
  Primary splits:
      Age < 5.5  to the right, improve=1.777778, (42 missing)

Node number 26: 117 observations,    complexity param=0.01169591
  predicted class=1  expected loss=0.4871795  P(node) =0.1313131
    class counts:    57    60
   probabilities: 0.487 0.513 
  left son=52 (8 obs) right son=53 (109 obs)
  Primary splits:
      Age < 12   to the left,  improve=3.900498, (42 missing)

Node number 27: 15 observations
  predicted class=1  expected loss=0.2666667  P(node) =0.01683502
    class counts:     4    11
   probabilities: 0.267 0.733 

Node number 52: 8 observations
  predicted class=0  expected loss=0  P(node) =0.008978676
    class counts:     8     0
   probabilities: 1.000 0.000 

Node number 53: 109 observations
  predicted class=1  expected loss=0.4495413  P(node) =0.1223345
    class counts:    49    60
   probabilities: 0.450 0.550 

> 
> # Vamos avaliar o ganho de previsÃ£o a cada novo split da Ã¡rvore
> # Esse ganho Ã© chamado "complexity parameter" (CP)
> printcp(mod1)

Classification tree:
rpart(formula = Survived ~ Sex + Age + Pclass, data = dados)

Variables actually used in tree construction:
[1] Age    Pclass Sex   

Root node error: 342/891 = 0.38384

n= 891 

        CP nsplit rel error  xerror     xstd
1 0.444444      0   1.00000 1.00000 0.042446
2 0.023392      1   0.55556 0.55556 0.035750
3 0.014620      2   0.53216 0.58480 0.036416
4 0.011696      4   0.50292 0.56725 0.036021
5 0.010000      6   0.47953 0.55848 0.035818
> 
> # Visualizando a relaÃ§Ã£o entre complexidade do modelo e erro de previsÃ£o
> # Eixo horizontal: complexidade
> # Eixo vertical: mÃ©dia e desvio padrÃ£o do erro no cross-validation
> plotcp(mod1)
> 
> # Podando a Ã¡rvore: escolhemos um valor c de CP, e eliminamos todos os splits que nÃ£o tenham ganho de qualidade de no mÃ­nimo c
> # Um mÃ©todo usual Ã© escolher como corte para o CP o valor que forneceu o mÃ­nimo erro de cross-validation (coluna xerror da cptable)
> # Neste caso, o menor erro de cross validation foi no mÃ­nimo valor de cp; portanto a poda nÃ£o terÃ¡ efeito
> pmod = prune(mod1, mod1$cptable[which.min(mod1$cptable[,"xerror"]),"CP"])
> rpart.plot(pmod)
> 
> # Para efeito de teste, vamos podar a Ã¡rvore com um valor maior de complexidade
> pmod = prune(mod1, 0.02)
> rpart.plot(pmod)
> 
> # Vamos observar agora as previsÃµes da Ã¡rvore
> # O mÃ©todo predict vai fornecer a previsÃ£o do modelo
> # No caso da classificaÃ§Ã£o binÃ¡ria, a previsÃ£o Ã© uma probabilidade (P(y = 0) ou P(y=1))
> prob = predict(mod1, dados %>% select(c(Age, Sex, Pclass)))
> 
> # Modelo com mais variÃ¡veis
> mod2 = rpart(Survived ~ Sex + Age + Pclass + SibSp + Parch + Fare, data = dados)
> prob2 = predict(mod2, dados %>% select(c(Age, Sex, Pclass, Parch, SibSp, Fare)))
> 
> mod3 = rpart(Survived ~ Sex + Age + Pclass + SibSp + Parch + Fare + Embarked, data = dados)
> prob3 = predict(mod3, dados)
> 
> # Pergunta: a parrtir de qual valor da probabilidade de y = 1 devemos classificar o indivÃ­duo como sobrevivente?
> # Vamos observar o que acontece na curva ROC
> # Utilizaremos o pacote AUC
> library(AUC)
Error in library(AUC) : there is no package called â€˜AUCâ€™
Execution halted
