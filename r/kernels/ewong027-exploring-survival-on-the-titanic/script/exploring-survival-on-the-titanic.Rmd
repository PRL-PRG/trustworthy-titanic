---
title: 'Exploring the Titanic Dataset'
author: 'Megan L. Risdal'
date: '6 March 2016'
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: readable
    highlight: tango
---

##Loading the data:
```{r}
train <- read.csv('../input/train.csv', header = TRUE)
test <- read.csv('../input/test.csv', header = TRUE)
```

##Exploring the Data:
```{r}
#Looking to see where the NAs are
sapply(train,function(x) sum(is.na(x)))
sapply(test,function(x) sum(is.na(x)))

#For train there are 177 missing values in Age, but Age seems to be important, so I can
#set their age to the average age of the people to "fix" the NA coersion problem
train$Age[is.na(train$Age)] <- mean(train$Age,na.rm=T)

#For test, there are 86 missing age values and 1 missing fare value, which I will
#replace with the average once again
test$Age[is.na(test$Age)] <- mean(test$Age,na.rm=T)
test$Fare[is.na(test$Fare)] <- mean(test$Fare,na.rm=T)
```

##Cleaning the data: 
Now I will remove some of the columns that I do not believe are necessary. For example, ticket and name are too specific for each person so I will remove those columns. Additionally, there are too many blanks for cabin so I will remove that from the data. I have also converted Sex and where they embarked into numeric values so they can be taken into account in the model.
```{r}
#deleting columns
train$Name <- NULL
test$Name <- NULL
train$Ticket <- NULL
test$Ticket <- NULL
train$Cabin <- NULL
test$Cabin <- NULL

#making variables into numbers
train$Sex <- as.numeric(train$Sex)
test$Sex <- as.numeric(test$Sex)
train$Embarked <- as.numeric(train$Embarked)
test$Embarked <- as.numeric(test$Embarked)
```


##Model Selection:
AIC:
```{r}
mod <- glm(Survived ~ ., family=binomial, data = train)
step(mod)
```

Through AIC, it selects the model Survived ~ Pclass + Sex + Age + SibSp + Embarked.

BIC:
```{r}
n = nrow(train)
step(mod, k = log(n))
```

Through BIC, it selects the model Survived ~ Pclass + Sex + Age + SibSp. 

Running Cross Validation to see which is the best Model between AIC and BIC:
```{r}
m1 = glm(Survived ~ Pclass + Sex + Age + SibSp + Embarked, family = binomial, data = train) 
m2 = glm(Survived ~ Pclass + Sex + Age + SibSp, family = binomial, data = train) 

cv.scores = rep(-999, 2)
cv.scores[1] = sum((m1$residuals^2)/((1 - influence(m1)$hat)^2))
cv.scores[2] = sum((m2$residuals^2)/((1 - influence(m2)$hat)^2))
cv.scores
which.min(cv.scores)
```

From this calculation, we find that model 1 has the smallest cross validation value, which is `r min(cv.scores)`. Model 1 is the **AIC model**, which has the explainatory variables Pclass, Sex, Age, SibSp, Embarked.

The model that I created has 5 variables. I found this model through variable selection. What variable selection does is that it removes some of the variables that do not effect the model. 

##Fit of the Model:
```{r}
m1 = glm(Survived ~ Pclass + Sex + Age + SibSp + Embarked, family = binomial, data = train)
summary(m1)
```

##Predicitng the Model:
Here we will get predictions about the survial status. Since our predictions will be decimals, we will need to decide where the cut off is between when y is 0 or 1. I will get this point to be .5 because it is right in the middle. So, if P(y=1|X) > 0.5 then y = 1 otherwise y=0. 
```{r}
predictions <- predict(m1,test, type = 'response')
predictions <- ifelse(predictions > 0.5,1,0)
write.csv(predictions, file = 'predictions.csv', row.names = F)