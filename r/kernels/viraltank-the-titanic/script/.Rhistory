accumulatedCommits = 0
d = 0
while (accumulatedCommits <= most_commits_bydev) {
d = d + 1
accumulatedCommits = accumulatedCommits + devs[d, 2]
}
# verify that we have the correct nunber
check(sum(devs[1:d-1,2]) < most_commits_bydev)
check(sum(devs[1:d,2]) > most_commits_bydev)
devs_prolific <- devs$author[1:d]
# Show the number of languages the most prolific authors committed for
data %>% filter(author %in% devs_prolific) -> devs_prolific
devs_prolific = devs_prolific %>% group_by(author,language) %>% dplyr::summarize(n=n()) %>% dplyr::summarize(l=n())  %>% arrange(desc(l))
devs_prolific
hist(devs_prolific$l)
```
# Modelling
## Basic analyses & graphs of the dataset
```{r}
dimnames(X)[[2]]
pairs(X[,3:7], gap=0, pch='.')
```
Let's log transform the data:
```{r}
Y = logTransform(X)
pairs(Y[,2:6], gap=0, pch='.')
```
## Language Frequencies
Explore the relative frequencies of the languages--how many projects use them in the dataset
```{r}
sort(table(Y$language))
```
The dataset contains 17 languages (16, if we remove TypeScript). They were represented by unevenly distributed numbers of projects (from 23 for Perl to 199 for Javascript).
Explore the relationship between the languages, and other measurements
```{r}
# (devs, tins, max_commit_age, commits, bcommits)
boxplot(split(Y$lbcommits, Y$language), las=2, main="Log(bugged commits, 0 replaced with 0.5)")
par(mfrow=c(1,2), oma=c(1,1,1,1), mar=c(5,1,2,1))
boxplot(split(Y$ldevs, Y$language), las=2, main="Log(devs)")
boxplot(split(Y$ltins, Y$language), las=2, main="Log(tins)")
boxplot(split(Y$lmax_commit_age, Y$language), las=2,
main="Log(max_commit_age)")
boxplot(split(Y$lcommits, Y$language), las=2, main="Log(commits)")
```
The distribution of bug-commits varied between the languages, but so did the distributions of the other measurements. It is difficult to see direct associations between languages and bugs from bivariate plots.
## Fitting the Negative Binomial
Let's now fit the negative binomial and compare to the original paper - this is the same code as in replication:
```{r}
nbfit = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language, contrasts = list(language = contr.Weights(Y$language)), data=Y)
nbfit_r = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language_r, contrasts = list(language_r = contr.Weights(Y$language_r)), data=Y)
# combine them into single result table
resultWeighted = combineModels(nbfit, nbfit_r, Y$language)
juxtWeighted = merge(resultWeighted, baselineFSE_RQ1(), by = 0, all = T, sort = F)
juxtWeighted$ok = checkPValues(juxtWeighted, "FSE_pv", "pVal")
juxtWeighted
```
And we see that just cleaning the data did invalidate several of the language claims made by the original paper.
## Fitting the zero-Sum Contrasts Negative Binomial
The zero-sum contrasts are preferred to weighted contrasts for their better stability. Let's look at how they look:
```{r}
contr.sum(length(levels(Y$language)))
```
And let's fit the model:
```{r}
nbfit = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language, contrasts = list(language = contr.sum), data=Y)
nbfit_r = glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language_r, contrasts = list(language_r = contr.sum), data=Y)
# combine them into single result table
resultZeroSum = combineModels(nbfit, nbfit_r, Y$language)
juxtZeroSum = merge(resultZeroSum, baselineFSE_RQ1(), by = 0, all = T, sort = F)
juxtZeroSum$ok = checkPValues(juxtZeroSum, "FSE_pv", "pVal")
juxtZeroSum
```
Some invalidatuions still.
##  Fit Negative Binomial regression without languages and compare the full and the reduced models
```{r}
nbfit_reduced <- glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits, data=Y)
summary(nbfit_reduced)
```
Comparing two nested models, i.e.$H_0$: reduced model vs $H_a$: full model, using F-test:
```{r}
anova(nbfit_reduced, nbfit)
```
```{r}
cat("AIC, full:", AIC(nbfit), "\n")
cat("AIC, reduced:", AIC(nbfit_reduced), "\n")
cat("BIC, full:", BIC(nbfit), "\n")
cat("BIC, reduced:", BIC(nbfit_reduced), "\n")
```
The difference between the models is borderline.
## Adjusting for Multiple Hypothesis
The original paper just compares the p-Values against thresholds, which is not correct way to do. In the presence of multiple hypothesis, the p-Values must be adjusted. There are various ways to adjust the p-Values, namely the Bonferroni and FDR (Benjamini & Hochberg). The FDR adjustment is more permissive and the Bonferrioni is the more conservative one. What we can do now is to revisit the juxtaposed tables and add extra columns for the FDR and Bonferroni adjustments:
```{r}
# the the pValues for the predictions, not for the control variables since these are ignored by the adjustments
pValWeighted = juxtWeighted$pVal[6:(5 + length(unique(Y$language)))]
# create the empty vectors with NAs instead of pValues
fdrWeighted = rep(NA, nrow(juxtWeighted))
bonfWeighted = rep(NA, nrow(juxtWeighted))
# update the relevant parts of the vectors with the adjusted pValues
fdrWeighted[6:(5+ length(pValWeighted))] = round(p.adjust(pValWeighted, "fdr"), 3)
bonfWeighted[6:(5+ length(pValWeighted))] = round(p.adjust(pValWeighted, "bonferroni"), 3)
# add the columns to the juxtaposed tables
juxtWeighted$pVal_fdr = fdrWeighted
juxtWeighted$pVal_bonf = bonfWeighted
# now remove the old ok column and add the ok, ok_fdr and ok_bonf columns
juxtWeighted = juxtWeighted %>% dplyr::select(-(ok))
# add the columns
juxtWeighted$ok = checkPValues(juxtWeighted, "FSE_pv", "pVal")
#juxtWeighted$ok_fdr = checkPValuesLevel(juxtWeighted, "FSE_pv", "pVal_fdr", 0.01)
#juxtWeighted$ok_bonf = checkPValuesLevel(juxtWeighted, "FSE_pv", "pVal_bonf", 0.01)
juxtWeighted$ok_fdr = checkPValues(juxtWeighted, "FSE_pv", "pVal_fdr")
juxtWeighted$ok_bonf = checkPValues(juxtWeighted, "FSE_pv", "pVal_bonf")
juxtWeighted
```
With the exception of the last language (Coffeescript), the FDR and Bonferroni are the same and they actually invalidate some of the original predictions. Let's look at the zero-sum contrasts version now:
```{r}
# the the pValues for the predictions, not for the control variables since these are ignored by the adjustments
pValZeroSum = juxtZeroSum$pVal[6:(5 + length(unique(Y$language)))]
# create the empty vectors with NAs instead of pValues
fdrZeroSum = rep(NA, nrow(juxtZeroSum))
bonfZeroSum = rep(NA, nrow(juxtZeroSum))
# update the relevant parts of the vectors with the adjusted pValues
fdrZeroSum[6:(5+ length(pValZeroSum))] = round(p.adjust(pValZeroSum, "fdr"), 3)
bonfZeroSum[6:(5+ length(pValZeroSum))] = round(p.adjust(pValZeroSum, "bonferroni"), 3)
# add the columns to the juxtaposed tables
juxtZeroSum$pVal_fdr = fdrZeroSum
juxtZeroSum$pVal_bonf = bonfZeroSum
# now remove the old ok column and add the ok, ok_fdr and ok_bonf columns
juxtZeroSum = juxtZeroSum %>% dplyr::select(-(ok))
# add the columns
juxtZeroSum$ok = checkPValues(juxtZeroSum, "FSE_pv", "pVal")
#juxtZeroSum$ok_fdr = checkPValuesLevel(juxtZeroSum, "FSE_pv", "pVal_fdr", 0.01)
#juxtZeroSum$ok_bonf = checkPValuesLevel(juxtZeroSum, "FSE_pv", "pVal_bonf", 0.01)
juxtZeroSum$ok_fdr = checkPValues(juxtZeroSum, "FSE_pv", "pVal_fdr")
juxtZeroSum$ok_bonf = checkPValues(juxtZeroSum, "FSE_pv", "pVal_bonf")
juxtZeroSum
```
The situation is fairly similar here.
## Statistical significance vs practical significance, for languages
Since the number of observations is large, statistical significance can be driven by the sample size without being meaningful in practice.
Here we contrast model-based confidence intervals and prediction intervals, on the log and on the original scale
```{r}
numLanguages = length(unique(Y$language))
# Create a new data structure for prediction
newY <- NULL
for (i in 1:numLanguages) {
newY <- rbind(newY,
data.frame(language=rep(levels(Y$language)[i], 100),
ldevs=rep(median(Y$ldevs), 100),
lcommits=seq(from=min(Y$lcommits), to=max(Y$lcommits), length=100),
ltins=rep(median(Y$ltins), 100),
lmax_commit_age=rep(median(Y$lmax_commit_age), 100)))
}
newY$commits <- exp(newY$lcommits)
# Make predictions
pr_nbfit <- predict(nbfit, type="response", newdata=newY, se.fit=TRUE)
newY$pr_mean <- pr_nbfit$fit
newY$pr_se <- pr_nbfit$se.fit
```
Consider languages with the most predicted bugs (C++) and fewest predicted bugs (Clojure).
Compute the log CI for C++ and Clojure and the log Prediction CI.
Then translate the intervals on the original scale.
```{r}
axfont  = 32 # size of axis title font
ticfont = 26 # size of axes' font
legfont = 28 # size of legend title
legtext = 26 # size of legend text
ptitle  = 30 # size of plot title letters
getConfInterval<-function(df,lang) {
df %>%
filter(language==lang)  %>%
mutate(language = lang,
x = lcommits, y = log(pr_mean),
yhigh = log(pr_mean + qnorm(1-0.01/numLanguages) * pr_se),
ylow =  log(pr_mean - qnorm(1-0.01/numLanguages) * pr_se)) %>%
dplyr::select(language, x, y, ylow, yhigh)
}
dfCI <- rbind(getConfInterval(newY, "C++"), getConfInterval(newY, "Clojure"))
getPredInterval<-function(df,lang) {
df %>%
filter(language==lang) %>%
mutate(language = lang,
x = lcommits, y = log(pr_mean),
yhigh = log(qnbinom(1-0.01/numLanguages, mu= pr_mean, size= nbfit$theta) ),
ylow = log(qnbinom(0.01/numLanguages, mu=pr_mean, size=nbfit$theta))) %>%
dplyr::select(language, x, y, ylow, yhigh)
}
dfPI <- rbind(getPredInterval(newY, "C++"), getPredInterval(newY, "Clojure"))
plotIt <- function(df) {
ggplot(data = df, aes(x=x,y=y,color=language)) + geom_line() +
geom_ribbon(aes(ymin=df$ylow, ymax=df$yhigh), linetype=2, alpha=0.1) +
labs(x="log of commits", y="log of bug-fixing commits") +
theme_light() +
theme(axis.title = element_text(size=axfont),
axis.text = element_text(size=ticfont),
plot.title = element_text(hjust = 0.5, size = ptitle))
}
plotIt(dfCI) + theme(legend.position = "none") + ggtitle("(a)") -> p1
plotIt(dfPI) + theme(legend.title = element_text(size=legfont),
legend.text = element_text(size=legtext)) +
guides(colour = guide_legend(nrow = 1)) +
ggtitle("(b)") -> p2
# Grab the legend to display it separately
tmp    <- ggplot_gtable(ggplot_build(p2))
l      <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
legend <- tmp$grobs[[l]]
legend
p2 + theme(legend.position = "none") -> p2
getConfInterval <- function(df,lang) {
df %>%
filter(language==lang)  %>%
mutate(language = lang,
x = commits,
y = pr_mean,
yhigh = pr_mean + qnorm(1-0.01/numLanguages) * pr_se,
ylow =  pr_mean - qnorm(1-0.01/numLanguages)* pr_se ) %>%
dplyr::select(language, x, y, ylow, yhigh)
}
dfCI <- rbind(getConfInterval(newY, "C++"), getConfInterval(newY, "Clojure"))
getPredInterval<-function(df,lang) {
df %>%
filter(language==lang) %>%
mutate(language = lang,
x = commits, y = pr_mean,
yhigh = qnbinom(1-0.01/numLanguages, mu= pr_mean, size= nbfit$theta) ,
ylow =qnbinom(0.01/numLanguages, mu=pr_mean, size=nbfit$theta)) %>%
dplyr::select(language, x,y,ylow,yhigh)
}
dfPI <- rbind(getPredInterval(newY, "C++"), getPredInterval(newY, "Clojure"))
plotIt <- function(df) {
ggplot(data = df, aes(x=x,y=y,color=language)) + geom_line() +
geom_ribbon(aes(ymin=df$ylow, ymax=df$yhigh), linetype=2, alpha=0.1) +
theme_light() +
theme(axis.title = element_text(size=axfont),
axis.text = element_text(size=ticfont),
plot.title = element_text(hjust = 0.5, size = ptitle),
legend.position = "none") +
labs(x="commits", y="bug-fixing commits")
}
plotIt(dfCI) + xlim(0,800) + ylim(0,400) + ggtitle("(c)") -> p3
plotIt(dfPI) + xlim(0,800) + ylim(0,620) + ggtitle("(d)") -> p4
```
```{r fig.width=32, fig.height=8}
pdf(paste(WORKING_DIR, "/Figures/intervals.pdf", sep = ""), width=32, height=8)
figs <- grid.arrange(arrangeGrob(p1, p2, p3, p4, nrow=1), arrangeGrob(legend), nrow=2, heights=c(4,1))
dev.off()
```
```{r}
# Plot predicted means for all the languages
par(mfrow=c(1,2))
# Log scale
plot(log(pr_mean)~lcommits,
main="log(Exp. values), all languages",
data=newY[newY$language==levels(Y$language)[1],],
type="l",
ylab="log(bugged commits)")
for (i in 2:numLanguages) {
lines(log(pr_mean)~lcommits,
data=newY[newY$language==levels(Y$language)[i],])
}
# Original scale
plot(pr_mean~commits,
main="Exp. values, all languages",
data=newY[newY$language==levels(Y$language)[1],],
type="l",
xlim=c(0, 800),
ylim=c(0,400),
ylab="bugged commits")
for (i in 2:numLanguages) {
lines(pr_mean~commits,
data=newY[newY$language==levels(Y$language)[i],])
}
```
## Add uncertainty in labels
```{r}
if (UNCERTAINTY) {
fp <- 0.36
fn <- 0.11
# Function to get parameter values
getParams <- function(Ystar) {
# Fit NB with standard contrasts
nbfit <- glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language,
contrasts = list(language = "contr.sum"), data=Ystar)
s <- summary(nbfit)$coefficients
# Fit the releveled model with standard contrasts,
# to get the parameter for Scala
nbfit_r <- glm.nb(bcommits~lmax_commit_age+ltins+ldevs+lcommits+language_r,
contrasts = list(language_r = "contr.sum"), data=Ystar)
s_r <- summary(nbfit_r)$coefficients
# Return params, incluing Scala
out <- c(s[,1], s_r[6, 1])
names(out) <- c(dimnames(s)[[1]][1:5], levels(Ystar$language))
out
}
numBootstrapIterations = 10000
# Perform sampling
set.seed(1)
paramNum <- numLanguages + 5
paramsLang <- matrix(rep(NA,paramNum*numBootstrapIterations), nrow=paramNum)
for (i in 1:numBootstrapIterations) {
# Sample rows
Ystar <- Y[sample(1:nrow(Y), replace = T),]
# Adjust for tp and fp:
# Reduce bcommits by prob fp
# Increase non-bugged commits by prob fn
tmp <- rbinom(n=nrow(Ystar), size=Ystar$bcommits, prob=1-fp) +
rbinom(n=nrow(Ystar), size=(Ystar$commits-Ystar$bcommits), prob=fn)
Ystar$bcommits <- tmp
# Get parameters
paramsLang[,i] <- getParams(Ystar)
}
}
```
To determine whether the bootstrapped values are statistically signifficant, we analyze whether the x and 1-x quantiles both have the same sign. If they do, the result is signifficant in that regard, if the do not, then the results is not statistically signifficant. Similarly to p-values, we can compare using different quantiles. The proper, conservative quantile is 0.01 divided by number of languages tested, the less conservative option is just 0.01.
```{r}
if (UNCERTAINTY) {
paramsRowNames = getModelRowNames(nbfit, Y$language)
result = data.frame(
row.names = paramsRowNames,
coef = rep(NA, length(paramsRowNames)),
se = rep(NA, length(paramsRowNames)),
sig = rep(NA, length(paramsRowNames)),
sigCons = rep(NA, length(paramsRowNames))
)
par(mfrow=c(2,4))
quant = 0.01
quantCons = 0.01 / numLanguages
for ( i in 1:length(paramsRowNames)) {
paramMean = mean(paramsLang[i,])
result$coef[[i]] = round(paramMean, digits = 2)
result$se[[i]] = round(sd(paramsLang[i,]), digits = 2)
qsigns = sign(quantile(paramsLang[i,], probs = c(quant, 1-quant, quantCons, 1-quantCons), na.rm = T))
result$sig[[i]] = qsigns[[1]] == qsigns[[2]]
result$sigCons[[i]] = qsigns[[3]] == qsigns[[4]]
# store the fake pValue here so that we can later try FDR. Since bootstrap does not really have pValues, we take the % that different result than the mean will be reported to be the pValue
if (paramMean > 0)
result$fakePVal[[i]] = sum(paramsLang[i,] < 0) / length(paramsLang[i,])
else
result$fakePVal[[i]] = sum(paramsLang[i,] > 0) / length(paramsLang[i,])
hist(paramsLang[i,],
xlab=paramsRowNames[i],
main=paste("mean =", round(mean(paramsLang[i,]), digits=2))
)
abline(v=0, col="red", lwd=2)
}
# calculate the bonferroni and FDR pVal Coefficients for the fake p values - note that we do not adjust the pValues for the control variables (top lines)
result$fakeBonf = result$fakePVal
result$fakeFdr = result$fakePVal
adjEnd = length(result$fakePVal)
adjStart = adjEnd - numLanguages
result$fakeBonf[adjStart:adjEnd] = round(p.adjust(result$fakePVal[adjStart:adjEnd], "bonferroni"), 3)
result$fakeFdr[adjStart:adjEnd] = round(p.adjust(result$fakePVal[adjStart:adjEnd], "fdr"), 3)
result$fakePValCheck = result$fakePVal <= 0.01
result$fakeBonfCheck = result$fakeBonf <= 0.01
result$fakeFdrCheck = result$fakeFdr <= 0.01
}
```
```{r}
if (UNCERTAINTY) {
result # %>% dplyr::select(sigCons, fakePVal, fakeBonf, fakeFdr)
}
```
Finally, let's juxtapose this to the baseline as usual:
```{r}
if (UNCERTAINTY) {
juxtBootstrap = merge(result, baselineFSE_RQ1(), by = 0, all = T, sort = F)
juxtBootstrap$ok = checkSignificance(juxtBootstrap, "FSE_pv", "sigCons")
juxtBootstrap
}
```
# More graphs: Bug rate over time
```{r echo=FALSE}
filter_proj <- function(df,proj,lang) {
data %>%
ungroup() %>%
filter(project == proj, language == lang) %>%
dplyr::select(commit_age, isbug) %>%
arrange(commit_age) -> bt
bt %>%
group_by(commit_age,isbug) %>%
dplyr::summarize(n = n()) %>%
spread(key = "isbug",value = "n") -> bt2
bt2 %>%
dplyr::mutate(br = round(`0`/(`0`+`1`),2),
month = as.integer(commit_age/30)) -> bt3
bt3  %>%
na.omit() %>%
dplyr::group_by(month) %>%
dplyr::summarize(n = n(), brs = sum(br), brm = brs/n) %>%
dplyr::mutate(name= paste0(proj," ",lang)) -> prj
rbind(df, prj)
}
filter_proj(NULL, "linux","C") %>%
filter_proj("mono","C#") %>%
filter_proj("homebrew","Ruby") %>%
filter_proj("WordPress","Php") %>%
filter_proj("salt","Python") %>%
filter_proj("mythtv","C++") %>%
filter_proj("jenkins","Java") %>%
filter_proj("akka","Scala") %>%
filter_proj("rabbitmq-server","Erlang") %>%
filter_proj("brackets","Javascript") %>%
filter_proj("yi","Haskell") %>%
filter_proj("overtone","Clojure")  ->prj
prj %>% ggplot( aes(x = month, y = brm)) +
geom_point(size=.4) +
theme_light() +
geom_smooth(method='lm',size=.5, formula = y ~ x) +
facet_wrap( ~ name,ncol=3) + labs(x="",y="") +
xlab("Project lifetime (months)") + ylab("Percent bug-labeled commits") +
theme(strip.text.x = element_text(size = 14), text = element_text(size=13)) +
theme(text = element_text(size=20))
ggsave(paste(WORKING_DIR, "/Figures/bugspermonth.pdf", sep = ""), width = 8, height = 8, dpi = 100)
```
# Commits for top-5 projects by language
```{r fig.width = 20, fig.height = 4, echo=FALSE}
data %>% group_by(language, project) %>% dplyr::summarize(n = n()) %>% arrange(desc(n)) %>% arrange(desc(language)) %>% top_n(5, wt = n) -> projsize_ordered2
projsize_ordered2
projsize_ordered2 %>% ggplot(aes(x = reorder(factor(project), -n), y = n)) +
geom_bar(stat="identity") +
facet_grid(. ~ language, scales = "free") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(x = "Project (by language)", y = "Number of Commits") +
scale_y_continuous(labels = scales::comma)
```
# Authors
```{r}
everything %>% group_by(author, sha) %>% dplyr::summarize(n=n()) %>% group_by(author) %>% dplyr::summarise(commits = n()) -> auth
auth %>% arrange(desc(commits))
tot <- sum(auth$commits)
check( tot == 1485049)
top <- sort(auth$commits, dec=T)[1:453]
sum(top)/tot ## 46 %
everything %>% group_by(author, project) %>% dplyr::summarize(n=n()) %>%
group_by(author) %>% dplyr::summarise(proj = n())   %>% arrange(desc(proj)) -> authproj
summary(authproj)  ## mean 1.2
```
# Writing the results
Finally, let's write the results of our analyses into CSV files so that they can be picked and analyzed later:
```{r}
weighted = data.frame(
coef = juxtWeighted$coef,
se = juxtWeighted$se,
pVal = round(juxtWeighted$pVal,3),
pVal_fdr = juxtWeighted$pVal_fdr,
pVal_bonf = juxtWeighted$pVal_bonf,
row.names = juxtWeighted$Row.names
)
write.csv(weighted, paste0(WORKING_DIR, "/Data/languages_weighed.csv"))
zeroSum = data.frame(
coef = juxtZeroSum$coef,
se = juxtZeroSum$se,
pVal = round(juxtZeroSum$pVal,3),
pVal_fdr = juxtZeroSum$pVal_fdr,
pVal_bonf = juxtZeroSum$pVal_bonf,
row.names = juxtZeroSum$Row.names
)
write.csv(zeroSum, paste0(WORKING_DIR, "/Data/languages_zeroSum.csv"))
# only store bootstrap if we have actually created it
if (UNCERTAINTY) {
bootstrap = data.frame(
coef = juxtBootstrap$coef,
se = juxtBootstrap$se,
sig = juxtBootstrap$sig,
sigCons = juxtBootstrap$sigCons,
row.names = juxtBootstrap$Row.names
)
write.csv(bootstrap, paste0(WORKING_DIR, "/Data/languages_bootstrap.csv"))
}
```
```{r}
remove(WORKING_DIR)
```
# load the file containing the actual implementation details
knitr::opts_chunk$set(echo = FALSE)
source("implementation.R")
initializeEnvironment("./artifact/repetition")
rm(list = ls(all.names = TRUE))
gc()
setwd("~/trustworthy_titanic")
dirs = list.dirs(path = './r/kernels', full.names = FALSE, recursive = FALSE)
dirs = rev(dirs)
dirs
detach.packages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
for (dir in dirs)
{
try(
{
print(dir)
temp.env <- new.env()
#detach.packages()
library(NCmisc)
library(jsonlite)
library(stringr)
d = paste("~/trustworthy_titanic/r/kernels/",dir, sep="")
d = paste(d, "/script", sep="")
print(d)
setwd(d)
f.name <- list.files(path = ".", pattern = "\\.R$", full.names = TRUE, recursive = TRUE, ignore.case = TRUE)
sys.source(f.name, envir = temp.env)
lst <- list.functions.in.file(f.name, alphabetic = TRUE)
exportJSON <- toJSON(lst)
write(exportJSON, paste(str_remove(f.name, ".R"), ".json", sep=""))
rm(list=setdiff(ls(), "dirs")) #will clear all objects includes hidden objects.
gc()
}
)
}
