
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> 
> # This R script will run on our backend. You can write arbitrary code here!
> 
> # Many standard libraries are already installed, such as randomForest
> library(randomForest)
randomForest 4.6-14
Type rfNews() to see new features/changes/bug fixes.
> library (rpart)
> library (caret)
Loading required package: lattice
Loading required package: ggplot2

Attaching package: ‘ggplot2’

The following object is masked from ‘package:randomForest’:

    margin

Warning message:
package ‘ggplot2’ was built under R version 3.6.2 
> #install.packages('rattle')
> #install.packages ('rpart.plot')
> #install.packages ('RColorBrewer')
> #library(rattle)
> library(rpart.plot)
> library(RColorBrewer)
> library (lattice)
> library (ggplot2)
> 
> 
> 
> # The train and test data is stored in the ../input directory
> train <- read.csv("../input/train.csv")
> test  <- read.csv("../input/test.csv")
> 
> 
> # *********************9-2-2016*************************
> 
> 
> # Let us split train into two variables (one of 80%, the other of 20%)
> #train2 <- train[1:712,]
> #train2
> #str(train2)
> 
> #train2cv <- train[713:891,]
> #train2cv
> 
> #How many survived in the train2 data set
> #Survived = 278 (39.04%), Died = 434 (60.96%), total 712 in train2 
> #table(train2$Survived)
> #prop.table(table(train2$Survived))
> 
> #How many males, females in train2 data set
> # Females = 256 (35.96%), Males = 456 (64.04%)
> #table(train2$Sex)
> #prop.table(table(train2$Sex))
> 
> #How many males, females survived
> # Females survived = 190 (74.22% of all women)
> # Males survived = 88 (19.30% of all males)
> #table(train2$Sex, train2$Survived)
> #prop.table(table(train2$Sex, train2$Survived),1)
> 
> # Now, to investigate Age
> # table(train2$Age,train2$Survived)
> 
> 
> #fit <- rpart(Survived ~ Sex + Age + SibSp + Pclass + Embarked + Parch,
> #        data = train2,
> #       method = "class")
> #rpart.plot(fit)
> 
> 
> #*****************************9-13-2016**********************************
> # First, clean up Pclass to be ordered using as.factor (so we do not get "1.5" in our decision trees)
> train$Pclass <- as.factor(train$Pclass)
> test$Pclass <- as.factor(test$Pclass)
> 
> 
> #Group the Fare feature into fewer categories
> train$Fare2 <- '30+'
> train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
> train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
> train$Fare2[train$Fare < 10] <- '<10'
> 
> test$Fare2 <- '30+'
> test$Fare2[test$Fare < 30 & test$Fare >= 20] <- '20 - 30'
> test$Fare2[test$Fare < 20 & test$Fare >= 10] <- '10 - 20'
> test$Fare2[test$Fare < 10] <- '<10'
> 
> #Break up train dataframe into 5 train frames and use one of them as the cv set.  Then rotate through.
> 
> train1 <- train[1:179,]
> train2 <-train[180:358,]
> train3 <- train[359:537,]
> train4 <- train[538:716,]
> train5 <- train[717:891,]
> 
> 
> # Setting up pairs of newtrain and cv to use on building our model.
> cv1 <- train1
> newtrain2345 <- rbind(train2,train3,train4,train5)
> table(newtrain2345$Survived)

  0   1 
429 283 
> table(newtrain2345$Sex)

female   male 
   252    460 
> table(newtrain2345$Sex,newtrain2345$Survived)
        
           0   1
  female  63 189
  male   366  94
> # USe first pair of data sets (cv1 and newtrain2345).
> #Construct a decision tree using rpart and examine outcomes
> #Refer to this as "model 1"
> fit <- rpart(Survived ~ Fare2 + Sex + Age + SibSp + Pclass + Embarked + Parch,
+         data = newtrain2345,
+        method = "class")
> predict(fit,cv1,interval = "confidence")
             0         1
1   0.82126697 0.1787330
2   0.04794521 0.9520548
3   0.42857143 0.5714286
4   0.04794521 0.9520548
5   0.82126697 0.1787330
6   0.82126697 0.1787330
7   0.82126697 0.1787330
8   0.16666667 0.8333333
9   0.42857143 0.5714286
10  0.04794521 0.9520548
11  0.42857143 0.5714286
12  0.04794521 0.9520548
13  0.82126697 0.1787330
14  0.82126697 0.1787330
15  0.42857143 0.5714286
16  0.04794521 0.9520548
17  0.16666667 0.8333333
18  0.82126697 0.1787330
19  0.42857143 0.5714286
20  0.42857143 0.5714286
21  0.82126697 0.1787330
22  0.82126697 0.1787330
23  0.42857143 0.5714286
24  0.82126697 0.1787330
25  0.79310345 0.2068966
26  0.79310345 0.2068966
27  0.82126697 0.1787330
28  0.82126697 0.1787330
29  0.42857143 0.5714286
30  0.82126697 0.1787330
31  0.82126697 0.1787330
32  0.04794521 0.9520548
33  0.42857143 0.5714286
34  0.82126697 0.1787330
35  0.82126697 0.1787330
36  0.82126697 0.1787330
37  0.82126697 0.1787330
38  0.82126697 0.1787330
39  0.42857143 0.5714286
40  0.42857143 0.5714286
41  0.42857143 0.5714286
42  0.04794521 0.9520548
43  0.82126697 0.1787330
44  0.04794521 0.9520548
45  0.42857143 0.5714286
46  0.82126697 0.1787330
47  0.82126697 0.1787330
48  0.42857143 0.5714286
49  0.82126697 0.1787330
50  0.42857143 0.5714286
51  0.82126697 0.1787330
52  0.82126697 0.1787330
53  0.04794521 0.9520548
54  0.04794521 0.9520548
55  0.82126697 0.1787330
56  0.82126697 0.1787330
57  0.04794521 0.9520548
58  0.82126697 0.1787330
59  0.04794521 0.9520548
60  0.82126697 0.1787330
61  0.82126697 0.1787330
62  0.04794521 0.9520548
63  0.82126697 0.1787330
64  0.16666667 0.8333333
65  0.82126697 0.1787330
66  0.82126697 0.1787330
67  0.04794521 0.9520548
68  0.82126697 0.1787330
69  0.42857143 0.5714286
70  0.82126697 0.1787330
71  0.82126697 0.1787330
72  0.79310345 0.2068966
73  0.82126697 0.1787330
74  0.82126697 0.1787330
75  0.82126697 0.1787330
76  0.82126697 0.1787330
77  0.82126697 0.1787330
78  0.82126697 0.1787330
79  0.16666667 0.8333333
80  0.42857143 0.5714286
81  0.82126697 0.1787330
82  0.82126697 0.1787330
83  0.42857143 0.5714286
84  0.82126697 0.1787330
85  0.04794521 0.9520548
86  0.42857143 0.5714286
87  0.82126697 0.1787330
88  0.82126697 0.1787330
89  0.04794521 0.9520548
90  0.82126697 0.1787330
91  0.82126697 0.1787330
92  0.82126697 0.1787330
93  0.82126697 0.1787330
94  0.82126697 0.1787330
95  0.82126697 0.1787330
96  0.82126697 0.1787330
97  0.82126697 0.1787330
98  0.82126697 0.1787330
99  0.04794521 0.9520548
100 0.82126697 0.1787330
101 0.42857143 0.5714286
102 0.82126697 0.1787330
103 0.82126697 0.1787330
104 0.82126697 0.1787330
105 0.82126697 0.1787330
106 0.82126697 0.1787330
107 0.42857143 0.5714286
108 0.82126697 0.1787330
109 0.82126697 0.1787330
110 0.79310345 0.2068966
111 0.82126697 0.1787330
112 0.42857143 0.5714286
113 0.82126697 0.1787330
114 0.42857143 0.5714286
115 0.42857143 0.5714286
116 0.82126697 0.1787330
117 0.82126697 0.1787330
118 0.82126697 0.1787330
119 0.82126697 0.1787330
120 0.79310345 0.2068966
121 0.82126697 0.1787330
122 0.82126697 0.1787330
123 0.82126697 0.1787330
124 0.04794521 0.9520548
125 0.82126697 0.1787330
126 0.82126697 0.1787330
127 0.82126697 0.1787330
128 0.82126697 0.1787330
129 0.79310345 0.2068966
130 0.82126697 0.1787330
131 0.82126697 0.1787330
132 0.82126697 0.1787330
133 0.42857143 0.5714286
134 0.04794521 0.9520548
135 0.82126697 0.1787330
136 0.82126697 0.1787330
137 0.04794521 0.9520548
138 0.82126697 0.1787330
139 0.82126697 0.1787330
140 0.82126697 0.1787330
141 0.42857143 0.5714286
142 0.42857143 0.5714286
143 0.42857143 0.5714286
144 0.82126697 0.1787330
145 0.82126697 0.1787330
146 0.82126697 0.1787330
147 0.82126697 0.1787330
148 0.79310345 0.2068966
149 0.82126697 0.1787330
150 0.82126697 0.1787330
151 0.82126697 0.1787330
152 0.04794521 0.9520548
153 0.82126697 0.1787330
154 0.82126697 0.1787330
155 0.82126697 0.1787330
156 0.82126697 0.1787330
157 0.42857143 0.5714286
158 0.82126697 0.1787330
159 0.82126697 0.1787330
160 0.82126697 0.1787330
161 0.82126697 0.1787330
162 0.04794521 0.9520548
163 0.82126697 0.1787330
164 0.82126697 0.1787330
165 0.16666667 0.8333333
166 0.82126697 0.1787330
167 0.04794521 0.9520548
168 0.79310345 0.2068966
169 0.82126697 0.1787330
170 0.82126697 0.1787330
171 0.82126697 0.1787330
172 0.16666667 0.8333333
173 0.42857143 0.5714286
174 0.82126697 0.1787330
175 0.82126697 0.1787330
176 0.82126697 0.1787330
177 0.82126697 0.1787330
178 0.04794521 0.9520548
179 0.82126697 0.1787330
> 
> #submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
> #write.csv(submit, file = "theyallperish.csv", row.names = FALSE)
> 
> 
> 
> 
> 
> cv2 <- train2
> newtrain1345 <- rbind(train1,train3,train4,train5)
> #table(newtrain1345$Survived)
> #table(newtrain1345$Sex)
> #table(newtrain1345$Sex,newtrain1345$Survived)
> # USe first pair of data sets (cv1 and newtrain2345).
> #Construct a decision tree using rpart and examine outcomes
> #Refer to this as "model 1"
> #fit <- rpart(Survived ~ Fare2 + Sex + Age + SibSp + Pclass + Embarked + Parch,
> #        data = newtrain1345,
> #       method = "class")
> #rpart.plot(fit)
> #predict(fit,cv2,interval = "confidence")
> 
> 
> 
> 
> cv3 <- train3
> newtrain1245 <- rbind(train1,train2,train4,train5)
> #table(newtrain1245$Survived)
> #table(newtrain1245$Sex)
> #table(newtrain1245$Sex,newtrain1245$Survived)
> # USe first pair of data sets (cv3 and newtrain1245).
> #Construct a decision tree using rpart and examine outcomes
> #Refer to this as "model 1"
> #fit <- rpart(Survived ~ Fare2 + Sex + Age + SibSp + Pclass + Embarked + Parch,
> #        data = newtrain1245,
> #       method = "class")
> #rpart.plot(fit)
> #predict(fit,cv3,interval = "confidence")
> 
> 
> 
> 
> cv4 <- train4
> newtrain1235 <- rbind(train1,train2,train3,train5)
> #table(newtrain1235$Survived)
> #table(newtrain1235$Sex)
> #table(newtrain1235$Sex,newtrain1235$Survived)
> # USe first pair of data sets (cv4 and newtrain1235).
> #Construct a decision tree using rpart and examine outcomes
> #Refer to this as "model 1"
> #fit <- rpart(Survived ~ Fare2 + Sex + Age + SibSp + Pclass + Embarked + Parch,
> #        data = newtrain1235,
> #       method = "class")
> #rpart.plot(fit)
> #predict(fit,cv4,interval = "confidence")
> 
> 
> 
> 
> cv5 <- train5
> newtrain1234 <- rbind(train1,train2,train3,train4)
> #table(newtrain1234$Survived)
> #table(newtrain1234$Sex)
> #table(newtrain1234$Sex,newtrain1234$Survived)
> # USe first pair of data sets (cv5 and newtrain1234).
> #Construct a decision tree using rpart and examine outcomes
> #Refer to this as "model 1"
> #fit <- rpart(Survived ~ Fare2 + Sex + Age + SibSp + Pclass + Embarked + Parch,
> #        data = newtrain1234,
> #       method = "class")
> #rpart.plot(fit)
> #predict(fit,cv5,interval = "confidence")
> 
> 
> #At the conclusion of running this model (fit) against all 5 sets of data,
> #it seems that the percentage accuracy in predicting survival is 78.77095% (median) or 79.2556% (average).
> #Let us make a submission based on this model against test and see where Kaggle rates us.....
> 
> 
> #fit <- rpart(Survived ~ Fare2 + Sex + Age + SibSp + Pclass + Embarked + Parch,
> #        data = newtrain2345,
> #      method = "class")
> 
> #predict(fit, test, interval = "confidence")
> #prediction <- predict(fit,test,interval = "confidence")
> #prediction.Survived
> 
> 
> 
> 
> 
> # Save the solution to a dataframe with two columns: PassengerId and Survived (prediction)
> #solution <- data.frame(PassengerId = test$PassengerId, Survived <- prediction)
> 
> #solution
> 
> 
> 
> 
> # Write the solution to file
> #write.csv(solution, file = 'rf_mod_Solution.csv', row.names = F)
> 
> 
> #submission <- test(PassengerId = test$PassengerId)
> #submission$Survived <- predict(fit, test, interval = "confidence")
> #write.csv(submission, file = "1_random_forest_r_submission.csv", row.names=FALSE)
> #submission
> 
> 
> 
> 
> 
> 
> 
> 
> #Investigating Random Forests
> #rf <- randomForest(extractFeatures(train), as.factor(train$Survived), ntree=100, importance=TRUE)
> 
> 
> # Here we will plot the passenger survival by class
> # train$Survived <- factor(train$Survived, levels=c(1,0))
> # levels(train$Survived) <- c("Survived", "Died")
> # train$Pclass <- as.factor(train$Pclass)
> # levels(train$Pclass) <- c("1st Class", "2nd Class", "3rd Class")
> 
> # png("1_survival_by_class.png", width=800, height=600)
> # mosaicplot(train$Pclass ~ train$Survived, main="Passenger Survival by Class",
> #           color=c("#8dd3c7", "#fb8072"), shade=FALSE,  xlab="", ylab="",
> #           off=c(0), cex.axis=1.4)
> #dev.off()
> 
> proc.time()
   user  system elapsed 
  1.695   0.141   1.845 
