---
title: "Kaggle's Titanic Survival Prediction Competition"
author: "Neill White"
date: "January 18, 2017"
output:
  html_document:
    toc: TRUE
    toc_depth: 4
    toc_float: FALSE
---

```{r getpics, echo=FALSE}
# Load images and store locally
library("curl");
imageFile = "titanic.jpg";
if ( ! file.exists( imageFile )){
    curl_download(url = "http://cbsnews1.cbsistatic.com/hub/i/r/2017/01/04/b1b74071-3301-49ee-93bd-82e47c67d3a8/thumbnail/1200x630/0f08f16522eb0723e8d147cc809bc3d1/0103-eve-titanicfire-phillips-1223384-640x360.jpg", destfile = imageFile);
}
imageFile = "TitanicModelSummary.png";
if ( ! file.exists( imageFile )){
    curl_download(url = "http://neillwhite.dynu.net/DataScience/TitanicModelSummary.png", destfile = imageFile);
}
```

## The Problem Statement
On April 14, 1912, the RMS Titanic struck an iceberg in the North Atlantic Ocean and sank.  Of the 2,224 people on board, only 706 survived.

The goal of this exercise is to predict survivors on the Titanic based on nine input variables, described below.  We are provided two datasets: (1) train.csv, containing 891 records and (2) test.csv, containing 418 records.  The two datasets are provided with the intent that models are formulated using the train dataset and model performance is evaluated on the test dataset.

<!--![](http://cbsnews1.cbsistatic.com/hub/i/r/2017/01/04/b1b74071-3301-49ee-93bd-82e47c67d3a8/thumbnail/1200x630/0f08f16522eb0723e8d147cc809bc3d1/0103-eve-titanicfire-phillips-1223384-640x360.jpg)-->
![](titanic.jpg)

***
## About the Data

Variable|Definition|Key
------- | ---------- | ---------
survival|Survival|0 = No, 1 = Yes
pclass|Ticket|class	1 = 1st, 2 = 2nd, 3 = 3rd
sex|Sex|
Age|Age in year|
sibsp|	# of siblings / spouses aboard the Titanic|
parch|	# of parents / children aboard the Titanic|
ticket|Ticket number|
fare|Passenger fare|
cabin|Cabin number|
embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|

### Variable Notes

**pclass**: A proxy for socio-economic status (SES)
1st = Upper
2nd = Middle
3rd = Lower

**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5

**sibsp**: The dataset defines family relations in this way...
Sibling = brother, sister, stepbrother, stepsister
Spouse = husband, wife (mistresses and fianc√©s were ignored)

**parch**: The dataset defines family relations in this way...
Parent = mother, father
Child = daughter, son, stepdaughter, stepson
Some children travelled only with a nanny, therefore parch=0 for them.

source: https://www.kaggle.com/c/titanic/data

### Sample Records
#### Training Data

```{r trainsample, echo=FALSE}
# Load raw data
train <- read.csv("../input/train.csv", header = TRUE);
edaTrain = train;
knitr::kable( head(train), format="markdown", longtable=TRUE)
```
#### Test Data

```{r testsample, echo=FALSE, warning=FALSE}
# Load libraries
library(stringr);
library(assertthat);
library(dplyr, quietly=TRUE);
library(leaps);
# Load raw data
test <- read.csv("../input/test.csv", header = TRUE);

# Add a "Survived" variable to the test set to allow for combining data sets
test.survived <- data.frame(Survived = rep(NA, nrow(test)), test[,]);

# Combine data sets
data_combined <- rbind(train, test.survived);
#data_combined$survived <- as.factor(data_combined$survived)

knitr::kable( head(test), format="markdown", longtable=TRUE)
```
***
## Exploratory Data Analysis
### Missing Values
The first step is to find any and all missing data in the train and test sets.  

#### Train Dataset
```{r eda1}
# go through each variable and if it's an empty factor or numeric NA, sum each column
nlTrainNA = sapply( train, function(x) switch( class(x), factor = sum(x==""), sum( is.na(x) ) ) );
tTrainNA = t(nlTrainNA);                                 # transpose named list
dfTrainNA = data.frame( tTrainNA );                      # convert to dataframe
```
The train.csv dataset had 3 columns with missing values: Age, Cabin, and Embarked.  Age is likely to be an important predictor of survival and we have data for 80% of the training subjects so imputing the missing values is likely to be beneficial.  The source of embarkation may not have obvious predictive power, but given that we have data for over 99% of the training subjects, imputing the missing values could provide value.  The Cabin variable is missing from over 77% of the test subjects.  At first impression, this variable seems like an unlikely candidate to impute values since so much source data is missing.
```{r eda1table, echo=FALSE}
knitr::kable( dfTrainNA, format="markdown", longtable=TRUE);
```

#### Test Dataset
```{r eda2}
nlTestNA = sapply( test, function(x) switch( class(x), factor = sum(x==""), sum( is.na(x) ) ) );
tTestNA = t(nlTestNA);                                 # transpose named list
dfTestNA = data.frame( tTestNA );                      # convert to dataframe
```
The test.csv dataset also had 3 columns with missing values: Age, Fare, and Cabin.  Like the train dataset, the Cabin variable is sparse, with over 78% subjects missing values.  The Age variable is populated for over 79% of the train subjects, and likely has good predictive power so it will likely be beneficial to impute values for subjects missing Age.  The Fare variable is missing for a single subject.  While Fare may not be an obvious predictor for survival, the fact that the dataset is over 99% complete for this variable indicates that it is a good candidate for imputation.
```{r eda2table, echo=FALSE, warning=FALSE}
library( ggplot2 );
knitr::kable( dfTestNA, format="markdown", longtable=TRUE);
```

### Variable Features

#### PassengerId
PassengerId is a primary key for each row of data in the train and test sets.  This variable will not be included in any of the predictive models.

#### Survived
Survived is the class we're trying to predict.

#### Pclass
Passenger class is either 1st, 2nd, or 3rd.
```{r eda_pclass1}
ggplot(train, aes(x = factor(Pclass), fill = factor(Pclass))) +
  geom_bar( show.legend=FALSE) +
  xlab("Pclass") +
  ylab("Total Count")
```

The Pclass variable shows that most passengers in the train set held a 3rd class ticket.  491 of the 891 passengers were 3rd class, more than 1st and 2nd class combined.  
```{r eda_pclass2}
ggplot(train, aes(x = factor(Pclass), fill = factor(Survived))) +
  geom_bar(width = 0.5, position="dodge") +
  xlab("Pclass") +
  ylab("Total Count") +
  labs(fill = "Survived")
```


If the Survived variable is plotted as a function of passenger class, it appears that Pclass will be a predictor for survivability.  A higher percentage of first class passengers survived than died, contrary to the overall trend, whereas a far higher percentage of 3rd class passengers died than survived.

#### Name
It may seem that the passenger name is a lot like the PassengerId in that each name acts as a sort of primary key into the data and using name as a model feature would not generalize well.  However, the name field could possibly provide value.  The first twenty names in the train dataset:
```{r eda_name}
head(as.character(train$Name),n=20);
```
Each name begins with a surname before a comma and a title.  If the passenger is a married woman, her maiden name appears in parentheses.  Some of the titles may help infer age (Master. and Miss.) and surnames could help determine extended family travelling together, even if they've purchased separate tickets and are not in the same cabin.  Additionally, the presence of diacritical marks in a name could indicate that the passenger is a non-English speaker who might have had difficulty understanding instructions or the gravity of the situation.

#### Sex
Sex is an unordered factor, male or female.
```{r eda_sex1}
ggplot(train, aes(x = factor(Sex), fill = factor(Sex))) +
  geom_bar( show.legend=FALSE) +
  xlab("Sex") +
  ylab("Total Count")
```

The Sex variable shows that most passengers in the training set were male (nearly 2/3 male).  577 of the 891 passengers were male, or 65%.
```{r eda_sex2}
ggplot(train, aes(x = factor(Sex), fill = factor(Survived))) +
  geom_bar(width = 0.5, position="dodge") +
  xlab("Sex") +
  ylab("Total Count") +
  labs(fill = "Survived")
```
If the Survived variable is plotted as a function of Sex, it appears that Sex will be a strong predictor for survivability.  Of the 314 female passengers in the training set, 233, or 74% survived.  On the other hand, of the 577 male passengers, only 109 survived, or 19%.

#### Age
As noted, there are age values for 80% of the training subjects, missing for 177 passengers.  The distribution of ages is slightly skewed right, with a median of 28 years and a mean of 29.7 years.
```{r eda_age1}
ggplot(subset(train, !is.na(Age)), aes(x = Age)) +
  geom_histogram(binwidth=4) +
  xlab("Age") +
  ylab("Total Count");
ggplot(subset(train,!is.na(Age)), aes(y=Age,x="")) + geom_boxplot();
```

Age as a predictor of survivability:
```{r eda_age2}
ggplot(subset(train,!is.na(Age)), aes(x = Age, fill = factor(Survived))) +
  geom_density( position="stack") +
  xlab("Age") +
  ylab("Total Count") +
  labs(fill = "Survived")
```

If we plot the Survived value as a function of the Age density, we see that there is a higher likelihood of younger passengers surviving over older passengers.  Up until the mid-to-late teens, a training set passenger is more likely to survive than die, so age is likely to be a useful predictor for survivability.

#### SibSp
This variable is unique in that it is combination of number of siblings or "1" if the passenger had a spouse on board the ship.  In some cases, it is not clear what the SibSp variable is encoding when a "1" is found - is the passenger travelling with a sibling or a spouse?  A SibSp of 2 or more is indicative of siblings.  It will likely be beneficial to disambiguate this variable into separate 'Siblings' and 'Spouses' variables.
```{r eda_sibsp1}
ggplot(train, aes(x = SibSp )) +
   geom_histogram(binwidth=0.5) +
   xlab("Number of Siblings/Spouses") +
   ylab("Total Count")
```

Most passengers in the training set (68%) had neither a spouse or sibling on board.  23% of the training set passengers had a single sibling or spouse on board.  The remaining 9% of the passengers had two or more (presumably) siblings on board.
```{r eda_sibsp2}
ggplot(train, aes(x = SibSp, fill = factor(Survived))) +
   geom_histogram(binwidth=0.5, position="dodge") +
   xlab("Number of Siblings/Spouses") +
   ylab("Total Count") +
   labs(fill = "Survived")
```

Survivability does appear to trend with the number of siblings/spouses on board.  A passenger having no siblings or spouses is most likely to have died, whereas a passenger with one or two siblings/spouses has around a 50% likelihood of surviving.  The remaining cases of three to eight siblings are likely too few from which to draw inferences individually, so it might make sense to pool the SibSp values as follows: 0, 1, 2, 3> to avoid overfitting to specific training cases.  A close examination of the seven instances of the SibSp variable in which SibSp equals 8 reveals that all the subjects were from the same family and were in the same cabin.  Predicting that all families of size 8 will perish is unlikley to generalize well.

#### Parch
Similar to SibSp, this variable convolves two separate pieces of data: the number of parents and the number of children this passenger has on board.  In some cases, it is not clear what the Parch variable is encoding when a "1" is found - is the passenger travelling with a parent or a child?  This can be inferred if the Age variable is present for the passenger, but if the Age is missing, it will be ambiguous and may need further analysis.  Perhaps the passenger's title (Mr., Miss., Master) could help.  Like SibSp, it will likely be beneficial to disambiguate this variable into separate 'Parents' and 'Children' variables.
```{r eda_parch1}
ggplot(train, aes(x = Parch )) +
   geom_histogram(binwidth=0.5) +
   xlab("Number of Parents/Children") +
   ylab("Total Count")
```

Most passengers in the training set (76%) had neither a parent or child on board.  13% of the training set passengers had a single parent or child on board.  About 9% of the passengers in the training set (80) had two or more parents or children on board.  The remaining 2% of the passengers had either 3, 4, 5, or 6 (presumably) children on board.
```{r eda_parch2}
ggplot(train, aes(x = Parch, fill = factor(Survived))) +
   geom_histogram(binwidth=0.5, position="dodge") +
   xlab("Number of Parents/Children") +
   ylab("Total Count") +
   labs(fill = "Survived")
```

Survivability does appear to trend with the number of parents/children on board.  A passenger having no parents or children is most likely to have died, whereas a passenger with one or two parents/children has around a 50% likelihood of surviving.  The remaining cases of three to six children are likely too few from which to draw inferences individually, so it might make sense to group the Parch values as follows: 0, 1, 2, 3>= to avoid overfitting to specific training cases.  

#### Ticket
The entries in the Ticket column do not seem to be of a uniform format.  Some ticket entries are just numbers - ranging from 693-392096.  Other ticket entries have character prefixes like "C.A." or "SOTON/O2", followed by a (presumably) ticket number.  
```{r eda_ticket}
head(as.character(train$Ticket),n=20);
```
An inspection of the set of tickets shows that presumed families tend to share a single ticket number.  The first impression is that the ticket number seems an unlikely predictor for survivability and could lead to overfitting the training set.  However, the ticket number might help populate the missing Cabin information - and Cabin might be a good predictor for survivability.

#### Fare
The fare (price paid per ticket) ranges from 0 to 512.3292.  The units are unclear, but are likely in English pounds.  The distribution is skewed to the right, with a median of 14.4542 and a mean of 32.20421.  A log transform of the data may be necessary to normalize the distribution of fares.  However, first the fare for passenger must be determined.  It appears to be the case that individual ticket numbers are not assigned per passenger, but rather a single ticket number is given to the purchaser of an allotment of tickets.  That is, families travelling together seem to be under the same ticket with the same fare.  So, it may be necessary to get to create an "Amount Paid per Passenger" feature that takes into account the number of people for which a fare was purchased on a single ticket.  

```{r eda_fare1}
ggplot(train, aes(x = Fare)) +
  geom_histogram(binwidth=4) +
  xlab("Fare") +
  ylab("Total Count");
ggplot(train, aes(y=Fare,x="")) + geom_boxplot();
```

Fare could conceivably be an important factor in determining survivability.  Perhaps the higher paying passengers received the first opportunity to board lifeboats.  Or perhaps, those higher paying passengers were more initially unwilling to leave their more comfortable accomodations for the plebian conditions aboard a lifeboat.  Fare as a predictor of survivability:
```{r eda_fare2}
ggplot(train, aes(x = Fare, fill = factor(Survived))) +
  geom_density( position="stack") +
  xlab("Fare") +
  ylab("Total Count") +
  labs(fill = "Survived")
```

#### Cabin
The Cabin feature could be another strong predictor for survivability.  Perhaps cabins located nearest the lifeboats afforded the best survivability.  But, the Cabin variable has many empty values.  The empty values could mean that the information was not captured or it could mean that not all passengers received cabins and stayed in other accomodations.  Being assigned a cabin could be a proxy for one's social status and wealth.  If so, the Pclass variable might be co-linear.
```{r eda_cabin1}
levels(train$Cabin);
```

The cabin name mostly adheres to the rule of a single letter A-F,G,T, followed by a number up to 3 digits.  There are cases where a passenger has multiple cabins, each separated by whitespace.  The beginning letter of each cabin could denote a deck or particular region of the ship - which could help with predicting survivability.  Alternatively, the number of the cabin could be more informative than the beginning letter.  Perhaps cabins "A19" and "B19" are located right next to one another, for instance.  
```{r eda_cabin2, warning=FALSE}
cabinLetter = ifelse(train$Cabin == "", NA, substr(train$Cabin,1,1));
cabinREs = gregexpr("\\d+",train$Cabin, perl=TRUE);
cnMatches = regmatches( train$Cabin, cabinREs);
cabinNumber = numeric(length(cnMatches));
for ( i in 1:length(cnMatches) ){
  cabinNumber[i] = mean( as.numeric( unlist( cnMatches[i] )));
}
edaTrain$CabinLetter <- as.factor(cabinLetter);
edaTrain$CabinNumber <- cabinNumber;
ggplot( edaTrain, aes(x=CabinNumber,y=Survived,color=Survived ) ) + 
  geom_point( shape=1,position=position_jitter(height=0.25)) +
  ggtitle("Survivability by Cabin Number") +
  xlab("Cabin Number") +
  ylab("Survived");
ggplot(subset(edaTrain, !is.na(cabinLetter)), aes(x = CabinLetter, fill = as.factor(Survived))) +
  geom_bar() +
  ggtitle("Survivability by Ticket Letter") +
  xlab("Cabin Letter") +
  ylab("Total Count") +
  labs(fill = "Survived");
```

When grouping the passengers by cabin number, there does not appear to be a relationship where survivability depends on cabin number.  If so, there should be identifiable pockets of clusters where there is a higher incidence of survivability.  If such clusters appeared to exist, the clusters could be defined and the group of clusters tested with a Chi-Square test to measure if survivability depends on cabin cluster.

It's not immediately obvious if there is a benefit to categorizing the cabins according to their first letter.  Are these groups statistically different from one another?  A Chi-Square test of independence should show if survivability is dependent on cabin letter or not.
```{r eda_cabin3, warning=FALSE}
tbl = table( edaTrain$Survived, edaTrain$CabinLetter);
tbl
chisq.test(tbl);
```

The p-value is 0.17 so at a confidence level of 0.05, we cannot reject the null hypothesis that survivability is independent of the starting cabin letter.
```{r eda_cabin4}
edaTrain$CabinAssignment[ edaTrain$Cabin != "" ] <- "Assigned";
edaTrain$CabinAssignment[ edaTrain$Cabin == "" ] <- "Unassigned";
data_combined$CabinAssignment[ data_combined$Cabin != "" ] <- "Assigned";
data_combined$CabinAssignment[ data_combined$Cabin == "" ] <- "Unassigned";
data_combined$CabinAssignment = factor( data_combined$CabinAssignment );
ggplot(edaTrain, aes(x=CabinAssignment, fill=factor(Survived))) +
  geom_bar() + 
  facet_wrap(~Pclass) + 
  ggtitle("Pclass") + 
  xlab("Cabin Assignment") +
  ylab("Total Count") +
  labs(fill="Survived");
```

From the graph, it appears that for each passenger class, if a passenger is assigned a cabin, their chances of surviving the disaster are better than if they had not been assigned a cabin.

#### Embarked

Passengers boarded the Titanic from one of three ports: (S)outhampton, England; (C)herbourg, France, or (Q)ueenstown, Ireland.  As noted, two passengers in the training set have no record of port of Embarkation.
```{r eda_embarked1, echo=FALSE}
knitr::kable( train[train$Embarked == "",], format="markdown", longtable=TRUE);
```

The two passengers are both female and in first class.
```{r eda_embarked2}
ggplot( edaTrain, aes(x=factor(Pclass),fill=factor(Survived)))+
  geom_bar() + 
  facet_wrap(~Embarked) +
  ggtitle("Survivability by Port of Embarkation and Passenger Class") +
  xlab("Passenger Class") +
  ylab("Count") +
  labs(fill="Survived");
```

***

### Errors in the Data

Errors in the input dataset are often not apparent until deeper analyses are performed, such as feature engineering and imputing missing data.  However, once the data is corrected, it is necessary to regenerate the columns and feature analyses that will feed the predictive models.  In this dataset, it was found that a 16-year-old member of a family was incorrectly identified as being the father:
```{r errors1, echo=FALSE}
knitr::kable( data_combined[ data_combined$Ticket == "W./C. 6608", ], format="markdown", longtable=TRUE)
```
In the Ford family, Mr. William Neal Ford has the value '1' for SibSp and '3' for Parch.  The value of '1' for SibSp would imply that William has either one spouse or one sibling.  In addition, William has a value of '3' for Parch.  This indicates that William has either 3 children, 1 parent and 2 children, 2 parents and 1 child, or 3 parents, which is clearly not possible according to the variable definition.  Since two of the children in the family, Miss. Doolina Margaret Ford (21), and Mr. Edward Watson Ford (18) are actually *older* than William, it is clear that he cannot be their father.  The other child, Miss. Robina Maggie Ford (9) is only 7 years younger than William, also indicating that William cannot be her father.  The matriarch of the family is Mrs. Edward Ford - not Mrs. William Neal Ford, which indicates that it's likely not a simple case of getting William Neal Ford's age wrong (for instance, 46 instead of 16).

Therefore, the best fix would be to change the Parch variables for Miss. Robina Maggie Ford, Miss. Doolina Margaret Ford, and Mr. Edward Watson Ford from '2' to '1' (due to the inclusion of Mr. William Neal as a brother).  In addition, the Parch variable of Mrs. Edward Ford would rise from '3' to '4', indicating she's travelling with four of her children and the Parch variable of Mr. William Neal Ford would change from '3' to '1' indicating that he's travelling with a single parent, his mother.  The SibSp variables would change from '2' to '3' for Miss. Robina Maggie Ford, Miss. Doolina Margaret Ford, and Mr. Edward Watson Ford, indicating that each of them are travelling with 3 siblings.  Similarly, Mr. William Neal Ford's SibSp variable would change from '1' to '3'.  Finally, Mrs. Edward Ford's SibSp would change from '1' to '0', indicating she was travelling without her husband (Mr. Edward Ford).  The changes appear below.
```{r errors2, echo=FALSE}
# Mr. William Neal Ford
data_combined[ data_combined$PassengerId ==  87,"SibSp"] = 3;  # From 1
data_combined[ data_combined$PassengerId ==  87,"Parch"] = 1;  # From 3
# Miss. Robina Maggie Ford
data_combined[ data_combined$PassengerId == 148,"SibSp"] = 3;  # From 2
data_combined[ data_combined$PassengerId == 148,"Parch"] = 1;  # From 2
# Miss. Doolina Margaret Ford
data_combined[ data_combined$PassengerId == 437,"SibSp"] = 3;  # From 2
data_combined[ data_combined$PassengerId == 437,"Parch"] = 1;  # From 2
# Mrs. Edward Ford
data_combined[ data_combined$PassengerId == 737,"SibSp"] = 1;  # From 1; Note: sister is Mrs. Eliza Johnston
data_combined[ data_combined$PassengerId == 737,"Parch"] = 4;  # From 3
# Mr. Edward Watson Ford
data_combined[ data_combined$PassengerId == 1059,"SibSp"] = 3;  # From 2
data_combined[ data_combined$PassengerId == 1059,"Parch"] = 1;  # From 2
knitr::kable( data_combined[ data_combined$Ticket == "W./C. 6608", ], format="markdown", longtable=TRUE)
```

Additionally, there are problems with the Abbott family with ticket C.A., 2673:
```{r errors3, echo=FALSE}
knitr::kable( data_combined[ data_combined$Ticket == "C.A. 2673", ], format="markdown", longtable=TRUE)
```

Thirteen year old Master. Eugene Joseph Abbott has a Parch value of '2', meaning he has two parents on board.  A female on the same ticket, Mrs. Stanton Abbott, is 22 years older than Eugene at 35 years of age.  Mrs. Stanton Abbott has a SibSp of '1' and a Parch of '1'.  The other person on the ticket is 16-year-old Mr. Rossmore Edward Abbott, with a SibSp of '1' and a Parch of '1'.  It appears that 16-year-old Mr. Rossmore Edward Abbott was incorrectly designated the spouse of Mrs. Stanton Abbott instead of her son.  The corrections appear below.
```{r errors4, echo=FALSE}
# Master. Eugene Joseph Abbott
data_combined[ data_combined$PassengerId == 1284,"SibSp"] = 1;  # From 0
data_combined[ data_combined$PassengerId == 1284,"Parch"] = 1;  # From 2
# Mrs. Stanton (Rosa Hunt)
data_combined[ data_combined$PassengerId == 280,"SibSp"] = 0;  # From 1
data_combined[ data_combined$PassengerId == 280,"Parch"] = 2;  # From 1
knitr::kable( data_combined[ data_combined$Ticket == "C.A. 2673", ], format="markdown", longtable=TRUE)
```

## Feature Engineering
When importing into R, factor/class variables solely composed of numeric entries are interpreted and imported as numeric types, implying an ordering and a distance between entries.  These variables include PassengerId, Survived, and Pclass.  PassengerId is like a primary key for each passenger, so it's not necessary to convert this variable into a factor.  However, the Survived data is encoded as a binary flag where 1=survived and 0=not survived.  Since our goal is to predict classes (i.e., survived or died), it is necessary to convert this value into a factor.  The Pclass variable is a factor, but it is an ordered factor.  An ordered factor indicates that there is an ordering between the classes (1st class, 2nd class, 3rd class), but no magnitude between each class level can be inferred.
```{r feature1}
train$Survived = as.factor(train$Survived);
train$Pclass = as.ordered(train$Pclass);
```

### Imputing Missing Data

#### Embarked
Two passengers had no information for their port of embarcation, Miss. Amelie Icard and Mrs. George Nelson Stone.  These first class passengers were both on the same ticket (113572) and stayed in Cabin B28.  The majority of passengers boarded at Southampton (70%), as compared to Cherbourg (21%) and Queenstown (9%).  There is quite a variety in the ticket numbers on the Titanic.  Their ticket number, 113572, is one in a series of similar ticket numbers, all in the 113XXX format.  There are 56 tickets in the 113XXX format and of the 54 that have a valid port of embarcation, 81% were from Southampton.  Considering the different data points, the two missing Embarked values will be considered 'S' for Southampton.
```{r impute_embarked}
data_combined[62,"Embarked"] = as.factor("S");
data_combined[830,"Embarked"] = as.factor("S");
data_combined$Embarked = factor( data_combined$Embarked );  # removes empty factor ""
```

#### Cabin
As determined in the EDA section, over 77% of the passengers have no recorded cabin information.  In addition, the assigned cabins did not seem to have a predictable pattern from the available data.  Further, cursory analyses did not indicate that cabin assignments were a good predictor of survivability.  As such, the missing cabin data will not be imputed.

#### Fare
Out of 1309 records, one fare is missing:
```{r impute_fare1, echo=FALSE}
knitr::kable( data_combined[1044,], format="markdown", longtable=TRUE)
```

The fare can be estimated by considering the variables that most likely affect the Fare variable, such as passenger class (Pclass), port of Embarcation (Embarked), Cabin, and how many other people are on the same ticket.  The passenger class and port of embarkation are readily available in each data row.  The Cabin variable, however, is not useful in this case since the Cabin variable is not defined for passenger 1044.  However, the fact that no Cabin was recorded for this passenger is significant since perhaps not all passengers were assigned cabins (i.e., wealthy passengers would be assigned cabins, and hence pay higher fares, while less privileged passengers may have to had large, unassigned community accomodations with a comparatively lower fare).  

```{r impute_fare2}
ticketChar = as.character( data_combined$Ticket );
numPassengers = nrow( data_combined );
data_combined$NumPassengersOnTicket = 1;
for (i in 1:numPassengers ){
    thisTicket = as.character( data_combined[i,"Ticket"] );
    idxPeople = which( thisTicket == ticketChar );
    numPeople = length( idxPeople );
    data_combined$NumPassengersOnTicket[i] = numPeople;
}
# now constrain to those that have just 1 person on the ticket, as passenger 1044
singlePassengers = data_combined$NumPassengersOnTicket == 1;
embarkedS = data_combined$Embarked == 'S';
unassignedCabins = data_combined$CabinAssignment == 'Unassigned';
pClass3 = data_combined$Pclass == 3;
similarPassengers = which( singlePassengers & embarkedS & unassignedCabins & pClass3 );
similarPassengersFareData = data_combined[similarPassengers,];  # include all columns but the one we have no Fare info
similarPassengersFareData = similarPassengersFareData[ similarPassengersFareData$PassengerId != 1044,];  # include all columns but the one we have no Fare info
medianFare = median( similarPassengersFareData$Fare );
data_combined[1044,"Fare"] = medianFare;
hist(similarPassengersFareData$Fare, xlab = "Fare", ylab = "Passengers", main = "Fare of Similar Passengers");
summary( similarPassengersFareData$Fare );
```

There are 318 passengers that share the same characteristics as passenger 1044: passenger class 3, port of embarkation Southampton, cabin assignment (unassigned), and the number of people with the same ticket (1).  Excluding a fare of 3.1708 and two at 19.9667, the remaining fares are between 6.2375 and 10.5167.  Passenger 1044 is assigned the median fare of 7.896.

#### Age
As found above, the Age data is missing for 177 of the train set cases and 86 of the test set cases.  Since Age is likely to be an important predictor of survival, the missing data should be imputed.  As will be shown in the next section on Feature Engineering, the passenger's title can be extracted from their name.  The title should allow for a better estimate of a passenger's age.

The missing passengers have the following titles:
```{r impute_age1, echo=FALSE}
data_combined$FixedAge = data_combined$Age;
passenger_names = as.character(data_combined$Name);
num_commas = unname(sapply( passenger_names, str_count, ","));
all_commas = assert_that( all( num_commas == 1 ) );  # Make sure each row has a comma
# Now, extract titles.  Split on comma and take the tail end of the character string
surnames = sapply( strsplit(as.character(passenger_names), ","), head, 1);
data_combined$Surname = as.factor( surnames );
given_name_string = sapply( strsplit(as.character(passenger_names), ","), tail, 1);
given_name_string = trimws( given_name_string, "left"); # remove leading whitespace
data_combined$NameString = paste( given_name_string, surnames );
# Remove leading whitespace, if any
given_name_string = trimws( given_name_string, "left");
# To extract the title from the Name character string, split each string 
name_tokens = strsplit( given_name_string, "\\s+");
# Exract first token as the title
titles = lapply(name_tokens,"[[",1);
given_names = lapply(name_tokens,"[[",2);
data_combined$GivenName = as.factor( unlist( given_names ) );
title_regs = regexpr( "[\\w+]+\\.", given_name_string, perl=TRUE );
re_titles = regmatches( given_name_string, title_regs );
data_combined$Title = as.factor( unlist( re_titles ));
# The set of titles that have missing age values
ageNA = is.na( data_combined$Age );
titleNA = data_combined$Title[ageNA];
# The set of titles that have missing age values
data_combined$FixedTitle = data_combined$Title;

FIX_TITLES = TRUE;
if ( FIX_TITLES ){
  # change the following titles to "Mr." or "Mrs."
  titlesToChange = c("Capt.", "Col.", "Countess.", "Don.", "Dona.", "Dr.", "Jonkheer.", "Lady.", "Major.", "Mlle.", "Mme.", "Ms.", "Rev.", "Sir.");
  for ( title in titlesToChange ){
    idxChange = which( data_combined$FixedTitle == title );
    idxChangeMale = intersect( idxChange, data_combined$Sex == "male");
    idxChangeFemale = setdiff( idxChange, idxChangeMale );
    data_combined[idxChangeMale,"FixedTitle"] = "Mr.";
    data_combined[idxChangeFemale,"FixedTitle"] = "Mrs.";
  }
  data_combined$FixedTitle = factor( data_combined$FixedTitle );
}

data_combined$AgeTitle = data_combined$Title;
data_combined$AgeTitle[ which( data_combined$Title == "Ms." & ageNA )] = as.factor( "Mrs." );
age_lm = lm( Age~AgeTitle, data=data_combined);
missingAgeIndices = which( is.na( data_combined$Age ) );
missingTitles = data_combined[missingAgeIndices,"AgeTitle"];
missingCount = table( missingTitles );
summary( missingTitles )
```

The resultant linear model uses the following average ages for each missing passenger title:
```{r impute_age2}
# model is simple: use the passenger's title (i.e., Mr., Master., Mrs.) to determine age
# May want to better this by using age of parents (if travelling with parents), age of siblings, spouse, etc.
age_lm = lm( Age~AgeTitle, data=data_combined);

missingAges = predict( age_lm, data_combined[missingAgeIndices,]);
data_combined[missingAgeIndices,"FixedAge"] = missingAges;
coeffs = coefficients( age_lm );
coeffNames = names(coeffs);
coeffValues = unname(coeffs);
# find (Intercept)
idxIntercept = match( "(Intercept)", coeffNames );
interceptValue = coeffValues[idxIntercept];
allOtherCoeffIndices = setdiff(1:length(coeffNames),idxIntercept);
allOtherCoeffValues = coeffValues[allOtherCoeffIndices];
titleAges = interceptValue + allOtherCoeffValues;
names(titleAges) =  gsub( "AgeTitle", "", coeffNames[allOtherCoeffIndices] );
knitr::kable( bind_rows(titleAges), format="markdown", longtable=TRUE)
```
***
### New Features

#### Title and AgeTitle
The Name variable in the test and train datasets has some structure - surname followed by a comma, then a title and a given name.  Additionally, a maiden name will be present in parentheses if the passenger is a married woman.
```{r feature2}
passenger_names = as.character(data_combined$Name);
head(passenger_names);
```
The implied structure is [SURNAME] COMMA [TITLE] [GIVEN NAME] (MAIDEN NAME if applicable).  Since the goal is to extract the title information from each row programmatically, it is  necessary to enforce some error checking - namely, that each row contain one and only one comma.  If this condition does not hold true, the title extraction task will be more tedious.

```{r feature3}
num_commas = unname(sapply( passenger_names, str_count, ","));
all_commas = assert_that( all( num_commas == 1 ) );  # Make sure each row has a comma
# Now, extract titles.  Split on comma and take the tail end of the character string
surnames = sapply( strsplit(as.character(passenger_names), ","), head, 1);
data_combined$Surname = as.factor( surnames );
given_name_string = sapply( strsplit(as.character(passenger_names), ","), tail, 1);
# Remove leading whitespace, if any
given_name_string = trimws( given_name_string, "left");
# To extract the title from the Name character string, split each string 
name_tokens = strsplit( given_name_string, "\\s+");
# Exract first token as the title
titles = lapply(name_tokens,"[[",1);
given_names = lapply(name_tokens,"[[",2);
data_combined$GivenName = as.factor( unlist( given_names ) );
```

The full set of extracted titles:
```{r feature4}
passenger_titles = unlist( titles );
table( passenger_titles );
```

Most of these look correct and reasonable as titles ("Don." and "Dona." are Italian honorifics, as is "Jonkheer." a Dutch honorific).  However, the title "the" is suspicious.  The complete row is as follows:
```{r feature5}
data_combined[ which( data_combined$Title == "Countess." ), ]
```

Although it makes little difference in this case since there is only a single "the Countess. of Rothes" on board, the title of this passenger should be changed from "the" to "Countess.".  This could be fixed by changing the single record, but the fact that this incorrect title occurred in the first place indicates that the methodology for finding titles isn't robust enough.  Since it appears that the only time a "." appears in the Name field is at the end of a title, that information could be used to find all titles.
```{r feature6}
title_regs = regexpr( "[\\w+]+\\.", given_name_string, perl=TRUE );
re_titles = regmatches( given_name_string, title_regs );
data_combined$Title = as.factor( unlist( re_titles ));
summary( data_combined$Title );
```

Of particular interest are the titles that are underrepresented and how they might relate to the most abundant titles (Mr., Mrs., Miss., and Master.).  For instance, the title "Ms." has only two occurences and as such, has limited predictive power.  "Ms." generally refers to an adult woman, either married or unmarried and so is much closer to either a "Mrs." or "Miss." title than a "Mr.", for instance.  Additionally, the French designations for "Miss." and "Mrs." appear as Madamoiselle ("Mlle.") and Madame ("Mme.").  These titles would likely provide better predictive power if they were switched to their English equivalents.

Additionally, many of the Age attributes are missing from the dataset and since it seems likely that Age would be a strong predictor of survivability.  Title should be a fairly robust predictor for Age, so engineering a title feature that is reflective of Age could be beneficial (i.e., a Doctor with a missing Age value is much more likely to be an adult than a child and adults have a generally lower survival rate than children).  The titles of the passengers missing Age information are as follows:
```{r feature7}
# The set of titles that have missing age values
ageNA = is.na( data_combined$Age );
titleNA = data_combined$Title[ageNA];
summary( titleNA );
```

For the purposes of estimating the Age variable for this group, the "Ms." entry is better suited as a "Mrs.".  Although the "Dr." title has a relatively small number of instances, it is likely enough from which to draw a reasonable age estimate.
```{r feature8}
# The set of titles that have missing age values
data_combined$AgeTitle = data_combined$Title;
data_combined$AgeTitle[ which( data_combined$Title == "Ms." & ageNA )] = as.factor( "Mrs." );
summary( data_combined$AgeTitle );
```

#### Parents and Children
Next, the Parch variable encodes two different measures: the number of parents a passenger has on board AND/OR the number of children a passenger has on board.  Since this variable is overloaded, a more informative variable set might disambiguate these measures into NumParents and NumChildren (aboard).  Such a set of variables may provide more predictive power than the single Parch variable.
```{r feature9}
numParents = integer( nrow( data_combined ) );
numChildren = integer( nrow( data_combined ) );

MAX_CHILD_AGE = 14;

# Find all Parch > 0
posParch = data_combined$Parch > 0;
idxParch = which( posParch );
counter = 0;
numCases = length( idxParch );
for (thisRow in idxParch){
  sibsp = data_combined[thisRow,"Sibsp"];
  # if sibsp > 1, then the passenger is travelling with siblings (and therefore, likely parents)
  counter = counter + 1;
  numParch = data_combined[thisRow,"Parch"];
  thisSurname = data_combined[thisRow,"Surname"];
  thisTicket = data_combined[thisRow,"Ticket"];
  thisTitle = as.character( data_combined[thisRow,"Title"] );
  thisAge = data_combined[thisRow,"Age"];
  if ( (thisTitle == "Master.") || (thisTitle == "Miss.") ){
    # if this passenger is a "Master." or Miss. and numParch <= 2, he/she must be someone's child
    numParents[thisRow] = numParch;
    #next;
  }
  if (!is.na( thisAge ) && thisAge <= MAX_CHILD_AGE ){
    # if this passenger is young, declare they cannot be parents, must be a child
    numParents[thisRow] = numParch;
    #next;
  }
  # get passenger rows on same ticket
  sameTickets = data_combined$Ticket == thisTicket;
  sameSurnames = data_combined$Surname == thisSurname;
  sameSurnameSameTicket = sameTickets & sameSurnames & posParch;
  ticketTitles = as.character( data_combined[ sameSurnameSameTicket, "Title"] );
  ticketParches = data_combined[ sameSurnameSameTicket, "Parch"];
  # now, look for passengers with the same surname on the ticket and check their titles and ages
  numSame = length( sameSurnameSameTicket );
  ages = sort( data_combined[sameSurnameSameTicket,"Age"] );
  thisAgePos = which( thisAge == ages )[1];  # find index of thisAge in sorted ages
  gaps = diff( ages );
  numGenerationalGaps = length( which( gaps > MAX_CHILD_AGE) );
  if ( numGenerationalGaps == 0 ){
    if ( !is.na( thisAge )){
      if ( ( thisAge >= 40 ) || (thisTitle == "Mrs." ) ){
        numChildren[thisRow] = numParch;
      }
      else if (numParch > 2 ){
        numChildren[thisRow] = numParch;
      }
      else{
        numParents[thisRow] = numParch;
      }
    }
    else{
      # no age information. If travelling with "kids", and title isn't a kid, then parent
      if ( ! ( thisTitle %in% c("Master.","Miss.") ) ){
        if( any( c("Master.","Miss.") %in% ticketTitles ) ){
          # now, if there are two Mr. in this group, we need to choose the real father
          # If there are three or more children, then Parch will be greater than 2 and
          # will indicate this is the father.  Else, it will be a child
          if( thisTitle == "Mr."){
            maxParches = max( ticketParches );
            if ( ( maxParches > 2 ) && ( maxParches == numParch ) ){
              numChildren[thisRow] = numParch;
            }
            else{
              numParents[thisRow] = numParch;
            }
          }
          else{
            numChildren[thisRow] = numParch;
          }
        }
        else{
          # all we have is a non Mr. or Miss. title.  Make them a child
            numChildren[thisRow] = numParch;
        }
      }
      else{
        numParents[thisRow] = numParch;
      }
    }
  }
  else{
    if ( !is.na(thisAge) ){  # use age in comparison to generation gap to classify kids/parents
      maxGapPos = which.max( gaps ) + 0.5;  # the 0.5 puts it in the middle of the kids/parents
      if ( thisAgePos < maxGapPos ){
        numParents[thisRow] = numParch;
      }
      else{
        numChildren[thisRow] = numParch;
      }
    }
    else{
      # no age info, have to go with titles
    }
  }
  totalParch = numParents[thisRow] + numChildren[thisRow];
  if ( totalParch != numParch ){
    stop( "Number of Parents/Children assigned (", totalParch, ") does not equal the Parch variable (", numParch, ") for passenger ", data_combined[thisRow,"PassengerId"], "\n");
  }
  data_combined$NumParents = numParents;
  data_combined$NumChildren = numChildren;
}
```

#### Siblings and Spouses
Similar to Parch, the SibSp variable indicates the number of Siblings AND/OR Spouses a passenger has on board.  A better set of variables would be separate variables for Siblings and Spouses as it would be possible to model the original SibSp vector as a simple combination of the separate columns.
```{r feature10}
numSiblings = integer( nrow( data_combined ) );
numSpouses = integer( nrow( data_combined ) );

MAX_CHILD_AGE = 14;

posSibSp = data_combined$SibSp > 0;
idxSibSp = which( posSibSp );
counter = 0;
numCases = length( idxSibSp );
for (thisRow in idxSibSp){
  counter = counter + 1;
  numSibSp = data_combined[thisRow,"SibSp"];
  numParch = data_combined[thisRow,"Parch"];
  thisSurname = data_combined[thisRow,"Surname"];
  thisTicket = data_combined[thisRow,"Ticket"];
  thisTitle = as.character( data_combined[thisRow,"Title"] );
  thisAge = data_combined[thisRow,"Age"];
  thisSex = as.character( data_combined[thisRow,"Sex"] );
  thisGivenName = as.character( data_combined[thisRow,"GivenName"] );
  thisPassengerId = data_combined[thisRow,"PassengerId"];
  if ( (thisTitle == "Master.") || (thisTitle == "Miss.") ){
    # if this passenger is a "Master." or Miss., this passenger is not married, so must be spouse
    numSiblings[thisRow] = numSibSp;
    next;
  }
  if (!is.na( thisAge ) && thisAge <= 10 ){
    # if this passenger is young, declare they cannot be parents, must be a child
    numSiblings[thisRow] = numSibSp;
    next;
  }
  # get passenger rows on same ticket
  sameTickets = data_combined$Ticket == thisTicket;
  sameSurnames = data_combined$Surname == thisSurname;
  sameSurnameSameTicket = sameTickets & sameSurnames & posSibSp;
  # if married, look for spouse on the same ticket
  # get given names on same ticket
  same_ticket_rows = data_combined[which(sameSurnameSameTicket),];
  given_names = as.character( same_ticket_rows$GivenName );
  titles = as.character( same_ticket_rows$Title );
  sexes = as.character( same_ticket_rows$Sex );
  passengerIds = same_ticket_rows$PassengerId;
  master_mask = ( titles != "Master." );  
  sex_mask = ( sexes != thisSex );  # opposite sex marriage
  miss_mask = ( titles != "Miss." );
  this_mask = ( passengerIds != thisPassengerId );
  idxMatch = which( ( thisGivenName == given_names ) & master_mask & sex_mask & miss_mask & this_mask);
  if ( length( idxMatch ) == 1 ){
    numSpouses[thisRow] = 1;
    next;
  }
  ticketTitles = as.character( data_combined[ sameSurnameSameTicket, "Title"] );
  ticketParches = data_combined[ sameSurnameSameTicket, "Parch"];
  # now, look for passengers with the same surname on the ticket and check their titles and ages
  numSame = length( sameSurnameSameTicket );
  ages = sort( data_combined[sameSurnameSameTicket,"Age"] );
  thisAgePos = which( thisAge == ages )[1];  # find index of thisAge in sorted ages
  gaps = diff( ages );
  numGenerationalGaps = length( which( gaps > MAX_CHILD_AGE) );
}

# do some manual corrections
numSiblings[168] = 0;  # Mrs. William Skoog
numSpouses[168] = 1;  # Mrs. William Skoog
numSiblings[361] = 0;  # Mr. Wilhelm Skoog
numSpouses[361] = 1;  # Mr. Wilhelm Skoog
numSiblings[679] = 0;  # Mrs. Frederick Goodwin
numSpouses[679] = 1;  # Mrs. Frederick Goodwin
numSiblings[1031] = 0;  # Mr. Charles Frederick Goodwin
numSpouses[1031] = 1;  # Mr. Charles Frederick Goodwin
numSiblings[557] = 0;  # Lady. Duff Gordon
numSpouses[557] = 1;  # Lady. Duff Gordon
numSiblings[600] = 0;  # Sir. Duff Gordon
numSpouses[600] = 1;  # Sir. Duff Gordon
numSiblings[746] = 0;  # Capt. Edward Gifford Crosby
numSpouses[746] = 1;  # Capt. Edward Gifford Crosby
numSiblings[1197] = 0;  # Mrs. Edward Gifford Crosby
numSpouses[1197] = 1;  # Mrs. Edward Gifford Crosby
numSiblings[1059] = 3;  # Mr. Edward Watson Ford
numSpouses[1059] = 0;  # Mr. Edward Watson Ford

counter = 0;
for (thisRow in idxSibSp){
  counter = counter + 1;
  numSibSp = data_combined[thisRow,"SibSp"];
  numParch = data_combined[thisRow,"Parch"];
  thisSurname = data_combined[thisRow,"Surname"];
  thisTicket = data_combined[thisRow,"Ticket"];
  thisTitle = as.character( data_combined[thisRow,"Title"] );
  thisAge = data_combined[thisRow,"Age"];
  thisSex = as.character( data_combined[thisRow,"Sex"] );
  thisGivenName = as.character( data_combined[thisRow,"GivenName"] );
  thisPassengerId = data_combined[thisRow,"PassengerId"];
  numSiblings[thisRow] = numSibSp - numSpouses[thisRow];
}

data_combined$Spouses = numSpouses;
data_combined$SpousesFactor = factor( numSpouses );
data_combined$Siblings = numSiblings;
```

#### FarePerPassenger
As determined in the EDA section, the Fare variable is not per passenger, it is the price paid for the ticket the passenger is traveling under (and multiple passengers may travel on the same ticket).  
```{r feature11}
ticketChar = as.character( data_combined$Ticket );
uniqueTickets = unique( ticketChar );
farePerPassenger = data_combined$Fare;
for (i in 1:length(uniqueTickets) ){
    thisTicket = uniqueTickets[i];
    idxPeople = which( ticketChar == thisTicket );
    theseFares = data_combined[idxPeople,"Fare"];
    #if ( !all( theseFares == theseFares[1] ) ){
    #    cat( thisTicket )
    #}
    # should only be a single fare
    
    numPeople = length( idxPeople );
    farePerPassenger[idxPeople] = theseFares[1]/numPeople;
}
data_combined$FarePerPassenger = farePerPassenger;
```

***
## Models
This is a classification problem with two classes: { Died, Survived }.  This is encoded in the 'Survived' variable in the train dataset. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

<!-- Globals -->
<!-- Data set -->

```{r include = FALSE}
MAX_HOURS = 10;
library( assertthat );
data_combined$pclass <- as.factor(data_combined$Pclass);

# A bit about R data types (e.g., factors)
summary(data_combined );

```

### Null Model
```{r null1, echo=FALSE}
numPassengers = length(train$Survived);
died = sum( train$Survived == 0);
survived = sum( train$Survived == 1);
#assert_that( died + survived == numPassengers);
mean_survived = survived/numPassengers;
cat( "Of ", numPassengers, " passengers, ", died, " died and ", survived, " survived.\nMean Survivability = ", survived/numPassengers );
```
The simplest model would be to assume that all test subjects are members of the most common class in the train dataset.  In the train dataset, 62% of the subjects died, so this simple model would assume that all the test passengers die.  This would yield a prediction accuracy of about 62%, and a corresponding misclassification rate of 38%, all false negatives.

This simple model is also the fastest to implement and serves as a good initial benchmark from which to improve upon.

Upon submission to the Kaggle site, the model yielded a score of 0.62679.  In terms of the bias/variance tradeoff, this is a very biased model (very inflexible) and the variance will likely be very small (i.e., the choice of training set will not affect predictions on the test set except in rare cases where more survivors are selected than non-survivors).

```{r null2, echo=FALSE}
# Time strip
nullModelTrain = data.frame(PassengerId=train$PassengerId, Survived=0);
nullModel = data.frame(PassengerId=test$PassengerId, Survived=0);
write.csv( nullModelTrain, "NullModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( nullModel, "NullModel.csv", row.names=FALSE, quote=FALSE );
nullModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=342/891);
write.csv( nullModelTrainProbs, "NullModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
nullModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=342/891);
write.csv( nullModelTestProbs, "NullModelProbs.csv", row.names=FALSE, quote=FALSE );

#hours = 1;
#par(mfrow=c(2,1));
#barplot( hours, width=0.9, main="Time", xlab="hours", ylim=c(0,1), xlim=c(0,MAX_HOURS), horiz=TRUE );
#bp = ggplot(data=hours,aes(x=hours,y=1)) + geom_bar(stat="identity") + coord_flip();
#bp
### Bias-Variance Strip

```

### Gender Submission Model
Along with the train.csv and test.csv file, Kaggle provides an additional file called 'gender_submission.csv'.  This file is a sample submission to Kaggle in which all the female passengers are predicted to survive while all of the male passengers are predicted to die.


```{r genderSubmissionModel, echo=FALSE}
femaleIndices = which( train$Sex == 'female' );
maleIndices = which( train$Sex == 'male' );
numFemales = length( femaleIndices );
numMales = length( maleIndices );
proportionFemaleSurvival = length( intersect( femaleIndices, which( train$Survived == 1)))/numFemales;
proportionMaleSurvival = length( intersect( maleIndices, which( train$Survived == 1)))/numMales;

genderModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=0);
genderModelTrainProbs$Survived[ femaleIndices ] = proportionFemaleSurvival;
genderModelTrainProbs$Survived[ maleIndices ] = proportionMaleSurvival;
write.csv( genderModelTrainProbs, "GenderModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
genderModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=0);
write.csv( genderModelTestProbs, "GenderModelProbs.csv", row.names=FALSE, quote=FALSE );
```

In the training set, of the 891 passengers, 342 are female.  233 of the 342 female passengers in the training set survived (74.2%).  Of the 549 male passengers in the train set, 468 did not survive (81.1%).  This yields an overall classification rate of 78.7% for the test set.  On the bias/variance continuum, this model is very biased and should have small variance.

Upon submission to Kaggle, this model scored 0.76555.

### Women and Children First Model
The canonical model for a sinking ship is that women and children get preferential access to lifeboats while men are expected to defer.  If 100% accordance to this model is assumed and all women and children are boarded onto lifeboats while the men are left to fight it out for flotsam in the frigid waters, women and children would be expected to survive while the men would perish.

Since the Age variable is missing for 177 of the train set cases and 86 test set cases, the imputed Age values from above will be used.  For this application, a child will be defined as a person under 15 years of age.
```{r womenAndChildrenFirstTrain, echo=FALSE}
library(caret, quietly=TRUE);
femaleIndices = which( train$Sex == 'female' );
data_combined[["SurvivedFactor"]] = factor(data_combined[["Survived"]]);
whiten = preProcess(data_combined, c("center","scale","BoxCox") );
whitened_data = data.frame( predict( whiten, data_combined ));
# now "unwhiten" some data like Survived and PassengerId
whitened_data$PassengerId = data_combined$PassengerId;
whitened_data$Survived = data_combined$Survived;
newTrain = subset( data_combined, PassengerId <= nrow(train) );
newTest = subset( data_combined, PassengerId > nrow(train) );
whiteTrain = subset( whitened_data, PassengerId <= nrow(train) );
whiteTest = subset( whitened_data, PassengerId > nrow(train) );
childIndices = which( newTrain$FixedAge < 15 );
womenOrChildren = union( femaleIndices, childIndices );
maleIndices = setdiff( 1:nrow(train), womenOrChildren );
womenOrChildrenSurvived = which( train[ womenOrChildren, "Survived"] == 1 );
menDied = which( train[ maleIndices, "Survived"] == 0);
numFemales = length( femaleIndices );
numMales = length( maleIndices );
trainProportionCorrect = ( length(womenOrChildrenSurvived) + length(menDied))/nrow(train);
```

In the training set, the proportion of correct predictions is 0.79.  

```{r womenAndChildrenFirstTest, echo=FALSE}
# start with all perished model, then set women and children to survived
womenAndChildrenFirstModelTrain = data.frame(PassengerId=train$PassengerId, Survived=0);
womenAndChildrenFirstModel = data.frame(PassengerId=test$PassengerId, Survived=0);
newTest = subset( data_combined, PassengerId > nrow(train) );
femaleIndices = which( newTest$Sex == 'female' );
trainFemaleIndices = which( newTrain$Sex == 'female' );
childIndices = which( newTest$FixedAge < 15 );
trainChildIndices = which( newTrain$FixedAge < 15 );
womenOrChildren = union( femaleIndices, childIndices );
womenOrChildrenTrain = union( trainFemaleIndices, trainChildIndices );
womenAndChildrenFirstModelTrain[womenOrChildrenTrain,"Survived"] = 1;
womenAndChildrenFirstModel[womenOrChildren,"Survived"] = 1;
write.csv( womenAndChildrenFirstModelTrain, "WomenAndChildrenFirstModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( womenAndChildrenFirstModel, "WomenAndChildrenFirstModel.csv", row.names=FALSE, quote=FALSE );

probWCSurvived = mean(newTrain$Survived[ womenOrChildrenTrain ]);
probMenSurvived = mean(newTrain$Survived[ -womenOrChildrenTrain ]);
wcModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=newTrain$Survived);
wcModelTrainProbs$Survived[womenOrChildrenTrain] = probWCSurvived;
wcModelTrainProbs$Survived[-womenOrChildrenTrain] = probMenSurvived;
write.csv( wcModelTrainProbs, "WCModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
wcModelTestProbs = data.frame(PassengerId=test$PassengerId, Survived=0);
wcModelTestProbs$Survived[womenOrChildren] = probWCSurvived;
wcModelTestProbs$Survived[-womenOrChildren] = probMenSurvived;
write.csv( wcModelTestProbs, "WCModelProbs.csv", row.names=FALSE, quote=FALSE );
```

Upon submission to Kaggle, the Women and Children First model scored 0.76076.

### Linear Discriminant Analysis (LDA) Models

In Linear Discriminant Analysis, the distributions of each of the predictors (Passenger Class, Age, Sex, etc.) are modeled separately for the Survived and Died classes.  Then, Bayes' theorem can be flipped around to get estimates for the probability of Survived or Died given the set of predictors.  LDA assumes that the observations in each class (Survived, Died) are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix common to both classes.  This assumption does not hold for some of the predictors in this analysis, as not only are they not all normal, some are categorical (Sex, Embarked).  Nevertheless, the implementation in R converts the variables to a numeric albeit not-normal representation.

#### LDA Model using all features
The first LDA model is one in which all predictors are considered: pClass, Sex, FixedAge, FarePerPassenger, FixedTitle, CabinAssignment, Embarked, NumPassengersOnTicket, NumParents, NumChildren, SpousesFactor, and Siblings.  Against the training data, the model scored 0.8373 (~84% accuracy).  When run against the test set, the model scored 0.78947.

```{r lda1, warning=FALSE}
library(MASS, quietly=TRUE);
xTrain = whiteTrain[,c("SurvivedFactor","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
xTest = whiteTest[,c("SurvivedFactor","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
lda.fit = lda( SurvivedFactor ~., data=xTrain);
lda.fit
plot( lda.fit );
lda.train.pred = predict( lda.fit, xTrain );  
ldaTrainPredictions = as.numeric( lda.train.pred$class ) - 1;
truth = newTrain$Survived;
confusionMatrix( ldaTrainPredictions, truth );
# scored a 0.8373 against the training data
lda.test.pred = predict( lda.fit, xTest );  
# lda.pred$class is a factor; need to convert to 0, 1
ldaPredictions = as.numeric( lda.test.pred$class ) - 1;
ldaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=ldaTrainPredictions);
ldaModel = data.frame(PassengerId=test$PassengerId, Survived=ldaPredictions);

write.csv( ldaModelTrain, "LDAModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( ldaModelTrain, "LDAModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( ldaModel, "LDAModel.csv", row.names=FALSE, quote=FALSE );
write.csv( ldaModel, "LDAModelProbs.csv", row.names=FALSE, quote=FALSE );
# scored a 0.78947
```

#### LDA Model with only significant features
In subsequent analyses (logistic regression, random forests), the predictors are measured for effect on the response variable.  The logistic regression analysis indicates which covariates have a low p-value (i.e., <0.05), indicating a significant effect.  Similarly, random forests produce a variable importance measure which shows which covariates are used most often in tree generation.  These measures give a good idea of which variables are likely to have the most predictive power.  Using these variables and leaving out the variables with higher p-values and those not often used in random forests will likely reduce overfitting.  The variables with the best predictive power in this set are pClass, Sex, FixedAge, FarePerPassenger, FixedTitle, CabinAssignment, NumChildren, SpousesFactor, and Siblings.

The LDA model using only significant features scored a 0.8339 against the training data and, similar to the full-featured LDA model, scored a 0.78947.

```{r lda2, echo=TRUE, warning=FALSE}
library(MASS);
xTrain = whiteTrain[,c("SurvivedFactor","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","NumChildren","SpousesFactor","Siblings")];
xTest = whiteTest[,c("SurvivedFactor","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","NumChildren","SpousesFactor","Siblings")];
lda.fit = lda( SurvivedFactor ~., data=xTrain);
plot( lda.fit );
lda.train.pred = predict( lda.fit, xTrain );  
ldaTrainPredictions = as.numeric( lda.train.pred$class ) - 1;
truth = newTrain$Survived;
confusionMatrix( ldaTrainPredictions, truth );
# scored a 0.8339 against the training data
lda.test.pred = predict( lda.fit, xTest );  
# lda.pred$class is a factor; need to convert to 0, 1
ldaPredictions = as.numeric( lda.test.pred$class ) - 1;
ldaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=ldaTrainPredictions);
ldaModel = data.frame(PassengerId=test$PassengerId, Survived=ldaPredictions);

write.csv( ldaModelTrain, "LDAReducedModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( ldaModelTrain, "LDAReducedModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( ldaModel, "LDAReducedModel.csv", row.names=FALSE, quote=FALSE );
write.csv( ldaModel, "LDAReducedModelProbs.csv", row.names=FALSE, quote=FALSE );
# scored a 0.78947
```

### Quadratic Discriminant Analysis (QDA) Models

Quadratic Discriminant Analysis (QDA) is very much similar to LDA but with one difference: there is no assumption that the classes have a common variance.  In this model, each class has its own covariance matrix.  QDA is so named because the discriminant functions are quadratic in the predictor vector, as opposed to linear with LDA.  Since the discriminant function is quadratic, the decision boundary is not constrained to a line as with LDA and can assume curved shapes.

#### QDA Model with all features

The QDA model with all variables scored a 0.8159 against the training set and 0.77033 against the test data.

```{r qda1, echo=TRUE, warning=FALSE}
library(MASS);
xTrain = whiteTrain[,c("SurvivedFactor","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
xTest = whiteTest[,c("SurvivedFactor","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
qda.fit = qda( SurvivedFactor ~., data=xTrain);
qda.train.pred = predict( qda.fit, xTrain );  
qdaTrainPredictions = as.numeric( qda.train.pred$class ) - 1;
truth = newTrain$Survived;
confusionMatrix( qdaTrainPredictions, truth );
# scored a 0.8159 against the training data
qda.test.pred = predict( qda.fit, xTest );  
# lda.pred$class is a factor; need to convert to 0, 1
qdaPredictions = as.numeric( qda.test.pred$class ) - 1;
qdaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=qdaTrainPredictions);
qdaModel = data.frame(PassengerId=test$PassengerId, Survived=qdaPredictions);

write.csv( qdaModelTrain, "QDAModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( qdaModelTrain, "QDAModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( qdaModel, "QDAModel.csv", row.names=FALSE, quote=FALSE );
write.csv( qdaModel, "QDAModelProbs.csv", row.names=FALSE, quote=FALSE );
# scored a 0.77033
```

#### QDA Model with only significant features

The QDA model using only significant features scored a 0.7845 against the training set and 0.78947 against the test data.

```{r qda2, echo=TRUE, warning=FALSE}
library(MASS);
xTrain = whiteTrain[,c("SurvivedFactor","pclass","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","NumChildren","Siblings")];
xTest = whiteTest[,c("SurvivedFactor","pclass","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","NumChildren","Siblings")];
qda.fit = qda( SurvivedFactor ~., data=xTrain);
qda.train.pred = predict( qda.fit, xTrain );  
qdaTrainPredictions = as.numeric( qda.train.pred$class ) - 1;
truth = newTrain$Survived;
confusionMatrix( qdaTrainPredictions, truth );
# scored a 0.7845 against the training data
qda.test.pred = predict( qda.fit, xTest );  
# lda.pred$class is a factor; need to convert to 0, 1
qdaPredictions = as.numeric( qda.test.pred$class ) - 1;
qdaModelTrain = data.frame(PassengerId=train$PassengerId, Survived=qdaTrainPredictions);
qdaModel = data.frame(PassengerId=test$PassengerId, Survived=qdaPredictions);

write.csv( qdaModelTrain, "QDASigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( qdaModelTrain, "QDASigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( qdaModel, "QDASigModel.csv", row.names=FALSE, quote=FALSE );
write.csv( qdaModel, "QDASigModelProbs.csv", row.names=FALSE, quote=FALSE );
# scored a 0.78947
```
### Logistic Regression Models

In a linear regression model, the probability of the response variable is modeled as a linear combination of predictors and multiplicative coefficients.  The encoding of the response variable (Survived) as 0 and 1 allows for a linear regression model but covariates that produce a response greater than 1 and less than 0 are considered errors and can negatively impact parameter estimation.  In logistic regression, the probability of the response variable is modeled with the logistic function, or y = e^X/(1+e^X).  Logistic regression is a classic model for binary classification.

#### Logistic Regression using all features
A logistic regression model is fit using the train data.  Selected variables are shown below.  Some variables could be treated as either numeric or factors (i.e., NumPassengersOnTicket and NumChildren).  Because there was a natural ordering of these variables, they are treated as numeric values rather than factors.  This was done to capture the linear dependence of each variable to the survival response.

```{r logisticRegression1, echo=TRUE, warning=FALSE}
all_features = c("pclass", "Sex", "FixedAge", "FarePerPassenger", "FixedTitle", "Siblings", "CabinAssignment", "Embarked", "NumPassengersOnTicket", "NumParents", "NumChildren", "SpousesFactor");
full_df = newTrain[, c("Survived", all_features)];
full.glm.fit = glm( Survived~., data=full_df, family=binomial);
summary( full.glm.fit );
trainSetProbs = predict( full.glm.fit, newTrain, type="response");  # now a vector of probabilities
# probabilities >= 0.5 mean survived, < 0.5 mean perished
# training set performance
threshold = 0.5;
predictions = as.numeric( trainSetProbs >= threshold );
confusionMatrix( predictions, newTrain$Survived );

# now, how'd we do against the test set?
testSetProbs = predict( full.glm.fit, newTest, type="response");
fullLogisticPredictions = as.numeric( testSetProbs >= threshold );
fullLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);
fullLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullLogisticPredictions);

fullLogisticRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=trainSetProbs);
fullLogisticRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=testSetProbs);

write.csv( fullLogisticRegressionModelTrainProbs, "LogisticRegressionFullModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( fullLogisticRegressionModelProbs, "LogisticRegressionFullModelProbs.csv", row.names=FALSE, quote=FALSE );

write.csv( fullLogisticRegressionModelTrain, "LogisticRegressionFullModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( fullLogisticRegressionModel, "LogisticRegressionFullModel.csv", row.names=FALSE, quote=FALSE );
# scored a 0.78947 - not expected
```

The logistic regression model was first fit with all 12 possible covariates. After fitting the model to the training data, the model was run on the training set yielding a training set test error of 0.1661055 (0.8338945 accuracy rate).  The model had an overall accuracy of 0.78947 on the test set (the score from Kaggle).  Since this was lower than the expected score (the training set yielded a score of 0.8338945), it appears the model may be overfitting the training data.  

```{r logisticRegression2, echo=TRUE, warning=FALSE}
library(caret);
library(lattice);
cat( "Training Set performance:\n");
confusionMatrix( predictions, newTrain$Survived);
```

Because the logistic regression model with all 12 predictors has a test set error somewhat larger than the training set error, the model is likely fitting noise in the training set.  It's very likely that some of the features are co-linear, for instance, Passenger Class (pclass) and FarePerPassenger.  If the number of predictors can be reduced, the variance of the model can likely be lowered.  After running the full set of predictors, the model shows several variables as significant: FixedTitle, Sex, and Siblings.  Other variables that show nearly significant correlation to the response are FixedAge, pclass, CabinAssignment, and NumChildren.  If those variables are brought forward and tested separately in different combinations, a model with a lower cross validation error might result.  As a final measure, a separate model was run with the remaining "leftover" features: FarePerPassenger, Embarked, NumPassengersOnTicket, SpousesFactor, and NumParents.  The idea is to see if any significant features can be determined in the absence of the significant features of the full model.

```{r logisticRegression3, echo=TRUE, warning=FALSE}
leftoverFeatures = c("FarePerPassenger","Embarked","NumPassengersOnTicket","SpousesFactor","NumParents");
leftover_df = newTrain[, c("Survived", leftoverFeatures)];
leftover.glm.fit = glm( Survived ~., data = leftover_df, family=binomial);
summary(leftover.glm.fit);
```

#### Logistic Regression using only significant features

The logistic regression model with the "leftover" features yielded two new feature possibilities: FarePerPassenger and NumParents.  These features are likely proxies for passenger class (pclass) and age (FixedAge).  i.e., passengers travelling with their parents are more likely to be children and thus, have a higher survival probability.  Similarly, FarePerPassenger is likely to reflect the passenger class separation.  However, unlike pclass, FarePerPassenger is a continuous variable and possibly more informative than the three passenger classes.  So, combined with the significant features from the full logistic regression model, a set of features can be drawn upon in different combinations in order to select the model with the lowest cross validation error.

```{r logisticRegression4, echo=TRUE, warning=FALSE}
library(boot, quietly = TRUE);
significant_features = c("FixedTitle", "Sex", "Siblings", "FixedAge", "CabinAssignment", "pclass", "NumChildren", "FarePerPassenger", "Embarked");
all_combinations_of_significant_features = lapply(1:length(significant_features), function(x) combn(length(significant_features),x));
numFeatureSetClasses = length( all_combinations_of_significant_features );
correct = list();
accuracies = list();
errors = list();
features = list();
numFeatures = list();
counter = 1;
for ( featureClass in 1:numFeatureSetClasses){
  featClassColumns = dim( all_combinations_of_significant_features[[featureClass]] )[2];
  for ( thisCol in 1:featClassColumns ){
    feature_set = significant_features[ all_combinations_of_significant_features[[featureClass]][,thisCol]];
    nFeatures = length( feature_set );
    df = newTrain[, c("Survived", feature_set)]
    glm.fit = glm( Survived~.,data=df,family=binomial);
    cv.glm.fit = cv.glm(df,glm.fit,K=10);
    cv.error = cv.glm.fit$delta[1];
    trainSetProbs = predict( glm.fit, newTrain, type="response");  # now a vector of probabilities
    predictions = as.numeric( trainSetProbs >= threshold );
    numRight = sum( predictions == newTrain$Survived );
    accuracy = mean( predictions == newTrain$Survived );
    features[counter] = list( feature_set );
    numFeatures[counter] = nFeatures;
    accuracies[counter] = accuracy;
    correct[counter] = numRight;
    errors[counter] = cv.error;
    counter = counter + 1;
  }
}
cvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), Features=I(features) );
cvResults = cvResults[order(cvResults$CVError),];
knitr::kable( head(cvResults,n=20L), format="markdown", longtable=TRUE)
```

Of the 512 different feature combinations, the top 20 are listed.  The cross validation error estimates were made using 10-fold cross validation.  The top 10 are retested using LOOCV:

```{r logisticRegression5, echo=TRUE, warning=FALSE}
correct = list();
accuracies = list();
errors = list();
features = list();
numFeatures = list();
counter = 1;
for ( rowNum in 502:511){
    feature_set = cvResults$Features[[rowNum]];
    nFeatures = cvResults$NumFeatures[rowNum];
    df = newTrain[, c("Survived", feature_set)]
    glm.fit = glm( Survived~.,data=df,family=binomial);
    cv.glm.fit = cv.glm(df,glm.fit);
    cv.error = cv.glm.fit$delta[1];
    trainSetProbs = predict( glm.fit, newTrain, type="response");  # now a vector of probabilities
    predictions = as.numeric( trainSetProbs >= threshold );
    numRight = sum( predictions == newTrain$Survived );
    accuracy = mean( predictions == newTrain$Survived );
    features[counter] = list( feature_set );
    numFeatures[counter] = nFeatures;
    accuracies[counter] = accuracy;
    correct[counter] = numRight;
    errors[counter] = cv.error;
    counter = counter + 1;
}
loocvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), Features=I(features) );
loocvResults = loocvResults[order(loocvResults$CVError),];
knitr::kable( head(loocvResults,n=10L), format="markdown", longtable=TRUE)
```

The model with the lowest cross validation error is a model with 8 features: FixedTitle, Sex, Siblings, FixedAge, CabinAssignment, pclass, NumChildren, and Embarked.  A logistic regression model is fit using these features and submitted to Kaggle.

```{r logisticRegression6, echo=TRUE, warning=FALSE}
best_features = c("pclass", "Sex", "FixedAge", "FixedTitle", "Siblings", "CabinAssignment", "Embarked", "NumChildren" );
best_df = newTrain[, c("Survived", best_features)];
best.glm.fit = glm( Survived~., data=best_df, family=binomial);
trainSetProbs = predict( best.glm.fit, newTrain, type="response");  # now a vector of probabilities
# probabilities >= 0.5 mean survived, < 0.5 mean perished
threshold = 0.5;
predictions = as.numeric( trainSetProbs >= threshold );
summary( best.glm.fit );

# How'd we do?
# now calculate the training set error
correctRate = mean( predictions == newTrain$Survived );
# scored a 0.8395062

# now, how'd we do against the test set?
testSetProbs = predict( best.glm.fit, newTest, type="response");
bestTestPredictions = as.numeric( testSetProbs >= threshold );
bestLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);
bestLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=bestTestPredictions);

bestLogisticRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=trainSetProbs);
bestLogisticRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=testSetProbs);

write.csv( bestLogisticRegressionModelTrainProbs, "LogisticRegressionSigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( bestLogisticRegressionModelProbs, "LogisticRegressionSigModelProbs.csv", row.names=FALSE, quote=FALSE );

write.csv( bestLogisticRegressionModelTrain, "LogisticRegressionSigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( bestLogisticRegressionModel, "LogisticRegressionSigModel.csv", row.names=FALSE, quote=FALSE );
# scored a 0.77990 - not as good as the full model
```
The model based on significant features scored lower on the test set than the full model.  To reduce the model fit to noise in the training data, the regularization methods of ridge regression and lasso are considered.

### Logistic Regression with Ridge Regression Regularization
Ridge regression introduces a coefficient penalty that reduces the effect of the covariates.  This can help with variance by making the solution slightly less flexible.  The glmnet package includes a method for determining the lambda penalty coefficient using cross validation (default is 10-fold cross validation).

<!--![](http://businessforecastblog.com/wp-content/uploads/2014/01/RRminization.png)-->
![](../RRminimization.png)

#### Logistic Ridge Regression on full feature set
```{r ridgeRegression1, echo=TRUE, warning=FALSE}
library( glmnet, quietly = TRUE );
lambdas = c();
set.seed(1);
# use ridge regression
xTrain = model.matrix( Survived~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,family=binomial);
xTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest,family=binomial);
#xTrain = model.matrix( Survived~.,data=full_df,family=binomial);
full_df_test = newTest[, all_features];
#xTest = model.matrix( ~.,data=full_df_test,family=binomial);
cv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=0, type.measure="class");
plot( cv.out );
lambdaMin = cv.out$lambda.min;
rrmodel = glmnet( xTrain, newTrain$Survived, alpha=0);
train.ridge.pred = predict( rrmodel, s=lambdaMin, newx=xTrain);
trainRidgePredictions = as.numeric( train.ridge.pred >= threshold );
numCorrect = sum(trainRidgePredictions == newTrain$Survived);
accuracy = numCorrect/nrow(newTrain);
ridge.pred = predict( rrmodel, s=lambdaMin, newx=xTest);
fullRidgePredictions = as.numeric( ridge.pred >= threshold );
fullRidgeRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainRidgePredictions);
fullRidgeRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullRidgePredictions);
fullRidgeRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=train.ridge.pred);
fullRidgeRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=ridge.pred);
write.csv( fullRidgeRegressionModelTrainProbs, "RidgeRegressionFullModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( fullRidgeRegressionModelProbs, "RidgeRegressionFullModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( fullRidgeRegressionModelTrain, "RidgeRegressionFullModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( fullRidgeRegressionModel, "RidgeRegressionFullModel.csv", row.names=FALSE, quote=FALSE );
# score is 0.78947
```

The training set mean squared error is the smallest at a lambda value of 0.02931267.  Against the training data, the model achieved an accuracy of 0.8226712.

```{r ridgeRegression1b, echo=TRUE}
confusionMatrix(trainRidgePredictions, newTrain$Survived);
```

When comparing to the full logistic regression solution, the ridge predictions are the same as the least-squares logistic regression in 398 of the 418 cases (95.2% of the test set).  The model scored a 0.78468 upon submission to the Kaggle site - slightly less than the full logistic regression solution (see comparison below).

```{r ridgeRegression2, echo=TRUE}
fullLogisticPredictions = bestTestPredictions;
confusionMatrix( fullRidgePredictions, fullLogisticPredictions );
```
Next, a ridge regression model is fit using only the significant features determined from the logistic regression solution.  From the training data, lambda was determined to be 0.02931267.  Against the training data, the model achieved an accuracy of 0.8249158.

#### Logistic Ridge Regression on significant features

```{r ridgeRegression3, echo=TRUE, warning=FALSE}
library( glmnet );
lambdas = c();
set.seed(1);
# use ridge regression
xTrain = model.matrix( Survived~.,data=best_df,family=binomial);
best_df_test = newTest[, best_features];
xTest = model.matrix( ~.,data=best_df_test,family=binomial);
cv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=0, type.measure="class");
plot( cv.out );
lambdaMin = cv.out$lambda.min;
rrmodel = glmnet( xTrain, newTrain$Survived, alpha=0);
train.ridge.pred = predict( rrmodel, s=lambdaMin, newx=xTrain);
trainRidgePredictions = as.numeric( train.ridge.pred >= threshold );
numCorrect = sum(trainRidgePredictions == newTrain$Survived);
accuracy = numCorrect/nrow(newTrain);
ridge.pred = predict( rrmodel, s=lambdaMin, newx=xTest);
bestRidgePredictions = as.numeric( ridge.pred >= threshold );
bestRidgeRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainRidgePredictions);
bestRidgeRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=bestRidgePredictions);

bestRidgeRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.ridge.pred));
bestRidgeRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(ridge.pred));
write.csv( bestRidgeRegressionModelTrainProbs, "RidgeRegressionSigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( bestRidgeRegressionModelProbs, "RidgeRegressionSigModelProbs.csv", row.names=FALSE, quote=FALSE );

write.csv( bestRidgeRegressionModelTrain, "RidgeRegressionSigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( bestRidgeRegressionModel, "RidgeRegressionSigModel.csv", row.names=FALSE, quote=FALSE );
```

When comparing to the full logistic regression solution, the best feature ridge predictions are the same as the least-squares logistic regression in 399 of the 418 cases (95.4% of the test set).  The model scored a 0.77990 upon submission to the Kaggle site - which is less than both the full logistic regression and full ridge regression models.

```{r ridgeRegression4, echo=TRUE}
fullLogisticPredictions = bestTestPredictions;
confusionMatrix( bestRidgePredictions, fullLogisticPredictions );
```


### Logistic Regression with Lasso Regularization
Similar to ridge regression, lasso regularization introduces a penalty to reduce the magnitude of coefficient estimates.  As with ridge regression, the glmnet implementation of the lasso uses cross validation to estimate the best value of lambda.  The Lasso regularization method multiplies lambda with the absolute value of the coefficients as the penalty and, unlike Ridge Regression, can be used to eliminate features altogether.

<!--![](http://www.statisticshowto.com/wp-content/uploads/2015/09/lasso-regression.png)-->
![](../lasso-regression.png)

#### Lasso using all features
```{r lassoRegression, echo=TRUE, warning=FALSE}
# on to the lasso
xTrain = model.matrix( Survived~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,family=binomial);
xTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest,family=binomial);
cv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=1, type.measure = "class");
plot( cv.out );
lambdaMin = cv.out$lambda.min;
lassomodel = glmnet( xTrain, newTrain$Survived, alpha=1);
train.lasso.pred = predict( lassomodel, s=lambdaMin, newx=xTrain);
trainLassoPredictions = as.numeric( train.lasso.pred >= threshold );
numCorrect = sum(trainLassoPredictions == newTrain$Survived);
accuracy = numCorrect/nrow(newTrain);
lasso.pred = predict( lassomodel, s=lambdaMin, newx=xTest);
lassoPredictions = as.numeric( lasso.pred >= threshold );
fullLassoRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainLassoPredictions);
fullLassoRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=lassoPredictions);
fullLassoRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.lasso.pred));
fullLassoRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(lasso.pred));
write.csv( fullLassoRegressionModelTrainProbs, "LassoRegressionFullModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( fullLassoRegressionModelProbs, "LassoRegressionFullModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( fullLassoRegressionModelTrain, "LassoRegressionFullModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( fullLassoRegressionModel, "LassoRegressionFullModel.csv", row.names=FALSE, quote=FALSE );
```

In the plot, as Lambda increases, the number of features in the model is reduced (see top axis).  In this application, no features were eliminated.  The Lasso model using all of the features had a training set error of 0.1638608 with a lambda of 0.00160114 (see confusion matrix below).  

```{r lassoRegression2, echo=TRUE, warning=FALSE}
confusionMatrix( trainLassoPredictions, newTrain$Survived );
```

When comparing to the full logistic regression model against the test data, the lasso model differed from the full logistic regression model in 11 cases, predicting the same outcome in 407 of 419 rows.  Against the Kaggle test data, the model scored 0.78947.

```{r lassoRegression3, echo=TRUE, warning=FALSE}
confusionMatrix( lassoPredictions, fullLogisticPredictions );
```

#### Lasso on significant features
The lasso model with only the significant features achieved a training set error of 0.1627385 using a lambda of 0.0002067951.
```{r lassoRegression4, echo=TRUE, warning=FALSE}
# on to the lasso
xTrain = model.matrix( Survived~.,data=best_df,family=binomial);
best_df_test = newTest[, best_features];
xTest = model.matrix( ~.,data=best_df_test,family=binomial);
cv.out = cv.glmnet( xTrain, newTrain$Survived, alpha=1, type.measure = "class");
plot( cv.out );
lambdaMin = cv.out$lambda.min;
lassomodel = glmnet( xTrain, newTrain$Survived, alpha=1);
train.lasso.pred = predict( lassomodel, s=lambdaMin, newx=xTrain);
trainLassoPredictions = as.numeric( train.lasso.pred >= threshold );
numCorrect = sum(trainLassoPredictions == newTrain$Survived);
accuracy = numCorrect/nrow(newTrain);
lasso.pred = predict( lassomodel, s=lambdaMin, newx=xTest);
lassoPredictions = as.numeric( lasso.pred >= threshold );
bestLassoRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=trainLassoPredictions);
bestLassoRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=lassoPredictions);

bestLassoRegressionModelTrainProbs = data.frame(PassengerId=train$PassengerId, Survived=as.numeric(train.lasso.pred));
bestLassoRegressionModelProbs = data.frame(PassengerId=test$PassengerId, Survived=as.numeric(lasso.pred));

write.csv( bestLassoRegressionModelTrainProbs, "LassoRegressionSigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( bestLassoRegressionModelProbs, "LassoRegressionSigModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( bestLassoRegressionModelTrain, "LassoRegressionSigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( bestLassoRegressionModel, "LassoRegressionSigModel.csv", row.names=FALSE, quote=FALSE );
```

The "best" feature Lasso model had a training set error of 0.1616162 with a lambda of 0.0005754197 (see confusion matrix below).  

```{r lassoRegression5, echo=TRUE, warning=FALSE}
confusionMatrix( trainLassoPredictions, newTrain$Survived );
```

When comparing to the full logistic regression model against the test data, the "best feature" lasso model differed from the full logistic regression model in 11 cases, predicting the same outcome in 407 of 419 rows.  Against the Kaggle test data, the model scored 0.79425.

```{r lassoRegression6, echo=TRUE, warning=FALSE}
confusionMatrix( lassoPredictions, fullLogisticPredictions );
```

The best feature lasso model differed from the full logistic regression model in 11 cases and from the best feature ridge regression model in 12 cases (see confusion matrices below).

```{r lassoRegression7, echo=TRUE, warning=FALSE}
confusionMatrix( lassoPredictions, fullLogisticPredictions );
confusionMatrix( bestRidgePredictions, lassoPredictions );
# score is 0.78947
```

### K-Nearest Neighbors Models

K-Nearest Neighbors is a conceptionally simple method of assigning the most common class label to a point based on the labels of the closest K points.  The lower the K, the higher the variance (at an extreme, K=1 will assign the class label based on the closest labeled point).  Conversely, the higher the K, the lower the variance and higher the bias.  With K as N-1 as an extreme, each point will be labeled as the most common class.  This method lends itself nicely to cross-validation to find the best value of K.

#### KNN using all features
Using Leave One Out Cross Validation, the KNN model was trained using all features.  The model was cross-validated using 20 different K-values from 5 to 43, odd numbered.  The model achieved a training set accuracy of 0.8563 with a K of 9.

```{r knn1, warning=FALSE}
library(caret);
trainCtrl = trainControl( method="LOOCV");
knn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain,method="knn", trControl=trainCtrl, preProcess=c("center","scale"), tuneLength=20 );
knn_fit
plot( knn_fit );
test_pred = predict( knn_fit, newdata=newTrain);
confusionMatrix( test_pred, newTrain$SurvivedFactor)
```

Against the Kaggle test data, the model scored 0.76555.

```{r knn2, echo=TRUE, warning=FALSE}
# now the whole training set
knnTrain = newTrain;
knnTest = newTest;
trainCtrl = trainControl( method="LOOCV" );
knn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=knnTrain,method="knn", trControl=trainCtrl, preProcess=c("center","scale"), tuneLength=20 );
knn_train_pred = predict( knn_fit, newdata=knnTrain);
knn_pred = predict( knn_fit, newdata=knnTest);
fullKNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);
fullKNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);
write.csv( fullKNNModelTrain, "KNNFullModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( fullKNNModel, "KNNFullModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( fullKNNModelTrain, "KNNFullModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( fullKNNModel, "KNNFullModel.csv", row.names=FALSE, quote=FALSE );
```

#### KNN with subsets of significant features

Since some features may not be informative and all features are used in finding closest points, it may be better to consider removing features that just add noise. The idea is to start with all significant features and then consider combinations as subsets for analysis.

```{r knn3, warning=FALSE}
significant_features = c("FixedTitle", "Sex", "Siblings", "FixedAge", "CabinAssignment", "pclass", "NumChildren", "FarePerPassenger", "Embarked");
all_combinations_of_significant_features = lapply(1:length(significant_features), function(x) combn(length(significant_features),x));
numFeatureSetClasses = length( all_combinations_of_significant_features );
correct = list();
accuracies = list();
errors = list();
features = list();
numFeatures = list();
k = list();
counter = 1;
numTrain = nrow( newTrain );
trainCtrl = trainControl( method="repeatedcv", number = 8, repeats = 1 );
for ( featureClass in 1:numFeatureSetClasses){
  featClassColumns = dim( all_combinations_of_significant_features[[featureClass]] )[2];
  for ( thisCol in 1:featClassColumns ){
    feature_set = significant_features[ all_combinations_of_significant_features[[featureClass]][,thisCol]];
    nFeatures = length( feature_set );
    df = newTrain[, c("SurvivedFactor", feature_set)]
    knn_fit = train( SurvivedFactor~.,data=df,method="knn", trControl=trainCtrl, preProcess=c("center","scale"), tuneLength=20 );
    knn_pred = predict( knn_fit, newdata=newTrain);
    numRight = sum( knn_pred == newTrain$SurvivedFactor );
    cv.error = ( numTrain - numRight )/numTrain;
    accuracy = mean( knn_pred == newTrain$SurvivedFactor );
    features[counter] = list( feature_set );
    numFeatures[counter] = nFeatures;
    accuracies[counter] = accuracy;
    correct[counter] = numRight;
    errors[counter] = cv.error;
    k[counter] = knn_fit$bestTune$k;
    counter = counter + 1;
  }
}
cvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), K=unlist(k), Features=I(features) );
cvResults = cvResults[order(cvResults$CVError),];
knitr::kable( head(cvResults,n=20L), format="markdown", longtable=TRUE);
```

Now, take the top 10 results and repeat the analysis using LOOCV.  LOOCV provides a better estimate of the error but is computationally costly.

```{r knn4, echo=TRUE, eval=FALSE, warning=FALSE}
correct = list();
accuracies = list();
errors = list();
features = list();
numFeatures = list();
k = list();
counter = 1;
numTrain = nrow( newTrain );
trainCtrl = trainControl( method="LOOCV" );
for ( rowNum in 1:10 ){
  feature_set = cvResults$Features[[rowNum]];
  nFeatures = cvResults$NumFeatures[rowNum];
  df = newTrain[, c("SurvivedFactor", feature_set)]
  knn_fit = train( SurvivedFactor~.,data=df,method="knn", trControl=trainCtrl, preProcess=c("center","scale"), tuneLength=20 );
  knn_pred = predict( knn_fit, newdata=newTrain);
  numRight = sum( knn_pred == newTrain$SurvivedFactor );
  cv.error = ( numTrain - numRight )/numTrain;
  accuracy = mean( knn_pred == newTrain$SurvivedFactor );
  features[counter] = list( feature_set );
  numFeatures[counter] = nFeatures;
  accuracies[counter] = accuracy;
  correct[counter] = numRight;
  errors[counter] = cv.error;
  k[counter] = knn_fit$bestTune$k;
  counter = counter + 1;
  cat( counter, "\n")
}
loocvResults = data.frame( NumFeatures=unlist(numFeatures), CVError=unlist(errors), TrainingScore=unlist(accuracies), NumCorrect=unlist(correct), K=unlist(k), Features=I(features) );
loocvResults = loocvResults[order(loocvResults$CVError),];
knitr::kable( head(loocvResults,n=10L), format="markdown", longtable=TRUE);
```
Now, take the best 9, 8, 7, and 6 knn models.

#### KNN with with best subset of 9 significant features

With 9 features, the model achieves a training set accuracy of 0.8754 using a cross-validated K of 5.  On the test set, the model scored 0.76076.

```{r knn5, echo=TRUE, warning=FALSE}
# best 9
knnTrain = newTrain;
knnTest = newTest;
trainCtrl = trainControl( method="LOOCV" );
knn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=knnTrain,method="knn", trControl=trainCtrl, preProcess=c("center","scale"), tuneLength=20 );
knn_fit
plot( knn_fit )
# training set performance
knn_train_pred = predict( knn_fit, newdata=knnTrain);
confusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);
# test set performance
knn_pred = predict( knn_fit, newdata=knnTest);
best9KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);
best9KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);
write.csv( best9KNNModelTrain, "KNNBest9ModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( best9KNNModel, "KNNBest9ModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( best9KNNModelTrain, "KNNBest9ModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( best9KNNModel, "KNNBest9Model.csv", row.names=FALSE, quote=FALSE );
```

#### KNN with with best subset of 8 significant features

With 8 features, the model achieves a training set accuracy of 0.881 using a cross-validated K of 5.  On the test set, the model scored 0.73684.

```{r knn6, warning=FALSE}
# best 8
trainCtrl = trainControl( method="LOOCV" );
knn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+Siblings,data=knnTrain,method="knn", trControl=trainCtrl, preProcess=c("center","scale"), tuneLength=20 );
knn_fit
plot( knn_fit )
# training set performance
knn_train_pred = predict( knn_fit, newdata=knnTrain);
confusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);
# test set performance
knn_pred = predict( knn_fit, newdata=knnTest);
best8KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);
best8KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);
write.csv( best8KNNModelTrain, "KNNBest8ModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( best8KNNModel, "KNNBest8ModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( best8KNNModelTrain, "KNNBest8ModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( best8KNNModel, "KNNBest8Model.csv", row.names=FALSE, quote=FALSE );
```

#### KNN with with best subset of 7 significant features

With 7 features, the model achieves a training set accuracy of 0.8765 using a cross-validated K of 5.  On the test set, the model scored 0.73684.

```{r knn7, echo=TRUE, warning=FALSE}
# best 7
trainCtrl = trainControl( method="LOOCV" );
knn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Siblings,data=knnTrain,method="knn", trControl=trainCtrl, preProcess=c("center","scale"), tuneLength=20 );
knn_fit
plot( knn_fit )
# training set performance
knn_train_pred = predict( knn_fit, newdata=knnTrain);
confusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);
# test set performance
knn_pred = predict( knn_fit, newdata=knnTest);
best7KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);
best7KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);
write.csv( best7KNNModelTrain, "KNNBest7ModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( best7KNNModel, "KNNBest7ModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( best7KNNModelTrain, "KNNBest7ModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( best7KNNModel, "KNNBest7Model.csv", row.names=FALSE, quote=FALSE );
```

#### KNN with with best subset of 6 significant features

With 6 features, the model achieves a training set accuracy of 0.8765 using a cross-validated K of 5.  On the test set, the model scored 0.71770.

```{r knn8, warning=FALSE}
# best 6
trainCtrl = trainControl( method="LOOCV" );
knn_fit = train( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+Siblings,data=knnTrain,method="knn", trControl=trainCtrl, preProcess=c("center","scale"), tuneLength=20 );
knn_fit
plot( knn_fit )
# training set performance
knn_train_pred = predict( knn_fit, newdata=knnTrain);
confusionMatrix(knn_train_pred, knnTrain$SurvivedFactor);
# test set performance
knn_pred = predict( knn_fit, newdata=knnTest);
best6KNNModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(knn_train_pred)-1);
best6KNNModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(knn_pred)-1);
write.csv( best6KNNModelTrain, "KNNBest6ModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( best6KNNModel, "KNNBest6ModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( best6KNNModelTrain, "KNNBest6ModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( best6KNNModel, "KNNBest6Model.csv", row.names=FALSE, quote=FALSE );
```

The test set score of 0.71770 is less than that of the simple Gender Submission Model (0.76555).

### Decision Tree Models

#### Classification Tree

A classification tree is used to model the data.  Since the features contain nominal data (Embarked, Sex, for example), a decision tree is appropriate.  After the tree is fit, terminal nodes are pruned using cross validation.  
The tree model scored 0.8507 against the training data and 0.7727 against the test data.

```{r treeModel1, echo=TRUE, warning=FALSE}
library(tree);
tree_model = tree( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain );
summary( tree_model )
cv_tree_model = cv.tree( tree_model, FUN=prune.misclass);
best_terminal_node_idx = which.min(cv_tree_model$dev);
best_terminal_nodes = cv_tree_model$size[best_terminal_node_idx];
pruned_tree = prune.misclass(tree_model,best=best_terminal_nodes);
plot( pruned_tree );
text( pruned_tree, pretty=0);
# training set performance against pruned tree
pruned_tree_train_pred = predict( pruned_tree, newTrain, type="class");
confusionMatrix(pruned_tree_train_pred, newTrain$SurvivedFactor);
# test set performance
tree_pred = predict( pruned_tree, newTest, type="class");
treeModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(pruned_tree_train_pred)-1);
treeModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(tree_pred)-1);
write.csv( treeModelTrain, "TreeModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( treeModel, "TreeModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( treeModelTrain, "TreeModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( treeModel, "TreeModel.csv", row.names=FALSE, quote=FALSE );
```

#### Tree model with significant features

Next, a tree model using only significant features is considered.  Again, cross-validation is used to prune the tree.  The model achieved an accuracy of 0.8541 on the training data and scored 0.7608 against the test data.

```{r treeModel2, warning=FALSE}
library(tree);
tree_model = tree( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain );
summary( tree_model )
cv_tree_model = cv.tree( tree_model, FUN=prune.misclass);
best_terminal_node_idx = which.min(cv_tree_model$dev);
best_terminal_nodes = cv_tree_model$size[best_terminal_node_idx];
pruned_tree = prune.misclass(tree_model,best=best_terminal_nodes);
plot( pruned_tree );
text( pruned_tree, pretty=0);
# training set performance against pruned tree
pruned_tree_train_pred = predict( pruned_tree, newTrain, type="class");
confusionMatrix(pruned_tree_train_pred, newTrain$SurvivedFactor);
# test set performance
tree_pred = predict( pruned_tree, newTest, type="class");
# scored 0.7608
treeModelTrain = data.frame(PassengerId=knnTrain$PassengerId, Survived=as.numeric(pruned_tree_train_pred)-1);
treeModel = data.frame(PassengerId=knnTest$PassengerId, Survived=as.numeric(tree_pred)-1);
write.csv( treeModelTrain, "TreeSigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( treeModel, "TreeSigModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( treeModelTrain, "TreeSigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( treeModel, "TreeSigModel.csv", row.names=FALSE, quote=FALSE );
```

### Bootstrap Aggregation (Bagging) Tree Models

Bootstrap Aggregation is a method of resampling the data many times to generate a series of samples from the original training set.  This allows for the fitting of many trees.  In Bagging tree models, the results of these trees are accumulated and the class that a particular sample assumes most often is the winner (by majority vote).  The averaging of many trees reduces variance.

#### Bagging model using all features

The Bagging model scored an impressive 0.9877 on the training set but only 0.7321 against the test set.  The high train set accuracy and relatively low test set accuracy is indicative of over-training.

```{r baggingModel1, warning=FALSE}
library(randomForest);
set.seed(1);
bag_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain, mtry=12, importance=TRUE );
bag_model
plot( bag_model )
# Get training set performance
bag_train_pred = predict( bag_model, newdata=newTrain);
confusionMatrix( bag_train_pred, newTrain$SurvivedFactor);
# now test set
bag_pred = predict( bag_model, newdata=newTest);
bagModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(bag_train_pred)-1);
bagModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(bag_pred)-1);
write.csv( bagModelTrain, "BagModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( bagModel, "BagModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( bagModelTrain, "BagModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( bagModel, "BagModel.csv", row.names=FALSE, quote=FALSE );
# scored 0.73205
```

Plot shows the OOB error (black) as well as the class errors (0/Died = red, 1/Survived = green).

#### Bagging model using only significant features

Like the full featured model, the significant feature bagging model scored an impressive 0.9854 on the training set but only 0.7273 against the test set.  The high train set accuracy and relatively low test set accuracy is indicative of over-training.

```{r baggingModel2, warning=FALSE}
library(randomForest);
set.seed(1);
bag_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain, mtry=9, importance=TRUE );
bag_model
plot( bag_model )
# Get training set performance
bag_train_pred = predict( bag_model, newdata=newTrain);
confusionMatrix( bag_train_pred, newTrain$SurvivedFactor);
# now test set
bag_pred = predict( bag_model, newdata=newTest);
bagModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(bag_train_pred)-1);
bagModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(bag_pred)-1);
write.csv( bagModelTrain, "BagSigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( bagModel, "BagSigModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( bagModelTrain, "BagSigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( bagModel, "BagSigModel.csv", row.names=FALSE, quote=FALSE );
# scored 0.7273
```

Plot shows the OOB error (black) as well as the class errors (0/Died = red, 1/Survived = green).

### Random Forest Models

A random forest is much like the bagging idea but has the constraint that each branch point is allowed a random selection of m features to use.  This random selection of predictors decorrelates the trees from one another and creates a much more diverse set of trees than does bagging.  As a rule of thumb, the number of predictors to choose from is set to the square root of the number of available predictors (for classification trees).

#### Random Forest Model using all features

Using the standard sqrt(predictors) = 3 for mtry resulted in another overfit model.  But reducing the mtry to 1 (the number of predictors considered at each branch point), the random forest model scored 0.8575 against the training data and 0.7847 against the test data.

```{r rfModel1, echo=TRUE, warning=FALSE}
library(randomForest);
set.seed(1);
rf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain, mtry=1, importance=TRUE );
rf_model
importance( rf_model );
varImpPlot( rf_model );
plot( rf_model )
# training set performance
rf_train_pred = predict( rf_model, newdata=newTrain);
confusionMatrix( rf_train_pred, newTrain$SurvivedFactor);
# test set performance
rf_pred = predict( rf_model, newdata=newTest);
rfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);
rfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);
write.csv( rfModelTrain, "RandomForestModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModel, "RandomForestModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModelTrain, "RandomForestModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModel, "RandomForestModel.csv", row.names=FALSE, quote=FALSE );
# scored 0.7847
```

#### Random Forest Model using only significant features

Using the standard sqrt(predictors) = 3 for mtry resulted in another overfit model.  But reducing the mtry to 1 (the number of predictors considered at each branch point), the random forest model scored 0.8608 against the training data and 0.7823 against the test data.

```{r rfModel2, echo=TRUE, warning=FALSE}
library(randomForest);
set.seed(1);
rf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumChildren+Siblings,data=newTrain, importance=TRUE, mtry = 1 );
rf_model
importance( rf_model );
varImpPlot( rf_model );
plot( rf_model )
# training set performance
rf_train_pred = predict( rf_model, newdata=newTrain);
confusionMatrix( rf_train_pred, newTrain$SurvivedFactor);
# test set performance
rf_pred = predict( rf_model, newdata=newTest);
rfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);
rfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);
write.csv( rfModelTrain, "RandomForestSigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModel, "RandomForestSigModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModelTrain, "RandomForestSigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModel, "RandomForestSigModel.csv", row.names=FALSE, quote=FALSE );
# scored 0.7823
```

#### Random Forest using only 9 selected features

The Random forest model using all features generated a variable importance plot.  The variable set that looked to be most informative is selected and used in this model.  This model scored 0.8485 on the training set and 0.7775 on the test set.

```{r rfModel3, echo=TRUE, warning=FALSE}
library(randomForest);
set.seed(1);
rf_model = randomForest( SurvivedFactor~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+NumParents+Siblings+NumPassengersOnTicket,data=newTrain, importance=TRUE, mtry=1 );
rf_model
importance( rf_model );
varImpPlot( rf_model );
plot( rf_model )
# training set performance
rf_train_pred = predict( rf_model, newdata=newTrain);
confusionMatrix( rf_train_pred, newTrain$SurvivedFactor);
# test set performance
rf_pred = predict( rf_model, newdata=newTest);
rfModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(rf_train_pred)-1);
rfModel = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(rf_pred)-1);
write.csv( rfModelTrain, "RandomForestTrimmedModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModel, "RandomForestTrimmedModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModelTrain, "RandomForestTrimmedModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( rfModel, "RandomForestTrimmedModel.csv", row.names=FALSE, quote=FALSE );
# scored 0.7775
```

### Boosted Classification Tree Models

Gradient Boosting applied to classification trees involves fitting classification trees sequentially.  That is, instead of generating a bunch of trees and averaging them as with bagging and random forests, boosting involves adding trees sequentially into a single model.  Trees are successively fit to the residuals of the previous tree ensemble. 

#### Gradient Boosting using all features

Since there are many parameters to consider for boosted classification trees, cross-validation is used to select the values which will generalize best.  This analysis is done using all predictors.

```{r boostedModel0, echo=TRUE, warning=FALSE, eval=FALSE}
# This is not run; ignore
library(gbm, quietly = TRUE);
tree_lengths = 1:4;
best_error = Inf;
best_idx = 0;
min_error_idx = tree_lengths;
min_errors = tree_lengths;
xTrain = newTrain[,c("Survived","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
xTest = newTest[,c("Survived","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
numSamples = 10;
trainingProportion = 0.8;
threshold = 0.5;
cvFolds = 8;  # n-fold validation
nTrees = 15000; 
treeDiv = 1000;
numIntTrees = nTrees/treeDiv - 1;
nCores = 7; # adjust according to computer

trainMat = matrix( nrow=numSamples, ncol=numIntTrees);
testMat = matrix( nrow=numSamples, ncol=numIntTrees);
for ( depth in tree_lengths ){
  set.seed(depth);
  sampleSet = createDataPartition(1:nrow(newTrain),numSamples,p=trainingProportion);
  for ( samp in 1:numSamples){
    trainSample = sampleSet[[samp]];
    testSample = setdiff(1:nrow(newTrain),trainSample);
    boosted_model = gbm( formula=Survived~.,data=xTrain[trainSample,], distribution="bernoulli", n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, n.cores = nCores, verbose=FALSE );
    best_tree_num = gbm.perf( boosted_model );
    train_scores = 0;
    test_scores = 0;
    for ( ntree in 1:numIntTrees){
      treelen = (ntree-1)*treeDiv;
      # now that the model is trained, see how it does on training set for n trees.
      train_pred = predict( boosted_model, newdata=xTrain[trainSample,], n.trees=treelen, type="response");
      train_predictions = as.numeric( train_pred >= threshold );
      train_score = sum( train_predictions == newTrain$Survived[trainSample] )/length(trainSample);
      train_scores[ ntree ] = train_score;
      trainMat[samp,ntree] = train_score;
      cat( "trainMat[", samp, ",", ntree, "] = ", train_score, "\n");
      # now that the model is trained, see how it does on test set for n trees.
      test_pred = predict( boosted_model, newdata=xTrain[testSample,], n.trees=treelen, type="response");
      test_predictions = as.numeric( test_pred >= threshold );
      test_score = sum( test_predictions == newTrain$Survived[testSample] )/length(testSample);
      test_scores[ ntree ] = test_score;
      testMat[samp,ntree] = test_score;
    }
  }
  boosted_model = gbm( formula=Survived~.,data=xTrain, distribution="bernoulli", n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, n.cores = nCores, verbose=FALSE );
  min_error_idx[depth] = which.min( boosted_model$cv.error );
  min_errors[depth] = min( boosted_model$cv.error );
  if ( min_errors[depth] < best_error ){
    best_error = min_errors[depth];
    best_idx = depth;
  }
}
gbtrees = data.frame( TreeDepth = tree_lengths, BernoulliDeviance = min_errors, NumTrees = min_error_idx );
knitr::kable( gbtrees, format="markdown", longtable=TRUE);
#summary( boosted_model );
#plot( boosted_model );
set.seed(best_idx);
best_model = gbm( formula=Survived~.,data=xTrain, distribution="bernoulli", n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, n.cores = nCores, verbose=FALSE );
gbm.perf( best_model );
boosted_train_pred = predict( best_model, newdata=xTrain, n.trees=min_error_idx[best_idx], type="response");
boosted_pred = predict( best_model, newdata=xTest, n.trees=min_error_idx[best_idx], type="response");
# probabilities >= 0.5 mean survived, < 0.5 mean perished
boosted_train_predictions = as.numeric( boosted_train_pred >= threshold );
boosted_predictions = as.numeric( boosted_pred >= threshold );
boostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_predictions);
boostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);
boostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));
boostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));
write.csv( boostedModelTrainProbs, "BoostedTreeModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModelProbs, "BoostedTreeModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModelTrain, "BoostedTreeModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModel, "BoostedTreeModel.csv", row.names=FALSE, quote=FALSE );
# originally scored 0.76076 with interaction depth =4, 0.77990 with interaction depth = 1
# after 10-fold x-validation, considering interaction depths of 1 to 4, scored a 0.76555
# same model , stopping at 4800 trees yields 0.77511
```

```{r boostedModel1, echo=TRUE, warning=FALSE}
library(gbm, quietly = TRUE);
tree_lengths = 1:10;
numLengths = length( tree_lengths );
best_error = Inf;
best_idx = 0;
best_shrinkage = 0;
best_steps = 0;
xTrain = newTrain[,c("Survived","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
xTest = newTest[,c("Survived","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
learningRates = c(0.1,0.07,0.05,0.03,0.02,0.01);
numLR = length( learningRates );
threshold = 0.5;
cvFolds = 10;  # n-fold validation
nTrees = 3000; 
nCores = 7; # adjust according to computer
tree_depths = rep(0,numLengths*numLR);
steps = rep(0,numLengths*numLR);
deviances = rep(0,numLengths*numLR);
shrinkages = rep(0,numLengths*numLR);
idx = 1;

for ( depth in tree_lengths ){
  set.seed(depth);
  for ( lrIdx in 1:numLR){
      learningRate = learningRates[lrIdx];
      boosted_model = gbm( formula=Survived~.,data=xTrain, distribution="bernoulli", n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, shrinkage = learningRate, n.cores = nCores, verbose=FALSE );
      idxCVErr = gbm.perf( boosted_model, plot.it=FALSE );
      cvError = boosted_model$cv.error[idxCVErr];
      if ( cvError < best_error ){
        best_error = cvError;
        best_idx = depth;
        best_shrinkage = learningRate;
        best_steps = idxCVErr;
      }
      tree_depths[idx] = depth;
      shrinkages[idx] = learningRate;
      steps[idx] = idxCVErr;
      deviances[idx] = cvError;
      idx = idx + 1;
  }
}
gbtrees = data.frame( TreeDepth = tree_depths, Shrinkage = shrinkages, NumTrees = steps, BernoulliDeviance = deviances );
knitr::kable( gbtrees, format="markdown", longtable=TRUE);
#summary( boosted_model );
#plot( boosted_model );
# create a line chart
xrange = range( shrinkages );
yrange = range( deviances );
plot( xrange, yrange, type="n", xlab="Shrinkage", ylab="CV Accuracy (Bernoulli Deviance)");
colors = rainbow( numLengths );
linetype = tree_lengths;
plotchar = seq(0,numLengths,1);
# add lines
for ( i in tree_lengths){
  thisDepthTrees = subset( gbtrees, gbtrees$TreeDepth == i );
  lines(thisDepthTrees$Shrinkage, thisDepthTrees$BernoulliDeviance, type="b", lwd=1.5, lty=linetype[i], col=colors[i], pch=plotchar[i]);
}
title( "Bernoulli Deviance by Shrinkage and Tree Depth");
legend( 0.08, yrange[2], 1:numLengths, cex=0.8, col=colors, pch=plotchar, lty=linetype, title="Tree Depth");
set.seed(best_idx);
best_model = gbm( formula=Survived~.,data=xTrain, distribution="bernoulli", n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, shrinkage = best_shrinkage, n.cores = nCores, verbose=FALSE );
gbm.perf( best_model );
boosted_train_pred = predict( best_model, newdata=xTrain, n.trees=best_steps, type="response");
boosted_train_calls = as.numeric( boosted_train_pred >= threshold );
confusionMatrix(boosted_train_calls,newTrain$Survived);
boosted_pred = predict( best_model, newdata=xTest, n.trees=best_steps, type="response");
# probabilities >= 0.5 mean survived, < 0.5 mean perished
boosted_predictions = as.numeric( boosted_pred >= threshold );
boostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_calls);
boostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);
boostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));
boostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));
write.csv( boostedModelTrainProbs, "BoostedTreeModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModelProbs, "BoostedTreeModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModelTrain, "BoostedTreeModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModel, "BoostedTreeModel.csv", row.names=FALSE, quote=FALSE );
# interaction.depth = 8, nTrees = 118, learningRate = 0.05
# scored 0.756
```

The Cross-validated parameters (interaction depth = 8, number of trees = 118, learning rate = 0.05) resulted in a training set accuracy of 0.8943 on the training set and 0.756 on the test set.

#### Gradient Boosting using only significant features

The same technique is used as above to find the best set of parameters for gradient boosting model for the significant features.

```{r boostedModel2, echo=TRUE, warning=FALSE}
library(gbm);
tree_lengths = 1:10;
numLengths = length( tree_lengths );
best_error = Inf;
best_idx = 0;
best_shrinkage = 0;
best_steps = 0;
significant_features = c("FixedTitle", "Sex", "Siblings", "FixedAge", "CabinAssignment", "pclass", "NumChildren", "FarePerPassenger", "Embarked");
xTrain = newTrain[,c("Survived", significant_features) ];
xTest = newTest[,c("Survived", significant_features) ];
learningRates = c(0.1,0.07,0.05,0.03,0.02,0.01);
numLR = length( learningRates );
threshold = 0.5;
cvFolds = 10;  # n-fold validation
nTrees = 3000; 
nCores = 7; # adjust according to computer
tree_depths = rep(0,numLengths*numLR);
steps = rep(0,numLengths*numLR);
deviances = rep(0,numLengths*numLR);
shrinkages = rep(0,numLengths*numLR);
idx = 1;

for ( depth in tree_lengths ){
  set.seed(depth);
  for ( lrIdx in 1:numLR){
      learningRate = learningRates[lrIdx];
      boosted_model = gbm( formula=Survived~.,data=xTrain, distribution="bernoulli", n.trees=nTrees, cv.folds=cvFolds, interaction.depth = depth, shrinkage = learningRate, n.cores = nCores, verbose=FALSE );
      idxCVErr = gbm.perf( boosted_model, plot.it=FALSE );
      cvError = boosted_model$cv.error[idxCVErr];
      if ( cvError < best_error ){
        best_error = cvError;
        best_idx = depth;
        best_shrinkage = learningRate;
        best_steps = idxCVErr;
      }
      tree_depths[idx] = depth;
      shrinkages[idx] = learningRate;
      steps[idx] = idxCVErr;
      deviances[idx] = cvError;
      idx = idx + 1;
  }
}
gbtrees = data.frame( TreeDepth = tree_depths, Shrinkage = shrinkages, NumTrees = steps, BernoulliDeviance = deviances );
knitr::kable( gbtrees, format="markdown", longtable=TRUE);
#summary( boosted_model );
#plot( boosted_model );
# create a line chart
xrange = range( shrinkages );
yrange = range( deviances );
plot( xrange, yrange, type="n", xlab="Shrinkage", ylab="Cross Validation Error (Bernoulli Deviance)");
colors = rainbow( numLengths );
linetype = tree_lengths;
plotchar = seq(0,numLengths,1);
# add lines
for ( i in tree_lengths){
  thisDepthTrees = subset( gbtrees, gbtrees$TreeDepth == i );
  lines(thisDepthTrees$Shrinkage, thisDepthTrees$BernoulliDeviance, type="b", lwd=1.5, lty=linetype[i], col=colors[i], pch=plotchar[i]);
}
title( "Bernoulli Deviance by Shrinkage and Tree Depth");
legend( 0.08, yrange[2], 1:numLengths, cex=0.8, col=colors, pch=plotchar, lty=linetype, title="Tree Depth");
set.seed(best_idx);
best_model = gbm( formula=Survived~.,data=xTrain, distribution="bernoulli", n.trees=nTrees, cv.folds=cvFolds, interaction.depth = best_idx, shrinkage = best_shrinkage, n.cores = nCores, verbose=FALSE );
gbm.perf( best_model );
boosted_train_pred = predict( best_model, newdata=xTrain, n.trees=best_steps, type="response");
boosted_train_calls = as.numeric( boosted_train_pred >= threshold );
confusionMatrix(boosted_train_calls,newTrain$Survived);
boosted_pred = predict( best_model, newdata=xTest, n.trees=best_steps, type="response");
# probabilities >= 0.5 mean survived, < 0.5 mean perished
boosted_predictions = as.numeric( boosted_pred >= threshold );
boostedModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=boosted_train_calls);
boostedModel = data.frame(PassengerId=newTest$PassengerId, Survived=boosted_predictions);
boostedModelTrainProbs = data.frame(PassengerId=newTrain$PassengerId, Survived=as.numeric(boosted_train_pred));
boostedModelProbs = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(boosted_pred));
write.csv( boostedModelTrainProbs, "BoostedTreeSigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModelProbs, "BoostedTreeSigModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModelTrain, "BoostedTreeSigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( boostedModel, "BoostedTreeSigModel.csv", row.names=FALSE, quote=FALSE );
# interaction.depth = 8, nTrees = 328, learningRate = 0.02
# scored 0.75119
```

The cross validated parameters were found to be an interaction depth of 8, 328 trees, and a learning rate of 0.02.  The model achieved a score of 0.8923 on the training set and a sad 0.7368 on the test set.

### Support Vector Machine (SVM) Model

Support Vector Machines seek to find the optimal separating hyperplane between two classes.  There are several parameters to consider: cost (of a constraint violation), choice of kernel (and corresponding gamma).  Cross-validation is used to estimate these parameters.

```{r svm1, echo=TRUE, warning=FALSE}
library(e1071);
library(caret);
#xTrain = newTrain[,c("pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
#yTrain = newTrain[,"SurvivedFactor"];
xTrain = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=whiteTrain);
yTrain = newTrain[,"SurvivedFactor"];
#xTest = newTest[,c("pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
xTest = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=whiteTest);
#xTrainSVM = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTrain);
#xTestSVM = model.matrix( ~pclass+Sex+FixedAge+FarePerPassenger+FixedTitle+CabinAssignment+Embarked+NumPassengersOnTicket+NumParents+NumChildren+SpousesFactor+Siblings,data=newTest);
costs = c(0.01, 0.1, 0.5, 0.9, 1, 1.1);
gammas = c(0.0001, 0.01, 0.05, 0.1, 0.5);
kernels = c("linear", "polynomial", "radial", "sigmoid");
bestCost = costs[1];
bestGamma = gammas[1];
bestKernel = kernels[1];
bestError = Inf;
errors = rep(Inf,1,length(kernels));
ctr = 1;
for ( kernel in kernels ){
  tune.out = tune( "svm", xTrain, train.y=yTrain, kernel=kernel, ranges=list( cost=costs,gamma=gammas) );
  tune.out$best.performance
  tune.out$best.model
  thisError = tune.out$best.performance;
  errors[ctr] = thisError;
  if ( thisError <  bestError ){
    bestError = thisError;
    bestKernel = kernel;
    bestGamma = tune.out$best.model$gamma;
    bestCost = tune.out$best.model$cost;
  }
  ctr = ctr + 1;
}
svm.fit = svm( x=xTrain, y=yTrain, kernel=bestKernel, cost=bestCost, gamma=bestGamma);
#plot( svm.fit, xTrain );
svm.train.pred = predict( svm.fit, xTrain );
svmTrainingPreds = as.numeric( svm.train.pred ) - 1;
truth = newTrain$Survived;
confusionMatrix( svmTrainingPreds, truth );
# scores a 0.8294 on the training set
svm.test.pred = predict( svm.fit, xTest );  
svmPredictions = as.numeric( svm.test.pred ) - 1;
svmModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=svmTrainingPreds);
svmModel = data.frame(PassengerId=newTest$PassengerId, Survived=svmPredictions);

write.csv( svmModelTrain, "SVMModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( svmModel, "SVMModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( svmModelTrain, "SVMModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( svmModel, "SVMModel.csv", row.names=FALSE, quote=FALSE );
# scored a 0.7727
```

The SVM model scored a 0.8328 against the training set and a 0.7727 against the test set.

### Neural Network Models

This neural network implementation uses a single layer.  The number of hidden nodes and learning rate is selected through 10-fold cross validation.

#### Neural Network with one hidden layer utilizing all features
```{r nnModel1, echo=TRUE, eval=FALSE, warning=FALSE}
### NN Model
library(nnet);
#xTrain = newTrain[,c("SurvivedFactor", significant_features) ];
#xTest = newTest[,c("SurvivedFactor", significant_features) ];

xTrain = whiteTrain[,c("SurvivedFactor","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];
xTest = whiteTest[,c("SurvivedFactor","pclass","Sex","FixedAge","FarePerPassenger","FixedTitle","CabinAssignment","Embarked","NumPassengersOnTicket","NumParents","NumChildren","SpousesFactor","Siblings")];

nn = nnet( SurvivedFactor ~ ., data = xTrain, size=12, maxit=500, trace=FALSE);
# How do we do on the training data?

nn_train_pred_class = predict( nn, xTrain, type="class" );  # yields "0", "1"
nn_train_pred = as.numeric( nn_train_pred_class );   # transform to 0, 1
confusionMatrix(nn_train_pred,xTrain$Survived);

# try on test data
nn_test_pred_class = predict( nn, xTest, type="class" );  # yields "0", "1"
nn_test_pred = as.numeric( nn_test_pred_class );   # transform to 0, 1
# write to  file
nnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=nn_train_pred);
nnModel = data.frame(PassengerId=newTest$PassengerId, Survived=nn_test_pred);
write.csv( nnModelTrain, "NeuralNetworkFullModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModel, "NeuralNetworkFullModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModelTrain, "NeuralNetworkFullModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModel, "NeuralNetworkFullModel.csv", row.names=FALSE, quote=FALSE );
# best "over"trained model scores 0.73205 (306/418)

# Now, we wish to select parameters for neural network by using 10-fold cross-validation with caret
tuningGrid = expand.grid(size=1:12,decay=c(0,0.0001,0.05,0.1));
set.seed( 1 );
trControl = trainControl( method="repeatedcv", number=10, repeats=10 );
nn_cv = train( SurvivedFactor ~ ., data=xTrain, method="nnet", trControl = trControl, tuneGrid=tuningGrid, verbose=FALSE, trace=FALSE);

best_size = nn_cv$bestTune[1,"size"];
best_decay = nn_cv$bestTune[1,"decay"];
best_nn = nnet( SurvivedFactor ~ ., data = xTrain, size=best_size, decay=best_decay, maxit=500, trace=FALSE);
best_nn_train_pred_class = predict( best_nn, newdata=xTrain, type="class" );  # yields "0", "1"
best_nn_train_pred = as.numeric( best_nn_train_pred_class );   # transform to 0, 1
confusionMatrix( best_nn_train_pred, xTrain$Survived);
# now do on test data
best_nn_test_pred_class = predict( best_nn, newdata=xTest, type="class" );  # yields "0", "1"
best_nn_test_pred = as.numeric( best_nn_test_pred_class );   # transform to 0, 1
# write to  file
nnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=best_nn_train_pred);
nnModel = data.frame(PassengerId=newTest$PassengerId, Survived=best_nn_test_pred);
write.csv( nnModelTrain, "NeuralNetworkFullModelCVTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModel, "NeuralNetworkFullModelCVProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModelTrain, "NeuralNetworkFullModelCVTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModel, "NeuralNetworkFullModelCV.csv", row.names=FALSE, quote=FALSE );
# Neural network model scores 0.7847
```

The cross validated Neural Network model scored 0.8418 against the training set and 0.7847 against the test set.

#### Neural Network with one hidden layer utilizing significant features
```{r nnModel2, echo=TRUE, eval=TRUE, warning=FALSE}
### NN Model
library(nnet);
xTrain = whiteTrain[,c("SurvivedFactor", significant_features) ];
xTest = whiteTest[,c("SurvivedFactor", significant_features) ];

nn = nnet( SurvivedFactor ~ ., data = xTrain, size=length(significant_features), maxit=500, trace=FALSE);
# How do we do on the training data?

nn_train_pred_class = predict( nn, xTrain, type="class" );  # yields "0", "1"
nn_train_pred = as.numeric( nn_train_pred_class );   # transform to 0, 1
confusionMatrix(nn_train_pred,xTrain$Survived);
# 0.9068 accuracy on the training set

# try on test data
nn_test_pred_class = predict( nn, xTest, type="class" );  # yields "0", "1"
nn_test_pred = as.numeric( nn_test_pred_class );   # transform to 0, 1
# write to  file
nnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=nn_train_pred);
nnModel = data.frame(PassengerId=newTest$PassengerId, Survived=nn_test_pred);
write.csv( nnModelTrain, "NeuralNetworkSigModelTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModel, "NeuralNetworkSigModelProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModelTrain, "NeuralNetworkSigModelTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModel, "NeuralNetworkSigModel.csv", row.names=FALSE, quote=FALSE );
# best "over"trained model scores 0.72727 (304/418)

# Now, we wish to select parameters for neural network by using 10-fold cross-validation with caret
tuningGrid = expand.grid(size=1:length(significant_features),decay=c(0,0.0001,0.05,0.1));
set.seed( 1 );
trControl = trainControl( method="repeatedcv", number=10, repeats=10 );
nn_cv = train( SurvivedFactor ~ ., data=xTrain, method="nnet", trControl = trControl, tuneGrid=tuningGrid, verbose=FALSE, trace=FALSE);

best_size = nn_cv$bestTune[1,"size"];
best_decay = nn_cv$bestTune[1,"decay"];
best_nn = nnet( SurvivedFactor ~ ., data = xTrain, size=best_size, decay=best_decay, maxit=500, trace=FALSE);
best_nn_train_pred_class = predict( best_nn, newdata=xTrain, type="class" );  # yields "0", "1"
best_nn_train_pred = as.numeric( best_nn_train_pred_class );   # transform to 0, 1
confusionMatrix( best_nn_train_pred, xTrain$Survived);
# now do on test data
best_nn_test_pred_class = predict( best_nn, newdata=xTest, type="class" );  # yields "0", "1"
best_nn_test_pred = as.numeric( best_nn_test_pred_class );   # transform to 0, 1
# write to  file
nnModelTrain = data.frame(PassengerId=newTrain$PassengerId, Survived=best_nn_train_pred);
nnModel = data.frame(PassengerId=newTest$PassengerId, Survived=best_nn_test_pred);
write.csv( nnModelTrain, "NeuralNetworkSigModelCVTrainProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModel, "NeuralNetworkSigModelCVProbs.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModelTrain, "NeuralNetworkSigModelCVTrain.csv", row.names=FALSE, quote=FALSE );
write.csv( nnModel, "NeuralNetworkSigModelCV.csv", row.names=FALSE, quote=FALSE );
# Neural network model scores 0.7847 (334/418) - same as full featured model
```

The cross validated Neural Network model with only significant features scored 0.853 against the training set and 0.7751 against the test set.

### Ensemble Model

Ensemble models use multiple models to make predictions.  Ensemble models usually fall into one of three different types: bagging, boosting, and stacking.  In a bagging model paradigm, a single model (usually) is run multiple times over different subsamples of the data.  These models are usually not complex to avoid overfitting and hopefully learn one part of the data and so are uncorrelated with one another.  The output of the models are then combined to make a prediction.  The Gradient Boosting model that we used with classification trees is an example of an ensemble model where successive models are employed to reduce the residuals of the previous model.  Finally, a stacking ensemble model is much like a traditional machine learning model only the features or inputs to the learning model are themselves outputs of different types of machine learning models.  The caveat, however, is that in order for ensembling to work, the models must be uncorrelated.

The idea in this section is to gather the results from all the trained models.  Where possible, I used the probabilties of individual predictions (i.e., 0.983 chance of survival) over class calls (binary 0=dead, 1=survived) as that data is a little more fine-tuned.  For this dataset, many of the models that I have trained are drastically overfit.  The gradient boosted tree models, with training accuracies of 98% are extremely overfit.  Other models are just not well-suited for this type of problem (KNN, LDA, QDA).  Once I pared the list of models to just include the models that I thought were well-suited for this problem, I computed all of the cross-correlations between models and then enumerated all of the sets of models that were below a specified cross-correlation (I picked 0.75 to signify a high correlation).  Once I did this, I found that *all* of the remaining models were highly correlated to one another, so I coudn't make an ensemble!  But since I'm determined to make an ensemble, I pressed on.

I added back the LDA, QDA, and neural network models and reran the correlation analysis.  I purposely excluded the KNN and Bagging models since I believe KNN to not be well-suited for this problem and the bagging models are massively overfit.  The correlation analysis showed which models could be combined into a single stacked model (see below).  I manually selected the set of 3 models: RandomForest, Gender model, and QDA, since they represent three models of different complexity (simple: gender model, balanced: Random Forest, complex: QDA), hoping the strengths of each would complement each other.

In addition, I decided to use all the models in a different ensemble model, just taking row means.  This set included all of the models, including the overfit bagging models, KNN, boosting, etc.  Many of these models are highly correlated so the ensemble method is just a simple mean across all of the models in each row.

```{r ensemble, echo=TRUE, eval=TRUE, warning=FALSE}
library(matlab,quietly=TRUE);
library(ROCR,quietly=TRUE);
library(tidyr,quietly=TRUE);
library(dplyr,quietly=TRUE);
library(tibble,quietly=TRUE);

all_train_models = c( "BagModelTrainProbs.csv", "BagSigModelTrainProbs.csv", "BoostedTreeModelTrainProbs.csv", "BoostedTreeSigModelTrainProbs.csv", "GenderModelTrainProbs.csv", "KNNBest6ModelTrainProbs.csv", "KNNBest7ModelTrainProbs.csv", "KNNBest8ModelTrainProbs.csv", "KNNBest9ModelTrainProbs.csv", "KNNFullModelTrainProbs.csv", "LassoRegressionFullModelTrainProbs.csv", "LassoRegressionSigModelTrainProbs.csv", "LDAModelTrainProbs.csv", "LDAReducedModelTrainProbs.csv", "LogisticRegressionFullModelTrainProbs.csv", "LogisticRegressionSigModelTrainProbs.csv", "NeuralNetworkSigModelCVTrainProbs.csv", "NeuralNetworkSigModelTrainProbs.csv", "NullModelTrainProbs.csv", "QDAModelTrainProbs.csv", "QDASigModelTrainProbs.csv", "RandomForestModelTrainProbs.csv", "RandomForestSigModelTrainProbs.csv", "RandomForestTrimmedModelTrainProbs.csv", "RidgeRegressionFullModelTrainProbs.csv", "RidgeRegressionSigModelTrainProbs.csv", "SVMModelTrainProbs.csv", "TreeModelTrainProbs.csv", "TreeSigModelTrainProbs.csv", "WCModelTrainProbs.csv" );

train_models = c( "BoostedTreeModelTrainProbs.csv", "BoostedTreeSigModelTrainProbs.csv", "GenderModelTrainProbs.csv", "LassoRegressionFullModelTrainProbs.csv", "LassoRegressionSigModelTrainProbs.csv", "LDAModelTrainProbs.csv", "LDAReducedModelTrainProbs.csv", "LogisticRegressionFullModelTrainProbs.csv", "LogisticRegressionSigModelTrainProbs.csv", "NeuralNetworkSigModelCVTrainProbs.csv", "NeuralNetworkSigModelTrainProbs.csv", "QDAModelTrainProbs.csv", "QDASigModelTrainProbs.csv", "RandomForestModelTrainProbs.csv", "RandomForestSigModelTrainProbs.csv", "RandomForestTrimmedModelTrainProbs.csv", "RidgeRegressionFullModelTrainProbs.csv", "RidgeRegressionSigModelTrainProbs.csv", "SVMModelTrainProbs.csv", "TreeModelTrainProbs.csv", "TreeSigModelTrainProbs.csv", "WCModelTrainProbs.csv" );

all_test_models = c( "BagModelProbs.csv", "BagSigModelProbs.csv", "BoostedTreeModelProbs.csv", "BoostedTreeSigModelProbs.csv", "GenderModelProbs.csv", "KNNBest6ModelProbs.csv", "KNNBest7ModelProbs.csv", "KNNBest8ModelProbs.csv", "KNNBest9ModelProbs.csv", "KNNFullModelProbs.csv", "LassoRegressionFullModelProbs.csv", "LassoRegressionSigModelProbs.csv", "LDAModelProbs.csv", "LDAReducedModelProbs.csv", "LogisticRegressionFullModelProbs.csv", "LogisticRegressionSigModelProbs.csv", "NeuralNetworkSigModelCVProbs.csv", "NeuralNetworkSigModelProbs.csv", "NullModelProbs.csv", "QDAModelProbs.csv", "QDASigModelProbs.csv", "RandomForestModelProbs.csv", "RandomForestSigModelProbs.csv", "RandomForestTrimmedModelProbs.csv", "RidgeRegressionFullModelProbs.csv", "RidgeRegressionSigModelProbs.csv", "SVMModelProbs.csv", "TreeModelProbs.csv", "TreeSigModelProbs.csv", "WCModelProbs.csv" );

test_models = c( "BoostedTreeModelProbs.csv", "BoostedTreeSigModelProbs.csv", "GenderModelProbs.csv", "LassoRegressionFullModelProbs.csv", "LassoRegressionSigModelProbs.csv", "LDAModelProbs.csv", "LDAReducedModelProbs.csv", "LogisticRegressionFullModelProbs.csv", "LogisticRegressionSigModelProbs.csv", "NeuralNetworkSigModelCVProbs.csv", "NeuralNetworkSigModelProbs.csv", "QDAModelProbs.csv", "QDASigModelProbs.csv", "RandomForestModelProbs.csv", "RandomForestSigModelProbs.csv", "RandomForestTrimmedModelProbs.csv", "RidgeRegressionFullModelProbs.csv", "RidgeRegressionSigModelProbs.csv", "SVMModelProbs.csv", "TreeModelProbs.csv", "TreeSigModelProbs.csv", "WCModelProbs.csv" );

trainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );
testModels = data.frame(PassengerId=1:nrow(newTest));

# read all the train model predictions and store in dataframe
numModels = length( train_models );
for ( i in 1:numModels ){
  trainFileName = train_models[i];
  fp = fileparts( trainFileName );
  modelName = gsub( "TrainProbs", "", fp$name );
  trainModelFrame = read.csv( trainFileName );
  trainModels[ modelName ] = trainModelFrame$Survived;
  # now for test
  testFileName = test_models[i];
  fp = fileparts( testFileName );
  modelName = gsub( "Probs", "", fp$name );
  testModelFrame = read.csv( testFileName );
  testModels[ modelName ] = testModelFrame$Survived;
}

allTrainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );
allTestModels = data.frame(PassengerId=1:nrow(newTest));

numAllModels = length( all_train_models );
for ( i in 1:numAllModels ){
  trainFileName = all_train_models[i];
  fp = fileparts( trainFileName );
  modelName = gsub( "TrainProbs", "", fp$name );
  trainModelFrame = read.csv( trainFileName );
  allTrainModels[ modelName ] = trainModelFrame$Survived;
  # now for test
  testFileName = all_test_models[i];
  fp = fileparts( testFileName );
  modelName = gsub( "Probs", "", fp$name );
  testModelFrame = read.csv( testFileName );
  allTestModels[ modelName ] = testModelFrame$Survived;
}

all_train_model_names = setdiff( names(allTrainModels), c("PassengerId", "Survived") );

# this calculates all correlations between models.  Some corr calcs break because
# they're all one value so standard deviation calc blows up.
modelCors <- trainModels[,!names(trainModels) %in% c("PassengerId","Survived")] %>% 
  as.matrix %>%
  cor %>%
  as.data.frame %>%
  rownames_to_column(var = 'Model_A') %>%
  gather(Model_B, correlation, -Model_A)

corrThresh = 0.75;
modelNames = unique(modelCors$Model_A);

# this recursive function is intended to output groups of models that 
# have inter correlations below threshold
addToModelSet = function( modelSet, corrs, model, corrThresh ){
    #modelSet[ length(modelSet)+1 ] = model;
    if ( ! model %in% modelSet ){
        modelSet = union( modelSet, model );
    }
    idxUsed = c();
    idxUnused = 1:nrow(corrs);
   
    toUse = c();
    for ( idx in idxUnused ){
        thisModel = corrs$Model_A[idx];
        if ( thisModel %in% modelSet ){
            toUse[ length(toUse) + 1] = idx;
        }
    }
   
    for ( idx in toUse ){
        addModel = TRUE;
        modelA = corrs$Model_A[idx];
        modelB = corrs$Model_B[idx];
        correlation = corrs$correlation[idx];
        if ( correlation < corrThresh ){
            # now, check corr with every other member of modelSet
            for ( model in modelSet ){
                # want to check modelB corr with all models in modelSet
              idxModelA = which( corrs$Model_A == model );
              idxModelB = which( corrs$Model_B == modelB );
                if ( model == modelB ){
                    idxUsed[ length(idxUsed) + 1 ] = idx;
                    # correlations of the same model
                    next;
                }
              rightRowIdx = intersect( idxModelA, idxModelB );
              if( length( rightRowIdx ) == 1 && rightRowIdx < nrow(corrs) ){
                rightRow = corrs[ rightRowIdx, ];
              }
              else{
                next;
              }
                rightCorr = rightRow$correlation;
                if ( rightCorr > corrThresh ){
                    idxUsed[ length(idxUsed) + 1 ] = idx;
                    addModel = FALSE;
                    break;
                }
            }
            idxUsed[ length(idxUsed) + 1 ] = idx;
        }
        else{
          addModel = FALSE;
        }
        if ( addModel ){
          idxToUse = setdiff( idxUnused, idxUsed );
          newCorrs = corrs[idxToUse,]
          #newCorrs = corrs;
          modelSet = addToModelSet( modelSet, newCorrs, modelB, corrThresh);
        }
    }
    
    return( modelSet );
}

modelSets = list();
print( "Model sets that have inter-correlations less than threshold:")
for ( model in modelNames ){
  modelSet = addToModelSet( c(), modelCors, model, corrThresh );
  cat( paste(paste(modelSet, collapse=", "),"\n"));
  modelSets[ length(modelSets) + 1 ] = list( modelSet );
}

# now go through each model and find the other models which are correlated below
# threshold (0.75)
# manually choose the set #14
uncorrelatedModelSet = modelSets[[14]];

# now for all models that have passed criteria, create a model
selectedTrainModels = trainModels[,c("Survived", uncorrelatedModelSet)];
ensemble_lr = glm( Survived~., data=selectedTrainModels, family=binomial);
summary( ensemble_lr );
trainSetProbs = predict( ensemble_lr, trainModels, type="response");  # now a vector of probabilities
# probabilities >= 0.5 mean survived, < 0.5 mean perished
threshold = 0.5;
selectedTrainPredictions = as.numeric( trainSetProbs >= threshold );
confusionMatrix( selectedTrainPredictions, trainModels$Survived );

# check to see if threshold 0.5 is appropriate
rocr.pred = prediction( trainSetProbs, trainModels$Survived);
acc.perf = performance( rocr.pred, measure="acc");
plot( acc.perf, main="Manual Ensemble Logistic Regression Model Training Set Accuracy by Threshold" );
# threshold 0.5 is appropriate

# now, how'd we do against the test set?
testSetProbs = predict( ensemble_lr, testModels, type="response");
testPredictions = as.numeric( testSetProbs >= threshold );
ensembleModelManual = data.frame(PassengerId=newTest$PassengerId, Survived=testPredictions);
write.csv( ensembleModelManual, "EnsembleModelManual.csv", row.names=FALSE, quote=FALSE );
ensembleModelTrainManual = data.frame(PassengerId=newTrain$PassengerId, Survived=selectedTrainPredictions);
write.csv( ensembleModelTrainManual, "EnsembleModelManualTrain.csv", row.names=FALSE, quote=FALSE );

# now try just an ensemble model, taking means
ensembleTrainModels = allTrainModels[,all_train_model_names];
# simply take the means across the rows to get "probabilities" of survival for each passenger
ensembleTrainPredictions = rowMeans( ensembleTrainModels );
ensembleTrainCalls = round( ensembleTrainPredictions );
confusionMatrix( ensembleTrainCalls, allTrainModels$Survived );

# check to see if threshold 0.5 is appropriate
rocr.pred = prediction( ensembleTrainPredictions, allTrainModels$Survived);
acc.perf = performance( rocr.pred, measure="acc");
plot( acc.perf, main="All Means Ensemble Logistic Regression Model Training Set Accuracy by Threshold" );
# threshold 0.5 is appropriate

# How'd the ensemble model do against the test set?
ensembleTestModels = testModels[,modelNames];
# simply take the means across the rows to get "probabilities" of survival for each passenger
ensembleTestPredictions = rowMeans( ensembleTestModels );
ensembleTestCalls = round( ensembleTestPredictions );

#fullLogisticRegressionModelTrain = data.frame(PassengerId=train$PassengerId, Survived=predictions);
#fullLogisticRegressionModel = data.frame(PassengerId=test$PassengerId, Survived=fullLogisticPredictions);
# now, create a new test and train data frame with these models as predictors

# write to  file
ensembleModelMeans = data.frame(PassengerId=newTest$PassengerId, Survived=ensembleTestCalls);
write.csv( ensembleModelMeans, "EnsembleModelMeans.csv", row.names=FALSE, quote=FALSE );
ensembleModelTrainMeans = data.frame(PassengerId=newTrain$PassengerId, Survived=ensembleTrainCalls);
write.csv( ensembleModelTrainMeans, "EnsembleModelMeansTrain.csv", row.names=FALSE, quote=FALSE );
```

The manually selected linear regression ensemble model was constructed using three predictors: Random Forest, Gender model, and QDA.  The training set score was 0.8575 and test set score was 0.7847.  The simple row means model scored a 0.8664 against the training set and a 0.7703 against the test set.  As would be expected, the row means model, which used some very overfit models, was itself overfit - scoring high against the training set and low against the test set - lower than the manually selected ensemble model.

### Cheat Models

Cheaters never prosper.  Nevertheless.

#### Google Data-Mining

All of the Titanic data is available online.  It is possible to find resource pages listing who died and who survived.  Of course, much of it is not in a standardized format and has to be screenscraped.  In addition, there are errors and discrepancies both in the Kaggle data set and in the data online.  Word for word searches fail in many cases.  

```{r cheatModel1, echo=TRUE, eval=FALSE, warning=FALSE}
library(RCurl);
library(XML);
library(readr);
#testIDs = 892:nrow(data_combined);
#userAgents = c( "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0", "Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0", "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36", "Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_1 like Mac OS X) AppleWebKit/603.1.30 (KHTML, like Gecko) Version/10.0 Mobile/14E304 Safari/602.1", "Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0)");
userAgents = c( "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36 Edge/15.15063" );
numUserAgents = length( userAgents );
trainIDs = 1:891;
testIDs = 892:nrow(data_combined);
setIDs = 1:nrow(data_combined);
trainURLS = c();
setHits = data_combined[,c("PassengerId","Survived")];
setHits$SurvivorHits = 0;
setHits$VictimHits = 0;
setHits$SurvivorFirst = 0;
setHits$SurvivedPrediction = 0;
for ( pass_idx in 1:length(setIDs) ){
  i = setIDs[pass_idx];
  name = gsub( " ", "+", as.character( data_combined[i,"NameString"] ) ); # remove whitespace
  sex = as.character( data_combined[i,"Sex"] );
  searchURL = paste0( "https://www.google.com/search?q=", name, "+", sex, "+titanic" );
  trainURLS[i] = searchURL;
  randomUserAgent = userAgents[ round( runif(1, 1, numUserAgents) ) ];
  wd = getwd();
  localFile = paste0( wd, "/html/", i, ".html" );
  if ( file.exists( localFile )){
    resultsHTML = read_file( localFile );
  }else{
    resultsHTML = getURL( searchURL, httpheader=c('User-Agent'=randomUserAgent) );
    write( resultsHTML, localFile );
    Sys.sleep( runif( 1, 27, 67)); # randomly sleep between 27 and 67 seconds
    # this is necessary because if Google thinks you're mining their data with a bot
    # you will be grounded and sent to your room for a timeout.
  }
  # now search for "victim" and "survivor"
  victimHits = gregexpr("victim", resultsHTML, ignore.case=TRUE);
  victimHitIndices = which( victimHits[[1]] > -1 );
  numVictimHits = length( victimHitIndices );
  setHits[pass_idx,"VictimHits"] = numVictimHits;
  firstVictimHit = victimHits[[1]][1];
  survivorHits = gregexpr("survivor", resultsHTML, ignore.case=TRUE);
  survivorHitIndices = which( survivorHits[[1]] > -1 );
  numSurvivorHits = length( survivorHitIndices );
  setHits[pass_idx,"SurvivorHits"] = numSurvivorHits;
  firstSurvivorHit = survivorHits[[1]][1];
  #cat( "row ", i, "\n")
  #assert_that( ( numSurvivorHits > 0 ) || (numVictimHits > 0) );
  if (!( ( numSurvivorHits > 0 ) || (numVictimHits > 0) ) ){
    cat( "Could not find 'victim' or 'survivor' keyword for PassengerID ", i, ".  Manually investigate and update .html file with 'survived' or 'victim'.\n")
  }
  survived = FALSE;
  if ( numSurvivorHits > numVictimHits ){
      survived = TRUE;
  }
  else if ( ( numSurvivorHits > 0 ) && ( numVictimHits > 0 ) ){
    # both are positive, take the min
    if ( firstSurvivorHit < firstVictimHit ){
      survived = TRUE;
    }
    else{
      survived = FALSE;
    }
    setHits[pass_idx,"SurvivorFirst"] = as.numeric( survived );
  }
  survived_flag = 0;
  if( survived ){
    survived_flag = 1;
  }
  setHits[pass_idx,"SurvivedPrediction"] = survived_flag;
  if ( !is.na( data_combined[i,"Survived"] ) && survived_flag != data_combined[i,"Survived"] ){
    #cat( "Prediction does not match truth at ", i, as.character( data_combined[i,"Name"] ), "\n");
  }
}
# evaluate training set performance 
trainingPredictions = setHits$SurvivedPrediction[1:891];
confusionMatrix( trainingPredictions, data_combined$Survived[1:891])

cheatTestPredictions = setHits$SurvivedPrediction[892:nrow(data_combined)];

write.csv( setHits, file="CheatModel1.csv", quote=FALSE);
```

#### Screen Scraping Titanic Resource Webpage

A webpage exists that lists all the survivors in bold and all the passengers who died in normal font.  It is possible to build a scraper to parse the passengers into two lists and then try to match the names in the Kaggle test set with the names in the Survived and Died sets derived from the webpage.  The idea is to use both a bag-of-words model and a distance model to capture both number of token hits and handle any differences in spelling.

```{r cheatModel2, echo=TRUE, eval=FALSE, warning=FALSE}
# read in CheatModel1
cheat <- read.csv("CheatModel1.csv", header = TRUE);
cheat$SurvivedFactor = as.factor( cheat$Survived );
cheat$SurvivedTokenMatches = 0;
cheat$DiedTokenMatches = 0;
cheat$SurvivedTokenDistance = 0;
cheat$DiedTokenDistance = 0;
resourceURL = "http://www.titanic-whitestarships.com/1st_Class_Pass.htm";
# Example HTML:
#<strong>Gordon, Sir Cosmo Duff<br>
#Gordon, Lady Lucile Duff<br>
#and Maid (Miss Laura Mabel Francatelli)<br>
#Gracie, Colonel Archibald IV</strong><br>
#Graham, Mr. George Edward<br>
#Graham1, Mr. George Edward<br>
#Graham2, Mr. George Edward<br>
#Graham3, Mr. George Edward<br>
#<strong>Graham, Mrs. William Thompson<br>
#(nee Edith Junkins)<br>
#Graham, Miss Margaret<br>
#Greenfield, Mrs. Leo David<br>
#(nee Blanche Strouse)<br>
#Greenfield, Mr. William Bertram</strong><br>
#Guggenheim, Mr. Benjamin<br>
#and Manservant (Victor Giglio)</font></p>
cheatSurvived = numeric(nrow(newTest));
wd = getwd();
localFile = file.path( wd, "TitanicResource.htm" );  
# know the file is approx 2100 lines.  Initialize each died/survived list to 3000
died = character(3000);
survived = character(3000);
if ( !file.exists( localFile )){  # Do this so we can load the URL once and not spam it everytime we run this script
  resultsHTML = getURL( resourceURL );
  write( resultsHTML, localFile );
}
# now, parse the file, line-by-line.  Survivors are in bold, denoted by the <strong> tag:
fid = file( localFile, "r");
isStrong = FALSE;
diedCtr = 1;
survivedCtr = 1;
while( TRUE ){
  line = readLines(fid,n=1);
  if( length(line) == 0 ){
    break;
  }
  # look for </strong>
  thisLineStartStrong = grepl( "^<strong>", line );
  thisLineEndStrong = grepl( "</strong>", line );
  # if line contains <strong
  # strip html tags
  rawText = gsub( "<.*?>", "", line );
  # skip if no comma
  thisLineHasComma = grepl( ",", rawText );
  if ( !thisLineHasComma ){
    next;
  }
  # remove punctuation
  rawText = gsub( "[[:punct:]]", "", rawText );
  # skip if line (name) is too long
  if ( nchar( rawText) > 100 ){
    next;
  }
  if( isStrong || thisLineStartStrong ){
    survived[survivedCtr] = rawText;
    survivedCtr = survivedCtr + 1;
  }
  else{
    died[diedCtr] = rawText;
    diedCtr = diedCtr + 1;
  }
  isStrong = thisLineStartStrong && !thisLineEndStrong;
}
close(fid);
# truncate the died/survived lists
survived = sort( survived[1:survivedCtr-1] );
died = sort( died[1:diedCtr-1] );  # yes, this includes crap lines at the top of list
#

# now have two lists and need to do bag of words searching vs. each name
# for each name, tokenize, then query the died/survived lists
for( i in 1:nrow( newTest ) ){
  thisName = newTest[i,"Name"];
  # remove non-word characters 
  thisName = gsub( "[[:punct:]]", "", thisName );
  # now tokenize each name and search each died/survived for distance from each token
  nameTokens = strsplit( thisName, "\\s+")[[1]];
  numTokens = length( nameTokens );
  # search through survived
  survivedTokenMatches = numeric(length(survived));
  survivedTokenDistance = numeric(length(survived));
  for ( j in 1:length( survived ) ){
    thisScore = 0;
    thisSurvived = survived[j];
    survivedTokens = strsplit( thisSurvived, "\\s+")[[1]];
    numSurvivedTokens = length( survivedTokens );
    # now, take each name token and search for match in survived tokens
    for ( k in 1:numTokens){
      thisNameToken = nameTokens[k];
      if ( thisNameToken %in% survivedTokens ){
        thisScore = thisScore + 1;
      }
    }
    survivedTokenMatches[j] = thisScore;
    # go through each name token and find the min distance to survived tokens
    totalDist = 0;
    for ( k in 1:numTokens){
      thisNameToken = nameTokens[k];
      wordDist = 10;
      distances = numeric(numSurvivedTokens);
      for ( m in 1:numSurvivedTokens){
        thisSurvivedToken = survivedTokens[m];
        distances[m] = adist( thisNameToken, thisSurvivedToken );
      }
      totalDist = totalDist + min( distances );
    }
    survivedTokenDistance[j] = totalDist;
  }
  bestSurvivedScore = max( survivedTokenMatches );
  bestSurvivedDistance = min( survivedTokenDistance );
  cheat[i,"SurvivedTokenHits"] = bestSurvivedScore;
  cheat[i,"SurvivedTokenDistance"] = bestSurvivedDistance;
  
  # search through died
  diedTokenMatches = numeric(length(died));
  diedTokenDistance = numeric(length(died));
  for ( j in 1:length( died ) ){
    thisScore = 0;
    thisDied = died[j];
    diedTokens = strsplit( thisDied, "\\s+")[[1]];
    numDiedTokens = length( diedTokens );
    # now, take each name token and search for match in survived tokens
    for ( k in 1:numTokens){
      thisNameToken = nameTokens[k];
      if ( thisNameToken %in% diedTokens ){
        thisScore = thisScore + 1;
      }
    }
    diedTokenMatches[j] = thisScore;
    # go through each name token and find the min distance to survived tokens
    totalDist = 0;
    for ( k in 1:numTokens){
      thisNameToken = nameTokens[k];
      wordDist = 10;
      distances = numeric(numDiedTokens);
      for ( m in 1:numDiedTokens){
        thisDiedToken = diedTokens[m];
        distances[m] = adist( thisNameToken, thisDiedToken );
      }
      totalDist = totalDist + min( distances );
    }
    diedTokenDistance[j] = totalDist;
  }
  
  bestDiedScore = max( diedTokenMatches );
  bestDiedDistance = min( diedTokenDistance );
  cheat[i,"DiedTokenHits"] = bestSurvivedScore;
  cheat[i,"DiedTokenDistance"] = bestSurvivedDistance;
  #cat( thisName, " bestScore = ", bestScore, ", bestMatch = ", survived[bestIdx], "\n");
}

# now that we have some cheating scores, build a simple tree model
library(tree);
cheat_train = cheat[1:891,];  # use the training data
cheat_test= cheat[892:nrow(cheat),];  # use the test data
tree_cheat_model = tree( SurvivedFactor~SurvivorHits+VictimHits+SurvivorFirst+SurvivedTokenHits+SurvivedTokenDistance+DiedTokenHits+DiedTokenDistance,data=cheat_train );
summary( tree_cheat_model )
plot( tree_cheat_model )
text( tree_cheat_model, pretty=0);
cheat_train_pred = predict( tree_cheat_model, cheat_train, type="class");
cheat_test_pred = predict( tree_cheat_model, cheat_test, type="class");
# performance
confusionMatrix(cheat_train_pred, cheat_train$Survived);

cheatModelTest = data.frame(PassengerId=newTest$PassengerId, Survived=as.numeric(cheat_test_pred)-1);
write.csv( cheatModelTest, "CheatModelFull.csv", row.names=FALSE, quote=FALSE );

```

### Summary

Here is the summary of training and test results for the different models:
```{r summary, eval=FALSE, echo=FALSE}
library(matlab,quietly=TRUE);
library(ROCR,quietly=TRUE);

# these csv models represent the binary 0, 1 predictions of Survival for the different models
models = c( "BagModel.csv", "BagSigModel.csv", "BoostedTreeModel.csv", "BoostedTreeSigModel.csv", "gender_submission.csv", "KNNBest6Model.csv", "KNNBest7Model.csv", "KNNBest8Model.csv", "KNNBest9Model.csv", "KNNFullModel.csv", "LassoRegressionFullModel.csv", "LassoRegressionSigModel.csv", "LDAModel.csv", "LDAReducedModel.csv", "LogisticRegressionFullModel.csv", "LogisticRegressionSigModel.csv", "NeuralNetworkFullModel.csv", "NeuralNetworkFullModelCV.csv", "NeuralNetworkSigModel.csv", "NeuralNetworkSigModelCV.csv", "QDAModel.csv", "QDASigModel.csv", "RandomForestModel.csv", "RandomForestSigModel.csv", "RandomForestTrimmedModel.csv", "RidgeRegressionFullModel.csv", "RidgeRegressionSigModel.csv", "SVMModel.csv", "NullModel.csv", "TreeModel.csv", "TreeSigModel.csv", "WomenAndChildrenFirstModel.csv", "EnsembleModelManual.csv", "EnsembleModelMeans.csv" );

trainModels = data.frame(PassengerId=1:nrow(newTrain), Survived=newTrain$Survived );
testModels = data.frame(PassengerId=1:nrow(newTest));
modelNames = c();

# testAnswers is a .csv file representing all the correct predictions for the test set.
# I created this through trial and error and some cheat models and use it here to compare
# predictions with the answer key to show in a table (easier than hitting Kaggle every
# time for a score calculation).  If you don't have this file, just make a default of
# everyone dying
testAnswers = data.frame(PassengerId=1:nrow(newTest), Survived=0);
if( file.exists("TestAnswers.csv")){
    testAnswers = read.csv( "TestAnswers.csv", header=TRUE );
}

trainScores = c();
testScores = c();

numModels = length( models );
for ( i in 1:numModels ){
  testFileName = models[i];
  fp = fileparts( testFileName );
  testName = fp$name;
  modelNames[i] = testName;
  trainName = paste0( testName, "Train" );
  trainFileName = paste0( trainName, fp$ext );
  trainModelFrame = read.csv( trainFileName );
  testModelFrame = read.csv( testFileName );
  trainScores[i] = mean( trainModelFrame$Survived == trainModels$Survived );
  testScores[i] = mean( testModelFrame$Survived == testAnswers$Survived );
  trainModelFrame$Score = trainScores[i];
  testModelFrame$Score = testScores[i];
}
types = c( rep("Training Score", length(modelNames)), rep("Test Score", length(modelNames)));
trainOrder = sort( trainScores, decreasing = TRUE, index.return = TRUE );
# now reorder according to training score
modelNames = modelNames[trainOrder$ix];
trainScores = trainScores[trainOrder$ix];
testScores = testScores[trainOrder$ix];
scores = data.frame( Model = c(modelNames,modelNames), Accuracy = c(trainScores,testScores), Type = types );
scores$Model = factor(scores$Model, levels=modelNames);
scores$Type = factor(scores$Type, levels=c("Training Score", "Test Score"));
scores$CVMethod = "-"
if( !file.exists("ModelResults.csv")){
    write.csv( scores, "ModelResults.csv", row.names=FALSE, quote=FALSE );
} else{
  # why read this in?  Because it might be annotated.  And if you're missing the TestAnswers.csv
  # file, all the test scores will be wrong
  scores = read.csv("ModelResults.csv", header=TRUE);
}

ggplot( data=scores, aes(x=Model, y=Accuracy, group=Type, colour=Type)) + 
  geom_line() + 
  geom_point() +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

model_results = scores;
models = as.character( model_results$Model );
uniqueModels= unique( as.character( model_results$Model) );
trainingScores = sort(model_results[model_results$Type=="Training Score","Accuracy"], decreasing=TRUE, index.return=TRUE);
trainingModels = models[trainingScores$ix];
cvMethods = model_results[model_results$Type=="Training Score","CVMethod"];
testScores = c();
for ( z in 1:length(trainingModels)){
  thisModel = trainingModels[z];
  idxTestScore = which(( model_results$Model == thisModel ) & (model_results$Type == "Test Score") );
  testScores[z] = model_results$Accuracy[idxTestScore];
}
annotatedCVMethods = cvMethods[trainingScores$ix];
if( ! file.exists("TestAnswers.csv")){
  testScores = c();
  annotatedCVMethods = c();
}
summaryTable = data.frame( Model = trainingModels, CVMethod=annotatedCVMethods, TrainingScore = trainingScores$x, TestScore = testScores);
knitr::kable( summaryTable, format="markdown", longtable=TRUE)
```

![Model Summary](TitanicModelSummary.png)

Model|CVMethod|Training Score|Test Score
-------------------------------- | ---------- | ---------- | -----------
Bag Model|-|0.9876543|0.7320574
Bag Sig Model|-|0.9854097|0.7272727
Neural Network Full Model|-|0.9203143|0.6746411
Neural Network Sig Model|-|0.9124579|0.7105263
Boosted Tree Model|8-fold CV|0.8933782|0.7559809
Boosted Tree Sig Model|8-fold CV|0.8922559|0.7368421
KNN Best 8 Model|LOOCV|0.8787879|0.7559809
KNN Best 6 Model|LOOCV|0.8754209|0.7200957
KNN Best 7 Model|LOOCV|0.8754209|0.7344498
KNN Best 9 Model|LOOCV|0.8742985|0.7631579
Ensemble Means Model|-|0.8664000|0.7703000
RandomForest Sig Model|-|0.8608305|0.7822967
Ensemble Manual Model|-|0.8575000|0.7847000
RandomForest Model|-|0.8574635|0.7846890
KNN Full Model|LOOCV|0.8563412|0.7607656
Tree Sig Model|Pruning|0.8540965|0.7607656
Neural Network Sig Model|10-fold CV|0.8529742|0.7751196
Tree Model|Pruning|0.8507295|0.7727273
RandomForest Trimmed Model|-|0.8484848|0.7775120
Neural Network Full Model|10-fold CV|0.8417508|0.7846890
Logistic Regression Sig Model|-|0.8395062|0.7607656
SVM Model|10-fold CV|0.8395062|0.7679426
Lasso Regression Sig Model|10-fold CV|0.8372615|0.7775120
LDA Model|-|0.8372615|0.7727273
Lasso Regression Full Model|10-fold CV|0.8361392|0.7703349
LDA Reduced Model|-|0.8338945|0.7727273
Logistic Regression Full Model|-|0.8338945|0.7607656
Ridge Regression Sig Model|10-fold CV|0.8249158|0.7870813
Ridge Regression Full Model|10-fold CV|0.8226712|0.7846890
QDAModel|-|0.8159371|0.7296651
WomenAndChildrenFirstModel|-|0.7901235|0.7511962
gender_submission|-|0.7867565|0.7631579
QDASigModel|-|0.7845118|0.7272727
NullModel|-|0.6161616|0.6244019

Now then, how does one go about picking a model out of all of these?  Clearly, the training scores of some of the more complex models cannot be trusted as they're obviously overfit.  The bagged tree model has training accuracies of about 0.98!  But how would we know that they're overfit without peeking at the test scores?  The boosted tree cross-validation accuracies were about 0.89 - but these also proved to be vastly overfit.

But suppose all of these training results were more or less equal - some at 0.84 cross-validated training set accuracy, some at 0.83 cross-validated training set accuracy, and one at 0.85 cross-validated training set accuracy.  What then would be the criterion for choosing a model?  At first blush, it might be obvious to choose the one that that has the highest cross-validated training set accuracy of 0.85.  But what if that model is very complex with a lot of parameterization and there is a much simpler model that scored at 0.84.  Then would it be better to choose the simpler, but lower-scoring model?  Is the difference between 0.84 and 0.85 statistically significant or is the difference just due to random chance?  Is there a way to quantify the statistical significance?  I've seen McNemar's test but it looks like it operates on the correct counts between only two different models, not when num_models > 2.  And if we run McNemar's test over all pairs of models, then we're likely to see something "significant" just through chance and need to somehow correct for it.

If I had to pick the "best" model out of these, it would probably be the ridge regression model.  The squared regularization penalty helps to drive down the coefficients of some of the predictors better than the similar lasso regression model.  

## Lessons Learned

This report is a little stream-of-consciousness as it is/was my first Kaggle competition and kernel.  But what a great introduction to overfitting with R.  The EDA portion was really enjoyable as was the feature engineering section.  I thought that time put in to the early part would pay off later on but in retrospect, I probably should have tried to regularize the data a little more with outlier detection and deletion.  But I was reluctant to delete any data that might help me eek out a few more percentage points!

Also, in retrospect, it is *much* easier to run all of the models through the caret framework.  In this model, a single feature set can be run through different models fairly seemlessly - and then even run as ensembles at a later step.

Thanks for reading along!