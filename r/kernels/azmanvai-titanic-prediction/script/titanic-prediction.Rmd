---
title: "Titanic Prediction"
author: "Nor Azman Zakaria"
date: "March 3, 2018"
output: 
  html_document: 
    number_sections: yes
    toc: yes
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(caret)
library(ggplot2)
library(lubridate)
```

# Introduction
This is my first data science competition after finishing the data science specialization. The problem is deifnitely 

# Data Load
The train.csv is saved into dataframe 'raw' and test.csv into 'submit'. I use the name 'raw' knowing it will be cleaned and separate into train and test datasets and I didn't want to confuse with the test.csv dataset. 
```{r, echo = TRUE, warning=FALSE}
raw <- read.csv("train.csv", sep = "," , strip.white = TRUE, stringsAsFactors = FALSE, header = TRUE, na.strings = "")
submit <- read.csv("test.csv", sep = "," , strip.white = TRUE, stringsAsFactors = FALSE, header = TRUE, na.strings = "")
```

Below is preview of both raw and submit datasets. Raw comes with 891 observations and 12 variables. Submit comes with 418 observations with 11 variables. The only difference is 'Survived' variable not included in Submit
```{r, echo = TRUE}
dim(raw); dim(submit)
names(raw); names(submit)
```

# Undertanding the variables
PassengerId is a unique identifier for each passenger. A quick check yields no duplication
```{r, echo = TRUE}
summary(raw$PassengerId)
sum(duplicated(raw$PassengerId))
```

Survived is and indication of survival where 0 = No and 1 = Yes. No missing values
```{r, echo = TRUE}
summary(raw$Survived)
sum(is.na(raw$Survived))
```

Pclass represent ticket class where 1 = 1st, 2 = 2nd, 3 = 3rd. 1st = Upper, 2nd = Middle, 3rd = Lower. No missing values
```{r, echo = TRUE}
summary(raw$Pclass)
sum(is.na(raw$Pclass))
```

Sex represents gender "male" or "female". No missing values
```{r, echo = TRUE}
str(raw$Sex)
sum(is.na(raw$Sex))
```

Age is age of passengers. Age is fractional if less than 1. No luck here with 177 missing values. We will decide on how to deal with this later.
```{r, echo = TRUE}
summary(raw$Age)
sum(is.na(raw$Age))
```

SibSp is the number of siblings or spouses abord the ship. Sibling = brother, sister, stepbrother, stepsister
Spouse = husband, wife (mistresses and fiancï¿½s were ignored). No missing values
```{r, echo = TRUE}
summary(raw$SibSp)
sum(is.na(raw$SibSp))
```

Parch is number of parents / children aboard the ship where Parent = mother, father
Child = daughter, son, stepdaughter, stepson. Some children travelled only with a nanny, therefore parch=0 for them. No missing values
```{r, echo = TRUE}
summary(raw$Parch)
sum(is.na(raw$Parch))
```

Ticket is Ticket number	for each passenger. About 210 duplications found. No missing values
```{r, echo = TRUE}
str(raw$Ticket)
sum(duplicated(raw$Ticket))
sum(is.na(raw$Ticket))
```

Fare is passenger fare. No missing values. Wow kinda interesting to see some class 3 passengers paying the rate of class 1!!
```{r, echo = TRUE}
str(raw$Fare)
sum(is.na(raw$Fare))
g <- ggplot(raw, aes(x = factor(Pclass), y = Fare)) + geom_boxplot() + ylim(c(0, 200))
g
```

Cabin indicates cabin number. No luck here with 687 missing values. 
```{r, echo = TRUE}
str(raw$Cabin)
sum(is.na(raw$Cabin))
```

Embarked indicates the port of embarkation. C = Cherbourg, Q = Queenstown, S = Southampton. Only 2 missing values. We will deal with this. 
```{r, echo = TRUE}
str(raw$Embarked)
sum(is.na(raw$Embarked))
```



# Data Preparation
My first take is always looking at missing values and decide whether to impute or simply discard them. Since we will be modifying the original dataset, I will create a new dataframe 'data'

## Formatting
Sex and Embarked should be assigned as factor. The rest looks fine at the moment
```{r, echo=TRUE}
data <- raw
data$Sex <- as.factor(data$Sex)
data$Embarked <- as.factor(data$Embarked)
data$Survived <- as.factor(data$Survived)
str(data)
```

## Missing Values
Lets look at overall missing values again
```{r, echo = TRUE}
nas <- apply(data, 2, function(x) sum(is.na(x)))
nas
```

Age, Cabin and Embarked have missing values. Lets start with the easiest one, Embarked. Passenger 62 and 830 travelled alone with no siblings, spouse or other family members. Interestingly they share same ticket, fare and cabin. 
```{r, echo = TRUE}
na <- data[is.na(data$Embarked),]
na
```

Lets try to look at passengers with similar ticket numbers (starting with "113") to guess the embarkation port. Out of 49 passengers, 42 embarked at Southampton and 7 from Cherbourg. If I have to make a guess it'd be Southampton
```{r, echo =TRUE}
ticket <- grep("113", data$Ticket)  # find passengers with ticket number starting from 113xxx
ticket
embark <- data[ticket,]
rownames(embark) <- c(1: nrow(embark)) # reset row names
table(embark$Embarked)
```

Lets fool around a bit and use ctree from party package to predict the port of embarkation for these two passengers. I exclude PassengerId, Name, Age and Cabin for this prediction
```{r, echo = TRUE}
set.seed(36)
library(party)

embark$Ticket <- as.numeric(embark$Ticket) #set ticket as numeric for this prediction only

train.embark <- embark[-c(6, 48), -c(1, 4, 6,11)]  #exclude passengers with missing port from training set
test.embark <- embark[c(6, 48),-c(1, 4, 6)]        #passengers with missing port
fit.ctree <- ctree(Embarked~., data = train.embark)
pred.embark <- predict(fit.ctree, test.embark)
pred.embark
```

As expected, our simple prediction result is Southampton. So we going to assign "S" (Southampton) for the 2 passengers and update 'data' 
```{r, echo = TRUE}
data$Embarked[62] <- "S"
data$Embarked[830] <- "S"
str(data)
```

The next variable to tackle is Age. We have got to impute 177 missing values. 
```{r, echo = TRUE}
sum(is.na(data$Age))
```

Again lets try to predict the missing 'Age' using ctree from party package. We are going to use a temporary dataframe 'ages' for this prediction. We are going to exclude PassengerId, Name, Ticket and Cabin for this run.
```{r, echo = TRUE}
set.seed(456)
library(party)
ages <- data

train.age <- ages[complete.cases(ages$Age),-c(1,4,9,11)]
test.age <- ages[is.na(ages$Age), -c(1,4,6, 9,11)]

fit.ctree <- ctree(Age~., data = train.age)
pred.age <- predict(fit.ctree, test.age)
summary(pred.age)
```

Lets combine this Age prediction into the testing dataframe to see whether it makes any sense. I think its logical that any person below 18 years should be accompanied by their parents or siblings right? Lets take a look
```{r echo=, warning=FALSE}
test.age$Age <- round(pred.age,1)
y <- test.age[test.age$Age < 20,]
y
```
Looks ok since imputed Age below 18 have at least 1 SibSp or Parch. Also when compared against Pclass, the imputed range looks reasonable. Pclass 2 is a little suspect since Fares are lower than Pclass 3. 
```{r, echo = TRUE}
g <- ggplot(test.age, aes(x = factor(Pclass), y = Fare, color = Age)) + geom_point() + ggtitle("Imputed Age")
g
```

So we are going to use the predicted values for the missing values in Age
```{r, echo = TRUE}
data$Age[is.na(data$Age)] <- round(pred.age,1)
sum(is.na(data$Age))
```

The last variable to deal is Cabin. Since the proportion of missing data is too high, we are going to exclude the variable
```{r, echo = TRUE}
nas <- apply(data, 2, function(x) sum(is.na(x)))
nas/nrow(data)
data <- data[,-c(1,11)]  #exclude PasengerId and Cabin
```

## Near Zero Variable
We need to test if there is any column with zero variance or near zero value. Everything is good here
```{r, echo=TRUE}
nzv <- nearZeroVar(data, saveMetrics = TRUE, names = TRUE)
nzv
```

## Feature Selection
Now that our data is clean, we will choose features which make sense for the prediction. Lets take a look at Name. As expected there's a variety of characters. We could do more advanced analysis to find patterns or generate new features but I like to keep things simple. We will exclude Name for the initial run and go back again if needed.
```{r, echo=TRUE}
head(data$Name)
data <- data[,-3]
```

Next is Ticket. Again we could find patterns but to keep things simple, I'll just exclude Ticket for the initial run
```{r, echo=TRUE}
head(data$Ticket)
data <- data[,-7]
```

Our final data looks like this. Distribution of response is a bit skewed towards Survived = 0, but its not too alarming
```{r, echo=TRUE}
names(data)
table(data$Survived)
```

## Splitting the data for training and testing
```{r, echo=TRUE}
index <- createDataPartition(data$Survived, p = 0.7, list = FALSE)
train <- data[index,]
test <- data[-index,]
x <- train[,-1]
y <- train[,1]
```

# Model Selection
For the moment of truth, I will be testing several models without tuning. In general, there will be GLM, Decision Tree, Ensembled Trees and Naive Bayes

## Generalized Linear Model
This is the classic method = 'glm' which can be used for both Regression and Classification. A model-specific variable importance metric is available but not used for this benchmark

```{r glm, echo=TRUE, warning=FALSE}

time.glm <- system.time(glm(Survived~., data = train, family = "binomial"))
fit.glm<- glm(Survived~., data = train, family = "binomial") 
pred.glm <- predict(fit.glm, test, type = "response")
pred.glm <- ifelse(pred.glm < 0.37, 0, 1)
acc.glm <- confusionMatrix(test$Survived, pred.glm)
acc.glm
glm <- c(family = "GLM", round(c(acc.glm$byClass, time.glm[3]),3))
glm
```

## GLMNET 
Method = 'glmnet' is an improved 'glm' method which includes regularization. It can be used for both Regression and Classification problems. Tuning parameters: alpha (Mixing Percentage) and lambda (Regularization Parameter). Required packages: glmnet, Matrix. A model-specific variable importance metric is available.

```{r glmnet, echo=TRUE}
library(glmnet)
library(Matrix)

xm <- data.matrix(x)
testxm <- data.matrix(test[,-1])
ym <- as.numeric(y) - 1
testym <- as.numeric(test$Survived) - 1

time.glmnet <- system.time(cv.glmnet(xm, ym, nfolds = 10))
fit.glmnet <- cv.glmnet(xm, ym, nfolds = 10)
pred.glmnet <- predict(fit.glmnet, testxm, s = fit.glmnet$lambda.min, type = "response")
pred.glmnet <- ifelse(pred.glmnet < 0.36, 0, 1)
acc.glmnet <- confusionMatrix(testym, pred.glmnet)
acc.glmnet
glmnet <- c(family = "GLM", round(c(acc.glmnet$byClass, time.glmnet[3]), 3))
glmnet
```

## Naive Bayes

  method = 'naive_bayes'
Type: Classification

Tuning parameters:
fL (Laplace Correction)
usekernel (Distribution Type)
adjust (Bandwidth Adjustment)
Required packages: naivebayes

```{r naivebayes, echo=FALSE, warning=FALSE}
library(naivebayes)

time.naivebayes <- system.time(naive_bayes(x, y ))
fit.naivebayes <- naive_bayes(x, y)
pred.naivebayes <- predict(fit.naivebayes, test)
acc.naivebayes <- confusionMatrix(test$Survived, pred.naivebayes)

naivebayes <- c(family = "Naive Bayes", round(c(acc.naivebayes$byClass, time.naivebayes[3]),3))
acc.naivebayes; naivebayes
```

## Conditional Inference Tree

  method = 'ctree'
Type: Classification, Regression

Tuning parameters:

mincriterion (1 - P-Value Threshold)
Required packages: party

```{r ctree, echo=FALSE, warning=FALSE}
library(party)

# some tuning
control <- ctree_control(testtype = "Teststatistic", mincriterion = 0.95, mtry = 0)

# run model
time.ctree <- system.time(ctree(Survived~., data = train, controls = control ))
fit.ctree <- ctree(Survived~., data = train, controls = control)
pred.ctree <- predict(fit.ctree, test, type = "response")

acc.ctree <- confusionMatrix(test$Survived, pred.ctree)

ctree <- c(family = "Decision trees", round(c(acc.ctree$byClass, time.ctree[3]),3))
ctree
```

## CART

  method = 'rpart'
Type: Regression, Classification

Tuning parameters:

cp (Complexity Parameter)
Required packages: rpart

A model-specific variable importance metric is available.


```{r rpart, echo=FALSE, warning=FALSE}
library(rpart)

# run model
time.rpart <- system.time(rpart(Survived ~ ., data = train, method = "class"))
fit.rpart <- rpart(Survived ~ ., data = train, method = "class")
cpmin <- printcp(fit.rpart)
control <- rpart.control(cp = 0.015, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30)
fit.rpart <- rpart(Survived ~ ., data = train, method = "class", control = control)
pred.rpart <- predict(fit.rpart, test, type = "class")
acc.rpart <- confusionMatrix(test$Survived, pred.rpart)

rpart <- c(family = "Decision trees", round(c(acc.rpart$byClass, time.rpart[3]),3))
rpart

```

## Random Forest
This is random forest from the ranger package. I like this because of the speed. 
 
```{r ranger}
library(ranger)


time.ranger <- system.time(csrf(Survived ~ ., training_data = train, test_data = test, params1 = list(num.trees = 300, mtry = 4), params2 = list(num.trees = 5)))
#fit.ranger <- ranger(Survived~., data = train)
#pred.ranger <- predict(fit.ranger, test)

tune.ranger <- csrf(Survived ~ ., training_data = train, test_data = test, params1 = list(num.trees = 300, mtry = 4), params2 = list(num.trees = 10))
acc.ranger <- confusionMatrix(tune.ranger, test$Survived)
ranger <- c(family = "Decision trees", round(c(acc.ranger$byClass, time.ranger[3]),3))
ranger

```

## Random Forest
Here I am using the randomForest package for the run. I tried some basic tuning to find the best mtry and it didn't change much compared to untuned model 

```{r randomForest, echo=FALSE, warning=FALSE}
library(randomForest)

# some tuning
cv.randomforest <- rfcv(x, y, cv.fold = 10, scale = "log", step = 0.5, recursive = TRUE)
tune.randomforest <- tuneRF(x, y, ntreeTry = 150, stepFactor = 2, doBest = TRUE)

# run model
time.randomforest <- system.time(tuneRF(x, y, ntreeTry = 150, stepFactor = 2, doBest = TRUE))

pred.randomforest <- predict(tune.randomforest, test) 
acc.randomforest <- confusionMatrix(pred.randomforest, test$Survived)

randomforest <- c(family = "Decision trees", round(c(acc.randomforest$byClass, time.randomforest[3]),3))
randomforest

```

## Models Comparison
You can see ensembled trees like ranger and randomforest giving highest accuracy compared to the rest. However both models lack in specificity which is prediction of Survived = 1. The rpart model is best in specificity, and having a more balanced accuracy comparared to the former. I am tempted to stack rpart and ranger together to get the best of both worlds but I will leave that for another day.
```{r comp, echo= FALSE}
comp <- data.frame(rbind(glm, glmnet,  rpart, ctree, ranger, randomforest), stringsAsFactors = FALSE)
comp[,13] <- as.numeric(comp[,13])
c <- comp[order(comp$Balanced.Accuracy, decreasing = TRUE), c(1,13,12, 2:5)]
c
```

# Final Submission
I am going to use randomForest for the submission. 

## Submission Data
Lets look at the submission dataset again. Everything looks similar to raw only without the Survived variable
```{r, echo = TRUE}
head(submit)
str(submit)
```

## Missing Values
Lots of missing values in Cabin, Age and Fare. We won't be using Cabin so we will have to impute Age and Fare. I am going to use same method as per the train dataset to impute the missing values
```{r, echo= TRUE}
na <- sum(is.na(submit))
na
nap <- apply(submit, 2, function(x) sum(is.na(x)))
nap
```

I will predict the missing 'Age' using ctree from party package. We are going to use a temporary dataframe 'ages' for this prediction. We are going to exclude PassengerId, Name, Ticket and Cabin for this run.
```{r, echo = TRUE}
library(party)
ages2 <- submit
ages2$Sex <- as.factor(ages2$Sex)
ages2$Embarked <- as.factor(ages2$Embarked)

train.age2 <- ages2[complete.cases(ages2$Age),-c(1,3, 8, 10)]  # complete Age
test.age2 <- ages2[is.na(ages2$Age), -c(1,3, 8, 10)]           # missing Age

fit.age2 <- ctree(Age~., data = train.age2)
pred.age2 <- predict(fit.age2, test.age2)
summary(pred.age2)
```

Lets combine this Age prediction into the testing dataframe to see whether it makes any sense. I think its logical that any person below 18 years should be accompanied by their parents or siblings right? Lets take a look
```{r echo=, warning=FALSE}
test.age2$Age <- round(pred.age2,1)
y2 <- test.age2[test.age2$Age < 18,]
y2
```
Looks ok since imputed Age below 18 have at least 1 SibSp or Parch. Also when compared against Pclass, the imputed range looks reasonable. Pclass 3 is a little suspect since some Fares are higher than Pclass 1. 
```{r, echo = TRUE}
g <- ggplot(test.age2, aes(x = factor(Pclass), y = Fare, color = Age)) + geom_point() + ggtitle("Imputed Age")
g
```

So we are going to use the predicted values for the missing values in Age
```{r, echo = TRUE}
submit$Age[is.na(submit$Age)] <- round(pred.age2,1)
sum(is.na(submit$Age))
```

Last one is Fare. I am going to use the mean for this one. Lets look at the missing Fare. Passenger is male, Age 60.5, Pclass 3 and embarked at Southampton. We can use that info to calculate the mean
```{r, echo = TRUE}
fare <- submit[is.na(submit$Fare),]
fare

```

I am going to filter the data based on sex = male, Age > 40, Pclass = 3 and embarked = S. Mean of fare is 20.6
```{r, echo = TRUE}
fare2 <- submit[complete.cases(submit$Fare),]
fare2 <- fare2[fare2$Pclass == 3,]
fare2 <- fare2[fare2$Sex == "male",]
fare2 <- fare2[fare2$Age >= 40,]
fare2 <- fare2[fare2$Embarked == "S",]
meanfare <- mean(fare2$Fare)
meanfare
```

Lets assign this fare to passenger 1044. We are ready to run our model!
```{r, echo = TRUE}
submit$Fare[submit$PassengerId == 1044] <- meanfare
apply(submit, 2, function(x) sum(is.na(x)))
```

## The Prediction 
Before running the final prediction, lets get the data in proper format. It should follow 'data'
```{r, echo=TRUE}
setdiff(names(submit), names(data))
```

I am going to exclude the above from submit
```{r, echo = TRUE}
names(submit)
submit <- submit[,-c(1,3,8,10)]
str(data)
```

Set Sex and Embarked as factor
```{r, echo = TRUE}
submit$Sex <- as.factor(submit$Sex)
submit$Embarked <- as.factor(submit$Embarked)
str(submit)
```

Now, for the moment of truth!
```{r, echo = TRUE}
pred.randomforest <- predict(tune.randomforest, submit) 
pred.randomforest
submit2 <- read.csv("test.csv", sep = "," , strip.white = TRUE, stringsAsFactors = FALSE, header = TRUE, na.strings = "")
submission <- data.frame(PassengerId = submit2$PassengerId, Survived = pred.randomforest)
submission
```