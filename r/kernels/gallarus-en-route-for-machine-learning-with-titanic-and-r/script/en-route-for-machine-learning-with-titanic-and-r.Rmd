---
title: '"En route" for machine Learning with titanic and R
'
author: "Adrien Le Guillou"
output:
  html_document: 
    toc: yes
  html_notebook: default
---

# Setup

# Introduction

This is my first attempt at the titanic challenge and also my first real machine 
learning project. 

You won't find anything new in that kernel. I have looked at a lot of great kernels
and use them to build this one. I will add references to them soon.

# Load and check data


The references for each of them can be found in the bibliography at the end. 

```{r libraries, message=FALSE}
# Load all the project packages
library("tidyverse")
library("forcats")
library("lubridate")
library("stringr")
library("ggpubr")
library("ggmosaic")
library("broom")
library("caret")
library("ranger")
library("parallel")
library("GGally")
library("doParallel")
library("mice")
library("randomForest")
library("e1071")
```

Once the libraries are loaded it's time for reading the data.

I put everything in a single dataframe and add a `set` column to distinguish
the training from the testing data.

```{r read_data, message=FALSE}
dataset <- 
  read_csv("../input/train.csv") %>%
  mutate(set = "train")

dataset <- 
  read_csv("../input/test.csv") %>%
  mutate(set = "test",
         Survived = NA) %>%
  bind_rows(dataset)
```

Then we take a glimpse at the data.

```{r check}
glimpse(dataset)
```

There is 12 variables plus the "set" variable I added.
And 1,309 observations.

Now I reorder and rename the columns following these rules:  

* snake_case
* somewhat logical names

```{r rename_columns}
dataset <- dataset %>%
  select(passenger_id    = PassengerId, # passenger identifier in the dataset
         name            = Name,        # name of the passenger
         sex             = Sex,         # sexe
         age             = Age,         # age
         family_horiz    = SibSp,       # number of family members on the same level (siblings or spouse)
         family_vert     = Parch,       # number of family members a level away (children or parents)
         cabin_number    = Cabin,       # cabin number
         ticket_number   = Ticket,      # ticket number 
         ticket_class    = Pclass,      # is the ticket 1st, 2nd or 3rd class
         ticket_fare     = Fare,        # price of the ticket  
         embark_port     = Embarked,    # embarkment port
         survived        = Survived,    # survival - output targer
         set)                         # is the data from training or test

glimpse(dataset) # glimpse again to see the changes
```

The variables `name`, `sexe`, `cabin_number`, `ticket_number` and `embark_port`
are strings and will need to be transformed correctly if we want to use
them for prediction.

# Missing data and feature engineering

First of I will set as factors the variable that I are categorical and that I
will not modify.

```{r factoring}
dataset <- dataset %>%
  mutate(sex = factor(sex),
         ticket_class = factor(ticket_class),
         survived = factor(survived),
         set = factor(set))
```

## What is missing

Dealing with missing values is crucial. I begin by simply counting the number
of missing values for each variable.

```{r missings}
map_df(dataset, function(x) {sum(is.na(x))}) %>% 
  gather(variable, missing) %>%
  arrange(desc(missing))
```

## Ticket fare

Because there is only one missing value for the `ticket_fare` we can explore it
directly.

```{r ticket_fare-missing}
dataset %>%
  filter(is.na(ticket_fare))
```

This passenger is not accompanied by family so price is probably for him alone. 
We can presume that the ticket fare is correlated with the ticket class and
the embarkment port.
I will infere as it's `ticket_fare` the median of price for passenger in
class 3 who embarked in Southampton. If there were more missing values this 
simple imputation would not be sufficient because of the several factors that 
can influence this variable. For exemple the family size.

```{r ticket_fare-impute}
dataset$ticket_fare[is.na(dataset$ticket_fare)] <- 
  dataset %>%
  filter(ticket_class == 3,
         embark_port == "S") %>% 
  pull(ticket_fare) %>%
  median(na.rm = TRUE)
```

## Embarkment port

There is two missing values for the embarkment port. 

```{r embark_port-missing}
dataset %>%
  filter(is.na(embark_port)) %>% 
  select(-family_horiz, -family_vert, -survived)
```

The two passengers were sharing a cabin and have the same ticket number.
The ticket cost 80\$ as is a first class.

With this data let's try to infere there embarkment port.

```{r embarkment_port-impute}
ggboxplot(dataset, x = "embark_port", y = "ticket_fare", 
          fill = "ticket_class") +
  geom_hline(aes(yintercept = 80), colour = "red")
```

The 80\$ price for a first class ticket correspond really well to an embarkment
at Charbourg. I will put that instead of NA.

```{r embark_port-replace}
# replace with S
dataset <- dataset %>%
  replace_na(list(embark_port = 'C')) %>%
  mutate(embark_port = factor(embark_port))
```

## Cabin number

Only `r 1309 - 1014` passenger have a known `cabin_number`. Before dropping 
that variable let's see if we can extract some information from it.

First I will extract the deck.

Warnings :  

* Some passengers have several cabin number but all on the same deck 
* Some cabin numbers begin with "F" then a regular number but it seems it 
  correspond to cabin on the F deck

So for my purpose I will extract the first chararcter of the 'cabin_number'

```{r deck}
dataset <- dataset %>%
  mutate(deck = str_sub(cabin_number, 1, 1))

dataset %>%
  filter(set == "train") %>%
  ggplot(aes(x = deck, fill = survived, label = ..count..)) +
    geom_bar(stat = "count", position = "fill") +
    geom_text(stat = "count", position = position_fill(vjust = 0.5))
```

It seems that the knowing the cabin's number is itself a factor of survival.
I will just keep a binary variable `cabin_known` then. I leave the `deck` 
variable there for know. Maybe we'll use it later.

```{r cabin}
dataset <- dataset %>%
  mutate(cabin_known = !is.na(cabin_number))

dataset %>%
  filter(set == "train") %>%
  ggplot(aes(x = cabin_known, fill = survived, label = ..count..)) +
    geom_bar(stat = "count", position = "fill") +
    geom_text(stat = "count", position = position_fill(vjust = 0.5))
```

## Titles and surname

There is a lot of missing data in the `age` and I don't think I have enough 
information yet to do some proper imputation so we will tackle the names first.

The name variable contains a lot of information :

* last name
* title
* first name (often the husband one if maried) 
* maiden name between parenthesis (the distinction between first name and 
  maiden family name is not obvious)
* surname between double quotes
* surname between parenthesis and double quotes (it seems to correspond to more 
  wealthy people)
  
Because of the wide variety of information let's just keep those :

* last name
* title
* presence of a surname 
* presence of a "formal" surname

```{r names}
dataset <- dataset %>%
  mutate(last_name    = str_extract(name, "^.+,") %>%
                        str_extract("[A-Z][A-Z 'a-z]*"),
         title        = str_extract(name, ", .+[.]") %>%
                        str_extract("[A-Z][A-Z 'a-z]*"),
         surname      = ifelse(!is.na(str_extract(name, "\\(\".*\"\\)")),
                               "formal",
                               ifelse(!is.na(str_extract(name, "\".*\"")),
                                      "informal",
                                      NA)))

dataset %>%
  filter(set == "train") %>%
  ggplot(aes(x = title, fill = survived, label = ..count..)) +
    geom_bar(stat = "count", position = "fill") +
    geom_text(stat = "count", position = position_fill(vjust = 0.5)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

We can see that there is a large number of different titles. They can represent
several things :

* the mark of nobility : sir, don, jonkheer...
* the type of profession : dr, reverend, capt, col...
* the age : master and miss for the children, Mr and Mrs for the adults
* the sexe : sir, lady, etc...

I want to remove the information we already have on other variables. So I will
normalise the titles. 

Because there is only a few occurence of the special titles I will create
a new variable *category* with *mr*, *mrs*, *miss*, *master* and *other* as
levels.

I will use a violin plot to confirm what title correspond to what age.

```{r category, message=FALSE}
ggviolin(data = dataset, x = "title", y = "age", fill = "title",
         add = "boxplot", add.params = list(fill = "white")) +
         theme(axis.text.x = element_text(angle = 45, hjust = 1))

dataset <- dataset %>% 
  mutate(category = as_factor(title) %>%
         # we add the foreign and variant to there group
           fct_collapse(mrs = c("Mrs", "Mme", "Ms"),
                        miss = c("Miss", "Mlle")) %>%
         # we keep only the four more frequent group and lump the rest together
           fct_lump(n = 4)) 

# we take a look again at the ages
ggviolin(data = dataset, x = "category", y = "age", fill = "category",
         add = "boxplot", add.params = list(fill = "white"))

# and another look at the survival by category
dataset %>%
  filter(set == "train") %>%
  ggplot(aes(x = category, fill = survived, label = ..count..)) +
    geom_bar(stat = "count", position = "fill") +
    geom_text(stat = "count", position = position_fill(vjust = 0.5))
```

## Age

With the titles and the family_size maybe I can do a better job at infering 
the ages. 

I will use the mice package following closely what Megan did on here kernel.

```{r age-missing}
# the mice package is used to impute the missng ages with a random forest
mice_model <- dataset %>% 
  select(age, 
         sex,
         family_horiz,
         family_vert,
         ticket_class,
         ticket_fare, 
         embark_port,
         set,
         category) %>%
  mice(method = 'rf')

mice_output <- complete(mice_model)
```

Now we explore the density of ages in the original and the imputed dataset.

```{r eval_mice, message=FALSE}
#bind dataset and mice_output and plot the density of age
dataset %>% 
  mutate(set = "original") %>% 
  bind_rows(mice_output %>% 
    mutate(set = "imputed")) %>% 
  mutate(set = factor(set)) %>%
  ggdensity(x = 'age', fill = "set", alpha = 0.5) 
```

It seems to be okay. Let's use these values.

```{r}
dataset$age <- mice_output$age
```

# Prediction

The pre-processing of the data is now complete. It is time for prediction.

As it is my first time doing real prediction I will use a random forest.
The model computation will be parallelized using the `doParallel` 
package and the `allowParallel = TRUE` option in `caret`.

```{r}
durations <- data.frame(model = as.character(),
                        duration)
```

## Random Forest

First I train a random forest using the `ranger` package. It is suposed to be
a more efficient implementation of the random forest algorithm.

Spliting the data :

```{r split-rf}
train <- dataset %>%
  filter(set == "train") %>%
  select(-set)

test <- dataset %>%
  filter(set == "test") %>%
  select(-set)
```

```{r random-forest}
start_time <- Sys.time()

# initiate the multicore
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

rf_model <- train(survived ~ sex +
                          age +
                          family_horiz +
                          family_vert +
                          ticket_class +
                          ticket_fare +
                          embark_port +
                          cabin_known +
                          category,
               data = train, 
               method = "ranger",
               tuneLength = 10,
	       #tuneGrid = data.frame(mtry = c(2, 4, 8, 12)),
               importance = "impurity",
               trControl = trainControl(
                 method = "repeatedcv", number = 10,
                 allowParallel = TRUE,
                 repeats = 10, verboseIter = TRUE))

# stop multicore
stopCluster(cluster)
registerDoSEQ()

end_time <- Sys.time()

durations <- durations %>%
  rbind(list(model = "random forest", 
          duration = end_time - start_time))

rf_model 
```

With my model created I can make my prediction and save the submition csv.

```{r, message=FALSE}
submission <- read_csv("../input/genderclassmodel.csv")
submission$Survived <- predict(rf_model, test, type = "raw")

write_csv(submission, "submission.csv")
```

When submitted on kaggle I achieve a 0.75119 success rate on my predictions.
