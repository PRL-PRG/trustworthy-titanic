
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> 
> train <- read.csv("../input/train.csv",stringsAsFactors=F)
> test <- read.csv("../input/test.csv",stringsAsFactors=F)
> test$Survived <- NA
> data <- rbind(train,test)
> 
> # create dictionary from training names
> words = paste(data$Name[1:891],collapse=' ')
> words = gsub('[.,"()/]','',words)
> words = gsub(' . |  ',' ',words)
> words = strsplit(words,' ')[[1]]
> freq = ave(1:length(words),words,FUN=length)
> dictionary = data.frame(Words=words,Freq=freq,stringsAsFactors=F)
> dictionary = dictionary[!duplicated(dictionary$Words) & dictionary$Freq>1,]
> dictionary <- dictionary[order(-dictionary$Freq),]
> 
> # create vectorization from dictionary
> data2 <- data.frame(Mr=rep(0,1309))
> for (i in dictionary$Words) data2[,i] <- 0
> for (i in 1:nrow(data)){
+     n = gsub('[.,"()/]','',data$Name[i])
+     n = gsub(' . |  ',' ',n)
+     n = strsplit(n,' ')[[1]]
+     for (j in n){
+         if (j %in% dictionary$Words){
+             data2[i,j] = 1
+         }
+     }
+ }
> 
> # calculate survival probability
> s=1:891
> dictionary$Survival <- NA
> for (i in 1:length(dictionary$Words)){
+     x = intersect(which(data2[,dictionary$Words[i]]>0),s)
+     dictionary$Survival[i] <- mean(data$Survive[x],na.rm=T)
+ }
> # display words and survival
> row.names(dictionary) <- 1:nrow(dictionary)
> cat("15 most frequent Words and their survival rate:\n")
15 most frequent Words and their survival rate:
> head(data.frame(n=1:nrow(dictionary),dictionary[order(-dictionary$Freq),]),15)
    n   Words Freq  Survival
1   1      Mr  521 0.1583012
2   2    Miss  182 0.6978022
3   3     Mrs  129 0.7906977
4   4 William   64 0.3492063
5   5    John   44 0.2500000
6   6  Master   40 0.5750000
7   7   Henry   35 0.2941176
8   8   James   24 0.2500000
9   9  George   24 0.3043478
10 10 Charles   23 0.3043478
11 11  Thomas   22 0.3636364
12 12    Mary   20 0.7500000
13 13  Edward   18 0.3333333
14 14    Anna   17 0.7058824
15 15  Joseph   16 0.3125000
> cat(sprintf('%d Words in dictionary\n',nrow(dictionary)))
415 Words in dictionary
> dictionary$Words[order(dictionary$Words)]
  [1] "Abbott"         "Abelson"        "Abraham"        "Ada"           
  [5] "Adele"          "Adolf"          "Adrian"         "Agnes"         
  [9] "Albert"         "Alexander"      "Alexandra"      "Alfons"        
 [13] "Alfred"         "Alfrida"        "Ali"            "Alice"         
 [17] "Allen"          "Allison"        "Amelia"         "Amin"          
 [21] "Amy"            "Anders"         "Anderson"       "Andersson"     
 [25] "Andreas"        "Andrew"         "Andrews"        "Ann"           
 [29] "Anna"           "Anne"           "Annie"          "Anthony"       
 [33] "Antoni"         "Antti"          "Arne"           "Arnold"        
 [37] "Arnold-Franchi" "Arthur"         "Arvid"          "Asplund"       
 [41] "Attalah"        "August"         "Augusta"        "Augustus"      
 [45] "Austin"         "Backstrom"      "Baclini"        "Baird"         
 [49] "Baptiste"       "Barbara"        "Baxter"         "Beane"         
 [53] "Becker"         "Beckwith"       "Bengtsson"      "Benjamin"      
 [57] "Berglund"       "Berk"           "Bertha"         "Bertram"       
 [61] "Bessie"         "Birkhardt"      "Bishop"         "Borland"       
 [65] "Boulos"         "Bourke"         "Bowerman"       "Bradley"       
 [69] "Braund"         "Bridget"        "Brogren"        "Brown"         
 [73] "Cacic"          "Caldwell"       "Calic"          "Campbell"      
 [77] "Carl"           "Carlsson"       "Carter"         "Castellana"    
 [81] "Catherine"      "Chambers"       "Chapman"        "Charles"       
 [85] "Charlotta"      "Charlotte"      "Christian"      "Christy"       
 [89] "Clara"          "Clarke"         "Col"            "Coleff"        
 [93] "Collyer"        "Constance"      "Courtenay"      "Coutts"        
 [97] "Crosby"         "Daisy"          "Daly"           "Danbom"        
[101] "Daniel"         "David"          "Davies"         "Davis"         
[105] "de"             "Dean"           "Delia"          "Denis"         
[109] "Dick"           "Dickinson"      "Doling"         "Dorothy"       
[113] "Douglas"        "Dr"             "Duane"          "Duff"          
[117] "Edgar"          "Edith"          "Edmond"         "Edvard"        
[121] "Edvin"          "Edward"         "Edwy"           "Eino"          
[125] "Elias"          "Elisabeth"      "Elizabeth"      "Ellen"         
[129] "Elmer"          "Elsie"          "Emil"           "Emilia"        
[133] "Emily"          "Emma"           "Erik"           "Ernest"        
[137] "Ernst"          "Ethel"          "Eugene"         "Eugenie"       
[141] "Eustis"         "Fischer"        "Fleming"        "Florence"      
[145] "Flynn"          "Ford"           "Fortune"        "Francis"       
[149] "Frank"          "Frauenthal"     "Frederic"       "Frederick"     
[153] "Futrelle"       "George"         "Gerious"        "Gertrude"      
[157] "Gilbert"        "Gillespie"      "Gladys"         "Godfrey"       
[161] "Goldenberg"     "Goldsmith"      "Goodwin"        "Gordon"        
[165] "Graham"         "Gretchen"       "Gustaf"         "Gustafsson"    
[169] "Hagland"        "Hakkarainen"    "Hall"           "Hamalainen"    
[173] "Hanna"          "Hannah"         "Hanora"         "Hans"          
[177] "Hansen"         "Harald"         "Harold"         "Harper"        
[181] "Harris"         "Harry"          "Hart"           "Harvey"        
[185] "Hays"           "Heath"          "Hedwig"         "Helen"         
[189] "Helene"         "Henrik"         "Henry"          "Herman"        
[193] "Hickman"        "Hippach"        "Hjalmar"        "Hocking"       
[197] "Holverson"      "Hoyt"           "Hudson"         "Hugh"          
[201] "Hughes"         "Hugo"           "Hunt"           "Ida"           
[205] "II"             "Impe"           "Ivan"           "Jacob"         
[209] "Jacobsohn"      "Jacques"        "Jakob"          "James"         
[213] "Jane"           "Jean"           "Jensen"         "Jessie"        
[217] "Joel"           "Johan"          "Johanna"        "Johannes"      
[221] "Johansson"      "John"           "Johnson"        "Johnston"      
[225] "Josef"          "Joseph"         "Jr"             "Juha"          
[229] "Juho"           "Julian"         "Jussila"        "Kantor"        
[233] "Karl"           "Karlsson"       "Kate"           "Katherine"     
[237] "Katie"          "Keane"          "Kelly"          "Lalio"         
[241] "Lam"            "Laroche"        "Larsson"        "Lauritz"       
[245] "Lawrence"       "Lee"            "Lefebre"        "Leo"           
[249] "Leonard"        "Leslie"         "Lewis"          "Lillian"       
[253] "Lily"           "Lobb"           "Louis"          "Louise"        
[257] "Lucile"         "Lucy"           "Luka"           "Mabel"         
[261] "Madeleine"      "Maggie"         "Major"          "Malkolm"       
[265] "Mallet"         "Manuel"         "Margaret"       "Maria"         
[269] "Marie"          "Marija"         "Marion"         "Marjorie"      
[273] "Mark"           "Marshall"       "Martha"         "Martin"        
[277] "Mary"           "Master"         "Mathias"        "Mathilde"      
[281] "Matilda"        "Matti"          "Maurice"        "Mauritz"       
[285] "Maxfield"       "May"            "McCoy"          "Mellinger"     
[289] "Meyer"          "Michael"        "Michel"         "Minahan"       
[293] "Miriam"         "Miss"           "Mlle"           "Monypeny"      
[297] "Moor"           "Moran"          "Morgan"         "Morley"        
[301] "Moubarek"       "Mr"             "Mrs"            "Murphy"        
[305] "Nakid"          "Nasser"         "Navratil"       "Neal"          
[309] "Nellie"         "Nelson"         "Newell"         "Nicholas"      
[313] "Nicola-Yarred"  "Nils"           "Nora"           "Norman"        
[317] "O'Brien"        "Olof"           "Olsen"          "Olsson"        
[321] "Oreskovic"      "Oscar"          "Oskar"          "Owen"          
[325] "Palsson"        "Panula"         "Parrish"        "Patrick"       
[329] "Pears"          "Peder"          "Pekka"          "Penasco"       
[333] "Percival"       "Persson"        "Peter"          "Petroff"       
[337] "Philip"         "Phillips"       "Pietari"        "Planke"        
[341] "Polk"           "Quick"          "Razi"           "Reginald"      
[345] "Rene"           "Renouf"         "Rev"            "Rice"          
[349] "Richard"        "Richards"       "Robert"         "Roger"         
[353] "Rosa"           "Rosalie"        "Rosblom"        "Ryerson"       
[357] "Saad"           "Sage"           "Samuel"         "Sandstrom"     
[361] "Satode"         "Sidney"         "Sigrid"         "Silvey"        
[365] "Sinai"          "Skoog"          "Sleeper"        "Smith"         
[369] "Sofia"          "Solomon"        "Sophia"         "Spencer"       
[373] "Stanley"        "Stephen"        "Stone"          "Strom"         
[377] "Susan"          "Svensson"       "Tannous"        "Taussig"       
[381] "Taylor"         "Thayer"         "Theodor"        "Thomas"        
[385] "Thompson"       "Thorne"         "Thorneycroft"   "Thornton"      
[389] "Timothy"        "Tobin"          "Turpin"         "van"           
[393] "Van"            "Vande"          "Vander"         "Victor"        
[397] "Viktor"         "Walter"         "Walton"         "Washington"    
[401] "Watson"         "Webber"         "West"           "White"         
[405] "Wick"           "Wilhelm"        "William"        "Williams"      
[409] "Wilson"         "Wright"         "Yasbeck"        "Youssef"       
[413] "Yousseff"       "Zabour"         "Zebley"        
> 
> data3 <- as.matrix(data2)
> # calculate mean and sd
> m = rep(0,ncol(data3))
> s = rep(0,ncol(data3))
> for (i in 1:ncol(data3)){
+     m[i] = mean(data3[,i])
+     s[i] = sd(data3[,i])
+ }
> # calculate covariance
> data4 = (1/1309)*t(data3) %*% data3 - m %*% t(m)
> # calculate correlation
> data5 = data4 / (s %*% t(s))
> # calculate principal components
> ev <- eigen(data4,symmetric=T)
> ev2 <- data.frame(values=ev[[1]],vectors=ev[[2]])
> cat('Principal components and correlations have been calculated.\n\n')
Principal components and correlations have been calculated.

> # find word pairs with frequency>=4 and high correlation
> cat("Correlations above 0.3 of Words with freq>=4 are:\n")
Correlations above 0.3 of Words with freq>=4 are:
> for (i in 1:length(which(dictionary$Freq>=4)))
+ for (j in (i+1):length(which(dictionary$Freq>=4))){
+     if (i!=j & abs(data5[i,j])>=0.3)
+         cat(sprintf("%s and %s have correlation r = %f\n",row.names(data5)[i],row.names(data5)[j],data5[i,j]))	
+ }
Mr and Miss have correlation r = -0.583480
Mr and Mrs have correlation r = -0.499178
Ernest and Carter have correlation r = 0.467785
Hart and Benjamin have correlation r = 0.375213
> #cat("\nCorrelations above 0.5 of Words with freq<=3\n")
> #for (i in length(which(dictionary$Freq>=4)):nrow(data5)-1)
> #for (j in (i+1):nrow(data5)-1){
> #    if (i!=j & abs(data5[i,j])>=0.5)
> #        cat(sprintf("%s and %s have correlation r = %f\n",row.names(data5)[i],row.names(data5)[j],data5[i,j]))
> #}
> 
> library(ggplot2)

Attaching package: ‘ggplot2’

The following object is masked _by_ ‘.GlobalEnv’:

    Position

Warning message:
package ‘ggplot2’ was built under R version 3.6.2 
> library(gridExtra)
> 
> #ggplot(data=data2, aes(x=data2$Charles,y=data2$Mr)) + 
> #    geom_jitter(width=0.1,height=0.1) +
> #    geom_smooth(method='lm') +
> #    labs(x='Charles',y='Mr',title='Corr(Charles,Mr) = 0.0828.\nLinear regression R^2 = 0.00686')
> 
> data3Transformed = (t(ev[[2]]))[,1:nrow(ev[[2]])] %*% t(data3)
> dataPC6 = data.frame(n=1:1309,Survived=data$Survived,t(data3Transformed)[,1:6])
> colnames(dataPC6) <- c('PassengerId','Survived',paste('PC',1:6,' wgt',sep=''))
> rownames(dataPC6) <- 1:1309
> head(data[,c('PassengerId','Name','Sex','Age')])
  PassengerId                                                Name    Sex Age
1           1                             Braund, Mr. Owen Harris   male  22
2           2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38
3           3                              Heikkinen, Miss. Laina female  26
4           4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35
5           5                            Allen, Mr. William Henry   male  35
6           6                                    Moran, Mr. James   male  NA
> cat('First 6 principal component weights for the first 6 passengers:\n')
First 6 principal component weights for the first 6 passengers:
> head(dataPC6[,-2])
  PassengerId    PC1 wgt     PC2 wgt    PC3 wgt   PC4 wgt      PC5 wgt
1           1  0.8289415 -0.13317819 -0.1955612 0.1749779 -0.005180128
2           2 -0.2575329  0.83496841 -0.1593632 0.2866029  1.050714017
3           3 -0.4762724 -0.64406901 -0.1454686 0.2761734  0.057313167
4           4 -0.2865486  0.75206427 -0.3313646 0.2078190 -0.027557796
5           5  0.8704663  0.00617076  0.6104579 0.9506416 -0.133330188
6           6  0.8409674 -0.10850758 -0.2034442 0.1891481  0.015216444
      PC6 wgt
1  0.02364561
2 -0.06646308
3  0.02475787
4  0.01588342
5  0.77599880
6 -0.12450352
> 
> library(caret)
Loading required package: lattice
> accuracy <- matrix(nrow=5,ncol=5)
> rownames(accuracy) <- paste('d=',3:7,sep='')
> colnames(accuracy) <- paste('k=',c(7,9,11,13,15),sep='')
> for (d in 1:5){
+     xt = (t(ev[[2]]))[1:(d+2),1:nrow(ev[[2]])] %*% t(data3)
+     dataPC = data.frame(Survived=data$Survived,t(xt))
+     for (k in 1:5){
+         trials = 100
+         total = 0
+         for (i in 1:trials){
+             s = sample(1:891,802)
+             s2 = (1:891)[-s]
+             model <- knn3(factor(Survived) ~ .,dataPC[s,],k=2*k+5)
+             p <- predict(model,newdata=dataPC[s2,])
+             p <- ifelse(p[,2]>=0.5,1,0)
+             # calculate one minus misclassification rate
+             x = 1-sum(abs(dataPC$Survived[s2]-p))/length(s2)
+             #if (i%%10==0) cat(sprintf("Trial %d has CV accuracy %f\n",i,x))
+             total = total + x
+     }
+         #cat(sprintf("For d=%d, k=%d, average CV accuracy of %d trials is %f\n"
+         #    ,d+2,2*k+5,trials,total/trials))
+     accuracy[d,k] <- total/trials
+     }
+ }
> cat('Cross validation accuracy using 10-fold CV:\n')
Cross validation accuracy using 10-fold CV:
> accuracy
          k=7       k=9      k=11      k=13      k=15
d=3 0.7793258 0.7974157 0.7937079 0.7859551 0.7751685
d=4 0.7955056 0.7947191 0.8013483 0.7885393 0.7910112
d=5 0.7983146 0.8051685 0.7912360 0.7957303 0.7853933
d=6 0.7828090 0.7855056 0.7912360 0.7877528 0.7871910
d=7 0.7922472 0.7867416 0.7859551 0.7796629 0.7805618
> 
> d=5; k=11;
> xt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% t(data3)
> dataPC = data.frame(Survived=data$Survived,t(xt))
> model <- knn3(factor(Survived) ~ .,dataPC[1:891,],k=k)
> p <- predict(model,newdata=dataPC[892:1309,])
> p <- ifelse(p[,2]>=0.5,1,0)
> submit = data.frame(PassengerId=892:1309,Survived=p)
> write.csv(submit,'PCA5kNN11.csv',row.names=F)
> 
> pimage <- function(x){
+     return(ggplot(data=ev2) + 
+            geom_line(aes(x=1:length(ev2[[x+1]]),y=ev2[[x+1]])) + 
+            labs(x='',y='',title=paste('PC',x,sep='')))
+ }
> x=0
> grid.arrange(pimage(x+1),pimage(x+2),pimage(x+3),pimage(x+4),pimage(x+5),pimage(x+6),
+     pimage(x+7),pimage(x+8),pimage(x+9),pimage(x+10),pimage(x+11),pimage(x+12),as.table=F)
There were 24 warnings (use warnings() to see them)
> 
> x=99
> grid.arrange(pimage(x+1),pimage(x+2),pimage(x+3),pimage(x+4),pimage(x+5),pimage(x+6),
+     pimage(x+7),pimage(x+8),pimage(x+9),pimage(x+10),pimage(x+11),pimage(x+12),as.table=F)
There were 24 warnings (use warnings() to see them)
> 
> prep <- function(data,s){
+     x=c(); y=c(); z=c()
+     for (i in 1:nrow(data))
+     for (j in 1:ncol(data)){
+         x=c(x,s*j)
+         y=c(y,s*i)
+         z=c(z,data[i,j])
+     }
+     return (data.frame(x=x,y=y,z=z))
+ }
> 
> x=diag(nrow(data5))
> g1 = ggplot(prep( x[4*1:100,4*1:100] ,4),aes(x,y)) +
+  geom_raster(aes(fill=z)) +
+  ylim(400,0) +
+  labs(title='Words in dictionary',x='',y='')
> d = 5
> x=diag(nrow(data5))
> xt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x
> x2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt
> g2 = ggplot(prep( x2[1:10,1:10] ,1),aes(x,y)) +
+  geom_raster(aes(fill=z)) +
+  ylim(10,0) +
+  labs(title='Words recreated from 5 PC',x='',y='')
> d=10
> xt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x
> x2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt
> g3 = ggplot(prep( x2[1:20,1:20] ,1),aes(x,y)) +
+  geom_raster(aes(fill=z)) +
+  ylim(20,0) +
+  labs(title='Words recreated from 10 PC',x='',y='')
> d=25
> xt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x
> x2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt
> g4 = ggplot(prep( x2[1:50,1:50] ,1),aes(x,y)) +
+  geom_raster(aes(fill=z)) +
+  ylim(50,0) +
+  labs(title='Words recreated from 25 PC',x='',y='')
> grid.arrange(g1,g2,g3,g4,nrow=2,ncol=2)
Warning messages:
1: Removed 100 rows containing missing values (geom_raster). 
2: Removed 10 rows containing missing values (geom_raster). 
3: Removed 20 rows containing missing values (geom_raster). 
4: Removed 50 rows containing missing values (geom_raster). 
> 
> d=50
> xt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x
> x2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt
> g5 = ggplot(prep( x2[1:100,1:100] ,1),aes(x,y)) +
+  geom_raster(aes(fill=z)) +
+  ylim(100,0) +
+  labs(title='Words recreated from 50 PC',x='',y='')
> d=100
> xt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x
> x2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt
> g6 = ggplot(prep( x2[2*1:100,2*1:100] ,2),aes(x,y)) +
+  geom_raster(aes(fill=z)) +
+  ylim(200,0) +
+  labs(title='Words recreated from 100 PC',x='',y='')
> d=200
> xt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x
> x2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt
> g7 = ggplot(prep( x2[4*1:100,4*1:100] ,4),aes(x,y)) +
+  geom_raster(aes(fill=z)) +
+  ylim(400,0) +
+  labs(title='Words recreated from 200 PC',x='',y='')
> d=300
> xt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x
> x2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt
> g8 = ggplot(prep( x2[4*1:100,4*1:100] ,4),aes(x,y)) +
+  geom_raster(aes(fill=z)) +
+  ylim(400,0) +
+  labs(title='Words recreated from 300 PC',x='',y='')
> grid.arrange(g5,g6,g7,g8,nrow=2,ncol=2)
Warning messages:
1: Removed 100 rows containing missing values (geom_raster). 
2: Removed 100 rows containing missing values (geom_raster). 
3: Removed 100 rows containing missing values (geom_raster). 
4: Removed 100 rows containing missing values (geom_raster). 
> 
> # returns mode of a set of numbers
> getmode <- function(v) {
+    uniqv <- unique(v)
+    uniqv[which.max(tabulate(match(v, uniqv)))]
+ }
> 
> # calculate average (mode) position of each word
> Position <- vector("list",length=nrow(dictionary))
> for (i in 1:nrow(data)){
+     n = gsub('[.,"()/]','',data$Name[i])
+     n = gsub(' . |  ',' ',n)
+     n = strsplit(n,' ')[[1]]
+     for (j in 1:length(n)){
+         k = which(dictionary$Words==n[j])[1]
+         if (!is.na(k)) Position[[k]] = c(Position[[k]],j)
+     }
+ }
> dictionary$Position <- NULL
> for (i in 1:nrow(dictionary)) dictionary$Position[i] <- getmode(Position[[i]])
> dictionary$Position <- pmin(4,dictionary$Position)
> 
> ggplot(data=dictionary) +
+ geom_jitter(height=0.005,aes(x=1:nrow(dictionary),y=dictionary$Survival,color=factor(dictionary$Position))) +
+ scale_colour_manual(values = c("red", "black", "blue","white"),labels=c('1','2','3','4+')) +
+ geom_hline(yintercept=0.384, linetype='dotted') +
+ geom_vline(xintercept=45) +
+ geom_vline(xintercept=57) +
+ geom_vline(xintercept=80) +
+ geom_vline(xintercept=115) +
+ geom_vline(xintercept=187) +
+ geom_vline(xintercept=6) +
+ annotate('text',x=-15,y=0.87,label='n<=6\nfreq\n>=35') +
+ annotate('text',x=98,y=0.87,label='freq\n>=4',color='gray70') +
+ #annotate('text',x=68,y=0.87,label='freq\n>=5',color='gray70') +
+ annotate('text',x=25,y=0.87,label='freq\n>=7',color='gray70') +
+ annotate('text',x=143,y=0.87,label='n>=116\nfreq<=3') +
+ annotate('text',x=215,y=0.87,label='n>=188\nfreq<=2') +
+ annotate('text',x=305,y=0.36,label='p = 0.384 = probability of survival') +
+ labs(x='Words order by frequency') +
+ labs(title='Each dot represents a Word in the dictionary.') +
+ labs(y='Probability Survival given Word',color='word\'s\nposition\nin name')
Warning messages:
1: Use of `dictionary$Survival` is discouraged. Use `Survival` instead. 
2: Use of `dictionary$Position` is discouraged. Use `Position` instead. 
> 
> # log odds
> logl <- function(p){
+     e = 0.001
+     q = 1 - p
+     return(log(max(p,e))-log(max(q,e)))
+ }
> # inverse logit
> ilogit <- function(z){
+     return (1/(1+exp(-z)))
+ }
> # cross validation trials
> trials = 100
> total = 0
> for (k in 1:trials){
+ if (k%%25==0) cat(sprintf("Begin trial %d\n completed",k))
+ s = sample(1:891,802)
+ s2 = (1:891)[-s]
+ 
+ # calculate the Tfreq and Survival within our training subset
+ dictionary$Survival <- NA
+ dictionary$Tfreq <- 0
+ for (i in 1:length(dictionary$Words)){
+     x = intersect(which(data2[,dictionary$Words[i]]>0),s)
+     dictionary$Survival[i] <- mean(data$Survive[x],na.rm=T)
+     dictionary$Tfreq[i] <- length(x)
+ }
+ dictionary$Survival[is.na(dictionary$Survival)] <- NA
+ dictionary$Tfreq[is.na(dictionary$Tfreq)] <- NA
+ dictionary$Logl <- sapply(dictionary$Survival,FUN=logl)
+ 
+ # calculate bias term for Oscar's naive Bayes
+ ps = sum(data$Survived[s])/length(s)
+ bias = logl(1-ps)
+ 
+ # the following line mimics PCA dimension reduction
+ dictionary2 <- dictionary[dictionary$Tfreq>=4,]
+ p = rep(0,891-length(s))
+ for (j in 1:length(s2)){
+     c = 0
+     slogl = 0
+     # perform Oscar's naive Bayes
+     for (i in 1:nrow(dictionary2)){
+         if (data2[s2[j],dictionary2$Words[i]]>0 & !is.na(dictionary2$Survival[i])){
+             slogl = slogl + dictionary2$Logl[i]
+             if (c>0) slogl = slogl + bias
+             c = 1
+         }
+     }
+     if (c!=0) p[j] = ilogit(slogl)
+     else p[j] = ps
+     if (k%%25==0 & j%%10==0) cat(sprintf(" j=%d ",j))
+ }
+ p <- ifelse(p>=0.5,1,0)
+ # calculate one minus misclassification rate
+ x = 1-sum(abs(data$Survived[s2]-p))/length(s2)
+ if (k%%25==0) cat(sprintf("\n Trial %d has CV accuracy %f\n",k,x))
+ total = total + x
+ }
Begin trial 25
 completed j=10  j=20  j=30  j=40  j=50  j=60  j=70  j=80 
 Trial 25 has CV accuracy 0.786517
Begin trial 50
 completed j=10  j=20  j=30  j=40  j=50  j=60  j=70  j=80 
 Trial 50 has CV accuracy 0.685393
Begin trial 75
 completed j=10  j=20  j=30  j=40  j=50  j=60  j=70  j=80 
 Trial 75 has CV accuracy 0.775281
Begin trial 100
 completed j=10  j=20  j=30  j=40  j=50  j=60  j=70  j=80 
 Trial 100 has CV accuracy 0.786517
> 
> cat(sprintf("Average CV accuracy of %d trials is %f\n",trials,total/trials))
Average CV accuracy of 100 trials is 0.796067
> 
> 
> proc.time()
   user  system elapsed 
 29.705   2.347  32.175 
