### Table of Contents
1. Introduction
2. Data and Research Questions
      a. Load packages and import data
      b. Data at a glance
      c. Visualize missing values
      d. Preliminary questions
3. Data Exploration
      a. Starter graphs
      b. Proportions
4. Feature Engineering
      a. Age Buckets
      b. Titles
      c. Family size
5. Data Imputation
      a. Fare
      b. Embarked
      c. Age
6. Prediction


### **1. Introduction**
This Kaggle competition is titled Titanic: Machine Learning from Disaster. It is designed as an introduction to data analysis and Kaggle. Using data of the RMS Titanic's shipwreck, Kagglers must predict whether passengers survive or die based on 10 variables: Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin and Embarked.

This was my first time participating in a Kaggle competition and performing data analysis. I treated it as an opportunity to apply my knowledge of R outside of the classroom, and become acquainted with the process of data analysis. For the structure of my report, I looked at Megan Risdal's Titanic tutorial (https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic). For additional guidance, I followed along Trevor Stephens' Titanic tutorial, which can be found at http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/. A last resource I used was the kernel of Kaggle user "Heads or Tails," named Tidy TitaRnic. It can be found at https://www.kaggle.com/headsortails/tidy-titarnic. Kernels are basically Kaggle's nickname for a report involving code and written analysis.

I began by examining our variables and missing values. Helpful functions here were str() and summary(). After getting a feel for the data, I asked some preliminary questions that would guide my data exploration. Next, I produced some plots in ggplot that helped to visualize relationships between variables and survival.<br><br>

### **2. Data and Research Questions**
#### 2a. Load packages and import data
First, we load relevant packages.
```{r load,message=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(kableExtra)
library(knitr)
library(rpart)
library(VIM)
```

Then inspect the data.
```{r}
train <- read.csv('../input/train.csv', stringsAsFactors = F)
test  <- read.csv('../input/test.csv', stringsAsFactors = F)

str(train)
str(test)
```

The train dataset has 891 observations and 12 variables. The test dataset has 418 observations and 11 variables. Taking a look at the columns in train first, we have PassengerID and Survived, followed by 10 other variables, which are the features of our dataset:

- Pclass (int) : ticket class, where 1st is upper class, 2nd is middle class, and 3rd is lower class
- Name (factor)
- Sex (factor)
- Age (num)
- SibSp (int): number of siblings and/or spouses onboard
- Parch (int): number of parents and/or children onboard
- Ticket (factor): ticket number
- Fare (num): passenger fare
- Cabin (factor): cabin number
- Embarked (factor): port of embarkation -- Southampton (England), Cherbourg (France), and Queenstown (Ireland)

These are the variables that we will use to build our prediction model.

There are only 11 columns in test, but because the Survived column is missing. It was purposely ommitted in test, but we would like test's columns to match with train so that we can merge the two and create new features conveniently. Our next step is to add an empty Survived column to test and combine the two datasets by row using rbind(). This merged dataset has 1309 observations and 12 variables.

```{r}
test$Survived <- NA
merged <- rbind(train,test)
str(merged)
```

<br>

#### 2b. Data at a glance
The summary function shows us rough statistics and missing values for each column.
```{r}
summary(merged)
```

The mean ticket class was around 2.3, which makes sense because there are far fewer 1st class tickets as compared to 2nd and 3rd. We see that there are nearly twice as many males as females, and that the typical age of the passengers hovers in the late twenties, but the age range of passengers is quite large. There is a child as young as a couple months old, and a passenger as old as 80. More than half of our passengers came alone, and most came without parents or children. However, it looks like there is one group consisting of 9 children (the max number of siblings is probably 8) and two parents. This is confirmed by the ticket variable, which has a group of 11 under the ticket CA. 2343. The second largest ticketing group is 8, followed by several groups of 7. The mean fare was $33 and median was $14, but apparently some--perhaps very young children--rode for free. Fare could be as expensive as $512 as well. The cabin groups do not reveal too much information as of yet, and there are three ports of embarkation, the majority coming from Southampton.

In our merged dataset, we have 418, 263, and 1 missing values (represented by NA) in Survived, Age, and Fare respectively. The missing values in our factor variables are more difficult to spot, but they are represented by blank values. Specifically, the first row of Cabin and Embarked reveal that they have 1014 and 2 NAs respectively. Beside Survived--which was ommitted in our test training set to test the accuracy of our predictive model--we will have to creatively account for all of these missing values later.

We replace blank values with NAs to make missing values in our factors more visible.
```{r}
merged$Fare[merged$Fare==""] <- NA
merged$Cabin[merged$Cabin==""] <- NA
merged$Embarked[merged$Embarked==""] <- NA
```

<br>

#### 2c. Visualize missing values
The inspiration for creating a missing data visualization came from the *Tidy TitaRnic* kernel, linked in the introduction. The VIM package's aggr() function uses base R to quickly plot missing values.<br>

```{r}
aggr(merged,prop=F,numbers=T,col=c("lightblue","indianred3"),sortVars=T,sortCombs=T)
```
First, the blank middle column blank represents PassengerId. I'm not sure why it doesn't show up, but there are no missing values for that feature.

The left plot missing values for each feature in descending order. Over 1000 of our 1309 passengers are missing Cabin information. As a result, our Cabin variable will most likely not be a reliable predictor of survival. Age has the second most NAs. At 400 this is far more manageable, and we will be able to estimate age using a data imputation package.

On the right is a tile plot of combinations of missing values (in red), along with the frequencies of those observations at the right. In the first row, for example, we have a single passenger with missing values in Cabin, Survived, and Fare. Not surprisingly, the most common combination of two missing variables is Cabin and Age.

Of the 1309 passengers on the Titanic, only 183 have complete information; only 87 rows of our test data are complete. Since a large proportion of our passengers have missing data, data imputation (estimation) will be an important component of achieving accurate predictions. There are packages such as MICE and Amelia which we can later use to complete our data.

#### 2c. Preliminary questions
Some questions to frame my initial data exploration...
1. Are women more likely to survive than men?
2. Does survival depend on age? Is there a significant difference in survival rate between children under 18 and adults?
3. Does higher fare result in higher survival? On related note, are 1st class passengers more likely to survive than 3rd class passengers?
4. Are larger families less likely to survive?

<br>

### **3 Data Exploration**
Using the ggplot2 data visualization package.

#### 3a. Starter graphs
```{r,warning=FALSE,message=FALSE}
stdtheme <- theme(plot.title=element_text(hjust=.5,size=12,face="bold"),legend.title=element_blank())

p.sex <- ggplot(data=train,mapping=aes(x=Sex,fill=factor(Survived,labels=c("Died","Survived")))) + 
  geom_bar(position="fill") + 
  labs(title="A. Sex and Survival") +
  stdtheme 

meanfare_0 <- mean(train$Fare[train$Survived==0])
meanfare_1 <- mean(train$Fare[train$Survived==1])
p.fare <- ggplot(data=train,mapping=aes(x=Fare,color=factor(Survived))) +
  geom_freqpoly() +
  labs(title="C. Fare and Survival") +
  geom_vline(xintercept=meanfare_0,color="red") +
  geom_vline(xintercept=meanfare_1,color="blue") +
  xlim(0,200) +
  stdtheme +
  guides(color=FALSE)

meanage_0 <- mean(train$Age[train$Survived==0],na.rm=TRUE)
meanage_1 <- mean(train$Age[train$Survived==1],na.rm=TRUE)
p.age <- ggplot(data=train,aes(x=Age,fill=factor(Survived))) +
  geom_histogram(binwidth=10) +
  labs(title="B. Age and Survival, Binwidth 10") +
  geom_vline(xintercept=meanage_0,color="red") +
  geom_vline(xintercept=meanage_1,color="blue") +
  scale_x_continuous(breaks=seq(0,100,10)) +
  stdtheme +
  guides(fill=FALSE)

layout = matrix(c(1,2,3,3),2,2,byrow=TRUE)
grid.arrange(p.sex,p.age,p.fare,layout_matrix=layout)
```

**Observations:**
- **Women are more likely to survive than men:** the position_fill() feature in plot A scales each category to 1, giving us proportions with which to compare gender survival. Nearly three-fourths of women in our training set survived, whereas less than a quarter of men survived. This is a reasonable since women received preferential treatment when it came to loading lifeboats.

- **At first glance, age doesn't seem a great predictor of survival:** the blue line represents survivors' mean age and the red line represents the mean age of those who died. The distributions and their means suggest that age is not a significant indicator of survival; however, we will come back to this age variable to look for more clues.

- **Higher fare means higher survival rate:** here, the blue and red lines represents the mean fare of passengers who 
survived and died respectively. Survivors paid a significantly higher fare, which potentially paid for greater access to emergency life boats and better cabin positions.

<br>

#### 3b. Proportions
```{r,warning=FALSE}
p2.age <- ggplot(train,mapping=aes(x=Age,fill=factor(Survived))) +
  geom_histogram(binwidth=5,position="fill") +
  guides(fill=FALSE) +
  labs(title="Survival rate by age, binwidth=5",y="Proportion") +
  scale_x_continuous(breaks=seq(0,100,10)) +
  stdtheme

p2.fare <- ggplot(train,mapping=aes(x=Fare,fill=factor(Survived))) +
  geom_histogram(binwidth=10,position="fill") +
  labs(title="Survival rate by fare, binwidth=10",y="Proportion") +
  scale_x_continuous(breaks=seq(0,550,50)) +
  stdtheme +
  guides(fill=FALSE)

# separate cabins
merged$CabinGr <- merged$Cabin %>% as.character() %>% substr(1,1)
train <- merged[1:891,]

p2.cabgr1 <- ggplot(data=subset(train,!is.na(CabinGr)),aes(x=CabinGr)) +
  geom_bar() +
  labs(title="Num passengers per cabin group",y="Count") +
  stdtheme

p2.cabgr2 <- ggplot(data=subset(train,!is.na(CabinGr)),aes(x=CabinGr,fill=factor(Survived))) +
  geom_bar(position="fill") +
  labs(title="Survival rate by cabin group",x="CabinGr",y="Proportion") +
  guides(fill=FALSE) +
  stdtheme

p.embarked <- ggplot(subset(train,!is.na(Embarked)),mapping=aes(x=Embarked,fill=factor(Survived,labels=c("Died","Survived")))) +
  geom_bar(position="fill") +
  labs(title="Port and Survival",y="Proportion",x="Port") +
  stdtheme  

grid.arrange(p2.age,p2.fare,p2.cabgr1,p2.cabgr2,p.embarked,layout_matrix=matrix(c(1,5,2,2,3,4),3,2,byrow=TRUE))
```

**Observations**
- **Age:** Children under 10 have significantly higher survival rates than passengers aged 20-60, probably because adolescents were prioritized in emergency procedures. Passengers around 80 also fare better, perhaps because they receive priority and are also able to pay for better care. We can make age buckets for 0-10, 10-60, and 60+ to reflect this data and help us make a prediction.

- **Ports:** Passengers who boarded the TItanic from port C fared best, followed by ports Q and S respectively.

- **Fare:** There are jumps in survival rate at $15 and $60. The survival rate for passengers who paid between $200 and $275 is high, but not as high as I would have expected.

- **Cabin Group:** I categorized cabin groups by their letters, which correspond to different decks on the ship. While there are noticeable differences in survival rate here--B, D, and E have a higher rate of survival than A and G for example--very few passengers' Cabins were actually accounted for. There are less than 20 individuals from A, F, and G. The plots help us confirm that the Cabin variable doesn't give us much to work with.

<br>

### **4. Feature Engineering**
This was my first encounter with feature engineering. As I understand, features are inputs for machine learning models, and picking the right combination results in a powerful model and accurate predictions. Here, I learned to create new features from existing data to enhance my model's accuracy. I followed Trevor Stephens' tutorial for the most part, but also used the other two kernels for inspiration and guidance.

#### 4a. Age Buckets
I create age buckets to group individuals by age range, then examine survival rates by category.
```{r}
merged$AgeBucket[!is.na(merged$Age)] <- "0-20"
merged$AgeBucket[merged$Age>20 & merged$Age<=40] <- "20-40" 
merged$AgeBucket[merged$Age>40 & merged$Age<=60] <- "40-60" 
merged$AgeBucket[merged$Age>60] <- "60+"

age.sex_surv <- aggregate(Survived ~ AgeBucket + Sex,
          data=merged,
          FUN=function(x) {sum(x)/length(x)})

kable(age.sex_surv[order(-age.sex_surv$Survived),],"html",row.names=FALSE) %>% 
  kable_styling(full_width=FALSE)

```

Women had high survival rates across the board. Interestingly, all women over 60 survived. It turns out though that our sample size is too small: there were only 3 women over 60. For expediency, I will hazard a guess and say that preferential policies towards elderly women would give women over 60 the highest survival rates, but this is bad practice. While eldelrly women fared the best, elderly men fared the worst, with a survival rate of .105. Moreover, there were 19 passengers in this category, which indicates that only 2 of these 19 survived. This is probably not by coincidence--perhaps in a noble sacrifice, these older men forfeited their seats to save their spouses, children, and grandchildren.

<br>

#### 4b. Titles
This was an interesting technique that I saw in a number of tutorials and kernels, but I followed along Trevor Stephens' in particular. Here, he categorized individuals by title, and lumped them under either "Mr", "Ms", or "Mrs". The motivation here is to capture the survival 

He converted the Name column of merged (originally a factor) to a character vector. Names are typically in the form "[last name], [title] [first name] [middle name, if applicable]." For example, the name of the first passenger is `Braund, Mr. Owen Harris`. Strsplit() splits up strings and stores their components as vectors. Its second argument tells it what to split a string on. We used `"[,.]"` to instruct strsplit() to separate strings by commas and colons. Thus `strsplit("Braund, Mr. Owen Harris","[,.]")` becomes a vector of three values: `Braund` ` Mr.` and ` Owen Harris`. After splitting on punctuation, we still have to remove blank spaces (i.e. the spaces before "Owen" and "Harris"). 
```{r}
names <- as.character(merged$Name)
titles <- strsplit(names,"[,.]") %>% sapply(FUN=function(x) x[2])
titles <- gsub("^ | $","",titles)
table(titles)
```

We group these titles by meaning. Jonkheer means young lord, so it goes under Mr.
```{r}
merged$Title = "Mr"
merged$Title[titles %in% c("Dona","Lady","Mme","Mrs","the Countess")] <- "Mrs"
merged$Title[titles %in% c("Miss","Mlle","Ms")] <- "Ms"
merged$Title <- factor(merged$Title)

prop.table(table(merged$Title,merged$Survived),1)
```

After creating these titles, we observe that married women have a nearly 10% higher survival rate than unmarried women. Thus we have managed to capture a bit more accuracy by creating this title feature.

<br>

#### 4c. Family size
This feature is self-explanatory. The reasoning for this feature is that larger families are more difficult to manage and are less flexible for lifeboat seating. It would seem natural that passengers in large groups have lower survival rates.
```{r}
merged$nFam <- merged$SibSp + merged$Parch

ggplot(data=merged[!is.na(merged$Survived),],aes(x=nFam,fill=factor(Survived))) +
  geom_bar(position="fill") +
  stdtheme +
  labs(title="Survival by num family members",x="Family members onboard",y="Survival rate") +
  scale_y_continuous(breaks=seq(0,1,.1)) +
  scale_x_continuous(breaks=0:10)
```

<br>

### **5. Data Imputation**
Before we can feed our features into our predictive model, we have to fill in our missing NAs for the features we choose. We will not attempt to fill in missing data for Cabin, because there are too many missing values.

#### 5a. Fare
We have a single missing value for Fare, so we can fill in that value manually be examining the passenger's other details.
```{r}
merged[is.na(merged$Fare),]
```

We note that our passenger is an older gentleman from 3rd class, so it would help to plot fare for 3rd class males as a function of age.
```{r}
ggplot(merged[!is.na(merged$Fare) & merged$Sex=="male" & merged$Pclass==3,],aes(x=Age,y=Fare)) +
  geom_point() +
  geom_smooth()
```

The Lowess smoother estimates the cost of the ticket to be around $9. Taking a look at 3rd class passengers 50 and above, this seems reasonable. To get our final result, we take the median of the 10 rightmost points on the graph, which turns out to be $7.75.

```{r}
missfare <- merged[merged$Pclass==3 & merged$Age>=50 & merged$Sex=="male",]$Fare %>% median(na.rm=TRUE)

merged$Fare[is.na(merged$Fare)] <- missfare
```

<br>

#### 5b. Embarked
For the two passengers who had missing values for Embarked, I simply did a quick Google search with their names to determine which port they boarded from. This method because I did not think that any of the given variables would be a good predictor of the port of embarkation. It turns out that the two of them, Mrs. Martha Evelyn Stone and her maid Miss. Amelie Icard, were traveling together from Southampton.
```{r}
merged[is.na(merged$Embarked),]
merged$Embarked[62] <- "S"
merged$Embarked[830] <- "S"
```

<br>  

#### 5c. Age
We will use the rprart library to fill in missing values for age. Rpart uses the random forest algorithm. I have limited machine learning knowledge at the moment, but my basic understanding of this algorithm is that it makes use of many weak decision trees, aggregates their results, and outputs a prediction. Random forests can be used for both classification and regression problems. Here, we use rpart() for regression, which we indicate with the `anova` method.
```{r}
agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title,
                data=merged[!is.na(merged$Age),],
                method="anova")
merged$Age[is.na(merged$Age)] <- predict(agefit,merged[is.na(merged$Age),])
```

We model our prediction only on data where Age is not missing, with the features that follow the tilde.

<br>

### 6. **Prediction**
In this last step, we use our newly engineered features and existing features to create a prediction model using a decision tree. In the end, this achieves a 76.6% accuracy of predicting survivors. There are better machine learning algorithms to be used here like random forests, which use a number of weak decision trees. However, as I have limited machine learning knowledge, I will stick to rpart, which uses a single decision tree to make predictions. 
```{r,eval=FALSE}
train <- merged[1:891,]
test <- merged[892:1309,]

sfit <- rpart(Survived ~ Pclass + Name + Sex + SibSp + Parch + Fare + Embarked + AgeBucket + Title + nFam,
              train,
              method="class",
              control=rpart.control(minsplit=5))

prediction <- predict(sfit,test,type="class")

submit <- data.frame(PassengerId=test$PassengerId,Survived=prediction)

write.csv(submit,file="rpart.csv",row.names=FALSE)
```








