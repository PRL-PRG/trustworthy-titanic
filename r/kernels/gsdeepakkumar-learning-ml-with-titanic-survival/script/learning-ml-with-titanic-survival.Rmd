---
title: "Learning ML with Titanic Survival"
author: "Deepak Kumar G S"
date: "November 23, 2017"
output: 
    html_document:
        code_folding: hide
        toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
```

# Introduction 

* As is the case with every programming language, this **Titanic:Machine Learning from Disaster** has become the *Hello World !* to machine learning.Therefore I try my hands on this dataset to begin my journey into the field of machine learning.

* I intend to do exploratory data analysis,missing value imputation on the dataset and then implement predictive modelling ,cross validate with test data and make the submission.Lets begin the journey.

* This is my first kernel in machine learning area.Any suggestions/comments/criticism are welcome.I have used kernels published by [Megan](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) ,[HeadsorTails](https://www.kaggle.com/headsortails/tidy-titarnic),[Andrew Kinsman](https://www.kaggle.com/varimp/a-mostly-tidyverse-tour-of-the-titanic),[Oscar Takeshita](https://www.kaggle.com/pliptor/divide-and-conquer-0-82296) and many other kernels for my reference.Thanks to all.Going through these kernels helped me develop knowledge and the right approach to a problem.

# Loading the libraries and dataset

```{r}
library(dplyr) # Data Munging
library(ggplot2) # Data Visualisation
library(ggthemes) #Themes
library(corrplot) #Correlation
library(rpart) #Data modelling
library(randomForest) #Data modelling
library(pscl)
library(Deducer) # Modelling
library(Amelia) #Missing values
library(forcats)
library(rpart.plot) # Random forest
library(Hmisc)
library(VIM)
train=read.csv("../input/train.csv",header=TRUE,stringsAsFactors = FALSE)
test=read.csv("../input/test.csv",header=TRUE,stringsAsFactors = FALSE)
```

# Glimpse of the data:

We then use the *summary* function on the dataset to understand the variable type and missing values.

```{r}
cat("There are ",nrow(train),"rows and",ncol(train),"columns in train dataset")
cat("There are",nrow(test),"rows and",ncol(train),"columns in test dataset")
summary(train)

summary(test)
```

**Train Dataset** - From the train summary it is clear that *Age* has 177 missing values.For a visually appealing view of the NA values,we use *missmap* from the library *Amelia*.For documentation refer this [link](https://r.iq.harvard.edu/docs/amelia/amelia.pdf).

```{r,fig.height=7}
missmap(train,main="Titanic Train Data-Missing Value Visualisation",col=c("red","green"),legend=FALSE)
missmap(test,main="Titanic Test Data-Missing Value Visualisation",col=c("red","green"),legend=FALSE)
```

From the missing value visualisation of train data,it is understood that Age has most missing values whereas in test dataset one row in fare is missing in addition to Age.

Now let us comine both the train and test data to do some EDA.

```{r}
titanic=full_join(train,test)
summary(titanic)
```


# Missing value imputation

Let us first focus on the missing value imputation and do some visual exploration after that.We first consider the age.

##Age

```{r}
prop.table(table(is.na(titanic$Age)))
```

20 % of the values in the Age column is missing.Therefore we use the **rpart (recursive partitioning)** to impute the age values.

```{r}
age=rpart(Age ~Pclass+Sex+SibSp+Parch+Fare+Embarked,data=titanic[!(is.na(titanic$Age)),],method="anova")
titanic$Age[is.na(titanic$Age)]=predict(age,titanic[is.na(titanic$Age),])
```

Let us check ,

```{r}
prop.table(table(is.na(titanic$Age)))
```

The column does not have any missing values.

```{r}
ggplot(titanic,aes(Age,fill="green"))+geom_density(alpha=0.4)+labs(x="Age",y="Count",title="Distribution of Age after imputation")+theme(legend.position="none")
```


##Fare:

```{r}
cat("There is",sum(is.na(titanic$Fare)),"missing value in Fare")
```
We find out the row and the details.

```{r}
which(is.na(titanic$Fare))
```

```{r}
titanic[1044,]
```


The passenger belongs to  3rd class and has emparked on S.The passenger is male.We again use the *rpart* function for imputation.

```{r}
fare=rpart(Fare ~Parch+SibSp+Sex+Pclass,data=titanic[!(is.na(titanic$Fare)),],method="anova")
titanic$Fare[(is.na(titanic$Fare))]=predict(fare,data=titanic[is.na(titanic$Fare),])
rpart.plot(fare,shadow.col="pink",box.col="gray",split.col="magenta",main="Decision Tree for Imputation")
```

From the decision tree,we interpret that passengers in 2nd or 3rd class paid lesser compared to 1st class and those having parents or childrens shelled out more compared to those who travelled alone.

```{r}
prop.table(table(is.na(titanic$Fare)))
```

We plot the density plot to understand about the fare dynamics.

```{r}
ggplot(titanic,aes(Fare,fill="green"))+geom_density(alpha=0.4)+labs(x="Fare",y="Fare Density",title="Distribution of Fare after imputation")+theme(legend.position="none")
```

Clearly we see that the data is highly skewed towards right.


# Data Wrangling and Visualisation:

We now focus on the name,sex,SibSp,Parch,Pclass to do some data wrangling and visualisation.We start with name.

```{r}
str(titanic$Name)
```

For better understanding of the data we extract the title from the Name.

```{r}
titanic$Title=gsub('(.*, )|(\\..*)','',titanic$Name)
head(titanic$Title)
table(titanic$Title,titanic$Sex)
```

We convert the variable into factor and using function from *forcats* library we collapse some of these levels.Following code is inspired from [Andrew Kinsman](https://www.kaggle.com/varimp/a-mostly-tidyverse-tour-of-the-titanic).

```{r}
titanic <- titanic %>% mutate(Title = factor(Title)) %>% mutate(Title = fct_collapse(Title, "Miss" = c("Mlle", "Ms"), "Mrs" = "Mme", "Ranked" = c( "Major", "Dr", "Capt", "Col", "Rev"),"Royalty" = c("Lady", "Dona", "the Countess", "Don", "Sir", "Jonkheer")))
str(titanic$Title)
```

Next we create a column called Family to know who all have travelled alone and who travelled with their families.This is achieved through simple ifelse statement where the condition will be *if a person has travelled with parents/children or sibling/spouse then the statement will be evaluated as true else it is false*.

```{r}
titanic$Families= factor(ifelse(titanic$SibSp + titanic$Parch + 1> 1,"Yes","No"))
prop.table(table(titanic$Families))
```

Almost 40 % of them have travelled with families.

Now let us look at the passenger class.

```{r}
prop.table(table(titanic$Pclass))
```

54 % of them have travelled in third class whereas an 25 % of them have been in 1st class.

##Surival Scenario

```{r}
titanic=titanic %>% mutate(Survived=factor(Survived)) %>% mutate(Survived=fct_recode(Survived,"No"="0","Yes"="1"))
# train=titanic[1:891,]
# test=titanic[1:1309,]
prop.table(table(train$Survived))
```

In the train data, only 38 % have survived whereas 61 % have perished.Lets dig deeper and see the trend.


###By Gender

```{r,fig.height=6}
ggplot(titanic[1:891,],aes(Sex,fill=Survived))+geom_bar(position="fill")+theme_fivethirtyeight()+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5))+labs(x="Gender",y="Survival Rate",title="Survival by Gender")
```

On board Titanic,the chances of survival being a male is very less whereas for female it is subtantially greater.This indicates that there was gender disparity in the ship while saving lives.


###By Passenger Class

```{r}
str(titanic$Pclass)
ggplot(titanic[1:891,],aes(Pclass,fill=Survived))+geom_bar(position="fill")+facet_wrap(~Sex)+theme_fivethirtyeight()+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5))+labs(x="Passenger Class",y="Survival Rate",title="Survival by Passenger Class Vs Gender")
```

From the two plots,it is understood that irrespective of the gender,there is a higher chance of survival if you were from 1st class.But,if you happen to be female 1st class passenger,then the chances of survival increases subtantially.Those who are unlucky are people from 2nd and 3rd class for male.

###By Title

Let us understand the scenario with the Title and survival rate.

```{r}
ggplot(titanic[1:891,],aes(Title,fill=Survived))+geom_bar(position="fill")+facet_wrap(Pclass~Sex)+theme_few()+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5),plot.subtitle = element_text(size=10),axis.text.x=element_text(angle=90))+labs(x="Title",y="Survival Rate",title="Survival by Title Vs Gender",subtitle="Visualizing by Passenger Class")
```

* From the plot,we understand that the survival rate is highly influenced by Passenger class and gender.
* As seen earlier,the survival rate is usually higher for female which is reinstated here.
* 1st class and 2nd class,female passengers had almost 100 % chance of survival compared to their male counterpart.
* Chanses of survival is 50% for female travelling in 3rd class.
* For male,the only way a male could have survived is that he should have been a boy (as indicated by master).For 1st and 2nd class,the survival is almost 100 % whereas it is 50 % in 3rd class.
* The probability of survival is worse for an adult male in 2nd and 3rd class whereas in 1st class it is around 50 %.


###By Embarkment:

```{r}
ggplot(titanic[1:891,],aes(Embarked,fill=Survived))+geom_bar(position="fill")+theme_few()+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5))+labs(x="Title",y="Survival Rate",title="Survival by Embarkment")
```

There seems to be one row with no value for embarkmemt.let us add this to the majority class.

```{r}
titanic =titanic %>% mutate(Embarked=ifelse(Embarked=="",names(which.max(table(titanic$Embarked))),Embarked))
ggplot(titanic[1:891,],aes(Embarked,fill=Survived))+geom_bar(position="fill")+theme_few()+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5))+labs(x="Title",y="Survival Rate",title="Survival by Embarkment")+facet_wrap(Pclass~Sex,scales="free")
```

* In each class,the chances of survival is high forthose who embarked on "C" port.
* All males who have embarked on "Q" port belonging to 2nd class have perished whereas the situation is just opposite for female from same class.This suggests that gender might be an more important factor in predicting the survival compared to the embarkment port.
* Same situation is observed in the case of 1st class passengers.


###By Families:

Let us now focus on the survival rates with respect to family.

```{r}
ggplot(titanic[1:891,],aes(Families,fill=Survived))+geom_bar(position="fill")+theme_few()+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5))+labs(x="With Family or Not",y="Survival Rate",title="Chance of Survival if travelled with Family")
```

* The chances of survival seems to be a bit higher for those who travel with their families.

###Median age of Survival

Let us draw a boxplot to understand the median age of survival with respect to gender.

```{r}
ggplot(titanic[1:891,],aes(Survived,Age,fill=Sex))+geom_boxplot()+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5))+labs(x="Survived",y="Age",title="Median age of Survival")
```

* The median age has been aroung 30 for both male and female surviors whereas the median age of female who have not survived is around 25 and for males it is around 28.


###By Cabin:

```{r}
str(titanic$Cabin)
```

We try to split the first character from the cabin variable and visualize the survival rate.

```{r}
titanic$Deck=factor(sapply(titanic$Cabin, function(x) strsplit(x, NULL)[[1]][1]))
str(titanic$Deck)
table(is.na(titanic$Deck)) #297 missing values
round(prop.table(table(titanic$Deck,titanic$Survived))*100,2)
```
C and B deck have higher rate of survival whereas the decks f,g,t have lower rates of survival.

We try to impute the missing values using **Hmisc** package.Other packages are also available to deal with missing values like **amelia**,**missForest**,**mice**,**mi** etc.Hmisc offers 2 powerful functions to impute missing values.They are *impute()* and *aregImpute()*

*impute()* function simply imputes missing value using user defined statistical method (mean, max, mean). It’s default is median. On the other hand, *aregImpute()* allows mean imputation using additive regression, bootstrapping, and predictive mean matching.

```{r}
set.seed(100)
titanic$Deck=with(titanic,impute(Deck,'random'))
ggplot(titanic[1:891,],aes(Deck,fill=Survived))+geom_bar(position="fill")+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5))+labs(x="Deck",y="Survival Rate",title="Survival by Deck")
```

###Family Size:

We create a separate column called family size and know about whether the chances of survival are higher if you are with family.

```{r}
titanic=titanic %>% mutate(FamilySize=SibSp+Parch+1) %>% mutate(Type=ifelse(FamilySize==1,"Single",ifelse(FamilySize>=3,"Large","2 People")))
ggplot(titanic[1:891,],aes(Type,fill=Survived))+geom_bar(position="fill")+theme(legend.position="bottom",plot.title=element_text(size=15,hjust=0.5))+labs(x="FamilyType",y="Survival Rate",title="Survival by FamilySize")
```


The survival rate is high if you are a 2 people family whereas for singleton,the survival rate is low.

# Modelling:

Now let us focus on the modelling part.I intent to use the random forest model .Let us first visualize whether relevant missing values are imputed correctly.

```{r}
aggr(titanic,prop=FALSE,combined=TRUE,sortVars=TRUE,sortCombs=TRUE,numbers=TRUE)
```

Thus from the data we understand that there are no missing value in training data.The test data has 418 missing values on Survived which needs to be predicted.Let us split the train and test dataset.



We convert the character variables into factors.

```{r}
titanic = titanic %>% mutate(Type=factor(Type)) %>% mutate(Embarked=factor(Embarked)) %>% mutate(Sex=factor(Sex))
```

```{r}
train=titanic[1:891,]
test=titanic[892:1309,]
names(train)
str(train)
```

```{r}
rfmodel=randomForest(factor(Survived) ~ Pclass+Sex+Age+Fare+Embarked+Title+Deck+FamilySize+Type+SibSp+Parch,data=train,importance=TRUE)
print(rfmodel)
```

The confusion matrix shows that the classification error is 30 %.

```{r}
plot(rfmodel, main="")
legend("topright", c("OOB", "0", "1"), text.col=1:6, lty=1:3, col=1:3)
title(main="Error Rates Random Forest")
```

The plot shows that somewhere between 0-100 trees,the optimum is reached and after that the OOB error becomes flat.Let us check the variable importance.

```{r}
varImpPlot(rfmodel)
```

The mean decrease in accuraccy is 100 % for pclass which means that if we do a random permutation on the variable,the decrease is 100%.

Let us tune our randomforest.

```{r}
variable=c("Pclass","Sex","Age","Fare","Embarked","Title","Deck","FamilySize","Type","SibSp","Parch")
tunedrfmodel=tuneRF(x=train[,variable],y=as.factor(train$Survived),mtryStart = 3,ntreeTry = 100,stepFactor = 2,improve=0.001,trace=FALSE,plot=FALSE,doBest = TRUE,nodesize=200,importance=TRUE)
varImpPlot(tunedrfmodel)
```

From the tuning,we see that title,sex are most important variable for our prediction.

Let us predict using the train data.

```{r}
trainpredict=table(predict(tunedrfmodel),train$Survived)
caret::confusionMatrix(trainpredict)
```

The accuracy is 80 %.Let us use the test data.

```{r}
test$Survived=NULL
titanicpred=predict(tunedrfmodel,test,OOB=TRUE,type="response")
titanicpred=ifelse(titanicpred=="No",0,1)
solution=data.frame(PassengerID=test$PassengerId,Survived=titanicpred)
write.csv(solution,file="submission.csv",row.names=F)
```


#Conclusion:

* This well known dataset in the kaggle community gives us the leverage to see the data in different angle gaining vast knowledge on how to visualise the data,impute missing values,various packages to model the data,accuraccy of the model etc.

* There is a scope of implementing logistic regression on the model which is left unexplored in this present kernel.This will be taken up in future to predict the survival rates.

* Overall,this dataset is an exposure to machine learning algorithms and interpretation of the same.

**Thank you for reading.Pls upvote if you like**

**Work in progress**